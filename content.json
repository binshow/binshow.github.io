{"pages":[],"posts":[{"title":"ElasticSearch从入门到实战（三）特殊查询和管道聚合","text":"本篇讲述了； 嵌套查询 地理位置查询 特殊查询 结果高亮和排序 聚合分析 Java客户端操作es 嵌套查询在关系型数据库中有表的关联关系，在 es 中，我们也有类似的需求，例如订单表和商品表，在 es 中，这样的一对多一般来说有两种方式： 嵌套文档（nested） 父子文档 嵌套文档假设：有一个电影文档，每个电影都有演员信息： 12345678910111213141516171819202122232425PUT movies{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;actors&quot;:{ &quot;type&quot;: &quot;nested&quot; } } }}PUT movies/_doc/1{ &quot;name&quot;:&quot;霸王别姬&quot;, &quot;actors&quot;:[ { &quot;name&quot;:&quot;张国荣&quot;, &quot;gender&quot;:&quot;男&quot; }, { &quot;name&quot;:&quot;巩俐&quot;, &quot;gender&quot;:&quot;女&quot; } ]} 注意 actors 类型要是 nested，nested 对象类型可以保持数组中每个对象的独立性（参考前面）。 缺点 查看文档数量： 1GET _cat/indices?v 查看结果如下： 12health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open movies _HXgjxGGQkuZFb_PMAi2kQ 1 1 3 0 5.3kb 5.3kb 这是因为 nested 文档在 es 内部其实也是独立的 lucene 文档，只是在我们查询的时候，es 内部帮我们做了 join 处理，所以最终看起来就像一个独立文档一样。因此这种方案性能并不是特别好。 嵌套查询这个用来查询嵌套文档： 123456789101112131415161718192021222324GET movies/_search{ &quot;query&quot;: { &quot;nested&quot;: { &quot;path&quot;: &quot;actors&quot;, &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match&quot;: { &quot;actors.name&quot;: &quot;张国荣&quot; } }, { &quot;match&quot;: { &quot;actors.gender&quot;: &quot;男&quot; } } ] } } } }} 父子文档相比于嵌套文档，父子文档主要有如下优势： 更新父文档时，不会重新索引子文档 创建、修改或者删除子文档时，不会影响父文档或者其他的子文档。 子文档可以作为搜索结果独立返回。 例如学生和班级的关系： 12345678910111213141516PUT stu_class{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;keyword&quot; }, &quot;s_c&quot;:{ &quot;type&quot;: &quot;join&quot;, &quot;relations&quot;:{ &quot;class&quot;:&quot;student&quot; } } } }} s_c 表示父子文档关系的名字，可以自定义。join 表示这是一个父子文档。relations 里边，class 这个位置是 parent，student 这个位置是 child。 接下来，插入两个父文档： 1234567891011121314PUT stu_class/_doc/1{ &quot;name&quot;:&quot;一班&quot;, &quot;s_c&quot;:{ &quot;name&quot;:&quot;class&quot; }}PUT stu_class/_doc/2{ &quot;name&quot;:&quot;二班&quot;, &quot;s_c&quot;:{ &quot;name&quot;:&quot;class&quot; }} 再来添加三个子文档： 123456789101112131415161718192021222324PUT stu_class/_doc/3?routing=1{ &quot;name&quot;:&quot;zhangsan&quot;, &quot;s_c&quot;:{ &quot;name&quot;:&quot;student&quot;, &quot;parent&quot;:1 }}PUT stu_class/_doc/4?routing=1{ &quot;name&quot;:&quot;lisi&quot;, &quot;s_c&quot;:{ &quot;name&quot;:&quot;student&quot;, &quot;parent&quot;:1 }}PUT stu_class/_doc/5?routing=2{ &quot;name&quot;:&quot;wangwu&quot;, &quot;s_c&quot;:{ &quot;name&quot;:&quot;student&quot;, &quot;parent&quot;:2 }} 首先大家可以看到，子文档都是独立的文档。特别需要注意的地方是，子文档需要和父文档在同一个分片上，所以 routing 关键字的值为父文档的 id。另外，name 属性表明这是一个子文档。 父子文档需要注意的地方： 每个索引只能定义一个 join filed 父子文档需要在同一个分片上（查询，修改需要routing） 可以向一个已经存在的 join filed 上新增关系 has_child query通过子文档查询父文档使用 has_child query。 12345678910111213GET stu_class/_search{ &quot;query&quot;: { &quot;has_child&quot;: { &quot;type&quot;: &quot;student&quot;, &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;wangwu&quot; } } } }} 查询 wangwu 所属的班级。 has_parent query通过父文档查询子文档： 12345678910111213GET stu_class/_search{ &quot;query&quot;: { &quot;has_parent&quot;: { &quot;parent_type&quot;: &quot;class&quot;, &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;二班&quot; } } } }} 查询二班的学生。但是大家注意，这种查询没有评分。 可以使用 parent id 查询子文档： 123456789GET stu_class/_search{ &quot;query&quot;: { &quot;parent_id&quot;:{ &quot;type&quot;:&quot;student&quot;, &quot;id&quot;:1 } }} 通过 parent id 查询，默认情况下使用相关性计算分数。 小结整体上来说： 普通子对象实现一对多，会损失子文档的边界，子对象之间的属性关系丢失。 nested 可以解决第 1 点的问题，但是 nested 有两个缺点：更新主文档的时候要全部更新，不支持子文档属于多个主文档。 父子文档解决 1、2 点的问题，但是它主要适用于写多读少的场景。 地理位置查询数据准备创建一个索引： 12345678910111213PUT geo{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;keyword&quot; }, &quot;location&quot;:{ &quot;type&quot;: &quot;geo_point&quot; } } }} 准备一个 geo.json 文件： 12345678910111213141516{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:1}}{&quot;name&quot;:&quot;西安&quot;,&quot;location&quot;:&quot;34.288991865037524,108.9404296875&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:2}}{&quot;name&quot;:&quot;北京&quot;,&quot;location&quot;:&quot;39.926588421909436,116.43310546875&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:3}}{&quot;name&quot;:&quot;上海&quot;,&quot;location&quot;:&quot;31.240985378021307,121.53076171875&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:4}}{&quot;name&quot;:&quot;天津&quot;,&quot;location&quot;:&quot;39.13006024213511,117.20214843749999&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:5}}{&quot;name&quot;:&quot;杭州&quot;,&quot;location&quot;:&quot;30.259067203213018,120.21240234375001&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:6}}{&quot;name&quot;:&quot;武汉&quot;,&quot;location&quot;:&quot;30.581179257386985,114.3017578125&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:7}}{&quot;name&quot;:&quot;合肥&quot;,&quot;location&quot;:&quot;31.840232667909365,117.20214843749999&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:8}}{&quot;name&quot;:&quot;重庆&quot;,&quot;location&quot;:&quot;29.592565403314087,106.5673828125&quot;} 最后，执行如下命令，批量导入 geo.json 数据： 1curl -XPOST &quot;http://localhost:9200/geo/_bulk?pretty&quot; -H &quot;content-type:application/json&quot; --data-binary @geo.json 可能用到的工具网站： http://geojson.io/#map=6/32.741/116.521 geo_distance query给出一个中心点，查询距离该中心点指定范围内的文档： 1234567891011121314151617181920212223GET geo/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match_all&quot;: {} } ], &quot;filter&quot;: [ { &quot;geo_distance&quot;: { &quot;distance&quot;: &quot;600km&quot;, &quot;location&quot;: { &quot;lat&quot;: 34.288991865037524, &quot;lon&quot;: 108.9404296875 } } } ] } }} 以(34.288991865037524,108.9404296875) 为圆心，以 600KM 为半径，这个范围内的数据。 geo_bounding_box query在某一个矩形内的点，通过两个点锁定一个矩形： 12345678910111213141516171819202122232425262728GET geo/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match_all&quot;: {} } ], &quot;filter&quot;: [ { &quot;geo_bounding_box&quot;: { &quot;location&quot;: { &quot;top_left&quot;: { &quot;lat&quot;: 32.0639555946604, &quot;lon&quot;: 118.78967285156249 }, &quot;bottom_right&quot;: { &quot;lat&quot;: 29.98824461550903, &quot;lon&quot;: 122.20642089843749 } } } } ] } }} 以南京经纬度作为矩形的左上角，以舟山经纬度作为矩形的右下角，构造出来的矩形中，包含上海和杭州两个城市。 geo_polygon query在某一个多边形范围内的查询。 12345678910111213141516171819202122232425262728293031323334GET geo/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match_all&quot;: {} } ], &quot;filter&quot;: [ { &quot;geo_polygon&quot;: { &quot;location&quot;: { &quot;points&quot;: [ { &quot;lat&quot;: 31.793755581217674, &quot;lon&quot;: 113.8238525390625 }, { &quot;lat&quot;: 30.007273923504556, &quot;lon&quot;:114.224853515625 }, { &quot;lat&quot;: 30.007273923504556, &quot;lon&quot;:114.8345947265625 } ] } } } ] } }} 给定多个点，由多个点组成的多边形中的数据。 geo_shape querygeo_shape 用来查询图形，针对 geo_shape，两个图形之间的关系有：相交、包含、不相交。 新建索引： 12345678910111213PUT geo_shape{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;keyword&quot; }, &quot;location&quot;:{ &quot;type&quot;: &quot;geo_shape&quot; } } }} 然后添加一条线： 1234567891011PUT geo_shape/_doc/1{ &quot;name&quot;:&quot;西安-郑州&quot;, &quot;location&quot;:{ &quot;type&quot;:&quot;linestring&quot;, &quot;coordinates&quot;:[ [108.9404296875,34.279914398549934], [113.66455078125,34.768691457552706] ] }} 接下来查询某一个图形中是否包含该线： 12345678910111213141516171819202122232425262728293031323334GET geo_shape/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match_all&quot;: {} } ], &quot;filter&quot;: [ { &quot;geo_shape&quot;: { &quot;location&quot;: { &quot;shape&quot;: { &quot;type&quot;: &quot;envelope&quot;, &quot;coordinates&quot;: [ [ 106.5234375, 36.80928470205937 ], [ 115.33447265625, 32.24997445586331 ] ] }, &quot;relation&quot;: &quot;within&quot; } } } ] } }} relation 属性表示两个图形的关系： within 包含 intersects 相交 disjoint 不相交 特殊查询more_like_this querymore_like_this query 可以实现基于内容的推荐，给定一篇文章，可以查询出和该文章相似的内容。 12345678910111213GET books/_search{ &quot;query&quot;: { &quot;more_like_this&quot;: { &quot;fields&quot;: [ &quot;info&quot; ], &quot;like&quot;: &quot;大学战略&quot;, &quot;min_term_freq&quot;: 1, &quot;max_query_terms&quot;: 12 } }} fields：要匹配的字段，可以有多个 like：要匹配的文本 min_term_freq：词项的最低频率，默认是 2。特别注意，这个是指词项在要匹配的文本中的频率，而不是 es 文档中的频率 max_query_terms：query 中包含的最大词项数目 min_doc_freq：最小的文档频率，搜索的词，至少在多少个文档中出现，少于指定数目，该词会被忽略 max_doc_freq：最大文档频率 analyzer：分词器，默认使用字段的分词器 stop_words：停用词列表 minmum_should_match script query脚本查询，例如查询所有价格大于 200 的图书： 1234567891011121314151617GET books/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;filter&quot;: [ { &quot;script&quot;: { &quot;script&quot;: { &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;if(doc['price'].size()!=0){doc['price'].value &gt; 200}&quot; } } } ] } }} percolate querypercolate query 译作渗透查询或者反向查询。 正常操作：根据查询语句找到对应的文档 query-&gt;document percolate query：根据文档，返回与之匹配的查询语句，document-&gt;query 应用场景： 价格监控 库存报警 股票警告 … 例如阈值告警，假设指定字段值大于阈值，报警提示。 percolate mapping 定义： 12345678910111213141516PUT log{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;threshold&quot;:{ &quot;type&quot;: &quot;long&quot; }, &quot;count&quot;:{ &quot;type&quot;: &quot;long&quot; }, &quot;query&quot;:{ &quot;type&quot;:&quot;percolator&quot; } } }} percolator 类型相当于 keyword、long 以及 integer 等。 插入文档： 123456789101112131415PUT log/_doc/1{ &quot;threshold&quot;:10, &quot;query&quot;:{ &quot;bool&quot;:{ &quot;must&quot;:{ &quot;range&quot;:{ &quot;count&quot;:{ &quot;gt&quot;:10 } } } } }} 最后查询： 12345678910111213141516171819202122232425GET log/_search{ &quot;query&quot;: { &quot;percolate&quot;: { &quot;field&quot;: &quot;query&quot;, &quot;documents&quot;: [ { &quot;count&quot;:3 }, { &quot;count&quot;:6 }, { &quot;count&quot;:90 }, { &quot;count&quot;:12 }, { &quot;count&quot;:15 } ] } }} 查询结果中会列出不满足条件的文档。 查询结果中的 _percolator_document_slot 字段表示文档的 position，从 0 开始计。 结果高亮和排序普通高亮，默认会自动添加 em 标签： 12345678910111213GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;大学&quot; } }, &quot;highlight&quot;: { &quot;fields&quot;: { &quot;name&quot;: {} } }} 查询结果如下： image-20201127170236189 正常来说，我们见到的高亮可能是红色、黄色之类的。 可以自定义高亮标签： 12345678910111213141516GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;大学&quot; } }, &quot;highlight&quot;: { &quot;fields&quot;: { &quot;name&quot;: { &quot;pre_tags&quot;: [&quot;&lt;strong&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/strong&gt;&quot;] } } }} 搜索结果如下： image-20201127170458679 有的时候，虽然我们是在 name 字段中搜索的，但是我们希望 info 字段中，相关的关键字也能高亮： 123456789101112131415161718192021GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;大学&quot; } }, &quot;highlight&quot;: { &quot;require_field_match&quot;: &quot;false&quot;, &quot;fields&quot;: { &quot;name&quot;: { &quot;pre_tags&quot;: [&quot;&lt;strong&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/strong&gt;&quot;] }, &quot;info&quot;: { &quot;pre_tags&quot;: [&quot;&lt;strong&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/strong&gt;&quot;] } } }} 搜索结果如下： image-20201127170726795 排序排序很简单，默认是按照查询文档的相关度来排序的，即（_score 字段）： 12345678910GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;java&quot; } } }} 等价于： 1234567891011121314151617GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;java&quot; } } }, &quot;sort&quot;: [ { &quot;_score&quot;: { &quot;order&quot;: &quot;desc&quot; } } ]} match_all 查询只是返回所有文档，不评分，默认按照添加顺序返回，可以通过 _doc 字段对其进行排序： 1234567891011121314GET books/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;sort&quot;: [ { &quot;_doc&quot;: { &quot;order&quot;: &quot;desc&quot; } } ], &quot;size&quot;: 20} es 同时也支持多字段排序。 12345678910111213141516171819GET books/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;sort&quot;: [ { &quot;price&quot;: { &quot;order&quot;: &quot;asc&quot; } }, { &quot;_doc&quot;: { &quot;order&quot;: &quot;desc&quot; } } ], &quot;size&quot;: 20} 聚合分析指标聚合Max Aggregation 最大值统计最大值。例如查询价格最高的书： 12345678910GET books/_search{ &quot;aggs&quot;: { &quot;max_price&quot;: { &quot;max&quot;: { &quot;field&quot;: &quot;price&quot; } } }} 查询结果如下： 12345&quot;aggregations&quot; : { &quot;max_price&quot; : { &quot;value&quot; : 269.0 }} 1234567891011GET books/_search{ &quot;aggs&quot;: { &quot;max_price&quot;: { &quot;max&quot;: { &quot;field&quot;: &quot;price&quot;, &quot;missing&quot;: 1000 } } }} 如果某个文档中缺少 price 字段，则设置该字段的值为 1000。 也可以通过脚本来查询最大值： 123456789101112GET books/_search{ &quot;aggs&quot;: { &quot;max_price&quot;: { &quot;max&quot;: { &quot;script&quot;: { &quot;source&quot;: &quot;if(doc['price'].size()!=0){doc.price.value}&quot; } } } }} 使用脚本时，可以先通过 doc['price'].size()!=0 去判断文档是否有对应的属性。 Min Aggregation 最小值统计最小值，用法和 Max Aggregation 基本一致： 1234567891011GET books/_search{ &quot;aggs&quot;: { &quot;min_price&quot;: { &quot;min&quot;: { &quot;field&quot;: &quot;price&quot;, &quot;missing&quot;: 1000 } } }} 脚本： 123456789101112GET books/_search{ &quot;aggs&quot;: { &quot;min_price&quot;: { &quot;min&quot;: { &quot;script&quot;: { &quot;source&quot;: &quot;if(doc['price'].size()!=0){doc.price.value}&quot; } } } }} Avg Aggregation统计平均值： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;avg_price&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } }}GET books/_search{ &quot;aggs&quot;: { &quot;avg_price&quot;: { &quot;avg&quot;: { &quot;script&quot;: { &quot;source&quot;: &quot;if(doc['price'].size()!=0){doc.price.value}&quot; } } } }} Sum Aggregation求和： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;sum_price&quot;: { &quot;sum&quot;: { &quot;field&quot;: &quot;price&quot; } } }}GET books/_search{ &quot;aggs&quot;: { &quot;sum_price&quot;: { &quot;sum&quot;: { &quot;script&quot;: { &quot;source&quot;: &quot;if(doc['price'].size()!=0){doc.price.value}&quot; } } } }} Cardinality Aggregationcardinality aggregation 用于基数统计。类似于 SQL 中的 distinct count(0)： text 类型是分析型类型，默认是不允许进行聚合操作的，如果相对 text 类型进行聚合操作，需要设置其 fielddata 属性为 true，这种方式虽然可以使 text 类型进行聚合操作，但是无法满足精准聚合，如果需要精准聚合，可以设置字段的子域为 keyword。 方式一： 重新定义 books 索引： 123456789101112131415161718192021222324252627282930PUT books{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;publish&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;fielddata&quot;: true }, &quot;type&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;author&quot;:{ &quot;type&quot;: &quot;keyword&quot; }, &quot;info&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;price&quot;:{ &quot;type&quot;: &quot;double&quot; } } }} 定义完成后，重新插入数据（参考之前的视频）。 接下来就可以查询出版社的总数量： 12345678910GET books/_search{ &quot;aggs&quot;: { &quot;publish_count&quot;: { &quot;cardinality&quot;: { &quot;field&quot;: &quot;publish&quot; } } }} 查询结果如下： 这种聚合方式可能会不准确。可以将 publish 设置为 keyword 类型或者设置子域为 keyword。 12345678910111213141516171819202122232425262728PUT books{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;publish&quot;:{ &quot;type&quot;: &quot;keyword&quot; }, &quot;type&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;author&quot;:{ &quot;type&quot;: &quot;keyword&quot; }, &quot;info&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;price&quot;:{ &quot;type&quot;: &quot;double&quot; } } }} 查询结果如下： 对比查询结果可知，使用 fileddata 的方式，查询结果不准确。 Stats Aggregation基本统计，一次性返回 count、max、min、avg、sum： 12345678910GET books/_search{ &quot;aggs&quot;: { &quot;stats_query&quot;: { &quot;stats&quot;: { &quot;field&quot;: &quot;price&quot; } } }} 123456789&quot;aggregations&quot; : { &quot;stats_query&quot; : { &quot;count&quot; : 888, &quot;min&quot; : 0.0, &quot;max&quot; : 269.0, &quot;avg&quot; : 29.40765765765766, &quot;sum&quot; : 26114.0 } } Extends Stats Aggregation高级统计，比 stats 多出来：平方和、方差、标准差、平均值加减两个标准差的区间： 12345678910GET books/_search{ &quot;aggs&quot;: { &quot;es&quot;: { &quot;extended_stats&quot;: { &quot;field&quot;: &quot;price&quot; } } }} Percentiles Aggregation百分位统计。 123456789101112131415161718192021GET books/_search{ &quot;aggs&quot;: { &quot;p&quot;: { &quot;percentiles&quot;: { &quot;field&quot;: &quot;price&quot;, &quot;percents&quot;: [ 1, 5, 10, 15, 25, 50, 75, 95, 99 ] } } }} Value Count Aggregation可以按照字段统计文档数量（包含指定字段的文档数量）： 12345678910GET books/_search{ &quot;aggs&quot;: { &quot;count&quot;: { &quot;value_count&quot;: { &quot;field&quot;: &quot;price&quot; } } }} 桶聚合Terms AggregationTerms Aggregation 用于分组聚合，例如，统计各个出版社出版的图书总数量: 1234567891011GET books/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 20 } } }} 统计结果如下： image-20201204200925589 在 terms 分桶的基础上，还可以对每个桶进行指标聚合。 统计不同出版社所出版的图书的平均价格： 123456789101112131415161718GET books/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 20 }, &quot;aggs&quot;: { &quot;avg_price&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } } }} 统计结果如下： image-20201204201400225 Filter Aggregation过滤器聚合。可以将符合过滤器中条件的文档分到一个桶中，然后可以求其平均值。 例如查询书名中包含 java 的图书的平均价格： 12345678910111213141516171819GET books/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;filter&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;java&quot; } }, &quot;aggs&quot;: { &quot;avg_price&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } } }} Filters Aggregation多过滤器聚合。过滤条件可以有多个。 例如查询书名中包含 java 或者 office 的图书的平均价格： 123456789101112131415161718192021222324252627GET books/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;filters&quot;: { &quot;filters&quot;: [ { &quot;term&quot;:{ &quot;name&quot;:&quot;java&quot; } },{ &quot;term&quot;:{ &quot;name&quot;:&quot;office&quot; } } ] }, &quot;aggs&quot;: { &quot;avg_price&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } } }} Range Aggregation按照范围聚合，在某一个范围内的文档数统计。 例如统计图书价格在 0-50、50-100、100-150、150以上的图书数量： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;range&quot;: { &quot;field&quot;: &quot;price&quot;, &quot;ranges&quot;: [ { &quot;to&quot;: 50 },{ &quot;from&quot;: 50, &quot;to&quot;: 100 },{ &quot;from&quot;: 100, &quot;to&quot;: 150 },{ &quot;from&quot;: 150 } ] } } }} Date Range AggregationRange Aggregation 也可以用来统计日期，但是也可以使用 Date Range Aggregation，后者的优势在于可以使用日期表达式。 造数据： 123456789101112131415PUT blog/_doc/1{ &quot;title&quot;:&quot;java&quot;, &quot;date&quot;:&quot;2018-12-30&quot;}PUT blog/_doc/2{ &quot;title&quot;:&quot;java&quot;, &quot;date&quot;:&quot;2020-12-30&quot;}PUT blog/_doc/3{ &quot;title&quot;:&quot;java&quot;, &quot;date&quot;:&quot;2022-10-30&quot;} 统计一年前到一年后的博客数量： 12345678910111213141516GET blog/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;date_range&quot;: { &quot;field&quot;: &quot;date&quot;, &quot;ranges&quot;: [ { &quot;from&quot;: &quot;now-12M/M&quot;, &quot;to&quot;: &quot;now+1y/y&quot; } ] } } }} 12M/M 表示 12 个月。 1y/y 表示 1年。 d 表示天 Date Histogram Aggregation时间直方图聚合。 例如统计各个月份的博客数量 1234567891011GET blog/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;date_histogram&quot;: { &quot;field&quot;: &quot;date&quot;, &quot;calendar_interval&quot;: &quot;month&quot; } } }} Missing Aggregation空值聚合。 统计所有没有 price 字段的文档： 12345678910GET books/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;missing&quot;: { &quot;field&quot;: &quot;price&quot; } } }} Children Aggregation可以根据父子文档关系进行分桶。 查询子类型为 student 的文档数量： 12345678910GET stu_class/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;children&quot;: { &quot;type&quot;: &quot;student&quot; } } }} Geo Distance Aggregation对地理位置数据做统计。 例如查询(34.288991865037524,108.9404296875)坐标方圆 600KM 和 超过 600KM 的城市数量。 12345678910111213141516171819GET geo/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;geo_distance&quot;: { &quot;field&quot;: &quot;location&quot;, &quot;origin&quot;: &quot;34.288991865037524,108.9404296875&quot;, &quot;unit&quot;: &quot;km&quot;, &quot;ranges&quot;: [ { &quot;to&quot;: 600 },{ &quot;from&quot;: 600 } ] } } }} IP Range AggregationIP 地址范围查询。 12345678910111213141516GET blog/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;ip_range&quot;: { &quot;field&quot;: &quot;ip&quot;, &quot;ranges&quot;: [ { &quot;from&quot;: &quot;127.0.0.5&quot;, &quot;to&quot;: &quot;127.0.0.11&quot; } ] } } }} 管道聚合管道聚合相当于在之前聚合的基础上，再次聚合。 Avg Bucket Aggregation计算聚合平均值。例如，统计每个出版社所出版图书的平均值，然后再统计所有出版社的平均值： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;book_count&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 3 }, &quot;aggs&quot;: { &quot;book_avg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } }, &quot;avg_book&quot;:{ &quot;avg_bucket&quot;: { &quot;buckets_path&quot;: &quot;book_count&gt;book_avg&quot; } } }} Max Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值中的最大值： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;book_count&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 3 }, &quot;aggs&quot;: { &quot;book_avg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } }, &quot;avg_book&quot;:{ &quot;max_bucket&quot;: { &quot;buckets_path&quot;: &quot;book_count&gt;book_avg&quot; } } }} Min Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值中的最小值： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;book_count&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 3 }, &quot;aggs&quot;: { &quot;book_avg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } }, &quot;avg_book&quot;:{ &quot;min_bucket&quot;: { &quot;buckets_path&quot;: &quot;book_count&gt;book_avg&quot; } } }} Sum Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值之和： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;book_count&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 3 }, &quot;aggs&quot;: { &quot;book_avg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } }, &quot;avg_book&quot;:{ &quot;sum_bucket&quot;: { &quot;buckets_path&quot;: &quot;book_count&gt;book_avg&quot; } } }} Stats Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值的各种数据： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;book_count&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 3 }, &quot;aggs&quot;: { &quot;book_avg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } }, &quot;avg_book&quot;:{ &quot;stats_bucket&quot;: { &quot;buckets_path&quot;: &quot;book_count&gt;book_avg&quot; } } }} Extended Stats Bucket Aggregation1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;book_count&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 3 }, &quot;aggs&quot;: { &quot;book_avg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } }, &quot;avg_book&quot;:{ &quot;extended_stats_bucket&quot;: { &quot;buckets_path&quot;: &quot;book_count&gt;book_avg&quot; } } }} Percentiles Bucket Aggregation1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;book_count&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 3 }, &quot;aggs&quot;: { &quot;book_avg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } }, &quot;avg_book&quot;:{ &quot;percentiles_bucket&quot;: { &quot;buckets_path&quot;: &quot;book_count&gt;book_avg&quot; } } }} Java客户端操作ES","link":"/2021/07/02/ElasticSearch%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89%E7%89%B9%E6%AE%8A%E6%9F%A5%E8%AF%A2%E5%92%8C%E7%AE%A1%E9%81%93%E8%81%9A%E5%90%88/"},{"title":"JVM（一）内存区域和对象详解","text":"Java是一门可以跨平台的语言，主要就是通过Java虚拟机来实现的: 编译器将Java文件编译为Java字节码文件（.class），接下来JVM对字节码文件进行解释，翻译成特定底层平台匹配的机器指令(win/linux/mac)并运行。 Java内存区域Java虚拟机定义了若干程序运行时的数据区，分为线程私有和公有的: 运行时数据区程序计数器程序计数器（Program Counter Register）也被称为PC寄存器，是一块较小的内存空间。 可以看作是当前线程所执行的字节码的行号指示器，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 线程私有的，在任意时刻，一条 Java 虚拟机线程只会执行一个方法的代码，这个正在被线程执行的方法称为该线程的当前方法。 程序计数器是唯一一个在虚拟机规范中没有规定OutOfMemoryError的区域 虚拟机栈Java虚拟机栈（Java Virtual Machine Stack）也是线程私有的，它的生命周期与线程相同。 每个方法被执行的时候，Java虚拟机都会同步创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态连接、方法出口等信息。每一个方法被调用直至执行完毕的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 局部变量表存放了编译期可知的各种Java虚拟机基本数据类型（boolean、byte、char、short、int、 float、long、double）、对象引用（reference类型，它并不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress 类型（指向了一条字节码指令的地址）。 这些数据类型在局部变量表中的存储空间以局部变量槽（Slot）来表示，其中64位长度的long和 double类型的数据会占用两个变量槽，其余的数据类型只占用一个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在栈帧中分配多大的局部变量空间是完全确定 的，在方法运行期间不会改变局部变量表的大小。 虚拟机栈会发生以下两种异常： 如果线程请求分配的栈容量超过 Java 虚拟机栈允许的最大容量时，Java 虚拟机将会抛出一个 StackOverflowError 异常。 如果 Java 虚拟机栈可以动态扩展，并且扩展的动作已经尝试过，但是目前无法申请到足够的内存去完成扩展，或者在建立新的线程时没有足够的内存去创建对应的虚拟机栈，那 Java 虚拟机将会抛出一个 OutOfMemoryError 异常。 本地方法栈地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别只是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的本地（Native）方法服务。 会发生的异常和虚拟机栈相同 Java堆Java堆（Java Heap）是虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，Java里“几乎”所有的对象实例都在这里分配内存。 Java堆是垃圾收集器管理的内存区域，因此一些资料中它也被称作“GC堆”（Garbage Collected Heap，）。从回收内存的角度看，由于现代垃圾收集器大部分都是基于分代收集理论设计的，所以Java堆中经常会出现“新生代”“老年代”“永久代”“Eden空间”“From Survivor空间”“To Survivor空间”等名词，需要注意的是这些区域划分仅仅是一部分垃圾收集器的共同特性或者说设计风格而已，而非某个Java虚拟机具体实现的固有内存布局，更不是《Java虚拟机规范》里对Java堆的进一步细致划分。 如果从分配内存的角度看，所有线程共享的Java堆中可以划分出多个线程私有的分配缓冲区 （Thread Local Allocation Buffer，TLAB），以提升对象分配时的效率。不过无论从什么角度，无论如何划分，都不会改变Java堆中存储内容的共性，无论是哪个区域，存储的都只能是对象的实例，将Java 堆细分的目的只是为了更好地回收内存，或者更快地分配内存。 方法区方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。 这区域的内存回收目标主要是针对常量池的回收和对类型的卸载 运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池表（Constant Pool Table），用于存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。 Class文件经过类加载器加载后，之前Class文件常量池的内容会存放到方法区的运行时常量池，需要注意的是Class文件常量池的符号引用会转变直接引用存入运行时常量池。 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量 一定只有编译期才能产生，也就是说，并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可以将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法。 直接内存在JDK 1.4中新加入了NIO（New Input/Output）类，引入了一种基于通道（Channel）与缓冲区 （Buffer）的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆里面的 DirectByteBuffer对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了 在Java堆和Native堆中来回复制数据。 JDK的内存演变 JDK1.6时期和我们上面讲的JVM内存区域是一致的： JDK1.7时发生了一些变化，将字符串常量池、静态变量，存放在堆上 在JDK1.8时彻底干掉了方法区，而在直接内存中划出一块区域作为元空间，运行时常量池、类常量池都移动到元空间。 思考一下，为什么使用元空间替换永久代？ 表面上看是为了避免OOM异常。因为通常使用PermSize和MaxPermSize设置永久代的大小就决定了永久代的上限，但是不是总能知道应该设置为多大合适, 如果使用默认值很容易遇到OOM错误。 当使用元空间时，可以加载多少类的元数据就不再由MaxPermSize控制, 而由系统的实际可用空间来控制。 更深层的原因还是要合并HotSpot和JRockit的代码，JRockit从来没有所谓的永久代，也不需要开发运维人员设置永久代的大小，但是运行良好。同时也不用担心运行性能问题了,在覆盖到的测试中, 程序启动和运行速度降低不超过1%，但是这点性能损失换来了更大的安全保障。 哪些地方会发生OOM 堆内存不足是最常见的OOM原因之一，抛出的错误信息是“java.lang.OutOfMemoryError:Java heap space”，原因可能千奇百怪，例如，可能存在内存泄漏问题；也很有可能就是堆的大小不合理，比如我们要处理比较可观的数据量，但是没有显式指定JVM堆大小或者指定数值偏小；或者出现JVM处理引用不及时，导致堆积起来，内存无法释放等。 对于Java虚拟机栈和本地方法栈，这里要稍微复杂一点。如果我们写一段程序不断的进行递归调用，而且没有退出条件，就会导致不断地进行压栈。类似这种情况，JVM实际会抛出StackOverFlowError；当然，如果JVM试图去扩展栈空间的的时候失败，则会抛出OutOfMemoryError。 对于老版本的Oracle JDK，因为永久代的大小是有限的，并且JVM对永久代垃圾回收（如，常量池回收、卸载不再需要的类型）非常不积极，所以当我们不断添加新类型的时候，永久代出现OutOfMemoryError也非常多见，尤其是在运行时存在大量动态类型生成的场合；类似Intern字符串缓存占用太多空间，也会导致OOM问题。对应的异常信息，会标记出来和永久代相关：“java.lang.OutOfMemoryError: PermGen space”。随着元数据区的引入，方法区内存已经不再那么窘迫，所以相应的OOM有所改观，出现OOM，异常信息则变成了：“java.lang.OutOfMemoryError: Metaspace”。 直接内存不足，也会导致OOM。 对象的创建都是在堆上吗我注意到有一些观点，认为通过逃逸分析，JVM会在栈上分配那些不会逃逸的对象，这在理论上是可行的，但是取决于JVM设计者的选择。据我所知，Oracle Hotspot JVM中并未这么做，这一点在逃逸分析相关的文档里已经说明，所以可以明确所有的对象实例都是创建在堆上。 目前很多书籍还是基于JDK 7以前的版本，JDK已经发生了很大变化，Intern字符串的缓存和静态变量曾经都被分配在永久代上，而永久代已经被元数据区取代。但是，Intern字符串缓存和静态变量并不是被转移到元数据区，而是直接在堆上分配，所以这一点同样符合前面一点的结论：对象实例都是分配在堆上。 字符串常量池的底层原理字符串常量池是全局的，JVM 中独此一份，因此也称为全局字符串常量池，在 jdk1.7（含）之后是在堆内存之中，存储的是字符串对象的引用，字符串实例是在堆中； 在HotSpot VM里实现线程池功能的是一个StringTable类，它是一个Hash表，默认值大小长度是1009；这个StringTable在每个HotSpot VM的实例只有一份，被所有的类共享。字符串常量由一个一个字符组成，放在了StringTable上。 12345678String str1 = &quot;图解Java&quot;; //先到常量池中查询有没有`&quot;图解Java&quot;`字符串的引用，如果没有，则会在`Java堆`上创建`&quot;图解Java&quot;`字符串，在常量池中存储字符串的地址，`str1`则指向字符串常量池的地址String str2 = new String(&quot;图解Java&quot;);//直接在Java堆中创建对象。`str2`指向堆中的地址。System.out.println(str1 == str2); //falseString str3 = &quot;图解Java&quot;; //str3发现字符串常量池中已经有了`&quot;图解Java&quot;`字符串的引用，则直接返回，不会创建新的对象System.out.println(str1 == str3); //true JVM中除了字符串常量池，8种基本数据类型中除了两种浮点类型剩余的6种基本数据类型的包装类，都使用了缓冲池技术，但是 Byte、Short、Integer、Long、Character 这5种整型的包装类也只是在对应值在 [-128,127] 时才会使用缓冲池，超出此范围仍然会去创建新的对象。 Java对象详解创建对象的4种方式 new 关键字： 在方法区的常量池中查看是否有new 后面参数（也就是类名）的符号引用，并检查是否有类的加载信息也就是是否被加载解析和初始化过。如果已经加载过了就不在加载，否则执行类的加载全过程。 给实例分配内存：此内存中存放对象自己的实例变量和从父类继承过来的实例变量（即使这些从超类继承过来的实例变量有可能被隐藏也会被分配空间），同时这些实例变量被赋予默认值（零值）； 调用构造函数，初始化成员字段：在Java对象初始化过程中，主要涉及三种执行对象初始化的结构，分别是实例变量初始化、实例代码块初始化以及构造函数初始化； user对象指向分配的内存空间： 注意：new操作不是原子操作，b和c的顺序可能会调换。 clone方法创建对象： 要想让一个对象支持clone，必须让这个对象对应的类实现Cloneable接口（标识接口），同时此类中也要重写clone方法 clone()方法是属于Object类的，clone是在堆内存中用二进制的方式进行拷贝，重新分配给对象一块内存 反射创建对象： 获取类的Class对象实例 1234获取方式如下：Class.forName(&quot;类全路径&quot;);类名.class; 如：Animal.class;对象名.getClass(); 通过反射创建类对象的实例对象 1Class.newInstance()：调用无参的构造方法，必需确保类中有无参数的可见的构造函数，否则将会抛出异常； 强制转换成用户所需类型 Java 反射机制是指在程序运行时，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性。这种动态的获取信息以及动态调用对象的方法的功能称为java 的反射机制。 反射机制很重要的一点就是“运行时”，其使得我们可以在程序运行时加载、探索以及使用编译期间完全未知的 .class 文件。换句话说，Java 程序可以加载一个运行时才得知名称的 .class 文件，然后获悉其完整构造，并生成其对象实体、或对其 fields（变量）设值、或调用其 methods（方法） 反序列化创建对象 Java中要序列化的类必须实现Serializable接口； 所有可在网络上传输的对象都必须是可序列化的；如RMI（remote method invoke，即远程方法调用），传入的参数或返回的对象都是可序列化的，否则会出错； 所有需要保存到磁盘的java对象都必须是可序列化的；通常建议：程序创建的每个JavaBean类都实现Serializeable接口； New对象创建过程我们以虚拟机遇到一个new指令开始： 首先检查这个指令的参数是否能在常量池中定位到一个类的符号引用 检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，就先执行相应的类加载过程 类加载检查通过后，接下来虚拟机将为新生对象分配内存。 内存分配有两种方式，指针碰撞（Bump The Pointer）、空闲列表（Free List） 指针碰撞：假设Java堆中内存是绝对规整的，所有被使用过的内存都被放在一边，空闲的内存被放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间方向挪动一段与对象大小相等的距离，这种分配方式称为“指针碰撞” 如果Java堆中的内存并不是规整的，已被使用的内存和空闲的内存相互交错在一起，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为“空闲列表” 两种方式的选择由Java堆是否规整决定 Java堆规整由所采用的垃圾收集器是否带有空间压缩整理（Compact）的能力决定 内存分配完成之后，虚拟机将分配到的内存空间（但不包括对象头）都初始化为零值。 设置对象头，请求头里包含了对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。 从虚拟机角度来看，设置完对象头信息以后初始化就已经完成了，但是对于Java程序而言，new指令之后会接着执行 ()方法，对对象进行初始化，这样一个真正可用的对象才算完全被构造出来。 分配对象内存的时候如何保证线程安全分配内存线程安全问题：对象创建在虚拟机中是非常频繁的行为，即使仅仅修改一个指针所指向的位置，在并发情况下也并不是线程安全的，可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。 线程安全问题有两种解可选方案： 一种是对分配内存空间的动作进行同步处理——实际上虚拟机是采用CAS配上失败重试的方式保证更新操作的原子性 另外一种是把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer，TLAB），哪个线程要分配内存，就在哪个线程的本地缓冲区中分配，只有本地缓冲区用完了，分配新的缓存区时才需要同步锁定。 对象的内存布局在HotSpot虚拟机里，对象在堆内存中的存储布局可以划分为三个部分：对象头（Header）、实例数据（Instance Data）和对齐填充（Padding） 在64位的HotSpot虚拟机中，如对象未被同步锁锁定的状态下，Mark Word的64个比特存储空间中的31个比特用于存储对象哈希码，4个比特用于存储对象分代年龄，2个比特用于存储锁标志位，在其他状态（轻量级锁、重量级锁、偏向锁）下对象的存储内容变化如图示。 对象头的另外一部分是类型指针，即对象指向它的类型元数据的指针，Java虚拟机通过这个指针来确定该对象是哪个类的实例。并不是所有的虚拟机实现都必须在对象数据上保留类型指针，查找对象的元数据信息并不一定要经过对象本身， 如果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据，因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小，但是如果数组的长度是不确定的，将无法通过元数据中的信息推断出数组的大小。 对象的访问定位Java程序会通过栈上的reference数据来操作堆上的具体对象。由于reference类型在《Java虚拟机规范》里面只规定了它是一个指向对象的引用，并没有定义这个引用应该通过什么方式去定位、访问到堆中对象的具体位置，所以对象访问方式也是由虚拟机实现而定的，主流的访问方式主要有使用句柄和直接指针两种： 如果使用句柄访问的话，Java堆中将可能会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自具体的地址信息，其结构如图所示： 如果使用直接指针访问的话，Java堆中对象的内存布局就必须考虑如何放置访问类型数据的相关信息，reference中存储的直接就是对象地址，如果只是访问对象本身的话，就不需要多一次间接访问的开销，如图所示： 这两种对象访问方式各有优势，使用句柄来访问的最大好处就是reference中存储的是稳定句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而reference本身不需要被修改。 使用直接指针来访问最大的好处就是速度更快，它节省了一次指针定位的时间开销，由于对象访问在Java中非常频繁，因此这类开销积少成多也是一项极为可观的执行成本。 HotSpot虚拟机主要使用直接指针来进行对象访问。 总结：","link":"/2021/05/10/JVM%EF%BC%88%E4%B8%80%EF%BC%89%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E5%92%8C%E5%AF%B9%E8%B1%A1%E8%AF%A6%E8%A7%A3/"},{"title":"ElasticSearch从入门到实战（二）映射字段和查询","text":"本篇讲述了； 映射Mapping 字段类型 映射参数 基础查询 复合查询 映射Mapping映射就是 Mapping，它用来定义一个文档以及文档所包含的字段该如何被存储和索引。所以，它其实有点类似于关系型数据库中表的定义。 映射分类动态映射顾名思义，就是自动创建出来的映射。es 根据存入的文档，自动分析出来文档中字段的类型以及存储方式，这种就是动态映射。 举一个简单例子，新建一个索引，然后查看索引信息： 1PUT blog 123456789101112131415161718192021222324252627282930313233343536373839404142{ &quot;version&quot;: 4, &quot;mapping_version&quot;: 1, &quot;settings_version&quot;: 1, &quot;aliases_version&quot;: 1, &quot;routing_num_shards&quot;: 1024, &quot;state&quot;: &quot;open&quot;, &quot;settings&quot;: { &quot;index&quot;: { &quot;routing&quot;: { &quot;allocation&quot;: { &quot;include&quot;: { &quot;_tier_preference&quot;: &quot;data_content&quot;}}}, &quot;number_of_shards&quot;: &quot;1&quot;, &quot;provided_name&quot;: &quot;blog&quot;, &quot;creation_date&quot;: &quot;1625109435679&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;SMZxI91HQMahoEudlSV92Q&quot;, &quot;version&quot;: { &quot;created&quot;: &quot;7130299&quot;}}}, &quot;mappings&quot;: { }, //mappings 为空，这个 mappings 中保存的就是映射信息。 &quot;aliases&quot;: [ ], &quot;primary_terms&quot;: { &quot;0&quot;: 1 }, &quot;in_sync_allocations&quot;: { &quot;0&quot;: [ &quot;L6J-WIOnQTms9nQm7NRhWw&quot; ]}, &quot;rollover_info&quot;: { }, &quot;system&quot;: false, &quot;timestamp_range&quot;: { &quot;unknown&quot;: true }} 现在我们向索引中添加一个文档，如下： 12345PUT blog/_doc/1{ &quot;title&quot;:&quot;1111&quot;, &quot;date&quot;:&quot;2021-07-01&quot;} 文档添加成功后，就会自动生成 Mappings： 12345678910111213141516171819&quot;mappings&quot;: { &quot;_doc&quot;: { &quot;properties&quot;: { &quot;date&quot;: { //date 字段的类型为 date &quot;type&quot;: &quot;date&quot; }, &quot;title&quot;: { //title 的类型有两个，text 和 keyword &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: { &quot;keyword&quot;: { &quot;ignore_above&quot;: 256, &quot;type&quot;: &quot;keyword&quot; } }}}} 可以看到，date 字段的类型为 date，title 的类型有两个，text 和 keyword。 默认情况下，文档中如果新增了字段，mappings 中也会自动新增进来。 有的时候，如果希望新增字段时，能够抛出异常来提醒开发者，这个可以通过 mappings 中 dynamic 属性来配置。 dynamic 属性有三种取值： true，默认即此。自动添加新字段。 false，忽略新字段。 strict，严格模式，发现新字段会抛出异常。 具体配置方式如下，创建索引时指定 mappings（这其实就是静态映射）： 1234567891011121314PUT blog{ &quot;mappings&quot;: { &quot;dynamic&quot;:&quot;strict&quot;, &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;: &quot;text&quot; }, &quot;age&quot;:{ &quot;type&quot;:&quot;long&quot; } } }} 然后向 blog 中索引中添加数据： 123456PUT blog/_doc/2{ &quot;title&quot;:&quot;1111&quot;, &quot;date&quot;:&quot;2020-11-11&quot;, &quot;age&quot;:99} 在添加的文档中，多出了一个 date 字段，而该字段没有预定义，所以这个添加操作就回报错： 12345678910111213{ &quot;error&quot; : { &quot;root_cause&quot; : [ { &quot;type&quot; : &quot;strict_dynamic_mapping_exception&quot;, &quot;reason&quot; : &quot;mapping set to strict, dynamic introduction of [date] within [_doc] is not allowed&quot; } ], &quot;type&quot; : &quot;strict_dynamic_mapping_exception&quot;, &quot;reason&quot; : &quot;mapping set to strict, dynamic introduction of [date] within [_doc] is not allowed&quot; }, &quot;status&quot; : 400} 动态映射还有一个日期检测的问题。 例如新建一个索引，然后添加一个含有日期的文档，如下： 1234PUT blog/_doc/1{ &quot;remark&quot;:&quot;2020-11-11&quot;} 添加成功后，remark 字段会被推断是一个日期类型。 12345678&quot;mappings&quot;: { &quot;_doc&quot;: { &quot;properties&quot;: { &quot;remark&quot;: { &quot;type&quot;: &quot;date&quot; } }} 此时，remark 字段就无法存储其他类型了。 1234PUT blog/_doc/1{ &quot;remark&quot;:&quot;javaboy&quot;} 此时报错如下： 123456789101112131415161718192021{ &quot;error&quot; : { &quot;root_cause&quot; : [ { &quot;type&quot; : &quot;mapper_parsing_exception&quot;, &quot;reason&quot; : &quot;failed to parse field [remark] of type [date] in document with id '1'. Preview of field's value: 'javaboy'&quot; } ], &quot;type&quot; : &quot;mapper_parsing_exception&quot;, &quot;reason&quot; : &quot;failed to parse field [remark] of type [date] in document with id '1'. Preview of field's value: 'javaboy'&quot;, &quot;caused_by&quot; : { &quot;type&quot; : &quot;illegal_argument_exception&quot;, &quot;reason&quot; : &quot;failed to parse date field [javaboy] with format [strict_date_optional_time||epoch_millis]&quot;, &quot;caused_by&quot; : { &quot;type&quot; : &quot;date_time_parse_exception&quot;, &quot;reason&quot; : &quot;Failed to parse with all enclosed parsers&quot; } } }, &quot;status&quot; : 400} 要解决这个问题，可以使用静态映射，即在索引定义时，将 remark 指定为 text 类型。也可以关闭日期检测。 123456PUT blog{ &quot;mappings&quot;: { &quot;date_detection&quot;: false }} 此时日期类型就回当成文本来处理。 静态映射 略。 类型推断es 中动态映射类型推断方式如下： JSON中的数据 自动推断出来的数据类型 null 没有字段被添加 true、false boolean 浮点数字 float 数字 long json对象 object 数组 数组中的第一个非空值来决定 string text、keyword、date、double、long都有可能 字段类型核心类型字符串类型 string：这是一个已经过期的字符串类型。在 es5 之前，用这个来描述字符串，现在的话，它已经被 text 和 keyword 替代了。 text：如果一个字段是要被全文检索的，比如说博客内容、新闻内容、产品描述，那么可以使用 text。用了 text 之后，字段内容会被分析，在生成倒排索引之前，字符串会被分词器分成一个个词项。text 类型的字段不用于排序，很少用于聚合。这种字符串也被称为 analyzed 字段。 keyword：这种类型适用于结构化的字段，例如标签、email 地址、手机号码等等，这种类型的字段可以用作过滤、排序、聚合等。这种字符串也称之为 not-analyzed 字段。 数字类型 在满足需求的情况下，优先使用范围小的字段。字段长度越短，索引和搜索的效率越高。 浮点数，优先考虑使用 scaled_float。 scaled_float 举例： 1234567891011121314PUT product{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;text&quot; }, &quot;price&quot;:{ &quot;type&quot;: &quot;scaled_float&quot;, &quot;scaling_factor&quot;: 100 } } }} 日期类型由于 JSON 中没有日期类型，所以 es 中的日期类型形式就比较多样： 2020-11-11 或者 2020-11-11 11:11:11 一个从 1970.1.1 零点到现在的一个秒数或者毫秒数。 es 内部将时间转为 UTC，然后将时间按照 millseconds-since-the-epoch 的长整型来存储。 自定义日期类型： 12345678910PUT product{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;date&quot;:{ &quot;type&quot;: &quot;date&quot; } } }} 这个能够解析出来的时间格式比较多。 123456789101112131415PUT product/_doc/1{ &quot;date&quot;:&quot;2020-11-11&quot;}PUT product/_doc/2{ &quot;date&quot;:&quot;2020-11-11T11:11:11Z&quot;}PUT product/_doc/3{ &quot;date&quot;:&quot;1604672099958&quot;} 上面三个文档中的日期都可以被解析，内部存储的是毫秒计时的长整型数。 布尔类型（boolean）JSON 中的 “true”、“false”、true、false 都可以。 二进制类型（binary）二进制接受的是 base64 编码的字符串，默认不存储，也不可搜索。 范围类型 integer_range float_range long_range double_range date_range ip_range 定义的时候，指定范围类型即可： 12345678910111213PUT product{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;date&quot;:{ &quot;type&quot;: &quot;date&quot; }, &quot;price&quot;:{ &quot;type&quot;:&quot;float_range&quot; } } }} 插入文档的时候，需要指定范围的界限： 12345678910111213PUT product{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;date&quot;:{ &quot;type&quot;: &quot;date&quot; }, &quot;price&quot;:{ &quot;type&quot;:&quot;float_range&quot; } } }} 指定范围的时，可以使用 gt、gte、lt、lte。 复合类型数组类型es 中没有专门的数组类型。默认情况下，任何字段都可以有一个或者多个值。需要注意的是，数组中的元素必须是同一种类型。 添加数组是，数组中的第一个元素决定了整个数组的类型。 对象类型（object）由于 JSON 本身具有层级关系，所以文档包含内部对象。内部对象中，还可以再包含内部对象。 1234567PUT product/_doc/2{ &quot;date&quot;:&quot;2020-11-11T11:11:11Z&quot;, &quot;ext_info&quot;:{ &quot;address&quot;:&quot;China&quot; }} 嵌套类型（nested）nested 是 object 中的一个特例。 如果使用 object 类型，假如有如下一个文档： 123456789101112{ &quot;user&quot;:[ { &quot;first&quot;:&quot;Zhang&quot;, &quot;last&quot;:&quot;san&quot; }, { &quot;first&quot;:&quot;Li&quot;, &quot;last&quot;:&quot;si&quot; } ]} 由于 Lucene 没有内部对象的概念，所以 es 会将对象层次扁平化，将一个对象转为字段名和值构成的简单列表。即上面的文档，最终存储形式如下： 1234{&quot;user.first&quot;:[&quot;Zhang&quot;,&quot;Li&quot;],&quot;user.last&quot;:[&quot;san&quot;,&quot;si&quot;]} 扁平化之后，用户名之间的关系没了。这样会导致如果搜索 Zhang si 这个人，会搜索到。 此时可以 nested 类型来解决问题，nested 对象类型可以保持数组中每个对象的独立性。nested 类型将数组中的每一饿对象作为独立隐藏文档来索引，这样每一个嵌套对象都可以独立被索引。 123456789{{&quot;user.first&quot;:&quot;Zhang&quot;,&quot;user.last&quot;:&quot;san&quot;},{&quot;user.first&quot;:&quot;Li&quot;,&quot;user.last&quot;:&quot;si&quot;}} 优点 文档存储在一起，读取性能高。 缺点 更新父或者子文档时需要更新更个文档。 地理类型使用场景： 查找某一个范围内的地理位置 通过地理位置或者相对中心点的距离来聚合文档 把距离整个到文档的评分中 通过距离对文档进行排序 geo_pointgeo_point 就是一个坐标点，定义方式如下： 12345678910PUT people{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;location&quot;:{ &quot;type&quot;: &quot;geo_point&quot; } } }} 创建时指定字段类型，存储的时候，有四种方式： 12345678910111213141516171819202122PUT people/_doc/1{ &quot;location&quot;:{ &quot;lat&quot;: 34.27, &quot;lon&quot;: 108.94 }}PUT people/_doc/2{ &quot;location&quot;:&quot;34.27,108.94&quot;}PUT people/_doc/3{ &quot;location&quot;:&quot;uzbrgzfxuzup&quot;}PUT people/_doc/4{ &quot;location&quot;:[108.94,34.27]} 注意，使用数组描述，先经度后纬度。 地址位置转 geo_hash：http://www.csxgame.top/#/ geo_shape 指定 geo_shape 类型： 12345678910PUT people{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;location&quot;:{ &quot;type&quot;: &quot;geo_shape&quot; } } }} 添加文档时需要指定具体的类型： 1234567PUT people/_doc/1{ &quot;location&quot;:{ &quot;type&quot;:&quot;point&quot;, &quot;coordinates&quot;: [108.94,34.27] }} 如果是 linestring，如下： 1234567PUT people/_doc/2{ &quot;location&quot;:{ &quot;type&quot;:&quot;linestring&quot;, &quot;coordinates&quot;: [[108.94,34.27],[100,33]] }} 特殊类型IP存储 IP 地址，类型是 ip： 12345678910PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;address&quot;:{ &quot;type&quot;: &quot;ip&quot; } } }} 添加文档： 1234PUT blog/_doc/1{ &quot;address&quot;:&quot;192.168.91.1&quot;} 搜索文档： 12345678GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;address&quot;: &quot;192.168.0.0/16&quot; } }} token_count用于统计字符串分词后的词项个数。 12345678910111213141516PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: { &quot;length&quot;:{ &quot;type&quot;:&quot;token_count&quot;, &quot;analyzer&quot;:&quot;standard&quot; } } } } }} 相当于新增了 title.length 字段用来统计分词后词项的个数。 添加文档： 1234PUT blog/_doc/1{ &quot;title&quot;:&quot;zhang san&quot;} 可以通过 token_count 去查询： 12345678GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;title.length&quot;: 2 } }} 24种映射参数analyzer定义文本字段的分词器。默认对索引和查询都是有效的。 假设不用分词器，我们先来看一下索引的结果，创建一个索引并添加一个文档： 123456PUT blogPUT blog/_doc/1{ &quot;title&quot;:&quot;定义文本字段&quot;} 查看词条向量（term vectors） 1234GET blog/_termvectors/1{ &quot;fields&quot;: [&quot;title&quot;]} 查看结果如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879{ &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 2, &quot;found&quot; : true, &quot;took&quot; : 19, &quot;term_vectors&quot; : { &quot;title&quot; : { &quot;field_statistics&quot; : { &quot;sum_doc_freq&quot; : 16, &quot;doc_count&quot; : 2, &quot;sum_ttf&quot; : 16 }, &quot;terms&quot; : { &quot;义&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 1, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2 } ] }, &quot;字&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 4, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 5 } ] }, &quot;定&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 0, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1 } ] }, &quot;文&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 2, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3 } ] }, &quot;本&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 3, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 4 } ] }, &quot;段&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 5, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 6 } ] } } } }} 可以看到，默认情况下，中文就是一个字一个字的分，这种分词方式没有任何意义。如果这样分词，查询就只能按照一个字一个字来查，像下面这样： 12345678GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;title&quot;: &quot;定&quot; } }} 无意义！！！ 所以，我们要根据实际情况，配置合适的分词器。 给字段设定分词器： 1234567891011PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot; } } }} 存储文档： 1234PUT blog/_doc/1{ &quot;title&quot;:&quot;定义文本字段的分词器。&quot;} 查看词条向量： 1234GET blog/_termvectors/1{ &quot;fields&quot;: [&quot;title&quot;]} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869{ &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;found&quot; : true, &quot;took&quot; : 2, &quot;term_vectors&quot; : { &quot;title&quot; : { &quot;field_statistics&quot; : { &quot;sum_doc_freq&quot; : 5, &quot;doc_count&quot; : 1, &quot;sum_ttf&quot; : 5 }, &quot;terms&quot; : { &quot;分词器&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 4, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 10 } ] }, &quot;字段&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 2, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 6 } ] }, &quot;定义&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 0, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2 } ] }, &quot;文本&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 1, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 4 } ] }, &quot;的&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 3, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 7 } ] } } } }} 然后就可以通过词去搜索了： 12345678GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;title&quot;: &quot;索引&quot; } }} search_analyzer查询时候的分词器。默认情况下，如果没有配置 search_analyzer，则查询时，首先查看有没有 search_analyzer，有的话，就用 search_analyzer 来进行分词，如果没有，则看有没有 analyzer，如果有，则用 analyzer 来进行分词，否则使用 es 默认的分词器。 normalizernormalizer 参数用于解析前（索引或者查询）的标准化配置。 比如，在 es 中，对于一些我们不想切分的字符串，我们通常会将其设置为 keyword，搜索时候也是使用整个词进行搜索。如果在索引前没有做好数据清洗，导致大小写不一致，例如 javaboy 和 JAVABOY，此时，我们就可以使用 normalizer 在索引之前以及查询之前进行文档的标准化。 先来一个反例，创建一个名为 blog 的索引，设置 author 字段类型为 keyword： 12345678910PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;author&quot;:{ &quot;type&quot;: &quot;keyword&quot; } } }} 添加两个文档： 123456789PUT blog/_doc/1{ &quot;author&quot;:&quot;javaboy&quot;}PUT blog/_doc/2{ &quot;author&quot;:&quot;JAVABOY&quot;} 然后进行搜索： 12345678GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;author&quot;: &quot;JAVABOY&quot; } }} 大写关键字可以搜到大写的文档，小写关键字可以搜到小写的文档。 如果使用了 normalizer，可以在索引和查询时，分别对文档进行预处理。 normalizer 定义方式如下： 123456789101112131415161718192021PUT blog{ &quot;settings&quot;: { &quot;analysis&quot;: { &quot;normalizer&quot;:{ &quot;my_normalizer&quot;:{ &quot;type&quot;:&quot;custom&quot;, &quot;filter&quot;:[&quot;lowercase&quot;] } } } }, &quot;mappings&quot;: { &quot;properties&quot;: { &quot;author&quot;:{ &quot;type&quot;: &quot;keyword&quot;, &quot;normalizer&quot;:&quot;my_normalizer&quot; } } }} 在 settings 中定义 normalizer，然后在 mappings 中引用。 测试方式和前面一致。此时查询的时候，大写关键字也可以查询到小写文档，因为无论是索引还是查询，都会将大写转为小写。 boostboost 参数可以设置字段的权重。 boost 有两种使用思路，一种就是在定义 mappings 的时候使用，在指定字段类型时使用；另一种就是在查询时使用。 实际开发中建议使用后者，前者有问题：如果不重新索引文档，权重无法修改。 mapping 中使用 boost（不推荐）： 1234567891011PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;content&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;boost&quot;: 2 } } }} 另一种方式就是在查询的时候，指定 boost 1234567891011GET blog/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;content&quot;: { &quot;query&quot;: &quot;你好&quot;, &quot;boost&quot;: 2 } } }} coercecoerce 用来清除脏数据，默认为 true。 例如一个数字，在 JSON 中，用户可能写错了： 1{&quot;age&quot;:&quot;99&quot;} 或者 ： 1{&quot;age&quot;:&quot;99.0&quot;} 这些都不是正确的数字格式。 通过 coerce 可以解决该问题。 默认情况下，以下操作没问题，就是 coerce 起作用： 123456789101112131415PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;age&quot;:{ &quot;type&quot;: &quot;integer&quot; } } }}POST blog/_doc{ &quot;age&quot;:&quot;99.0&quot;} 如果需要修改 coerce ，方式如下： 12345678910111213141516PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;age&quot;:{ &quot;type&quot;: &quot;integer&quot;, &quot;coerce&quot;: false } } }}POST blog/_doc{ &quot;age&quot;:99} 当 coerce 修改为 false 之后，数字就只能是数字了，不可以是字符串，该字段传入字符串会报错。 copy_to这个属性，可以将多个字段的值，复制到同一个字段中。 定义方式如下： 123456789101112131415161718192021222324252627282930313233PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;copy_to&quot;: &quot;full_content&quot; }, &quot;content&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;copy_to&quot;: &quot;full_content&quot; }, &quot;full_content&quot;:{ &quot;type&quot;: &quot;text&quot; } } }}PUT blog/_doc/1{ &quot;title&quot;:&quot;你好江南一点雨&quot;, &quot;content&quot;:&quot;当 coerce 修改为 false 之后，数字就只能是数字了，不可以是字符串，该字段传入字符串会报错。&quot;}GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;full_content&quot;: &quot;当&quot; } }} doc_values 和 fielddataes 中的搜索主要是用到倒排索引，doc_values 参数是为了加快排序、聚合操作而生的。当建立倒排索引的时候，会额外增加列式存储映射。 doc_values 默认是开启的，如果确定某个字段不需要排序或者不需要聚合，那么可以关闭 doc_values。 大部分的字段在索引时都会生成 doc_values，除了 text。text 字段在查询时会生成一个 fielddata 的数据结构，fieldata 在字段首次被聚合、排序的时候生成。 doc_values 默认开启，fielddata 默认关闭。 doc_values 演示： 1234567891011121314151617181920212223242526272829303132333435PUT usersPUT users/_doc/1{ &quot;age&quot;:100}PUT users/_doc/2{ &quot;age&quot;:99}PUT users/_doc/3{ &quot;age&quot;:98}PUT users/_doc/4{ &quot;age&quot;:101}GET users/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;sort&quot;:[ { &quot;age&quot;:{ &quot;order&quot;: &quot;desc&quot; } } ]} 由于 doc_values 默认时开启的，所以可以直接使用该字段排序，如果想关闭 doc_values ，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445PUT users{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;age&quot;:{ &quot;type&quot;: &quot;integer&quot;, &quot;doc_values&quot;: false } } }}PUT users/_doc/1{ &quot;age&quot;:100}PUT users/_doc/2{ &quot;age&quot;:99}PUT users/_doc/3{ &quot;age&quot;:98}PUT users/_doc/4{ &quot;age&quot;:101}GET users/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;sort&quot;:[ { &quot;age&quot;:{ &quot;order&quot;: &quot;desc&quot; } } ]} dynamicenabledes 默认会索引所有的字段，但是有的字段可能只需要存储，不需要索引。此时可以通过 enabled 字段来控制： 123456789101112131415161718192021222324PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;enabled&quot;: false } } }}PUT blog/_doc/1{ &quot;title&quot;:&quot;javaboy&quot;}GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;title&quot;: &quot;javaboy&quot; } }} 设置了 enabled 为 false 之后，就可以再通过该字段进行搜索了。 format日期格式。format 可以规范日期格式，而且一次可以定义多个 format。 123456789101112131415161718192021PUT users{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;birthday&quot;:{ &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd||yyyy-MM-dd HH:mm:ss&quot; } } }}PUT users/_doc/1{ &quot;birthday&quot;:&quot;2020-11-11&quot;}PUT users/_doc/2{ &quot;birthday&quot;:&quot;2020-11-11 11:11:11&quot;} 多个日期格式之间，使用 || 符号连接，注意没有空格。 如果用户没有指定日期的 format，默认的日期格式是 strict_date_optional_time||epoch_mills 另外，所有的日期格式，可以在 https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html 网址查看。 ignore_aboveigbore_above 用于指定分词和索引的字符串最大长度，超过最大长度的话，该字段将不会被索引，这个字段只适用于 keyword 类型。 123456789101112131415161718192021222324252627282930PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 10 } } }}PUT blog/_doc/1{ &quot;title&quot;:&quot;javaboy&quot;}PUT blog/_doc/2{ &quot;title&quot;:&quot;javaboyjavaboyjavaboy&quot;}GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;title&quot;: &quot;javaboyjavaboyjavaboy&quot; } }} ignore_malformedignore_malformed 可以忽略不规则的数据，该参数默认为 false。 12345678910111213141516171819202122232425262728293031323334PUT users{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;birthday&quot;:{ &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd||yyyy-MM-dd HH:mm:ss&quot; }, &quot;age&quot;:{ &quot;type&quot;: &quot;integer&quot;, &quot;ignore_malformed&quot;: true } } }}PUT users/_doc/1{ &quot;birthday&quot;:&quot;2020-11-11&quot;, &quot;age&quot;:99}PUT users/_doc/2{ &quot;birthday&quot;:&quot;2020-11-11 11:11:11&quot;, &quot;age&quot;:&quot;abc&quot;}PUT users/_doc/2{ &quot;birthday&quot;:&quot;2020-11-11 11:11:11aaa&quot;, &quot;age&quot;:&quot;abc&quot;} include_in_all这个是针对 _all 字段的，但是在 es7 中，该字段已经被废弃了。 indexindex 属性指定一个字段是否被索引，该属性为 true 表示字段被索引，false 表示字段不被索引。 12345678910111213141516171819202122232425PUT users{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;age&quot;:{ &quot;type&quot;: &quot;integer&quot;, &quot;index&quot;: false } } }}PUT users/_doc/1{ &quot;age&quot;:99}GET users/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;age&quot;: 99 } }} 如果 index 为 false，则不能通过对应的字段搜索。 index_optionsindex_options 控制索引时哪些信息被存储到倒排索引中（用在 text 字段中），有四种取值： normsnorms 对字段评分有用，text 默认开启 norms，如果不是特别需要，不要开启 norms。 null_value在 es 中，值为 null 的字段不索引也不可以被搜索，null_value 可以让值为 null 的字段显式的可索引、可搜索： 1234567891011121314151617181920212223242526PUT users{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;keyword&quot;, &quot;null_value&quot;: &quot;javaboy_null&quot; } } }}PUT users/_doc/1{ &quot;name&quot;:null, &quot;age&quot;:99}GET users/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;javaboy_null&quot; } }} position_increment_gap被解析的 text 字段会将 term 的位置考虑进去，目的是为了支持近似查询和短语查询，当我们去索引一个含有多个值的 text 字段时，会在各个值之间添加一个假想的空间，将值隔开，这样就可以有效避免一些无意义的短语匹配，间隙大小通过 position_increment_gap 来控制，默认是 100。 1234567891011121314151617PUT usersPUT users/_doc/1{ &quot;name&quot;:[&quot;zhang san&quot;,&quot;li si&quot;]}GET users/_search{ &quot;query&quot;: { &quot;match_phrase&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;sanli&quot; } } }} sanli 搜索不到，因为两个短语之间有一个假想的空隙，为 100。 1234567891011GET users/_search{ &quot;query&quot;: { &quot;match_phrase&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;san li&quot;, &quot;slop&quot;: 101 } } }} 可以通过 slop 指定空隙大小。 也可以在定义索引的时候，指定空隙： 123456789101112131415161718192021222324252627PUT users{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;position_increment_gap&quot;: 0 } } }}PUT users/_doc/1{ &quot;name&quot;:[&quot;zhang san&quot;,&quot;li si&quot;]}GET users/_search{ &quot;query&quot;: { &quot;match_phrase&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;san li&quot; } } }} propertiessimilaritysimilarity 指定文档的评分模型，默认有三种： store默认情况下，字段会被索引，也可以搜索，但是不会存储，虽然不会被存储的，但是 _source 中有一个字段的备份。如果想将字段存储下来，可以通过配置 store 来实现。 term_vectorsterm_vectors 是通过分词器产生的信息，包括： 一组 terms 每个 term 的位置 term 的首字符/尾字符与原始字符串原点的偏移量 term_vectors 取值： fieldsfields 参数可以让同一字段有多种不同的索引方式。例如： 1234567891011121314151617181920212223242526272829PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: { &quot;raw&quot;:{ &quot;type&quot;:&quot;keyword&quot; } } } } }}PUT blog/_doc/1{ &quot;title&quot;:&quot;javaboy&quot;}GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;title.raw&quot;: &quot;javaboy&quot; } }} https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-params.html ElasticSearch 搜索入门搜索分为两个过程： 当向索引中保存文档时，默认情况下，es 会保存两份内容，一份是 _source 中的数据，另一份则是通过分词、排序等一系列过程生成的倒排索引文件，倒排索引中保存了词项和文档之间的对应关系。 搜索时，当 es 接收到用户的搜索请求之后，就会去倒排索引中查询，通过的倒排索引中维护的倒排记录表找到关键词对应的文档集合，然后对文档进行评分、排序、高亮等处理，处理完成后返回文档。 搜索数据导入 在江南一点雨微信公众号后台回复 bookdata.json 下载脚本。 创建索引： 1234567891011121314151617181920212223242526272829PUT books{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;publish&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;type&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;author&quot;:{ &quot;type&quot;: &quot;keyword&quot; }, &quot;info&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;price&quot;:{ &quot;type&quot;: &quot;double&quot; } } }} 在下载的json文件所在目录终端下执行如下脚本导入命令： 1curl -XPOST &quot;http://localhost:9200/books/_bulk?pretty&quot; -H &quot;content-type:application/json&quot; --data-binary @bookdata.json 简单搜索查询所有文档： 123456GET books/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }} 查询结果中hits 中就是查询结果，total 是符合查询条件的文档数。 简单搜索可以简写为： 1GET books/_search 简单搜索默认查询 10 条记录。 词项查询term 查询，就是根据词去查询，查询指定字段中包含给定单词的文档，term 查询不被解析，只有搜索的词和文档中的词精确匹配，才会返回文档。应用场景如：人名、地名等等。 查询 name 字段中包含 材料 的文档。 12345678GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;材料&quot; } }} 分页查询默认返回前 10 条数据，es 中也可以像关系型数据库一样，给一个分页参数： 1234567891011GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;十一五&quot; } }, &quot;size&quot;: 5, &quot;from&quot;: 2 //从第几个查询结果开始展示size个结果} 过滤返回字段如果返回的字段比较多，又不需要这么多字段，此时可以指定返回的字段： 1234567891011GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;十一五&quot; } }, &quot;size&quot;: 10, &quot;from&quot;: 10, &quot;_source&quot;: [&quot;name&quot;,&quot;author&quot;] //返回的字段就只有 name 和 author 了。} 最小评分有的文档得分特别低，说明这个文档和我们查询的关键字相关度很低。我们可以设置一个最低分，只有得分超过最低分的文档才会被返回。 12345678910GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;十一五&quot; } }, &quot;_source&quot;: [&quot;name&quot;, &quot;author&quot;], &quot;min_score&quot;:1.75 //得分低于 1.75 的文档将直接被舍弃。} 高亮查询关键字高亮： 123456789101112131415GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;十一五&quot; } }, &quot;min_score&quot;:1.75, &quot;_source&quot;: [&quot;name&quot;,&quot;author&quot;], &quot;highlight&quot;: { &quot;fields&quot;: { &quot;name&quot;: {} } }} ElasticSearch 全文查询match querymatch query 会对查询语句进行分词，分词后，如果查询语句中的任何一个词项被匹配，则文档就会被索引到。 12345678GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;美术计算机&quot; } }} 这个查询首先会对 美术计算机 进行分词，分词之后，再去查询，只要文档中包含一个分词结果，就回返回文档。换句话说，默认词项之间是 OR 的关系，如果想要修改，也可以改为 AND。 1234567891011GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;美术计算机&quot;, &quot;operator&quot;: &quot;and&quot; } } }} 此时就回要求文档中必须同时包含 美术 和 计算机 两个词。 match_phrase querymatch_phrase query 也会对查询的关键字进行分词，但是它分词后有两个特点： 分词后的词项顺序必须和文档中词项的顺序一致 所有的词都必须出现在文档中 示例如下： 1234567891011GET books/_search{ &quot;query&quot;: { &quot;match_phrase&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;十一五计算机&quot;, &quot;slop&quot;: 7 } } }} query 是查询的关键字，会被分词器进行分解，分解之后去倒排索引中进行匹配。 slop 是指关键字之间的最小距离，但是注意不是关键之间间隔的字数。文档中的字段被分词器解析之后，解析出来的词项都包含一个 position 字段表示词项的位置，查询短语分词之后 的 position 之间的间隔要满足 slop 的要求。 match_phrase_prefix query这个类似于 match_phrase query，只不过这里多了一个通配符，match_phrase_prefix 支持最后一个词项的前缀匹配，但是由于这种匹配方式效率较低，因此大家作为了解即可。 12345678GET books/_search{ &quot;query&quot;: { &quot;match_phrase_prefix&quot;: { &quot;name&quot;: &quot;计&quot; } }} 这个查询过程，会自动进行单词匹配，会自动查找以计开始的单词，默认是 50 个，可以自己控制： 1234567891011GET books/_search{ &quot;query&quot;: { &quot;match_phrase_prefix&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;计&quot;, &quot;max_expansions&quot;: 3 } } }} match_phrase_prefix 是针对分片级别的查询，假设 max_expansions 为 1，可能返回多个文档，但是只有一个词，这是我们预期的结果。有的时候实际返回结果和我们预期结果并不一致，原因在于这个查询是分片级别的，不同的分片确实只返回了一个词，但是结果可能来自不同的分片，所以最终会看到多个词。 multi_match querymatch 查询的升级版，可以指定多个查询域： 123456789GET books/_search{ &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;java&quot;, &quot;fields&quot;: [&quot;name&quot;,&quot;info&quot;] } }} 这种查询方式还可以指定字段的权重： 123456789GET books/_search{ &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;阳光&quot;, &quot;fields&quot;: [&quot;name^4&quot;,&quot;info&quot;] } }} 这个表示关键字出现在 name 中的权重是出现在 info 中权重的 4 倍。 query_string queryquery_string 是一种紧密结合 Lucene 的查询方式，在一个查询语句中可以用到 Lucene 的一些查询语法： 123456789GET books/_search{ &quot;query&quot;: { &quot;query_string&quot;: { &quot;default_field&quot;: &quot;name&quot;, &quot;query&quot;: &quot;(十一五) AND (计算机)&quot; } }} simple_query_string这个是 query_string 的升级，可以直接使用 +、|、- 代替 AND、OR、NOT 等。 123456789GET books/_search{ &quot;query&quot;: { &quot;simple_query_string&quot;: { &quot;fields&quot;: [&quot;name&quot;], &quot;query&quot;: &quot;(十一五) + (计算机)&quot; } }} 查询结果和 query_string。 term query词项查询。词项查询不会分析查询字符，直接拿查询字符去倒排索引中比对。 12345678GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;程序设计&quot; } }} terms query词项查询，但是可以给多个关键词。 12345678GET books/_search{ &quot;query&quot;: { &quot;terms&quot;: { &quot;name&quot;: [&quot;程序&quot;,&quot;设计&quot;,&quot;java&quot;] } }} range query范围查询，可以按照日期范围、数字范围等查询。 range query 中的参数主要有四个： gt lt gte lte 案例： 123456789101112131415161718GET books/_search{ &quot;query&quot;: { &quot;range&quot;: { &quot;price&quot;: { &quot;gte&quot;: 10, &quot;lt&quot;: 20 } } }, &quot;sort&quot;: [ { &quot;price&quot;: { &quot;order&quot;: &quot;desc&quot; } } ]} exists queryexists query 会返回指定字段中至少有一个非空值的文档： 12345678GET books/_search{ &quot;query&quot;: { &quot;exists&quot;: { &quot;field&quot;: &quot;javaboy&quot; } }} 注意，空字符串也是有值。null 是空值。 prefix query前缀查询，效率略低，除非必要，一般不太建议使用。 给定关键词的前缀去查询： 12345678910GET books/_search{ &quot;query&quot;: { &quot;prefix&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;大学&quot; } } }} wildcard querywildcard query 即通配符查询。支持单字符和多字符通配符： ？表示一个任意字符。 * 表示零个或者多个字符。 查询所有姓张的作者的书： 12345678910GET books/_search{ &quot;query&quot;: { &quot;wildcard&quot;: { &quot;author&quot;: { &quot;value&quot;: &quot;张*&quot; } } }} 查询所有姓张并且名字只有两个字的作者的书： 12345678910GET books/_search{ &quot;query&quot;: { &quot;wildcard&quot;: { &quot;author&quot;: { &quot;value&quot;: &quot;张?&quot; } } }} regexp query支持正则表达式查询。 查询所有姓张并且名字只有两个字的作者的书： 12345678GET books/_search{ &quot;query&quot;: { &quot;regexp&quot;: { &quot;author&quot;: &quot;张.&quot; } }} fuzzy query在实际搜索中，有时我们可能会打错字，从而导致搜索不到，在 match query 中，可以通过 fuzziness 属性实现模糊查询。 fuzzy query 返回与搜索关键字相似的文档。怎么样就算相似？以LevenShtein 编辑距离为准。编辑距离是指将一个字符变为另一个字符所需要更改字符的次数，更改主要包括四种： 更改字符（javb–〉java） 删除字符（javva–〉java） 插入字符（jaa–〉java） 转置字符（ajva–〉java） 为了找到相似的词，模糊查询会在指定的编辑距离中创建搜索关键词的所有可能变化或者扩展的集合，然后进行搜索匹配。 12345678GET books/_search{ &quot;query&quot;: { &quot;fuzzy&quot;: { &quot;name&quot;: &quot;javba&quot; } }} ids query根据指定的 id 查询。 12345678GET books/_search{ &quot;query&quot;: { &quot;ids&quot;:{ &quot;values&quot;: [1,2,3] } }} 复合查询constant_score query当我们不关心检索词项的频率（TF）对搜索结果排序的影响时，可以使用 constant_score 将查询语句或者过滤语句包裹起来。 12345678910111213GET books/_search{ &quot;query&quot;: { &quot;constant_score&quot;: { &quot;filter&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;java&quot; } }, &quot;boost&quot;: 1.5 } }} bool querybool query 可以将任意多个简单查询组装在一起，有四个关键字可供选择，四个关键字所描述的条件可以有一个或者多个。 must：文档必须匹配 must 选项下的查询条件。 should：文档可以匹配 should 下的查询条件，也可以不匹配。 must_not：文档必须不满足 must_not 选项下的查询条件。 filter：类似于 must，但是 filter 不评分，只是过滤数据。 例如查询 name 属性中必须包含 java，同时书价不在 [0,35] 区间内，info 属性可以包含 程序设计 也可以不包含程序设计： 123456789101112131415161718192021222324252627282930313233GET books/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;java&quot; } } } ], &quot;must_not&quot;: [ { &quot;range&quot;: { &quot;price&quot;: { &quot;gte&quot;: 0, &quot;lte&quot;: 35 } } } ], &quot;should&quot;: [ { &quot;match&quot;: { &quot;info&quot;: &quot;程序设计&quot; } } ] } }} 这里还涉及到一个关键字，minmum_should_match 参数。 minmum_should_match 参数在 es 官网上称作最小匹配度。在之前学习的 multi_match 或者这里的 should 查询中，都可以设置 minmum_should_match 参数。 假设我们要做一次查询，查询 name 中包含 语言程序设计 关键字的文档： 12345678GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;语言程序设计&quot; } }} 在这个查询过程中，首先会进行分词，分词结果为：语言、程序设计、程序、设计 分词后的 term 会构造成一个 should 的 bool query，每一个 term 都会变成一个 term query 的子句。换句话说，上面的查询和下面的查询等价： 12345678910111213141516171819202122232425262728293031323334353637GET books/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;语言&quot; } } }, { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;程序设计&quot; } } }, { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;程序&quot; } } }, { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;设计&quot; } } } ] } }} 在这两个查询语句中，都是文档只需要包含词项中的任意一项即可，文档就回被返回，在 match 查询中，可以通过 operator 参数设置文档必须匹配所有词项。 如果想匹配一部分词项，就涉及到一个参数，就是 minmum_should_match，即最小匹配度。即至少匹配多少个词。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;语言程序设计&quot;, &quot;operator&quot;: &quot;and&quot; } } }}GET books/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;语言&quot; } } }, { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;程序设计&quot; } } }, { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;程序&quot; } } }, { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;设计&quot; } } } ], &quot;minimum_should_match&quot;: &quot;50%&quot; } }, &quot;from&quot;: 0, &quot;size&quot;: 70} 50% 表示词项个数的 50%。 如下两个查询等价（参数 4 是因为查询关键字分词后有 4 项）： 12345678910111213141516171819202122GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;语言程序设计&quot;, &quot;minimum_should_match&quot;: 4 } } }}GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;语言程序设计&quot;, &quot;operator&quot;: &quot;and&quot; } } }} dis_max query假设现在有两本书： 123456789101112131415161718192021222324252627PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;content&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; } } }}POST blog/_doc{ &quot;title&quot;:&quot;如何通过Java代码调用ElasticSearch&quot;, &quot;content&quot;:&quot;松哥力荐，这是一篇很好的解决方案&quot;}POST blog/_doc{ &quot;title&quot;:&quot;初识 MongoDB&quot;, &quot;content&quot;:&quot;简单介绍一下 MongoDB，以及如何通过 Java 调用 MongoDB，MongoDB 是一个不错 NoSQL 解决方案&quot;} 现在假设搜索 Java解决方案 关键字，但是不确定关键字是在 title 还是在 content，所以两者都搜索： 12345678910111213141516171819GET blog/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ { &quot;match&quot;: { &quot;title&quot;: &quot;java解决方案&quot; } }, { &quot;match&quot;: { &quot;content&quot;: &quot;java解决方案&quot; } } ] } }} 搜索结果如下： 12345678910111213141516171819202122232425262728293031323334353637383940{ &quot;took&quot; : 3, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : { &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; }, &quot;max_score&quot; : 1.1972204, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;YnERZXoBw3hORU3ogqsC&quot;, &quot;_score&quot; : 1.1972204, &quot;_source&quot; : { &quot;title&quot; : &quot;如何通过Java代码调用ElasticSearch&quot;, &quot;content&quot; : &quot;松哥力荐，这是一篇很好的解决方案&quot; } }, { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;Y3ERZXoBw3hORU3ogqsW&quot;, &quot;_score&quot; : 1.1069256, &quot;_source&quot; : { &quot;title&quot; : &quot;初识 MongoDB&quot;, &quot;content&quot; : &quot;简单介绍一下 MongoDB，以及如何通过 Java 调用 MongoDB，MongoDB 是一个不错 NoSQL 解决方案&quot; } } ] }} 肉眼观察，感觉第二个和查询关键字相似度更高，但是实际查询结果并非这样。 要理解这个原因，我们需要来看下 should query 中的评分策略： 首先会执行 should 中的两个查询 对两个查询结果的评分求和 对求和结果乘以匹配语句总数 在对第三步的结果除以所有语句总数 反映到具体的查询中： 前者 title 中 包含 java，假设评分是 1.1 content 中包含解决方案，假设评分是 1.2 有得分的 query 数量，这里是 2 总的 query 数量也是 2 最终结果：（1.1+1.2）*2/2=2.3 后者 title 中 不包含查询关键字，没有得分 content 中包含解决方案和 java，假设评分是 2 有得分的 query 数量，这里是 1 总的 query 数量也是 2 最终结果：2*1/2=1 在这种查询中，title 和 content 相当于是相互竞争的关系，所以我们需要找到一个最佳匹配字段。 为了解决这一问题，就需要用到 dis_max query（disjunction max query，分离最大化查询）：匹配的文档依然返回，但是只将最佳匹配的评分作为查询的评分。 12345678910111213141516171819GET blog/_search{ &quot;query&quot;: { &quot;dis_max&quot;: { &quot;queries&quot;: [ { &quot;match&quot;: { &quot;title&quot;: &quot;java解决方案&quot; } }, { &quot;match&quot;: { &quot;content&quot;: &quot;java解决方案&quot; } } ] } }} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839{ &quot;took&quot; : 7, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : { &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; }, &quot;max_score&quot; : 1.1069256, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;Y3ERZXoBw3hORU3ogqsW&quot;, &quot;_score&quot; : 1.1069256, &quot;_source&quot; : { &quot;title&quot; : &quot;初识 MongoDB&quot;, &quot;content&quot; : &quot;简单介绍一下 MongoDB，以及如何通过 Java 调用 MongoDB，MongoDB 是一个不错 NoSQL 解决方案&quot; } }, { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;YnERZXoBw3hORU3ogqsC&quot;, &quot;_score&quot; : 0.62177753, &quot;_source&quot; : { &quot;title&quot; : &quot;如何通过Java代码调用ElasticSearch&quot;, &quot;content&quot; : &quot;松哥力荐，这是一篇很好的解决方案&quot; } } ] }} 在 dis_max query 中，还有一个参数 tie_breaker（取值在0～1），在 dis_max query 中，是完全不考虑其他 query 的分数，只是将最佳匹配的字段的评分返回。但是，有的时候，我们又不得不考虑一下其他 query 的分数，此时，可以通过 tie_breaker 来优化 dis_max query。tie_breaker 会将其他 query 的分数，乘以 tie_breaker，然后和分数最高的 query 进行一个综合计算。 function_score query场景：例如想要搜索附近的肯德基，搜索的关键字是肯德基，但是我希望能够将评分较高的肯德基优先展示出来。但是默认的评分策略是没有办法考虑到餐厅评分的，他只是考虑相关性，这个时候可以通过 function_score query 来实现。 准备两条测试数据： 1234567891011121314151617181920212223242526PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;votes&quot;:{ &quot;type&quot;: &quot;integer&quot; } } }}PUT blog/_doc/1{ &quot;title&quot;:&quot;Java集合详解&quot;, &quot;votes&quot;:100}PUT blog/_doc/2{ &quot;title&quot;:&quot;Java多线程详解，Java锁详解&quot;, &quot;votes&quot;:10} 现在搜索标题中包含 java 关键字的文档： 12345678GET blog/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839{ &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : { &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; }, &quot;max_score&quot; : 0.22534126, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.22534126, &quot;_source&quot; : { &quot;title&quot; : &quot;Java多线程详解，Java锁详解&quot;, &quot;votes&quot; : 10 } }, { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.21799318, &quot;_source&quot; : { &quot;title&quot; : &quot;Java集合详解&quot;, &quot;votes&quot; : 100 } } ] }} 默认情况下，id 为 2 的记录得分较高，因为他的 title 中包含两个 java。 如果我们在查询中，希望能够充分考虑 votes 字段，将 votes 较高的文档优先展示，就可以通过 function_score 来实现。 具体的思路，就是在旧的得分基础上，根据 votes 的数值进行综合运算，重新得出一个新的评分。 具体有几种不同的计算方式： weight random_score script_score field_value_factor weight weight 可以对评分设置权重，就是在旧的评分基础上乘以 weight，他其实无法解决我们上面所说的问题。具体用法如下： 1234567891011121314151617GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;weight&quot;: 10 } ] } }} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839{ &quot;took&quot; : 8, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : { &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; }, &quot;max_score&quot; : 2.2534127, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 2.2534127, &quot;_source&quot; : { &quot;title&quot; : &quot;Java多线程详解，Java锁详解&quot;, &quot;votes&quot; : 10 } }, { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 2.1799319, &quot;_source&quot; : { &quot;title&quot; : &quot;Java集合详解&quot;, &quot;votes&quot; : 100 } } ] }} 可以看到，此时的评分，在之前的评分基础上*10 random_score random_score 会根据 uid 字段进行 hash 运算，生成分数，使用 random_score 时可以配置一个种子，如果不配置，默认使用当前时间。 1234567891011121314151617GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;random_score&quot;: {} } ] } }} script_score 自定义评分脚本。假设每个文档的最终得分是旧的分数加上votes。查询方式如下： 12345678910111213141516171819202122GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;script_score&quot;: { &quot;script&quot;: { &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;_score + doc['votes'].value&quot; } } } ] } }} 现在，最终得分是 (oldScore+votes)*oldScore。 如果不想乘以 oldScore，查询方式如下： 1234567891011121314151617181920212223GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;script_score&quot;: { &quot;script&quot;: { &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;_score + doc['votes'].value&quot; } } } ], &quot;boost_mode&quot;: &quot;replace&quot; } }} 通过 boost_mode 参数，可以设置最终的计算方式。该参数还有其他取值： multiply：分数相乘 sum：分数相加 avg：求平均数 max：最大分 min：最小分 replace：不进行二次计算 field_value_factor 这个的功能类似于 script_score，但是不用自己写脚本。 假设每个文档的最终得分是旧的分数乘以votes。查询方式如下： 12345678910111213141516171819GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;field_value_factor&quot;: { &quot;field&quot;: &quot;votes&quot; } } ] } }} 默认的得分就是oldScore*votes。 还可以利用 es 内置的函数进行一些更复杂的运算： 123456789101112131415161718192021GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;field_value_factor&quot;: { &quot;field&quot;: &quot;votes&quot;, &quot;modifier&quot;: &quot;sqrt&quot; } } ], &quot;boost_mode&quot;: &quot;replace&quot; } }} 此时，最终的得分是（sqrt(votes)）。 modifier 中可以设置内置函数，其他的内置函数还有： 另外还有个参数 factor ，影响因子。字段值先乘以影响因子，然后再进行计算。以 sqrt 为例，计算方式为 sqrt(factor*votes)： 12345678910111213141516171819202122GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;field_value_factor&quot;: { &quot;field&quot;: &quot;votes&quot;, &quot;modifier&quot;: &quot;sqrt&quot;, &quot;factor&quot;: 10 } } ], &quot;boost_mode&quot;: &quot;replace&quot; } }} 还有一个参数 max_boost，控制计算结果的范围： 123456789101112131415161718192021GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;field_value_factor&quot;: { &quot;field&quot;: &quot;votes&quot; } } ], &quot;boost_mode&quot;: &quot;sum&quot;, &quot;max_boost&quot;: 100 } }} max_boost 参数表示 functions 模块中，最终的计算结果上限。如果超过上限，就按照上线计算。 boosting queryboosting query 中包含三部分： positive：得分不变 negative：降低得分 negative_boost：降低的权重 123456789101112131415161718GET books/_search{ &quot;query&quot;: { &quot;boosting&quot;: { &quot;positive&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;java&quot; } }, &quot;negative&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;2008&quot; } }, &quot;negative_boost&quot;: 0.5 } }} 查询结果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647{ &quot;took&quot; : 15, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : { &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; }, &quot;max_score&quot; : 4.5299835, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;books&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;549&quot;, &quot;_score&quot; : 4.5299835, &quot;_source&quot; : { &quot;name&quot; : &quot;全国计算机等级考试笔试＋上机全真模拟：二级Java语言程序设计（最新版）&quot;, &quot;publish&quot; : &quot;高等教育出版社&quot;, &quot;type&quot; : &quot;考试认证&quot;, &quot;author&quot; : &quot;&quot;, &quot;info&quot; : &quot;为了更好地服务于考生，引导考生尽快掌握考试大纲中要求的知识点和技能，顺利通过计算机等级考试，根据最新的考试大纲，高等教育出版社组织长期从事计算机等级考试命题研究和培训工作的专家编写了这套“笔试+上机考试全真模拟”，全面模拟考试真题，让考生在做题的同时全面巩固复习考点，提前熟悉考试环境，在短时间内冲刺过关。本书内容包括20套笔试模拟题和20套上机模拟题，还给出了参考答案和解析，尤其适合参加计算机等级考试的考生考前实战演练。&quot;, &quot;price&quot; : 30 } }, { &quot;_index&quot; : &quot;books&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;86&quot;, &quot;_score&quot; : 2.5018067, &quot;_source&quot; : { &quot;name&quot; : &quot;全国计算机等级考试2级教程：Java语言程序设计（2008年版）&quot;, &quot;publish&quot; : &quot;高等教育出版社&quot;, &quot;type&quot; : &quot;计算机考试&quot;, &quot;author&quot; : &quot;&quot;, &quot;info&quot; : &quot;由国家教育部考试中心推出的计算机等级考试是一种客观、公正、科学的专门测试计算机应用人员的计算机知识与技能的全国性考试，它面向社会，服务于社会。本书在教育部考试中心组织下、在全国计算机等级考试委员会指导下，由有关专家执笔编写而成。本书按照《全国计算机等级考试二级Java语言程序设计考试大纲（2007年版）》的要求编写，内容包括：Java体系结构、基本数据类型、流程控制语句、类、数组和字符串操作、输入输出及文件操作、图形用户界面编写、线程和串行化技术、A程序设计以及应用开发工具和安装使用等。本书是参加全国计算机等级考试二级Java语言程序设计的考生的良师益友，是教育部考试中心指定教材，也可作为欲学习Java编程的读者的参考书。&quot;, &quot;price&quot; : 37 } } ] }}","link":"/2021/07/01/ElasticSearch%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89%E6%98%A0%E5%B0%84%E5%AD%97%E6%AE%B5%E5%92%8C%E6%9F%A5%E8%AF%A2/"},{"title":"JVM（二）垃圾回收和内存分配","text":"Java技术体系中所提倡的 自动内存管理 最终可以归结为自动化地解决了两个问题：给对象分配内存 以及 回收分配给对象的内存，而且这两个问题针对的内存区域就是Java内存模型中的 堆区 垃圾回收在Java的内存区域中： 程序计数器、虚拟机栈、本地方法栈3个区域随线程而生，随线程而灭，栈中的栈帧随着方法的进入和退出而有条不紊地执行着出栈和入栈操作，所以这几个区域的内存回收是确定的，随着方法结束或者线程结束，内存自然回收。 Java堆和方法区这两个区域则有着很显著的不确定性：一个接口的多个实现类需要的内存可能会不一样，一个方法所执行的不同条件分支所需要的内存也可能不一样，只有处于运行期间，我们才能知道程序究竟会创建哪些对象，创建多少个对象，这部分内存的分配和回收是动态的。垃圾收集器所关注的正是这部分内存该如何管理 如何判断是否是垃圾 引用计数法：在对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加一；当引用失效时，计数器值就减一；任何时刻计数器为零的对象就是不可能再被使用的。 需要额外的空间来存储计数器，以及繁琐的更新操作，引用计数法还有一个重大的漏洞，那便是无法处理循环引用对象。 12345678910111213141516public static void main(String[] args) { Person father = new Person(); Person son = new Person(); father.setSon(son); son.setFather(father); father = null; son = null; /** * 调用此方法表示希望进行一次垃圾回收。但是它不能保证垃圾回收一定会进行， * 而且具体什么时候进行是取决于具体的虚拟机的，不同的虚拟机有不同的对策。 */ System.gc();} 可达性分析算法:通过一系列名为“GC Roots” 的对象作为终点，当一个对象到GC Roots 之间无法通过引用到达时，便可以进行回收了。 在Java技术体系里面，固定可作为GC Roots的对象包括以下几种： 在虚拟机栈（栈帧中的本地变量表）中引用的对象在方法区中类静态属性引用的对象，譬如Java类的引用类型静态变量。在方法区中常量引用的对象，譬如字符串常量池（String Table）里的引用。在本地方法栈中JNI（即通常所说的Native方法）引用的对象。Java虚拟机内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象（比如 NullPointExcepiton、OutOfMemoryError）等，还有系统类加载器。所有被同步锁（synchronized关键字）持有的对象。反映Java虚拟机内部情况的JMXBean、JVMTI中注册的回调、本地代码缓存等。 Java中的引用分类Java中的引用有四种，分为强引用（Strongly Reference）、软引用（Soft Reference）、弱引用（Weak Reference）和虚引用（Phantom Reference）4种，这4种引用强度依次逐渐减弱。 强引用是最传统的“引用”的定义，是指在程序代码之中普遍存在的引用赋值，无论任何情况下，只要强引用关系还存在，垃圾收集器就永远不会回 收掉被引用的对象。 1Object obj =new Object(); 软引用是用来描述一些还有用，但非必须的对象。只被软引用关联着的对象，在系统将要发生内存溢出异常前，会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存， 才会抛出内存溢出异常。在JDK 1.2版之后提供了SoftReference类来实现软引用。 12345Object obj = new Object();ReferenceQueue queue = new ReferenceQueue();SoftReference reference = new SoftReference(obj, queue);//强引用对象滞空，保留软引用obj = null; 弱引用也是用来描述那些非必须对象，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生为止。当垃圾收集器开始工作，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。在JDK 1.2版之后提供了WeakReference类来实现弱引用。 12345Object obj = new Object();ReferenceQueue queue = new ReferenceQueue();WeakReference reference = new WeakReference(obj, queue);//强引用对象滞空，保留软引用obj = null; 虚引用也称为“幽灵引用”或者“幻影引用”，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的只是为了能在这个对象被收集器回收时收到一个系统通知。在JDK 1.2版之后提供了PhantomReference类来实现虚引用。 12345Object obj = new Object();ReferenceQueue queue = new ReferenceQueue();PhantomReference reference = new PhantomReference(obj, queue);//强引用对象滞空，保留软引用obj = null; 分代收集理论它建立在两个分代假说之上： 弱分代假说（Weak Generational Hypothesis）：绝大多数对象都是朝生夕灭的。 强分代假说（Strong Generational Hypothesis）：熬过越多次垃圾收集过程的对象就越难以消亡。 就是把Java堆划分为**新生代 （Young Generation）**和**老年代（Old Generation）两个区域**，新生代存放存活时间短的对象，而每次回收后存活的少量对象，将会逐步晋升到老年代中存放。 堆有新生代和老年代两块区域组成，而新生代区域又分为三个部分，分别是 Eden,From Surivor,To Survivor ,比例是8:1:1。 新生代采用复制算法，每次使用一块Eden区和一块Survivor区，当进行垃圾回收时，将Eden和一块Survivor区域的所有存活对象复制到另一块Survivor区域，然后清理到刚存放对象的区域，依次循环。 老年代采用标记-清除或者标记-整理算法，根据使用的垃圾回收器来进行判断。 基于分代，产生了一些垃圾收集的类型划分： 部分收集（Partial GC）：指目标不是完整收集整个Java堆的垃圾收集，其中又分为： 新生代收集（Minor GC/Young GC）：指目标只是新生代的垃圾收集。 老年代收集（Major GC/Old GC）：指目标只是老年代的垃圾收集。目前只有CMS收集器会有单独收集老年代的行为。 混合收集（Mixed GC）：指目标是收集整个新生代以及部分老年代的垃圾收集。目前只有G1收集器会有这种行为。 整堆收集（Full GC）：收集整个Java堆和方法区的垃圾收集。 垃圾回收算法标记清除标记-清除（Mark-Sweep）算法分为两个阶段： 标记 : 标记出所有需要回收的对象 清除：回收所有被标记的对象 标记-清除算法比较基础，但是主要存在两个缺点： 执行效率不稳定，如果Java堆中包含大量对象，而且其中大部分是需要被回收的，这时必须进行大量标记和清除的动作，导致标记和清除两个过程的执行效率都随对象数量增长而降低。 内存空间的碎片化问题，标记、清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 标记-清除算法主要用于老年代，因为老年代可回收的对象比较少。 复制标记-复制算法解决了标记-清除算法面对大量可回收对象时执行效率低的问题。 过程也比较简单：将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 这种算法存在一个明显的缺点：一部分空间没有使用，存在空间的浪费。 新生代垃圾收集主要采用这种算法，因为新生代的存活对象比较少，每次复制的只是少量的存活对象。 一般虚拟机的具体实现不会采用1:1的比例划分，以HotSpot为例，HotSpot虚拟机将内存分为一块较大的Eden空间和两块较小的 Survivor空间，每次分配内存只使用Eden和其中一块Survivor。发生垃圾搜集时，将Eden和Survivor中仍然存活的对象一次性复制到另外一块Survivor空间上，然后直接清理掉Eden和已用过的那块Survivor空间。默认Eden和Survivor的大小比例是8∶1。 标记整理为了降低内存的消耗，引入一种针对性的算法：标记-整理（Mark-Compact）算法。 其中的标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存。 标记-整理算法主要用于老年代，在老年代这种大量对象存活的区域，移动对象是个很大的负担，而且这种对象移动操作必须全程暂停用户应用程序（Stop The World）才能进行。 垃圾处理器盘点 Serial收集器它是一个单线程工作的收集器，使用一个处理器或一条收集线程去完成垃圾收集工作。并且进行垃圾收集时，必须暂停其他所有工作线程，直到垃圾收集结束——这就是所谓的“Stop The World”。 Serial/Serial Old收集器的运行过程如图： ParNew收集器ParNew收集器实质上是Serial收集器的多线程并行版本，使用多条线程进行垃圾收集。 ParNew收集器的工作过程如图所示： Parallel Scavenge收集器Parallel Scavenge收集器是一款新生代收集器，基于标记-复制算法实现，也能够并行收集。和ParNew有些类似，但Parallel Scavenge主要关注的是垃圾收集的吞吐量。 所谓吞吐量指的是运行用户代码的时间与处理器总消耗时间的比值。这个比例越高，证明垃圾收集占整个程序运行的比例越小。 Parallel Scavenge收集器提供了两个参数用于精确控制吞吐量: -XX：MaxGCPauseMillis，最大垃圾回收停顿时间。这个参数的原理是空间换时间，收集器会控制新生代的区域大小，从而尽可能保证回收少于这个最大停顿时间。简单的说就是回收的区域越小，那么耗费的时间也越小。 所以这个参数并不是设置得越小越好。设太小的话，新生代空间会太小，从而更频繁的触发GC。 -XX：GCTimeRatio，垃圾收集时间与总时间占比。这个是吞吐量的倒数，原理和MaxGCPauseMillis相同。 由于与吞吐量关系密切，Parallel Scavenge收集器也经常被称作“吞吐量优先收集器”。 Serial Old收集器Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法。 Parallel Old收集器Parallel Old是Parallel Scavenge收集器的老年代版本，支持多线程并发收集，基于标记-整理算法实现。 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器，同样是老年代的收集齐，采用标记-清除算法。 垃圾收集器： 初始标记（CMS initial mark）：单线程运行，需要Stop The World，标记GC Roots能直达的对象。 并发标记（（CMS concurrent mark）：无停顿，和用户线程同时运行，从GC Roots直达对象开始遍历整个对象图。 重新标记（CMS remark）：多线程运行，需要Stop The World，标记并发标记阶段产生对象。 并发清除（CMS concurrent sweep）：无停顿，和用户线程同时运行，清理掉标记阶段标记的死亡的对象。 Concurrent Mark Sweep收集器运行示意图如下： 优点：CMS最主要的优点在名字上已经体现出来——并发收集、低停顿。 缺点：CMS同样有三个明显的缺点。 Mark Sweep算法会导致内存碎片比较多 CMS的并发能力比较依赖于CPU资源，并发回收时垃圾收集线程可能会抢占用户线程的资源，导致用户程序性能下降。 并发清除阶段，用户线程依然在运行，会产生所谓的“浮动垃圾”（Floating Garbage），本次垃圾收集无法处理浮动垃圾，必须到下一次垃圾收集才能处理。如果浮动垃圾太多，会触发新的垃圾回收，导致性能降低。 Garbage First收集器G1把连续的Java堆划分为多个大小相等的独立区域（Region），每一个Region都可以根据需要，扮演新生代的Eden空间、Survivor空间，或者老年代空间。收集器能够对扮演不同角色的Region采用不同的策略去处理。 这样就避免了收集整个堆，而是按照若干个Region集进行收集，同时维护一个优先级列表，跟踪各个Region回收的价值，优先收集价值高的Region。 G1收集器的运行过程大致可划分为以下四个步骤： 初始标记（initial mark），标记了从GC Root开始直接关联可达的对象。STW（Stop the World）执行。 并发标记（concurrent marking），和用户线程并发执行，从GC Root开始对堆中对象进行可达性分析，递归扫描整个堆里的对象图，找出要回收的对象、 最终标记（Remark），STW，标记再并发标记过程中产生的垃圾。 筛选回收（Live Data Counting And Evacuation），制定回收计划，选择多个Region 构成回收集，把回收集中Region的存活对象复制到空的Region中，再清理掉整个旧 Region的全部空间。需要STW。 相比CMS，G1的优点有很多，可以指定最大停顿时间、分Region的内存布局、按收益动态确定回收集。 只从内存的角度来看，与CMS的“标记-清除”算法不同，G1从整体来看是基于“标记-整理”算法实现的收集器，但从局部（两个Region 之间）上看又是基于“标记-复制”算法实现，无论如何，这两种算法都意味着G1运作期间不会产生内存空间碎片，垃圾收集完成之后能提供规整的可用内存。 内存分配原则MinorGC/MajorGC/FullGC的区别 ①、Minor GC 也叫Young GC，指的是新生代 GC，发生在新生代（Eden区和Survivor区）的垃圾回收。因为Java对象大多是朝生夕死的，所以 Minor GC 通常很频繁，一般回收速度也很快。 ②、Major GC 也叫Old GC，指的是老年代的 GC，发生在老年代的垃圾回收，该区域的对象存活时间比较长，通常来讲，发生 Major GC时，会伴随着一次 Minor GC，而 Major GC 的速度一般会比 Minor GC 慢10倍。 ③、Full GC 指的是全区域（整个堆）的垃圾回收，通常来说和 Major GC 是等价的。 内存溢出和内存泄漏的区别 内存溢出（Out Of Memory） ：就是申请内存时，JVM没有足够的内存空间。通俗说法就是去蹲坑发现坑位满了。 内存泄露 （Memory Leak）：就是申请了内存，但是没有释放，导致内存空间浪费。通俗说法就是有人占着茅坑不拉屎。 内存分配的5个策略 对象优先在Eden区分配，当 Eden 区没有足够的空间进行分配时，虚拟机将会发起一次 Minor GC(新生代GC)。 大对象直接分配在老年代，比较典型的就是那种很长的字符串以及数组。 长期存活的对象将进入老年代，新生代对象每熬过一次 Minor GC，年龄就增加1，当它的年龄增加到一定阈值时（默认是15岁），就会被晋升到老年代中。 新生代Survivor 区相同年龄所有对象之和大于 Survivor 所有对象之和的一半，大于等于该年龄的对象进入老年代 空间分配担保原则： 新生代内存分为一块 Eden区，和两块 Survivor 区域，当发生一次 Minor GC时，虚拟机会将Eden和一块Survivor区域的所有存活对象复制到另一块Survivor区域，通常情况下，Java对象朝生夕死，一块 Survivor 区域是能够存放GC后剩余的对象的，但是极端情况下，GC后仍然有大量存活的对象，那么一块 Survivor 区域就会存放不下这么多的对象，那么这时候就需要老年代进行分配担保，让无法放入 Survivor 区域的对象直接进入到老年代，当然前提是老年代还有空间能够存放这些对象。但是实际情况是在完成GC之前，是不知道还有多少对象能够存活下来的，所以老年代也无法确认是否能够存放GC后新生代转移过来的对象，那么这该怎么办呢? 前面我们介绍的都是Minor GC,那么何时会发生 Full GC？ 在发生 Minor GC 时，虚拟机会检测之前每次晋升到老年代的平均大小是否大于老年代的剩余空间，如果大于，则改为 Full GC。如果小于，则查看 HandlePromotionFailure 设置是否允许担保失败，如果允许，那只会进行一次 Minor GC，如果不允许，则也要进行一次 Full GC。 1-XX:-HandlePromotionFailure 回到第一个问题，老年代也无法确认是否能够存放GC后新生代转移过来的对象，那么这该怎么办呢? 也就是取之前每一次回收晋升到老年代对象容量的平均大小作为经验值，然后与老年代剩余空间进行比较，来决定是否进行 Full GC，从而让老年代腾出更多的空间。 通常情况下，我们会将 HandlePromotionFaile 设置为允许担保失败，这样能够避免频繁的发生 Full GC。 总结​","link":"/2021/05/10/JVM%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%92%8C%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"},{"title":"JavaSE(一)基本程序设计结构","text":"基本程序设计结构包括一下入门的概念和实例： 一个简单的Java应用程序 注释 数据类型 变量 运算符 字符串 输入输出 控制流 大数值 数组 一个简单的Java应用程序12345678910111213/** * 1. java区分大小写 * 2. public 为访问修饰符，用于控制程序的其他部分对这段代码的访问级别 * 3. 关键字class表示Java程序中的全部内容都包含在类中 * 4. class 后面跟着类名，表明Java程序中的全部内容都包含在类中。 * 5. 运行已编译的程序时，Java 虚拟机将从指定类中的main方法开始执行 */public class HelloWorld { public static void main(String[] args) { System.out.println(&quot;Hello,World!&quot;); }} 注释123456789101112/** * 注释类 */public class Comments { public static void main(String[] args) { //1. 第一种注释方式 // 一直到本行结束 //2. 第二种注释方式 ：/* ---- */ //3. 第三种注释方式 ：/** ---- */ } } 数据类型 整型 类型 字节数 取值范围 byte 1 -128 ~ 127 short 2 -32678 ~ 32767 int 4 -2147483648 ~ 2147483647（2开头的10位数） long 8 long型数值后面有一个后缀L 浮点类型 类型 字节数 取值范围 float 4 float型数值有一个后缀F double 8 char类型：表示单个字符 boolean类型。 变量与常量字符串Java没有内置的字符串类型，而是在标准Java类库中提供了一个预定义类，很自然地叫做String。每个用双引号括起来的字符串都是String 类的一一个实例。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import java.time.LocalDate;import java.util.Date;/** * 1. java区分大小写 * 2. public 为访问修饰符，用于控制程序的其他部分对这段代码的访问级别 * 3. 关键字class表示Java程序中的全部内容都包含在类中 * 4. class 后面跟着类名，表明Java程序中的全部内容都包含在类中。 * 5. 运行已编译的程序时，Java 虚拟机将从指定类中的main方法开始执行 */public class HelloWorld { public static void main(String[] args) { String s = &quot;abcdef&quot;; //1. 截取子串方法substring String substring = s.substring(0, 2); System.out.println(&quot;substring = &quot; + substring); //substring = ab //2. 字符串拼接，+ String a = &quot;abc&quot;; String b = &quot;def&quot;; System.out.println(a + b); //abcdef //3. 字符串类型是不可变的,下面的改动只是修改字面量，让他去引用另外一个字符串。 /** * 不可变字符串却有一个优点:编译器可以让字符串共享。 * 为了弄清具体的工作方式，可以想象将各种字符串存放在公共的存储池中。字符串变量 * 指向存储池中相应的位置。如果复制一个字符串变量，原始字符串与复制的字符串共享相同的字符. * Java 的设计者认为共享带来的高效率远远胜过于提取、拼接字符串所带来的低效率。 * 查看一下程序会发现:很少需要修改字符串，而是往往需要对字符串进行比较(有 */ s = s.substring(0,2) + &quot;aa&quot;; System.out.println(&quot;s = &quot; + s); //s = abaa //4. 字符串比较是否相等 equals 方法 /** * 一定不要使用==运算符检测两个字符串是否相等!这个运算符只能够确定两个字符串 * 是否放置在同一个位置上。当然，如果字符串放置在同一个位置上，它们必然相等。 */ String c = &quot;abc&quot;; System.out.println(c.equals(a)); // true // 5. 空串和null串// 空串是一个Java对象，有自己的串长度(0)和内容(空)。不过，String 变量还可以存// 放一个特殊的值，名为nul,这表示目前没有任何对象与该变量关联(关于null的更多信息// 请参见第4章)。要检查-一个字符串是否为 null,要使用以下条件: String d = &quot;&quot;; //空串 String e = null; // null }} 输入和输出12345678910111213141516171819202122/** * 输入输出测试 */public class InputTest { public static void main(String[] args) { Scanner scanner = new Scanner(System.in); System.out.println(&quot;What is your name? &quot;); String name = scanner.nextLine(); //读取输入的下一行内容 System.out.println(&quot;How old are you?&quot;); int age = scanner.nextInt(); // 读取下一个整数 System.out.println(&quot;Hello, &quot; + name + &quot;. Next year ,you will be &quot; + (age+1)); // %s 表示字符串替换格式化 // %d 表示十进制整数替换格式化 System.out.printf(&quot;Hello, %s. Next year , you will be %d &quot; , name ,age); }} 大数1234567891011121314151617181920212223/** * 如果基本的整数和浮点数精度不能够满足需求，那么可以使用math包下的两个类： * BigInteger ： 可以实现任意精度的整数计算 * BigDecimal ： 可以实现任意精度的浮点数计算 */public class BigIntegerTest { public static void main(String[] args) { Scanner in = new Scanner(System.in); System.out.println(&quot;How many numbers do you need to draw? &quot;); int k = in.nextInt(); System.out.println(&quot;What is the highest number you can draw?&quot;); int n = in.nextInt(); BigInteger bigInteger = BigInteger.valueOf(1); for (int i = 1; i &lt;= k ; i++) { // 大数不能用常见的加减乘除算术运算符来计算，而是要用指定的方法来。 bigInteger = bigInteger.multiply(BigInteger.valueOf(n - i + 1)).divide(BigInteger.valueOf(i)); } System.out.println(&quot;Your odds are 1 in &quot; + bigInteger + &quot;.Good Luck！&quot;); }} 数组","link":"/2021/08/01/JavaSE-%E4%B8%80-%E5%9F%BA%E6%9C%AC%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%BB%93%E6%9E%84/"},{"title":"JVM（三）类文件和类加载","text":"java跨平台的实现是基于JVM虚拟机的，编写的java源码，编译后会生成一种.class文件，称为字节码文件。 java虚拟机就是负责将字节码文件翻译成特定平台下的机器码然后运行。 为了保证Class文件在多个平台的通用性，java官方制定了严格的Class文件格式。 了解Class文件结构，有利于我们反编译 .class 文件或在程序编译期间修改字节码做代码注入。 Class文件结构Class文件中包含了Java虚拟机指令集、符号表以及若干其他辅助信息。 每一个 Class 文件对应于一个如下所示的 ClassFile 结构体： 12345678910111213141516171819ClassFile { u4 magic; u2 minor_version; u2 major_version; u2 constant_pool_count; cp_info constant_pool[constant_pool_count-1]; u2 access_flags; u2 this_class; u2 super_class; u2 interfaces_count; u2 interfaces[interfaces_count]; u2 fields_count; field_info fields[fields_count]; u2 methods_count; method_info methods[methods_count]; u2 attributes_count; attribute_info attributes[attributes_count];}复制代码 简单看一下各项的含义： 由于 Class 文件结构没有任何分隔符，所以无论是每个数据项的的顺序还是数量，都是严格限定的，哪个字节代表什么含义，长度多少，先后顺序如何，都是不允许改变的 魔数将class文件用16进制打开的话 第一行中有一串特殊的字符 cafebabe，它就是一个魔数，是 JVM 识别 class 文件的标志，JVM 会在验证阶段检查 class 文件是否以该魔数开头，如果不是则会抛出 ClassFormatError。 版本号第 5 和第 6 个字节是次版本号（Minor Version），第 7 和第 8 个字节是主版本号（Major Version）。高版本的 JDK 能向下兼容以前版本的 Class 文件，但不能运行以后版本的 Class 文件，即使文件格式未发生变化。 常量池常量池中主要存放两大类常量：字面量（Literal）和符号引用（Symbolic References）。字面量比较接近于Java语言层面的常量概念，如文本字符串、被声明为final的常量值等。而符号引用则属于编译原理方面的概念，主要包括下面几类常量： 被模块导出或者开放的包（Package） 类和接口的全限定名（Fully Qualified Name） 字段的名称和描述符（Descriptor） 方法的名称和描述符 方法句柄和方法类型（Method Handle、Method Type、Invoke Dynamic） 动态调用点和动态常量（Dynamically-Computed Call Site、Dynamically-Computed Constant） 这17类常量结构只有一个相同之处，表结构起始的第一位是个u1类型的标志位（tag），代表着当前常量属于哪种常量类型。 17种常量类型所代表的具体含义如表所示： 类型 标志 描述 CONSTANT_Utf8_info 1 UTF-8 编码的字符串 CONSTANT_Integer_info 3 整型字面量 CONSTANT_Float_info 4 浮点型字面量 CONSTANT_Long_info 5 长整型型字面量 CONSTANT_Double_info 6 双精度浮点型字面量 CONSTANT_Class_info 7 类或接口的符号引用 CONSTANT_String_info 8 字符串类型字面量 CONSTANT_Fieldref_info 9 字段的符号引用 CONSTANT_Methodref_info 10 类中方法的符号引用 CONSTANT_InterfaceMethodref_info 11 接口中方法的符号引用 CONSTANT_NameAndType_info 12 字段或方法的部分符号引用 CONSTANT_MethodHandle_info 15 表示方法句柄 CONSTANT_MethodType_info 16 表示方法类型 CONSTANT_Dynamic_info 17 表示一个动态计算常量 CONSTANT_InvokeDynamic_info 18 表示一个动态方法调用点 CONSTANT_Moudle_info 19 表示一个模块 CONSTANT_Package_info 20 表示一个模块中开放或者导出的包 常量池非常繁琐，17种常量类型各自有着完全独立的数据结构，彼此之间没有什么共性和联系。 访问标志在常量池结束之后，紧接着的2个字节代表访问标志（access_flags），这个标志用于识别一些类或者接口层次的访问信息，包括：这个Class是类还是接口；是否定义为public类型；是否定义为abstract类型；如果是类的话，是否被声明为final等等。 具体的标志位以及标志的含义如表： 标志名称 标志值 含义 ACC_PUBLIC 0x0001 是否为 Public 类型 ACC_FINAL 0x0010 是否被声明为 final，只有类可以设置 ACC_SUPER 0x0020 是否允许使用 invokespecial 字节码指令的新语义 ACC_INTERFACE 0x0200 标志这是一个接口 ACC_ABSTRACT 0x0400 是否为 abstract 类型，对于接口或者抽象类来说，次标志值为真，其他类型为假 ACC_SYNTHETIC 0x1000 标志这个类并非由用户代码产生 ACC_ANNOTATION 0x2000 标志这是一个注解 ACC_ENUM 0x4000 标志这是一个枚举 access_flags中一共有16个标志位可以使用，当前只定义了其中9个，没有使用到的标志位要求一 律为零。 类索引、父类索引、接口索引这三者用来确定类的继承关系。 类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名。由于Java语言不允许多重继承，所以父类索引只有一个，除了java.lang.Object之外，所有的Java类都有父类，因此除了 java.lang.Object外，所有Java类的父类索引都不为0。 接口索引集合就用来描述这个类实现了哪些接口，这些被实现的接口将按implements关键字（后的接口顺序从左到右排列在接口索引集合中。 字段表集合接口索引结束后，接着是字段表（field_info），它用于描述接口或者类中声明的变量——这里的字段（Field）只包括类级变量以及实例级变量，不包括在方法内部声明的局部变量。 描述的主要信息包括： ①、字段的作用域（public，protected，private修饰） ②、是类级变量还是实例级变量（static修饰） ③、是否可变（final修饰） ④、并发可见性（volatile修饰，是否强制从主从读写） ⑤、是否可序列化（transient修饰） ⑥、字段数据类型（8种基本数据类型，对象，数组等引用类型） ⑦、字段名称 字段表的结构如下： 类型 名称 数量 u2 access_flags 1 u2 name_index 1 u2 descriptor_index 1 u2 attributes_count 1 attribute_info attributes attributes_count access_flags是该字段的的访问标志，它和类中的访问标志很类似，用以描述该字段的权限类型：private、protected、public；并发可见性：volatile；可变性：final； 访问标志详情如下图所示： 由于Java语法规则的约束，ACC_PUBLIC、ACC_PRIVATE、ACC_PROTECTED三个标志最多只能选择其一，ACC_FINAL、ACC_VOLATILE不能同时选择。接口之中的字段必须有ACC_PUBLIC、ACC_STATIC、ACC_FINAL标志。 方法表集合方法表的结构如同字段表一样，依次包括访问标志（access_flags）、名称索引（name_index）、描述符索引（descriptor_index）、属性表集合（attributes）几项，如表所示： 有区别的部分只有方法访问标志 access_flag, 因为volatile关键字和transient关键字不能修饰方法。 方法表标志位及其取值如下： 属性表集合接下来终于到了最后一项：属性表集合。 前面提到的Class文件、字段表、方法表都可以携带自己的属性表集合，就是引用的这里。 属性表集合中的属性如下所示： 与Class文件中其他的数据项目要求严格的顺序、长度和内容不同，属性表集合的限制宽松一些，不再要求各个属性表具有严格顺序，并且《Java虚拟机规范》允许只要不与已有属性名重复，任何人实现的编译器都可以向属性表中写入自己定义的属性信息，Java虚拟机运行时会忽略掉它不认识的属性。 举例查看12345public class Hello { public static void main(String[] args) { System.out.println(&quot;Hello World&quot;); }} 12javac Hello.java //javac 命令编译成 jvm 能识别的 class 文件xxd Hello.class //以 16 进制的方式查看这个 class 文件 16进制如下： 1234567891011121314151617181920212223242526272800000000: cafe babe 0000 0034 001d 0a00 0600 0f09 .......4........ //cafe babe为魔数00000010: 0010 0011 0800 120a 0013 0014 0700 1507 ................00000020: 0016 0100 063c 696e 6974 3e01 0003 2829 .....&lt;init&gt;...()00000030: 5601 0004 436f 6465 0100 0f4c 696e 654e V...Code...LineN00000040: 756d 6265 7254 6162 6c65 0100 046d 6169 umberTable...mai00000050: 6e01 0016 285b 4c6a 6176 612f 6c61 6e67 n...([Ljava/lang00000060: 2f53 7472 696e 673b 2956 0100 0a53 6f75 /String;)V...Sou00000070: 7263 6546 696c 6501 000a 4865 6c6c 6f2e rceFile...Hello.00000080: 6a61 7661 0c00 0700 0807 0017 0c00 1800 java............00000090: 1901 000b 4865 6c6c 6f20 576f 726c 6407 ....Hello World.000000a0: 001a 0c00 1b00 1c01 0021 636f 6d2f 7869 .........!com/xi000000b0: 6173 6d2f 6173 6d64 656d 6f2f 636c 6173 asm/asmdemo/clas000000c0: 7374 6573 742f 4865 6c6c 6f01 0010 6a61 stest/Hello...ja000000d0: 7661 2f6c 616e 672f 4f62 6a65 6374 0100 va/lang/Object..000000e0: 106a 6176 612f 6c61 6e67 2f53 7973 7465 .java/lang/Syste000000f0: 6d01 0003 6f75 7401 0015 4c6a 6176 612f m...out...Ljava/00000100: 696f 2f50 7269 6e74 5374 7265 616d 3b01 io/PrintStream;.00000110: 0013 6a61 7661 2f69 6f2f 5072 696e 7453 ..java/io/PrintS00000120: 7472 6561 6d01 0007 7072 696e 746c 6e01 tream...println.00000130: 0015 284c 6a61 7661 2f6c 616e 672f 5374 ..(Ljava/lang/St00000140: 7269 6e67 3b29 5600 2100 0500 0600 0000 ring;)V.!.......00000150: 0000 0200 0100 0700 0800 0100 0900 0000 ................00000160: 1d00 0100 0100 0000 052a b700 01b1 0000 .........*......00000170: 0001 000a 0000 0006 0001 0000 0003 0009 ................00000180: 000b 000c 0001 0009 0000 0025 0002 0001 ...........%....00000190: 0000 0009 b200 0212 03b6 0004 b100 0000 ................000001a0: 0100 0a00 0000 0a00 0200 0000 0500 0800 ................000001b0: 0600 0100 0d00 0000 0200 0e ........... 1javap -c xxx 是用来对class文件进行反编译 12345678910111213141516171819xiasmdeMacBook-Pro:test xiasm$ javap -c Hello警告: 二进制文件Hello包含com.xiasm.asmdemo.classtest.Hello1 Compiled from &quot;Hello.java&quot;2 public class com.xiasm.asmdemo.classtest.Hello {3 public com.xiasm.asmdemo.classtest.Hello();4 Code:5 0: aload_06 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V7 4: return89 public static void main(java.lang.String[]);10 Code:11 0: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream;12 3: ldc #3 // String Hello World13 5: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V14 8: return15 } 第5行：aload_x 操作码用来把 对象引用 加载到 操作数栈，非静态的函数都有第一个默认参数，那就是 this，这里的 aload_0 就是把 this 入栈 第6行：invokespecial #1 invokespecial指令调用实例初始化方法、私有方法、父类方法，#1 指的是常量池中的第一个，这里是方法引用java/lang/Object.””:()V，也即构造器函数 第7行：return，这个操作码属于 ireturn、lreturn、freturn、dreturn、areturn 和 return 操作码组中的一员，其中 i 表示 int，返回整数，同类的还有 l 表示 long，f 表示 float，d 表示 double，a 表示 对象引用。没有前缀类型字母的 return 表示返回 void 到此，构造器函数就结束了，接下来是 main 函数： 第11行：getstatic #2 getstatic获取指定类的静态域，并将其值压入栈顶，#2 代表常量池中的第 2 个，这里表示的是java/lang/System.out:Ljava/io/PrintStream;，其实就是java.lang.System 类的静态变量 out（类型是 PrintStream） 第12行：ldc #3 ldc表示将int, float或String型常量值从常量池中推送至栈顶，#3 代表常量池的第三个（字符串 Hello, World） 第13行：invokevirtual #4 invokevirutal 指令调用一个对象的实例方法，#4表示 PrintStream.println(String) 函数引用，并把栈顶两个元素出栈 类加载过程 加载就是把字节码文件从IO或内存加载到内存中的过程；初始化就是使用()进行类初始化的过程，这不同于调用构造函数；使用就是字面意思；卸载就是从方法区移除类型。 类加载过程加载 ①、通过一个类的全限定名来获取定义此类的二进制字节流。 ②、将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 ③、在Java堆中生成一个代表这个类的java.lang.Class对象，作为方法区这些数据的访问入口。 它是Java将字节码数据从不同的数据源读取到JVM中，并映射为JVM认可的数据结构（Class对象），这里的数据源可能是各种各样的形态，如jar文件、class文件，甚至是网络数据源等；如果输入数据不是ClassFile的结构，则会抛出ClassFormatError。 加载阶段是用户参与的阶段，我们可以自定义类加载器，去实现自己的类加载过程。 定义此类的二进制流的获取方式有多种： 1、从 ZIP 包中读取。这称为后面的 JAR、EAR、WAR 格式的基础。 2、从网络中获取。比较典型的应用就是 Applet。 3、运行时计算生成。这就是动态代理技术。 4、由其它文件生成。比如 JSP 应用。 5、从数据库中读取。 加载阶段完成后，虚拟机外部的二进制字节流就按照虚拟机所需的格式存储在方法区中，然后在Java堆中实例化一个 java.lang.Class 类的对象，这个对象将作为程序访问方法区中这些类型数据的外部接口。 注意，加载阶段与连接阶段的部分内容（如一部分字节码文件的格式校验）是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始了。 验证作用是为了确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 ①、文件格式验证 ②、元数据验证 ③、字节码验证 ④、符号引用验证 准备创建类或接口中的静态变量，并初始化静态变量的初始值。但这里的“初始化”和下面的显式初始化阶段是有区别的，侧重点在于分配所需要的内存空间，不会去执行更进一步的JVM指令。 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存是在方法区中进行分配。 注意： 一、上面说的是类变量，也就是被 static 修饰的变量，不包括实例变量。实例变量会在对象实例化时随着对象一起分配在堆中。 二、初始值，指的是一些数据类型的默认值。基本的数据类型初始值如下（引用类型的初始值为null）： 解析解析阶段是虚拟机将常量池中的符号引用替换为直接引用的过程。 符号引用（Symbolic References）：符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义的定位到目标即可。符号引用与虚拟机实现的内存布局无关，引用的目标不一定已经加载到内存中。 直接引用（Direct References）：直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用是与虚拟机实现内存布局相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那么引用的目标必定已经在内存中存在。 解析动作主要针对类或接口、字段、类方法、接口方法四类符号引用，分别对应于常量池的 CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info、CONSTANTS_InterfaceMethodref_info四种类型常量。 初始化初始化阶段是执行类构造器() 方法的过程。 这一步真正去执行类初始化的代码逻辑，包括静态字段赋值的动作，以及执行类定义中的静态初始化块内的逻辑，编译器在编译阶段就会把这部分逻辑整理好，父类型的初始化逻辑优先于当前类型的逻辑。 ①、() 方法 是由编译器自动收集类中的所有类变量的赋值动作和静态语句块（static{}）中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序所决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块中可以赋值，但是不能访问。 比如如下代码会报错： 但是你把第 14 行代码放到 static 静态代码块的上面就不会报错了。或者不改变代码顺序，将第 11 行代码移除，也不会报错。 ②、() 方法与类的构造函数（或者说是实例构造器()方法）不同，它不需要显示的调用父类构造器，虚拟机会保证在子类的()方法执行之前，父类的()方法已经执行完毕。因此虚拟机中第一个被执行的()方法的类肯定是 java.lang.Object。 ③、由于父类的() 方法先执行，所以父类中定义的静态语句块要优先于子类的变量赋值操作。 ④、() 方法对于接口来说并不是必须的，如果一个类中没有静态语句块，也没有对变量的赋值操作，那么编译器可以不为这个类生成() 方法。 ⑤、接口中不能使用静态语句块，但仍然有变量初始化的赋值操作，因此接口与类一样都会生成() 方法。但接口与类不同的是，执行接口中的() 方法不需要先执行父接口的() 方法。只有当父接口中定义的变量被使用时，父接口才会被初始化。 ⑥、接口的实现类在初始化时也一样不会执行接口的() 方法。 ⑦、虚拟机会保证一个类的() 方法在多线程环境中被正确的加锁和同步。如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的() 方法，其他的线程都需要阻塞等待，直到活动线程执行() 方法完毕。如果在一个类的() 方法中有很耗时的操作，那么可能造成多个进程的阻塞。 比如对于如下代码： View Code 运行结果如下： 线程1抢到了执行() 方法，但是该方法是一个死循环，线程2将一直阻塞等待。 知道了类的初始化过程，那么类的初始化何时被触发呢？JVM大概规定了如下几种情况： ①、当虚拟机启动时，初始化用户指定的类。 ②、当遇到用以新建目标类实例的 new 指令时，初始化 new 指定的目标类。 ③、当遇到调用静态方法的指令时，初始化该静态方法所在的类。 ④、当遇到访问静态字段的指令时，初始化该静态字段所在的类。 ⑤、子类的初始化会触发父类的初始化。 ⑥、如果一个接口定义了 default 方法，那么直接实现或间接实现该接口的类的初始化，会触发该接口的初始化。 ⑦、使用反射 API 对某个类进行反射调用时，会初始化这个类。 ⑧、当初次调用 MethodHandle 实例时，初始化该 MethodHandle 指向的方法所在的类。 类加载器分类①、启动类加载器（Bootstrap ClassLoader） 负责将存放在 /lib 目录中的，或者被**-Xbootclasspath** 参数所指定的路径中的，并且是虚拟机按照文件名识别的（仅按照文件名识别，如rt.jar，名字不符合的类库即使放在lib目录中也不会被加载）类库加载到虚拟机内存中。**** 启动类加载器无法被Java程序直接引用。**** JDK 中的源码类大都是由启动类加载器加载，比如前面说的 java.lang.String，java.util.List等，需要注意的是，启动类 main Class 也是由启动类加载器加载。 ②、扩展类加载器（Extension ClassLoader） 这个类加载器由 sun.misc.Launcher$ExtClassLoader 实现，负责加载＜JAVA_HOME＞/lib/ext 目录中的，或者被 java.ext.dirs 系统变量所指定的路径中的所有类库。 开发者可以直接使用扩展类加载器。 ③、应用程序类加载器（Application ClassLoader） 由 sun.misc.Launcher$AppClassLoader 实现。由于这个类加载器是 ClassLoader.getSystemClassLoader() 方法的返回值，所以一般也称它为系统类加载器。 它负责加载用户类路径ClassPath上所指定的类库，开发者可以直接使用这个类加载器。如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 通常项目中自定义的类，都会放在类路径下，由应用程序类加载器加载。 ④、自定义类加载器（User ClassLoader） 这是由用户自己定义的类加载器，一般情况下我们不会自定义类加载器，但有些特殊情况，比如JDBC能够通过连接各种不同的数据库就是自定义类加载器来实现的，具体用处会在后文详细介绍。 双亲委派模型双亲委派机制就是如果一个类加载器收到了类加载请求，它首先不会自己尝试去加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有父类加载器反馈到无法完成这个加载请求（它的搜索范围没有找到这个类），子加载器才会尝试自己去加载。 12345678910111213/** * Create by YSOcean */public class ClassLoadTest { public static void main(String[] args) { ClassLoader classLoader1 = ClassLoadTest.class.getClassLoader(); ClassLoader classLoader2 = classLoader1.getParent(); ClassLoader classLoader3 = classLoader2.getParent(); System.out.println(classLoader1); System.out.println(classLoader2); System.out.println(classLoader3); }} 双亲委派机制有什么好处呢? 回到上面提出的问题，如果你自定义了一个 java.lang.String类，你会发现这个自定义的String.java可以正常编译，但是永远无法被加载运行。因为加载这个类的加载器，会一层一层的往上推，最终由启动类加载器来加载，而启动类加载的会是源码包下的String类，不是你自定义的String类。 实现源码： 12345678910111213141516171819202122232425262728293031323334353637protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) { c = parent.loadClass(name, false); } else { c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // ClassNotFoundException thrown if class not found // from the non-null parent class loader } if (c == null) { // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; } } 自定义类加载器先说说我们为什么要自定义类加载器？ ①、加密 我们知道Java字节码是可以进行反编译的，在某些安全性高的场景，是不允许这种情况发生的。那么我们可以将编译后的代码用某种加密算法进行加密，加密后的文件就不能再用常规的类加载器去加载类了。而我们自己可以自定义类加载器在加载的时候先解密，然后在加载。 ②、动态创建 比如很有名的动态代理。 ③、从非标准的来源加载代码 我们不用非要从class文件中获取定义此类的二进制流，还可以从数据库，从网络中，或者从zip包等。 明白了为什么要自定义类加载器，接下来我们再来详述如何自定义类加载器。 通过第 3 小节的 java.lang.ClassLoader 类的源码分析，类加载时根据双亲委派模型会先一层层找到父加载器，如果加载失败，则会调用当前加载器的 findClass() 方法来完成加载。因此我们自定义类加载器，有两个步骤： 1、继承 ClassLoader 2、覆写 findClass() 方法 破坏双亲委派模型的情况 重写 loadClass() 方法 逆向使用类加载器，引入线程上下文类加载器，如果 API 中的基础类想要调用用户的代码(JNDI/JDBC 等),此时双亲委派模型就不能完成.为了解决这个问题,java 设计团队只好 使用一个不优雅的设计方案:Thread 的上下文类加载器,默认就是应用程序的类加载器。 追求程序的动态性：代码热替换、模块热部署等技术，希望应用程序不用重启就可以加载最新的字节码文件.此时就需要破坏双亲委派模型 动态代理的实现对于一个普通的Java动态代理，其实现过程可以简化成为： 提供一个基础的接口，作为被调用类型（com.mycorp.HelloImpl）和代理类之间的统一入口，如com.mycorp.Hello。 实现InvocationHandler，对代理对象方法的调用，会被分派到其invoke方法来真正实现动作。 通过Proxy类，调用其newProxyInstance方法，生成一个实现了相应基础接口的代理类实例，可以看下面的方法签名。 123public satic Object newProxyInsance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) 我们分析一下，动态代码生成是具体发生在什么阶段呢？ 不错，就是在newProxyInstance生成代理类实例的时候。我选取了JDK自己采用的ASM作为示例，一起来看看用ASM实现的简要过程，请参考下面的示例代码片段。 第一步，生成对应的类，其实和我们去写Java代码很类似，只不过改为用ASM方法和指定参数，代替了我们书写的源码。 123456789101112131415161718ClassWriter cw = new ClassWriter(ClassWriter.COMPUTE_FRAMES);cw.visit(V1_8, // 指定Java版本ACC_PUBLIC, // 说明是public类型&quot;com/mycorp/HelloProxy&quot;, // 指定包和类的名称null, // 签名，null表示不是泛型&quot;java/lang/Object&quot;, // 指定父类new String[]{ &quot;com/mycorp/Hello&quot; }); // 指定需要实现的接口更进一步，我们可以按照需要为代理对象实例，生成需要的方法和逻辑。MethodVisitor mv = cw.visitMethod(ACC_PUBLIC, // 声明公共方法&quot;sayHello&quot;, // 方法名称&quot;()Ljava/lang/Object;&quot;, // 描述符null, // 签名，null表示不是泛型null); // 可能抛出的异常，如果有，则指定字符串数组mv.visitCode();// 省略代码逻辑实现细节cw.visitEnd(); // 结束类字节码生成 上面的代码虽然有些晦涩，但总体还是能多少理解其用意，不同的visitX方法提供了创建类型，创建各种方法等逻辑。ASM API，广泛的使用了Visitor模式，如果你熟悉这个模式， 就会知道它所针对的场景是将算法和对象结构解耦，非常适合字节码操纵的场合，因为我们大部分情况都是依赖于特定结构修改或者添加新的方法、变量或者类型等。 按照前面的分析，字节码操作最后大都应该是生成byte数组，ClassWriter提供了一个简便的方法。 cw.toByteArray(); 然后，就可以进入我们熟知的类加载过程了， 总结","link":"/2021/05/11/JVM%EF%BC%88%E4%B8%89%EF%BC%89%E7%B1%BB%E6%96%87%E4%BB%B6%E5%92%8C%E7%B1%BB%E5%8A%A0%E8%BD%BD/"},{"title":"ElasticSearch从入门到实战","text":"本篇主要讲述了elasticsearch入门的一些基本概念（来源：江南一点雨） 简介 安装 十大核心概念 分词器 索引基本操作 文档基本操作 版本控制 倒排索引 ElasticSearch简介LuceneLucene 是一个开源、免费、高性能、纯 Java 编写的全文检索引擎，可以算作是开源领域最好的全文检索工具包。 在实际开发中，Lucene 几乎适用于任何需要全文检索的场景，所以 Lucene 先后发展出好多语言版本，例如 C++、C#、Python 等。 早在 2005 年，Lucene 就升级为 Apache 顶级开源项目。它的作者是 Doug Cutting，有的人可能没听过这这个人，不过你肯定听过他的另一个大名鼎鼎的作品 Hadoop。 不过需要注意的是，Lucene 只是一个工具包，并非一个完整的搜索引擎，开发者可以基于 Lucene 来开发完整的搜索引擎。比较著名的有 Solr、ElasticSearch，不过在分布式和大数据环境下，ElasticSearch 更胜一筹。 Lucene 主要有如下特点： 简单 跨语言 强大的搜索引擎 索引速度快 索引文件兼容不同平台 ElasticSearchElasticSearch 是一个分布式、可扩展、近实时性的高性能搜索与数据分析引擎。ElasticSearch 基于 Java 编写，通过进一步封装 Lucene，将搜索的复杂性屏蔽起来，开发者只需要一套简单的 RESTful API 就可以操作全文检索。 ElasticSearch 在分布式环境下表现优异，这也是它比较受欢迎的原因之一。它支持 PB 级别的结构化或非结构化海量数据处理 整体上来说，ElasticSearch 有三大功能： 数据搜集 数据分析 数据存储 ElasticSearch 的主要特点： 分布式文件存储。 实时分析的分布式搜索引擎。 高可拓展性。 可插拔的插件支持。 ElasticSearch安装（homebrew）首先打开 Es 官网，找到 Elasticsearch： https://www.elastic.co/cn/elasticsearch/ 点击 https://www.elastic.co/guide/en/elasticsearch/reference/7.13/brew.html，按照提示安装 打开终端 123456789101112131415161718192021222324252627282930shengbinbin@192 ~ % brew list //查看所有homebrew安装的软件==&gt; Formulaeautoconf icu4c maven pkg-configbrotli jemalloc mysql protobufc-ares kibana-full nghttp2 rediselasticsearch-full libev node tcl-tkfreetype libpng openjdkgit-gui libuv openjdk@11gradle m4 openssl@1.1shengbinbin@192 ~ % brew info elasticsearch-full //查看安装的elasticsearch-full详细信息elastic/tap/elasticsearch-full: stable 7.13.2Distributed search &amp; analytics enginehttps://www.elastic.co/products/elasticsearchConflicts with: elasticsearch/opt/homebrew/Cellar/elasticsearch-full/7.13.2 (957 files, 504.8MB) * Built from source on 2021-06-29 at 20:46:00From: https://github.com/elastic/homebrew-tap/blob/HEAD/Formula/elasticsearch-full.rb==&gt; CaveatsData: /opt/homebrew/var/lib/elasticsearch/elasticsearch_shengbinbin/Logs: /opt/homebrew/var/log/elasticsearch/elasticsearch_shengbinbin.logPlugins: /opt/homebrew/var/elasticsearch/plugins/Config: /opt/homebrew/etc/elasticsearch/To have launchd start elastic/tap/elasticsearch-full now and restart at login: brew services start elastic/tap/elasticsearch-fullOr, if you don't want/need a background service you can just run: elasticsearch 目录含义如下： 目录 含义 modules 依赖模块目录 lib 第三方依赖库 logs 输出日志目录 plugins 插件目录 bin 可执行文件目录 config 配置文件目录 data 数据存储目录 启动方式：打开终端，直接输入elasticsearch即可启动。看到 started 表示启动成功。 默认监听的端口是 9200，所以浏览器直接输入 localhost:9200 可以查看节点信息。 节点的名字以及集群（默认是 elasticsearch）的名字，我们都可以自定义配置。 打开 config/elasticsearch.yml 文件，可以配置集群名称以及节点名称。配置文件内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.# Before you set out to tweak and tune the configuration, make sure you# understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:#cluster.name: elasticsearch_shengbinbin //可以在这里修改集群名称## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:##node.name: node-1## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#path.data: /opt/homebrew/var/lib/elasticsearch/## Path to log files:#path.logs: /opt/homebrew/var/log/elasticsearch/## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: true## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## By default Elasticsearch is only accessible on localhost. Set a different# address here to expose this node on the network:##network.host: 192.168.0.1## By default Elasticsearch listens for HTTP traffic on the first free port it# finds starting at 9200. Set a specific HTTP port here:##http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when this node is started:# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]##discovery.seed_hosts: [&quot;host1&quot;, &quot;host2&quot;]## Bootstrap the cluster using an initial set of master-eligible nodes:##cluster.initial_master_nodes: [&quot;node-1&quot;, &quot;node-2&quot;]## For more information, consult the discovery and cluster formation module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true HEAD 插件安装Elasticsearch-head 插件，可以通过可视化的方式查看集群信息。 Chrome 直接在 App Store 搜索 Elasticsearch-head，点击安装即可。 2.3 分布式安装假设： 一主二从 master 的端口是 9200，slave 端口分别是 9201 和 9202 首先修改 master 的 config/elasticsearch.yml 配置文件： 12node.master: truenetwork.host: 127.0.0.1 配置完成后，重启 master。 将 es 的压缩包解压两份，分别命名为 slave01 和 slave02，代表两个从机。 分别对其进行配置。 slave01/config/elasticsearch.yml： 123456# 集群名称必须保持一致cluster.name: javaboy-esnode.name: slave01network.host: 127.0.0.1http.port: 9201discovery.zen.ping.unicast.hosts: [&quot;127.0.0.1&quot;] slave02/config/elasticsearch.yml： 123456# 集群名称必须保持一致cluster.name: javaboy-esnode.name: slave02network.host: 127.0.0.1http.port: 9202discovery.zen.ping.unicast.hosts: [&quot;127.0.0.1&quot;] 然后分别启动 slave01 和 slave02。启动后，可以在 head 插件上查看集群信息。 Kibana 安装Kibana 是一个 Elastic 公司推出的一个针对 es 的分析以及数据可视化平台，可以搜索、查看存放在 es 中的数据。 下载 Kibana：https://www.elastic.co/cn/downloads/kibana 使用homebrew安装：https://www.elastic.co/guide/en/kibana/7.13/brew.html 终端输入kibana启动 localhost:5601 查看默认窗口 ElasticSearch 十大核心概念3.1.1 集群（Cluster）一个或者多个安装了 es 节点的服务器组织在一起，就是集群，这些节点共同持有数据，共同提供搜索服务。 一个集群有一个名字，这个名字是集群的唯一标识，该名字成为 cluster name，默认的集群名称是 elasticsearch，具有相同名称的节点才会组成一个集群。 可以在 config/elasticsearch.yml 文件中配置集群名称： 1cluster.name: javaboy-es 在集群中，节点的状态有三种：绿色、黄色、红色： 绿色：节点运行状态为健康状态。所有的主分片、副本分片都可以正常工作。 黄色：表示节点的运行状态为警告状态，所有的主分片目前都可以直接运行，但是至少有一个副本分片是不能正常工作的。 红色：表示集群无法正常工作。 3.1.2 节点（Node）集群中的一个服务器就是一个节点，节点中会存储数据，同时参与集群的索引以及搜索功能。一个节点想要加入一个集群，只需要配置一下集群名称即可。默认情况下，如果我们启动了多个节点，多个节点还能够互相发现彼此，那么它们会自动组成一个集群，这是 es 默认提供的，但是这种方式并不可靠，有可能会发生脑裂现象。所以在实际使用中，建议一定手动配置一下集群信息。 3.1.3 索引（Index）索引可以从两方面来理解： 名词 具有相似特征文档的集合。 动词 索引数据以及对数据进行索引操作。 3.1.4 类型（Type）类型是索引上的逻辑分类或者分区。在 es6 之前，一个索引中可以有多个类型，从 es7 开始，一个索引中，只能有一个类型。在 es6.x 中，依然保持了兼容，依然支持单 index 多个 type 结构，但是已经不建议这么使用。 3.1.5 文档（Document）一个可以被索引的数据单元。例如一个用户的文档、一个产品的文档等等。文档都是 JSON 格式的。 3.1.6 分片（Shards）索引都是存储在节点上的，但是受限于节点的空间大小以及数据处理能力，单个节点的处理效果可能不理想，此时我们可以对索引进行分片。当我们创建一个索引的时候，就需要指定分片的数量。每个分片本身也是一个功能完善并且独立的索引。 默认情况下，一个索引会自动创建 1 个分片，并且为每一个分片创建一个副本。 3.1.7 副本（Replicas）副本也就是备份，是对主分片的一个备份。 3.1.8 Settings集群中对索引的定义信息，例如索引的分片数、副本数等等。 3.1.9 MappingMapping 保存了定义索引字段的存储类型、分词方式、是否存储等信息。 3.1.10 Analyzer字段分词方式的定义。 3.2 ElasticSearch Vs 关系型数据库 内置分词器ElasticSearch 核心功能就是数据检索，首先通过索引将文档写入 es。查询分析则主要分为两个步骤： 词条化：分词器将输入的文本转为一个一个的词条流。 过滤：比如停用词过滤器会从词条中去除不相干的词条（的，嗯，啊，呢）；另外还有同义词过滤器、小写过滤器等。 ElasticSearch 中内置了多种分词器可以供使用。 内置分词器： 4.2 中文分词器在 Es 中，使用较多的中文分词器是 elasticsearch-analysis-ik，这个是 es 的一个第三方插件，代码托管在 GitHub 上： https://github.com/medcl/elasticsearch-analysis-ik 4.2.1 安装两种使用方式： 第一种： 首先打开分词器官网：https://github.com/medcl/elasticsearch-analysis-ik。 在 https://github.com/medcl/elasticsearch-analysis-ik/releases 页面找到最新的正式版，下载下来。我们这里的下载链接是 https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.9.3/elasticsearch-analysis-ik-7.9.3.zip。 将下载文件解压。 在 es/plugins 目录下，新建 ik 目录，并将解压后的所有文件拷贝到 ik 目录下。 重启 es 服务。 第二种： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152shengbinbin@192 plugins % brew list==&gt; Formulaeautoconf icu4c maven pkg-configbrotli jemalloc mysql protobufc-ares kibana-full nghttp2 rediselasticsearch-full libev node tcl-tkfreetype libpng openjdkgit-gui libuv openjdk@11gradle m4 openssl@1.1shengbinbin@192 plugins % brew info elasticsearch-fullelastic/tap/elasticsearch-full: stable 7.13.2Distributed search &amp; analytics enginehttps://www.elastic.co/products/elasticsearchConflicts with: elasticsearch/opt/homebrew/Cellar/elasticsearch-full/7.13.2 (957 files, 504.8MB) * Built from source on 2021-06-29 at 20:46:00From: https://github.com/elastic/homebrew-tap/blob/HEAD/Formula/elasticsearch-full.rb==&gt; CaveatsData: /opt/homebrew/var/lib/elasticsearch/elasticsearch_shengbinbin/Logs: /opt/homebrew/var/log/elasticsearch/elasticsearch_shengbinbin.logPlugins: /opt/homebrew/var/elasticsearch/plugins/Config: /opt/homebrew/etc/elasticsearch/To have launchd start elastic/tap/elasticsearch-full now and restart at login: brew services start elastic/tap/elasticsearch-fullOr, if you don't want/need a background service you can just run: elasticsearchshengbinbin@192 plugins % cd /opt/homebrew/Cellar/elasticsearch-full/7.13.2shengbinbin@192 7.13.2 % lsINSTALL_RECEIPT.json binLICENSE.txt homebrew.mxcl.elasticsearch-full.plistNOTICE.txt libexecREADME.asciidoc //安装命令shengbinbin@192 7.13.2 % ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.13.2/elasticsearch-analysis-ik-7.13.2.zipwarning: usage of JAVA_HOME is deprecated, use ES_JAVA_HOMEFuture versions of Elasticsearch will require Java 11; your Java version from [/Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home/jre] does not meet this requirement. Consider switching to a distribution of Elasticsearch with a bundled JDK. If you are already using a distribution with a bundled JDK, ensure the JAVA_HOME environment variable is not set.-&gt; Installing https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.13.2/elasticsearch-analysis-ik-7.13.2.zip-&gt; Downloading https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.13.2/elasticsearch-analysis-ik-7.13.2.zip[=================================================] 100%@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: plugin requires additional permissions @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@* java.net.SocketPermission * connect,resolveSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.htmlfor descriptions of what these permissions allow and the associated risks.Continue with installation? [y/N]y-&gt; Installed analysis-ik-&gt; Please restart Elasticsearch to activate any plugins installedshengbinbin@192 7.13.2 % 1./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.13.2/elasticsearch-analysis-ik-7.13.2.zip 4.2.2 测试es 重启成功后，首先创建一个名为 test 的索引： 接下来，在该索引中进行分词测试： 4.2.3 自定义扩展词库4.2.3.1 本地自定义在 es/plugins/ik/config 目录下，新建 ext.dic 文件（文件名任意），在该文件中可以配置自定义的词库。 如果有多个词，换行写入新词即可。 然后在 es/plugins/ik/config/IKAnalyzer.cfg.xml 中配置扩展词典的位置： 4.2.3.2 远程词库也可以配置远程词库，远程词库支持热更新（不用重启 es 就可以生效）。 热更新只需要提供一个接口，接口返回扩展词即可。 具体使用方式如下，新建一个 Spring Boot 项目，引入 Web 依赖即可。然后在 resources/stastic 目录下新建 ext.dic 文件，写入扩展词： 接下来，在 es/plugins/ik/config/IKAnalyzer.cfg.xml 文件中配置远程扩展词接口： 配置完成后，重启 es ，即可生效。 热更新，主要是响应头的 Last-Modified 或者 ETag 字段发生变化，ik 就会自动重新加载远程扩展 索引的基本操作新建索引通过 head 插件新建索引在 head 插件中，选择 索引选项卡，然后点击新建索引。新建索引时，需要填入索引名称、分片数以及副本数。 索引创建成功后，如下图： 0、1、2、3、4 分别表示索引的分片，粗框表示主分片，细框表示副本（点一下框，通过 primary 属性可以查看是主分片还是副本）。 通过 kibana 发送请求创建创建索引请求： 1PUT user 创建成功后，返回如下信息： 123456{ &quot;acknowledged&quot; : true, &quot;shards_acknowledged&quot; : true, &quot;index&quot; : &quot;user&quot;} 查看user这个索引的信息： 123456789101112131415161718192021222324252627282930313233343536373839404142{ &quot;version&quot;: 4, &quot;mapping_version&quot;: 1, &quot;settings_version&quot;: 1, &quot;aliases_version&quot;: 1, &quot;routing_num_shards&quot;: 1024, &quot;state&quot;: &quot;open&quot;, &quot;settings&quot;: { &quot;index&quot;: { &quot;routing&quot;: { &quot;allocation&quot;: { &quot;include&quot;: { &quot;_tier_preference&quot;: &quot;data_content&quot;}}}, &quot;number_of_shards&quot;: &quot;1&quot;, &quot;provided_name&quot;: &quot;user&quot;, &quot;creation_date&quot;: &quot;1625048332811&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;IPeNFtZcQ8OEUfNEVryexg&quot;, &quot;version&quot;: { &quot;created&quot;: &quot;7130299&quot;}}}, &quot;mappings&quot;: { }, &quot;aliases&quot;: [ ], &quot;primary_terms&quot;: { &quot;0&quot;: 1}, &quot;in_sync_allocations&quot;: { &quot;0&quot;: [ &quot;ely2hxs1QdiRcTy0YB9Rmw&quot;]}, &quot;rollover_info&quot;: { }, &quot;system&quot;: false, &quot;timestamp_range&quot;: { &quot;unknown&quot;: true}} 索引的要求 索引名称不能有大写字母 123456789101112131415161718{ &quot;error&quot; : { &quot;root_cause&quot; : [ { &quot;type&quot; : &quot;invalid_index_name_exception&quot;, &quot;reason&quot; : &quot;Invalid index name [uSER], must be lowercase&quot;, &quot;index_uuid&quot; : &quot;_na_&quot;, &quot;index&quot; : &quot;uSER&quot; } ], &quot;type&quot; : &quot;invalid_index_name_exception&quot;, &quot;reason&quot; : &quot;Invalid index name [uSER], must be lowercase&quot;, &quot;index_uuid&quot; : &quot;_na_&quot;, &quot;index&quot; : &quot;uSER&quot; }, &quot;status&quot; : 400} 索引名是唯一的，不能重复，重复创建会出错 1234567891011121314151617{ &quot;error&quot; : { &quot;root_cause&quot; : [ { &quot;type&quot; : &quot;resource_already_exists_exception&quot;, &quot;reason&quot; : &quot;index [user/IPeNFtZcQ8OEUfNEVryexg] already exists&quot;, &quot;index_uuid&quot; : &quot;IPeNFtZcQ8OEUfNEVryexg&quot;, &quot;index&quot; : &quot;user&quot; } ], &quot;type&quot; : &quot;resource_already_exists_exception&quot;, &quot;reason&quot; : &quot;index [user/IPeNFtZcQ8OEUfNEVryexg] already exists&quot;, &quot;index_uuid&quot; : &quot;IPeNFtZcQ8OEUfNEVryexg&quot;, &quot;index&quot; : &quot;user&quot; }, &quot;status&quot; : 400} 更新索引索引创建好之后，可以修改其属性。 修改副本数例如修改索引的副本数： 1234PUT user/_settings{ &quot;number_of_replicas&quot;: &quot;2&quot;} 更新分片数也是一样。 向索引中写入文档1234PUT user/_doc/1{ &quot;title&quot;: &quot;三国演义&quot;} 123456789101112131415161718192021222324{ &quot;_index&quot; : &quot;user&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : { &quot;total&quot; : 3, &quot;successful&quot; : 1, &quot;failed&quot; : 0 }, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1}![](https://i.loli.net/2021/06/30/ZLHmCIfcAMgJvKX.png)### 修改索引的读写权限默认情况下，索引是具备读写权限的，当然这个读写权限可以关闭。例如，关闭索引的写权限： PUT user/_settings{ “blocks.write”: “true”} 123关闭之后，就无法添加文档了。关闭了写权限之后，如果想要再次打开，方式如下： PUT user/_settings{ “blocks.write”: “true”} 123456789101112131415其他类似的权限有：- blocks.write- blocks.read- blocks.read_only## 查看索引信息- head 插件查看方式如下- 请求查看方式如下： GET book/_settings 12345678910111213141516171819202122232425262728​```JSON{ &quot;user&quot; : { &quot;settings&quot; : { &quot;index&quot; : { &quot;routing&quot; : { &quot;allocation&quot; : { &quot;include&quot; : { &quot;_tier_preference&quot; : &quot;data_content&quot; } } }, &quot;number_of_shards&quot; : &quot;1&quot;, &quot;blocks&quot; : { &quot;write&quot; : &quot;false&quot; }, &quot;provided_name&quot; : &quot;user&quot;, &quot;creation_date&quot; : &quot;1625048332811&quot;, &quot;number_of_replicas&quot; : &quot;2&quot;, &quot;uuid&quot; : &quot;IPeNFtZcQ8OEUfNEVryexg&quot;, &quot;version&quot; : { &quot;created&quot; : &quot;7130299&quot; } } } }} 也可以同时查看多个索引信息： 1GET user,test/_settings 也可以查看所有索引信息： 1GET _all/_settings 删除索引 head 插件可以删除索引： 请求删除如下： 1DELETE test 删除一个不存在的索引会报错。 索引打开/关闭关闭索引： 1POST book/_close 打开索引： 1POST book/_open 当然，可以同时关闭/打开多个索引，多个索引用 , 隔开，或者直接使用 _all 代表所有索引。 复制索引索引复制，只会复制数据，不会复制索引配置。 12345POST _reindex{ &quot;source&quot;: {&quot;index&quot;:&quot;book&quot;}, &quot;dest&quot;: {&quot;index&quot;:&quot;book_new&quot;}} 复制的时候，可以添加查询条件。 索引别名可以为索引创建别名，如果这个别名是唯一的，该别名可以代替索引名称。 1234567891011POST /_aliases{ &quot;actions&quot;: [ { &quot;add&quot;: { &quot;index&quot;: &quot;book&quot;, &quot;alias&quot;: &quot;book_alias&quot; } } ]} 将 add 改为 remove 就表示移除别名： 1234567891011POST /_aliases{ &quot;actions&quot;: [ { &quot;remove&quot;: { &quot;index&quot;: &quot;book&quot;, &quot;alias&quot;: &quot;book_alias&quot; } } ]} 查看某一个索引的别名： 1GET /book/_alias 查看某一个别名对应的索引（book_alias 表示一个别名）： 1GET /book_alias/_alias 可以查看集群上所有可用别名： 1GET /_alias 文档的基本操作新建文档新建索引blog，在添加文档： 123456PUT blog/_doc/1{ &quot;title&quot;:&quot;文档基本操作&quot;, &quot;date&quot;:&quot;2021-06-30&quot;, &quot;content&quot;:&quot;添加的文档的基本操作&quot;} 123456789101112131415{ &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, //如果更新文档，版本会自动 + 1 &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : { //分片信息 &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 }, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1} 添加文档如果不指定id，系统会默认给一个id，但是需要修改成post请求： 123456POST blog/_doc{ &quot;title&quot;:&quot;666&quot;, &quot;date&quot;:&quot;2021-06-30&quot;, &quot;content&quot;:&quot;添加的文档的基本操作&quot;} 1234567891011121314{ &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1hrCXHoBjgQwlk7iYnof&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : { &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 }, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1} 查询文档get api： 1GET blog/_doc/1 1234567891011121314{ &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : { &quot;title&quot; : &quot;文档基本操作&quot;, &quot;date&quot; : &quot;2021-06-30&quot;, &quot;content&quot; : &quot;添加的文档的基本操作&quot; }} 批量查询文档： 1234GET blog/_mget{ &quot;ids&quot;:[&quot;1&quot;,&quot;1hrCXHoBjgQwlk7iYnof&quot;]} 为什么这里的get请求可以携带请求体呢？ 某些特定的语言比如JavaScript的http请求中不允许get请求有请求体，但是在RFC7231文档中，并没有规定GET请求的请求体如何处理。es为了保证兼容性，get和post都可以 更新文档 文档更新1次，version就+1 1234PUT blog/_doc/1hrCXHoBjgQwlk7iYnof{ &quot;title&quot;:&quot;777&quot;} 注 ： 这种方式更新的文档会覆盖掉原文档 大多数时候我们只是想更新文档中的字段： 12345678910111213141516171819POST blog/_update/1{ &quot;script&quot;: { &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;ctx._source.title=params.title&quot;, &quot;params&quot;: { &quot;title&quot;:&quot;666666&quot; } }}更新的请求格式：POST {index}/_update/{id}{ lang 表示脚本语言 painless 是es中内置的一种脚本语言 source 表示具体执行的脚本 ctx 是一个上下文对象，通过ctx可以访问到_source、_title等} 向文档中添加tag字段： 1234567POST blog/_update/1{ &quot;script&quot;: { &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;ctx._source.tag=[\\&quot;java\\&quot;,\\&quot;php\\&quot;]&quot; }} 1234567POST blog/_update/1{ &quot;script&quot;: { &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;ctx._source.tag.add(\\&quot;js\\&quot;)&quot; }} 用if、else语句：如果tags里面含有Java，就删除这个文档。 1234567POST blog/_update/1{ &quot;script&quot;: { &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;if(ctx._source.tag.contains(\\&quot;java\\&quot;)){ ctx.op = \\&quot;delete\\&quot;}else{ctx.op=\\&quot;none\\&quot;}&quot; }} 查询更新通过条件查询到文档，然后再去更新 12345678910111213讲title中包含666的文档的内容修改为888POST blog/_update_by_query{ &quot;script&quot;: { &quot;source&quot;: &quot;ctx._source.content=\\&quot;888\\&quot;&quot;, &quot;lang&quot;: &quot;painless&quot; }, &quot;query&quot;: { &quot;term&quot;: { &quot;title&quot;:&quot;777&quot; } }} 删除文档1DELETE blog/_doc/1hrCXHoBjgQwlk7iYnof 查询删除123456789删除 title 中包含 666 的文档POST blog/_delete_by_query{ &quot;query&quot;:{ &quot;term&quot;:{ &quot;title&quot;:&quot;666&quot; } }} 123456789删除某一个索引下的所有文档POST blog/_delete_by_query{ &quot;query&quot;:{ &quot;match_all&quot;:{ } }} 批量操作es 中通过 Bulk API 可以执行批量索引、批量删除、批量更新等操作。 首先需要将所有的批量操作写入一个 JSON 文件中，然后通过 POST 请求将该 JSON 文件上传并执行。 1curl -XPOST &quot;http://localhost:9200/user/_bulk&quot; -H &quot;content-type:application/json&quot; --data-binary @aaa.json 执行完成后，就会创建一个名为 user 的索引，同时向该索引中添加一条记录，再修改该记录，最终结果如下： 文档路由你的数据到底存在哪一个分片上？ 1GET _cat/shards/blog?v 可以查看文档在哪个分片上 1234567891011index shard prirep state docs store ip nodeblog 4 p STARTED 0 7.9kb 127.0.0.1 192.168.0.107blog 4 r UNASSIGNED blog 1 p STARTED 2 8.3kb 127.0.0.1 192.168.0.107blog 1 r UNASSIGNED blog 2 p STARTED 1 3.6kb 127.0.0.1 192.168.0.107blog 2 r UNASSIGNED blog 3 p STARTED 1 3.6kb 127.0.0.1 192.168.0.107blog 3 r UNASSIGNED blog 0 p STARTED 1 3.6kb 127.0.0.1 192.168.0.107blog 0 r UNASSIGNED es 中的路由机制是通过哈希算法，将具有相同哈希值的文档放到一个主分片中，分片位置的计算方式如下： shard=hash(routing) % number_of_primary_shards routing 可以是一个任意字符串，es 默认是将文档的 id 作为 routing 值，通过哈希函数根据 routing 生成一个数字，然后将该数字和分片数取余，取余的结果就是分片的位置。 默认的这种路由模式，最大的优势在于负载均衡，这种方式可以保证数据平均分配在不同的分片上。但是他有一个很大的劣势，就是查询时候无法确定文档的位置，此时它会将请求广播到所有的分片上去执行。另一方面，使用默认的路由模式，后期修改分片数量不方便。 然开发者也可以自定义 routing 的值，方式如下： 1234PUT blog/_doc/d?routing=javaboy{ &quot;title&quot;:&quot;d&quot;} 如果文档在添加时指定了 routing，则查询、删除、更新时也需要指定 routing。 1GET blog/_doc/d?routing=javaboy 自定义 routing 有可能会导致负载不均衡，这个还是要结合实际情况选择。 典型场景： 对于用户数据，我们可以将 userid 作为 routing，这样就能保证同一个用户的数据保存在同一个分片中，检索时，同样使用 userid 作为 routing，这样就可以精准的从某一个分片中获取数据。 ES的版本控制当我们使用 es 的 API 去进行文档更新时，它首先读取原文档出来，然后对原文档进行更新，更新完成后再重新索引整个文档。不论你执行多少次更新，最终保存在 es 中的是最后一次更新的文档。但是如果有两个线程同时去更新，就有可能出问题。 要解决问题，就是锁。在 es 中，实际上使用的就是乐观锁。 版本控制es6.7之前 在 es6.7 之前，使用 version+version_type 来进行乐观并发控制。根据前面的介绍，文档每被修改一个，version 就会自增一次，es 通过 version 字段来确保所有的操作都有序进行。 version 分为内部版本控制和外部版本控制。 内部版本es 自己维护的就是内部版本，当创建一个文档时，es 会给文档的版本赋值为 1。 每当用户修改一次文档，版本号就回自增 1。 如果使用内部版本，es 要求 version 参数的值必须和 es 文档中 version 的值相当，才能操作成功。 外部版本也可以维护外部版本。 在添加文档时，就指定版本号： 1234PUT blog/_doc/1?version=200&amp;version_type=external{ &quot;title&quot;:&quot;2222&quot;} 以后更新的时候，版本要大于已有的版本号。 vertion_type=external 或者 vertion_type=external_gt 表示以后更新的时候，版本要大于已有的版本号。 vertion_type=external_gte 表示以后更新的时候，版本要大于等于已有的版本号。 最新方案（Es6.7 之后）现在使用 if_seq_no 和 if_primary_term 两个参数来做并发控制。 seq_no 不属于某一个文档，它是属于整个索引的（version 则是属于某一个文档的，每个文档的 version 互不影响）。现在更新文档时，使用 seq_no 来做并发。由于 seq_no 是属于整个 index 的，所以任何文档的修改或者新增，seq_no 都会自增。 现在就可以通过 seq_no 和 primary_term 来做乐观并发控制。 1234PUT blog/_doc/2?if_seq_no=5&amp;if_primary_term=1{ &quot;title&quot;:&quot;6666&quot;} 倒排索引倒排索引是 es 中非常重要的索引结构，是从文档词项到文档 ID 的一个映射过程。 “正排索引”我们在关系型数据库中见到的索引，就是“正排索引”。 关系型数据库中的索引如下，假设我有一个博客表： id 作者 标题 内容 1 binshow 哈哈哈哈 xxxx 2 zkd 嘻嘻嘻嘻 xxxxx 我们可以针对这个表建立索引（正排索引）： 索引 内容 1 xxxx 2 xxxxx 哈哈哈哈 xxxx 嘻嘻嘻嘻 xxxxx 当我们通过 id 或者标题去搜索文章时，就可以快速搜到。 但是如果我们按照文章内容的关键字去搜索，就只能去内容中做字符匹配了。为了提高查询效率，就要考虑使用倒排索引。 倒排索引倒排索引就是以内容的关键字建立索引，通过索引找到文档 id，再进而找到整个文档。 索引 文档id=1 文档id=2 Java ✅ es ✅ ✅ 索引 ✅ php ✅ 一般来说，倒排索引分为两个部分： 单词词典（记录所有的文档词项，以及词项到倒排列表的关联关系） 倒排列表（记录单词与对应的关系，由一系列倒排索引项组成，倒排索引项指：文档 id、词频（TF）（词项在文档中出现的次数，评分时使用）、位置（Position，词项在文档中分词的位置）、偏移（记录词项开始和结束的位置）） 当我们去索引一个文档时，就回建立倒排索引，搜索时，直接根据倒排索引搜索。","link":"/2021/06/29/ElasticSearch%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E6%88%98/"},{"title":"JavaSE-五-异常处理","text":"程序在运行的过程中肯定会发生各种各样的异常，当发生异常时，我们该如何处理呢？ 异常处理 异常分类 捕获异常 finally子句 异常处理错误用户期望在程序运行在出现错误时，程序能够采用一些理智的行为，比如： 返回到一种安全的状态，并能够让用户执行一些其他的命令 允许用户保存所有操作的结果，并以妥善的方式来终止程序 异常处理的任务就是将控制权从错误产生的地方转移到能够处理这种情况的错误处理器。 在Java中，如果某个方法不能够采用正常的途径完整它的任务，就可以通过另外一个路径退出方法。在这种情况下，方法并不返回任何值，而是抛出( throw) 一个封装了错误信息的对象。需要注意的是，这个方法将会立刻退出，并不返回任何值。此外，调用这个方法的代码也将无法继续执行，取而代之的是，异常处理机制开始搜索能够处理这种异常状况的异常处理器(exceptionhandler)。 异常分类异常具有自己的语法和特定的继承结构，异常对象都是Throwable类的一个实例。如果Java内置的异常类不能满足需求，用户还可以创建自己的异常类。 Throwable类有两个子类，分别是 Error和 Exception： Error类描述了Java运行时系统的内部错误和资源耗尽错误。应用程序不应该抛出这种类型的对象。如果出现了这样的内部错误，除了通告给用户，并尽力使程序安全地终止之外，再也无能为力了。 Exception层次结构又分解为两个分支:一个分支派生于RuntimeException;另一个分支包含其他异常。划分两个分支的规则是:由程序错误导致的异常属于RuntimeException ;而程序本身没有问题，但由于像I/O错误这类问题导致的异常属于其他异常。 一些常见的RuntimeException（如果出现了基本都是你的问题，类似于）: 错误的类型转换 数组访问越界 访问null指针 还有一种说法是 Error和RuntimeException类的所有异常称为 非受检异常（unchecked），所有其他的异常称为受检异常（check）。编译器将核查是否为所有的受检异常提供异常处理器。 声明受检异常在程序运行时，遇到了如下的情况： 读取文件的代码发现读取的文件不存在 处理过程中IO发生了异常 12345//java.io.FileInputStream#FileInputStream(java.io.File)// 根据给定的String参数产生一个FileInputStream对象，但是如果没有这个文件就会抛出异常 FileNotFoundExceptionpublic FileInputStream(File file) throws FileNotFoundException { //...} 自己编写代码的时候，遇到下面4种情况需要抛出异常： 调用一个抛出受查异常的方法，例如，FileInputStream构造器。 程序运行过程中发现错误，并且利用throw语句抛出一个受查异常。 程序出现错误，例如，a[-1]=0 会抛出一个ArrayIndexOutOfBoundsException这样的非受查异常。 Java虚拟机和运行时库出现的内部错误。 编写异常类1234567891011121314151617181920212223242526272829303132public class FileFormatException extends IOException { public FileFormatException(){} //构造一个新的 throwable对象。带有特定的详细描述信息。这样超类的toString方法就会打印出这些详细信息，调试中很有用 public FileFormatException(String gripe){ super(gripe); }}public class Test { //main方法也没有处理这个异常，继续往上抛 public static void main(String[] args) throws FileFormatException { read(0); } //方法需要抛出这个异常 public static void read(int x ) throws FileFormatException { if (x == 0){ throw new FileFormatException(&quot;x不能为0，格式错误&quot;); } System.out.println(x); }}运行之后就会发生：Exception in thread &quot;main&quot; exception.FileFormatException: x不能为0，格式错误 at exception.Test.read(Test.java:13) at exception.Test.main(Test.java:6) 捕获异常如果某个异常发生的时候没有任何地方进行捕获。那程序就会终止执行，并在控制台上打印出异常信息。包括异常的类型和堆栈的内容等。如上面代码块所示。 捕获异常需要通过try/catch语句块(可以对不同类型的异常做不同的处理，多个catch)： 1234567891011121314151617181920public static void main(String[] args) { try { read(0); //业务代码 //如果在这里的任何代码抛出了一个catch子句中说明的异常类，那么程序将会 // 1. 跳过try语句块中的其他代码 // 2. 执行catch子句中的处理器代码 } catch (FileFormatException e) { //handle for this Exception System.out.println(&quot;read的方法参数不能为0&quot;); } } public static void read(int x ) throws FileFormatException { if (x == 0){ throw new FileFormatException(&quot;x不能为0，格式错误&quot;); } System.out.println(x); } finally子句当代码抛出一个异常时就会终止方法中剩余代码的处理。如果方法获得了一些本地资源，又如何保证在退出方法之前可以回收掉所有的资源呢？– finally子句。 不管是否有异常被捕获，finally子句中 的代码都会被执行： 1234567891011121314151617public static void main(String[] args) throws FileNotFoundException { InputStream in = new FileInputStream(&quot;aa&quot;); try{ // ====== 1 ===== // code might throw exceptions // ====== 2 ===== }catch (IOException e){ // ===== 3 ===== // show error message // ===== 4 ===== }finally { // ===== 5 ===== in.close(); } // ===== 6 ===== } 代码无异常，首先执行try语句块的全部代码，然后执行finally子句中的代码，再执行后面的语句。 1256 抛出了一个可以catch子句捕获的异常。程序会执行try语句块的代码直到发生异常，再跳过剩余代码转而执行catch子句中的代码 如果catch子句中没有抛出异常，执行顺序为 13456 如果catch子句中也抛出了异常，异常将会抛回这个方法的调用者。执行顺序为 135 抛出了一个不在catch子句中的异常，程序会执行try语句块的代码直到发生异常，再跳过剩余代码转而执行finally子句中的代码，并将异常抛给这个方法的调用者，执行顺序为15 建议这样写： 1234567891011121314151617181920public static void main(String[] args) throws FileNotFoundException { InputStream in = new FileInputStream(&quot;aa&quot;); try{ try{ //业务代码 } finally { in.close(); } }catch (IOException e){ // show error message } // 内层的 try语句只有一个职责就是确保关闭输入流 // 外层的 try语句也只有一个职责，就是确保报告出现的错误。这样设计还会报告finally子句中出现的错误 } 当finally子句中包含return语句时会出现意外情况。当利用return语句从try语句块中退出，在方法返回前finally子句的内容还是会被执行。如果finally子句中也有一个return语句，就会覆盖掉原来的返回值。 1234567891011System.out.println(f(4)); //16System.out.println(f(2)); //0 public static int f(int x){ try { int r = x * x; return r; }finally { if (x == 2) return 0; }} EffectiveJava中对异常的规范 异常是专门为异常情况设计的。不要用来做普通的流控制 对于可以恢复的情况使用受检异常，编程错误使用运行时异常。 如果期望调用者如果期望调用者可以对异常进行恢复，就应该使用受检异常。抛出一个受检异常，强迫调用者必须在一个catch块里处理这个异常，或者向外抛出这个异常。对于可以恢复的情况，抛出受检异常，而对于程序错误，抛出非受检的异常。无法确定是否可以恢复的时候，抛出非受检异常。不要定义任何的既不是受检异常也不是运行时异常得的可抛出结构。给你的受检异常提供一些方法来帮助恢复异常。 避免受检异常的不必要的使用 优先使用标准的异常 抛出和抽象对应的异常 为每个方法抛出的所有异常建立文档 在细节信息中包含失败-捕获信息 努力保持失败的原子性 不要忽略异常","link":"/2021/08/03/JavaSE-%E4%BA%94-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"},{"title":"JavaSE-四-接口与内部类","text":"接口设计 接口（Interface）技术主要用来描述类具有什么功能，而并不给出每个功能的具体实现。 接口概念如果类遵从某个特定接口，那么就要履行这项服务。比如Arrays类中的sort方法可以对对象数组进行排序，但有一个要求就是对象所属的类必须实现 了Comparable接口。任何实现Comparable接口的类都需要包含compareTo方法，如果没有给出具体的泛型的话，这个方法的参数是一个Object对象，返回一个整型数值。 1234567891011121314java.util.Arrays#sort(T[], java.util.Comparator&lt;? super T&gt;) /**Sorts the specified array of objects according to the order induced by the specified comparator. All elements in the array must be mutually comparable by the specified comparator (that is, c.compare(e1, e2) must not throw a ClassCastException for any elements e1 and e2 in the array).*/ public static &lt;T&gt; void sort(T[] a, Comparator&lt;? super T&gt; c) { if (c == null) { sort(a); } else { if (LegacyMergeSort.userRequested) legacyMergeSort(a, c); else TimSort.sort(a, 0, a.length, c, null, 0, 0); } } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class Employee implements Comparable&lt;Employee&gt; { //instance fields private String name; private double salary; private LocalDate hireDay; public Employee(String name, double salary) { this.name = name; this.salary = salary; } public Employee(String name, double salary, int year , int mouth , int day) { this.name = name; this.salary = salary; this.hireDay = LocalDate.of(year,mouth,day); } public String getName() { return name; } public double getSalary() { return salary; } public LocalDate getHireDay() { return hireDay; } public void raiseSalary(double byPercent){ double raise = this.salary * byPercent / 100; this.salary += raise; } @Override // 按照薪水排序 public int compareTo(Employee o) { return Double.compare(salary , o.getSalary()); }}// Test给Employee排序public class Test { public static void main(String[] args) { Employee[] staff = new Employee[3]; staff[0] = new Employee(&quot;a&quot;,100); staff[1] = new Employee(&quot;b&quot;,200); staff[2] = new Employee(&quot;c&quot;,300); // 如果 Employee 未实现 Comparable 接口，排序的时候就会爆出如下异常： // data.Employee cannot be cast to java.lang.Comparable Arrays.sort(staff); for (Employee e : staff){ System.out.println(&quot;name = &quot; + e.getName() + &quot;, salary = &quot; + e.getSalary()); } }} 接口的特性 接口中的所有方法都自动属于public 接口可以包含多个方法，可以定义常量（不推荐，因为任何实现接口的类都会继承这些常量） 接口中绝不能含有实例域。 接口中可以含有默认的方法实现 接口不能通过new实例化，但是可以声明一个接口的变量引用实现了接口的类对象。 12Comparable&lt;Object&gt; objectComparable = new Comparable&lt;&gt;();//error Comparable x = new Employee(&quot;d&quot;,400); 接口也可以继承。 接口和抽象类为什么不直接把Comparable设计成抽象类呢？ Java中只能实现单继承，但是可以实现多个接口。 静态方法Java8之后允许在接口中增加静态方法了。 目前为止通用的做法都是将静态方法放在伴随类中。比如标准库中的Collection/Collections或Path/Paths。 默认方法实现可以在接口中的方法提供一个默认实现，必须要用default修饰符来标记。 12345//java.util.Collection#stream//Collection 接口的默认对流的实现：default Stream&lt;E&gt; stream() { return StreamSupport.stream(spliterator(), false); } 默认实现的方法可以不在接口实现类中重写。为什么会这样发展呢？ 是为了兼容以前的代码，否则如果在接口中添加新的方法，之前的接口实现类都要重写该方法。 解决默认方法冲突情景：如果在一个接口中将一个方法定义为默认方法，然后又在父类或另一个接口中定义了同样的方法： 超类优先（确保兼容性） 接口冲突 接口实例接口和回调回调( callback)是一种常见的程序设计模式。在这种模式中，可以指出某个特定事件发生时应该采取的动作。例如，可以指出在按下鼠标或选择某个菜单项时应该采取什么行动。然而，由于至此还没有介绍如何实现用户接口，所以只能讨论一些与上述操作类似，但比较简单的情况。 123456789101112131415161718192021222324252627/** * @author shengbinbin * 定时器测试 */public class TimerTest { public static void main(String[] args) { TimePrinter listener = new TimePrinter(); Timer timer = new Timer(1000, listener); timer.start(); //计时器开始 JOptionPane.showMessageDialog(null,&quot;Quit program?&quot;); //展示仪表盘 System.exit(0); }}/** * @author shengbinbin * 打印时间的类,实现了监听事件的接口，重写了事件发生后的回调方法 */public class TimePrinter implements ActionListener { @Override public void actionPerformed(ActionEvent e) { System.out.println(&quot;At the tone , the time is &quot; + new Date()); Toolkit.getDefaultToolkit().beep();//响一声 }} Cloneable接口Cloneable接口指示一个类提供了一个安全的clone方法。但是clone方法是Object类的一个protected方法。 Object类如何实现clone方法的，它对这个对象一无所知，所以只能逐个域的进行拷贝。如果对象中的所有数据域都是数值或其他基本类型，拷贝这些域没有任何问题，但是如果对象包含子对象的引用，拷贝域就会得到相同子对象的另一个引用，这样的话原对象和克隆的对象仍然会共享一些信息。 默认的克隆操作是浅拷贝：并不会克隆对象中引用的其他对象。如果原对象和浅克隆的对象共享的子对象是不可变的（比如String）这种情况下是安全的。 不给通常的子对象都是可变的，必须要重新定义clone方法来实现一个深拷贝，同时克隆出所有的子对象。比如在这个例子中hireDay是一个可变的 Date类，所以也需要克隆（如果是个不可变的LocalDate类的实例就不需要处理了）。 对于每一个类，需要确定： 实现Cloneable接口，指定public修饰符。 默认的clone方法是否满足要求 是否在可变的子对象上调用clone来修补默认的clone方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Employee implements Comparable&lt;Employee&gt;, Cloneable { // instance fields private String name; private double salary; private Date hireDay; public Employee(String name, double salary) { this.name = name; this.salary = salary; } public String getName() { return name; } public double getSalary() { return salary; } public Date getHireDay() { return hireDay; } public void setHireDay(Date hireDay) { this.hireDay = hireDay; } public void raiseSalary(double byPercent) { double raise = this.salary * byPercent / 100; this.salary += raise; } @Override public int compareTo(Employee o) { return Double.compare(salary, o.getSalary()); } @Override public Employee clone() throws CloneNotSupportedException { Employee cloned = (Employee) super.clone(); cloned.hireDay = (Date) hireDay.clone(); // 建立深拷贝 return cloned; }}","link":"/2021/08/02/JavaSE-%E5%9B%9B-%E6%8E%A5%E5%8F%A3%E4%B8%8E%E5%86%85%E9%83%A8%E7%B1%BB/"},{"title":"JavaSE(二)对象和类","text":"Java是一门面向对象（OOP）的语言，本节介绍了： oop概述 使用预定义类和用户自定义类 静态字段和方法 方法参数 对象构造 类设计技巧 对象和类OOP设计概述Java是面向对象的。面向对象的程序是由对象组成的，每个对象都包含了对用户公开的特定功能部分和隐藏的实现部分。程序中的很多对象来自标准库，还有一些是自定义的。 OOP将数据放在第一位，然后再考虑操作数据的算法。 类 类是构造对象的模板。由类构造对象的过程称为创建类的实例。 封装( encapsulation,有时称为数据隐藏)是与对象有关的-一个重要概念。从形式上看，封装不过是将数据和行为组合在-一个包中，并对对象的使用者隐藏了数据的实现方式。对象中的数据称为实例域( instance field)，操纵数据的过程称为方法( method)。对于每个特定的类实例(对象)都有一组特定的实例域值。这些值的集合就是这个对象的当前状态( state)。无论何时，只要向对象发送-一个消息，它的状态就有可能发生改变。实现封装的关键在于绝对不能让类中的方法直接地访问其他类的实例域。程序仅通过对象的方法与对象数据进行交互。封装给对象赋予了“黑盒”特征，这是提高重用性和可靠性的关键。这意味着-一个类可以全面地改变存储数据的方式，只要仍旧使用同样的方法操作数据。 对象的三个特性要想使用OOP，- -定要清楚对象的三个主要特性:●对象的行为(behavior)一-可以对对象施加哪些操作，或可以对对象施加哪些方法?●对象的状态(state)一当施加那些方法时，对象如何响应?●对象标识(identity) 一如何辨别具有相同行为与状态的不同对象?同一个类的所有对象实例，由于支持相同的行为而具有家族式的相似性。对象的行为是月可调用的方法定义的。 识别类类与类直接的关系使用预定义的类使用用户自定义的类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package data;import java.time.LocalDate;public class Employee { //instance fields private String name; private double salary; private LocalDate hireDay; public Employee(String name, double salary, int year ,int mouth ,int day) { this.name = name; this.salary = salary; this.hireDay = LocalDate.of(year,mouth,day); } public String getName() { return name; } public double getSalary() { return salary; } public LocalDate getHireDay() { return hireDay; } public void raiseSalary(double byPercent){ double raise = salary * byPercent / 100; salary += raise; }}package data;public class EmployeeTest { public static void main(String[] args) { //1. 构建一个Employee数组 Employee[] employees = new Employee[3]; employees[0] = new Employee(&quot;binshow&quot;,20000,1997,1,8); employees[1] = new Employee(&quot;zkd&quot;,30000,1996,10,22); employees[2] = new Employee(&quot;aaa&quot;,10000,1997,7,18); //2. 将每个员工的工资都提高 for (Employee e : employees){ e.raiseSalary(5); } //3. 打印每个员工的信息 for (Employee e : employees){ System.out.println(&quot;name = &quot; + e.getName() + &quot;,salary = &quot; + e.getSalary() + &quot;,hireday = &quot; + e.getHireDay()); } }} 构造器与类重名，构造Employee类实例的对象时，构造器会运行将实例域初始化为所定义的状态 构造器与类重名 每个类可以有1个或多个构造器 构造器可以有0，1或多个参数 构造器没有返回值 构造器总是伴随着new操作一起调用 隐式参数和显示参数 12345678910 Employee employee = new Employee(&quot;binshow&quot;,20000,1997,1,8)employee.raiseSalary(5); //employee 为隐式参数 、 5 为显示参数//在每个方法中，可以用this表示隐式参数，比如下面：public void raiseSalary(double byPercent){ double raise = this.salary * byPercent / 100; this.salary += raise; } 封装的优点：getName、getSalary 、getHireDay等 12345678910111213141516 //返回实例域值，称为域访问器 public String getName() { return name; } public double getSalary() { return salary; } public LocalDate getHireDay() { return hireDay; }1. name是一个只读域，一旦在构造器中设置完毕，没有任何一个方法可以对其修改，确保了name域不会受到外界的破坏。2. 在本类中 salary不是只读域，但是它只能用raiseSalary方法来修改。一旦这个域出现错误，只要找到这个方法就可以调试了。 不能返回引用可变对象的访问器方法 1234567891011121314151617181920public class Person { // 如果用Date类型的域 private Date birthday; public Person(Date birthday) { this.birthday = birthday; } public Date getBirthday() { return birthday; // 这样是有问题的 }}//test Date date = new Date(); System.out.println(date); Person person = new Person(date); Date d = person.getBirthday(); d.setTime(d.getTime() - 500000); //Date对象是可变的，破坏了封装性。因为 d 和 person.birthday 指向的是同一个对象 System.out.println(person.getBirthday()); 12345678910111213//正确做法是：首先对他进行克隆,克隆完之后的对象是存在另一个位置的副本public class Person { private Date birthday; public Person(Date birthday) { this.birthday = birthday; } public Date getBirthday() { return (Date) birthday.clone(); }} 基于类的访问权限：一个方法可以访问所属类的所有对象的私有数据。 私有方法 final实例域：final修饰实例域表明在构造该类对象的时候必须初始化这样的域。并且在后面的操作中不能被修改。 final修饰符大部分都应用于修饰基本数据类型或者不可变的类（比如String类，类中的每个方法都不会改变其对象） 123456789public class Person { private final String name; //必须要有基于name的构造方法，且不能有对应的set方法 public Person(String name) { this.name = name; }} 静态域和静态方法 静态域： 1234567public class Person { //静态域，是类所共有的，而不是属于单个对象 // 新建100个person实例，每个person都有自己的id，但是它们共有一个personId private static int personId; private int id ;} 静态常量：static + final 共同修饰的域即为常量。 静态方法：可以认为是没有隐式参数的方法。不能对对象实施操作的方法，可以通过类名.方法来实施。 工厂方法： 1 方法参数（值传递和引用传递）将参数传递给方法(或函数)的一些专业术语： 按值调用( call by value)表示方法接收的是调用者提供的值。 按引用调用( call by reference)表示方法接收的是调用者提供的变量地址。 一个方法可以修改传递引用所对应的变量值，而不能修改传递值调用所对应的变量值。 Java程序设计语言总是采用按值调用。也就是说，方法得到的是所有参数值的一个拷贝，特别是，方法不能修改传递给它的任何参数变量的内容。 12double percent = 10; employees[1].raiseSalary(percent); //这个方法调用完之后，percent的值还是10 1234567891011121314151617 double percent = 10; tripleValue(percent); System.out.println(percent); //还是 10 也就是说方法不能改变基本数据类型的参数 private static void tripleValue(double x){ x = 3 * x; }分析具体的执行过程： 1. x被初始化为percent值的一个拷贝(也就是10) 2. x被乘以3后等于30。但是percent仍然是10。 3. 这个方法结束之后，参数变量x不再使用 然而，方法参数共有两种类型:●基本数据类型(数字、布尔值)。●对象引用。 12345678910111213Employee binshow = new Employee(&quot;binshow&quot;, 20000, 1997, 1, 8);System.out.println(&quot;tripleValue方法调用前：&quot; + binshow.getSalary()); // 20000.0tripleValue(binshow);System.out.println(&quot;tripleValue方法调用后：&quot; + binshow.getSalary()); // 60000.0private static void tripleValue(Employee x){ x.raiseSalary(200); }分析具体的执行过程:1. x被初始化为binshow值的拷贝，这里是一个对象的引用2. raiseSalary 方法应用于这个对象引用。x和binshow同时引用的那个Employee对象的薪金提高了200%。3. 方法结束后，参数变量x不再使用。当然，对象变量binshow继续引用那个薪金增至3倍的雇员. 方法得到的是对象引用的拷贝，对象引用和其他的拷贝同时引用同一个对象。 Java是引用调用吗？其实不是 123456789101112Employee binshow = new Employee(&quot;binshow&quot;, 20000, 1997, 1, 8);Employee zkd = new Employee(&quot;zkd&quot;, 30000, 1996, 10, 22);swap(binshow,zkd);//如果java采用的是引用传递，这一步之后binshow和zkd应该会交换数据,实际上并没有System.out.println(binshow.getName()); //binshowSystem.out.println(zkd.getName()); //zkdpublic static void swap(Employee a , Employee b){ Employee tem = a; a = b; b = tem; } 这个过程说明: Java程序设计语言对对象采用的不是引用调用，实际上，对象引用是按值传递的。下面总结一-下Java中方法参数的使用情况:●一个方法不能修改–个基本数据类型的参数(即数值型或布尔型)。●一个方法可以改变-一个对象参数的状态。●一个方法不能让对象参数引用-一个新的对象。 1234567891011121314151617181920212223242526272829303132333435363738/** * 方法参数的测试类 */public class ParamTest { public static void main(String[] args) { System.out.println(&quot;Test tripleValue:方法不能改变基础数据类型&quot;); double precent = 10; System.out.println(&quot;before: precent = &quot; + precent); tripleValue(precent); System.out.println(&quot;after: precent = &quot; + precent); System.out.println(&quot;Test tripleSalary：方法可以改变对象类型的状态&quot;); Employee binshow = new Employee(&quot;binshow&quot;, 20000); System.out.println(&quot;before: binshow的salary = &quot; + binshow.getSalary()); tripleSalary(binshow); System.out.println(&quot;after: binshow的salary = &quot; + binshow.getSalary()); } private static void tripleValue(double x){ x = 3 * x; System.out.println(&quot;End the Method, x = &quot; + x); } private static void tripleSalary(Employee x){ x.raiseSalary(200); System.out.println(&quot;End the Method, salary = &quot; + x.getSalary()); } public static void swap(Employee a , Employee b){ Employee tem = a; a = b; b = tem; }} 对象构造包类路径注释类设计的技巧继承异常异常处理的任务是将控制权从错误产生的地方转移给能够处理这种情况的错误处理器。 异常分类异常对象都是派生于Throwable类的一个实例。如果Java内置的异常类不能满足需求，用户还可以创建自己的异常类。 由程序错误导致的异常属于 RuntimeException。而程序本身没有问题，发生像IO错误这类的异常属于其他异常。 RunTimeException分类： 错误的类型转换 数组访问越界 访问null指针。（使用变量之前需要检测是否为null来避免空指针） 其他异常分类： 试图在文件尾部后面读取数据 试图打开不存在的文件 试图根据给定的字符串查找Class对象，但这个字符串表示的类并不存在。 Error和RunTimeException属于非受检异常。 声明受检异常一个方法不仅需要告诉编译器将要返回什么值，还要告诉编译器有可能发生什么错误。也就是说一个方法必须要声明所有可能抛出的受检异常。而非受检异常要么不可控制Error，要么就应该避免发生RunTimeException 12345java.io.FileInputStream#FileInputStream(java.lang.String) //构造方法会抛出异常：文件找不到异常public FileInputStream(String name) throws FileNotFoundException { this(name != null ? new File(name) : null); } 自定义异常类123456789101112131415161718192021222324252627282930313233public class FileFormatException extends IOException { public FileFormatException(){} //构造一个新的 throwable对象。带有特定的详细描述信息 public FileFormatException(String gripe){ super(gripe); }}public class Test { //main方法也没有处理这个异常，继续往上抛 public static void main(String[] args) throws FileFormatException { read(0); } //方法需要抛出这个异常 public static void read(int x ) throws FileFormatException { if (x == 0){ throw new FileFormatException(&quot;x不能为0，格式错误&quot;); } System.out.println(x); }}运行之后就会发生：Exception in thread &quot;main&quot; exception.FileFormatException: x不能为0，格式错误 at exception.Test.read(Test.java:13) at exception.Test.main(Test.java:6) 捕获异常如果某个异常发生的时候没有任何地方进行捕获。那程序就会终止执行，并在控制台上打印出异常信息。包括异常的类型和堆栈的内容等。如上面代码块所示。 捕获异常需要通过try/catch语句块： 1234567891011121314151617181920public static void main(String[] args) { try { read(0); //业务代码 //如果在这里的任何代码抛出了一个catch子句中说明的异常类，那么程序将会跳过try语句块中的其他代码 // 执行catch子句中的处理器代码 } catch (FileFormatException e) { //handle for this Exception System.out.println(&quot;read的方法参数不能为0&quot;); } } public static void read(int x ) throws FileFormatException { if (x == 0){ throw new FileFormatException(&quot;x不能为0，格式错误&quot;); } System.out.println(x); } finally子句当代码抛出一个异常时就会终止方法中剩余代码的处理。如果方法获得了一些本地资源，又如何保证在退出方法之前可以回收掉所有的资源呢？– finally子句。 不管是否有异常被捕获，finally子句中 的代码都会被执行。 1234567891011121314151617public static void main(String[] args) throws FileNotFoundException { InputStream in = new FileInputStream(&quot;aa&quot;); try{ // ====== 1 ===== // code might throw exceptions // ====== 2 ===== }catch (IOException e){ // ===== 3 ===== // show error message // ===== 4 ===== }finally { // ===== 5 ===== in.close(); } // ===== 6 ===== } 代码无异常，首先执行try语句块的全部代码，然后执行finally子句中的代码，再执行后面的语句。 1256 抛出了一个可以catch子句捕获的异常。程序会执行try语句块的代码直到发生异常，再跳过剩余代码转而执行catch子句中的代码 如果catch子句中没有抛出异常，执行顺序为 13456 如果catch子句中也抛出了异常，异常将会抛回这个方法的调用者。执行顺序为 135 抛出了一个不在catch子句中的异常，程序会执行try语句块的代码直到发生异常，再跳过剩余代码转而执行finally子句中的代码，并将异常抛给这个方法的调用者，执行顺序为15 建议这样写： 1234567891011121314151617181920public static void main(String[] args) throws FileNotFoundException { InputStream in = new FileInputStream(&quot;aa&quot;); try{ try{ //业务代码 } finally { in.close(); } }catch (IOException e){ // show error message } // 内层的 try语句只有一个职责就是确保关闭输入流 // 外层的 try语句也只有一个职责，就是确保报告出现的错误。这样设计还会报告finally子句中出现的错误 } 当finally子句中包含return语句时会出现意外情况。当利用return语句从try语句块中退出，在方法返回前finally子句的内容还是会被执行。如果finally子句中也有一个return语句，就会覆盖掉原来的返回值。 1234567891011System.out.println(f(4)); //16System.out.println(f(2)); //0 public static int f(int x){ try { int r = x * x; return r; }finally { if (x == 2) return 0; }} 断言断言的概念断言机制允许在测试期间向代码中插入一些检查语句。当代码发布时，这些插入的检测语句会被自动移走。","link":"/2021/08/01/JavaSE-%E4%BA%8C-%E5%AF%B9%E8%B1%A1%E5%92%8C%E7%B1%BB/"},{"title":"JavaSE-泛型","text":"Java基础之泛型的使用 泛型的需求：在拥有泛型之前，你必须对每个从集合中读出来的对象进行类型转换。如果有人不小心插入了一个错误类型的对象，这个类型转换就会再运行时失败。使用泛型，你就可以告诉编译器，那些类型的对象允许加入这个集合。编译器会自动地为你进行转换，然后当你要插入一个错误类型的时候，在编译地时候就会报错 Item26 不要使用原生类型每一个泛型都包括一组参数类型，其构成方式如下：首先是类或者接口的名字，然后紧跟着用&lt;&gt;括起来的实际类型参数，实际类型参数和泛型中的形式类型参数对应 原生类型指的是没有任何类型参数的泛型类型的名称。 1234567891011121314151617181920212223242526272829// 只是为了兼容之前的代码,不建议直接使用原生类型SetSet set = new Set;// 错误的插入了不同类型的 数据类型，编译时不会报错set.add(1);set.add(&quot;String&quot;);set.add('a');同理：public static void main(String[] args) { List&lt;String&gt; strings = new ArrayList&lt;&gt;(); //1. 添加的时候不会报错，编译也能通过 unsafeAdd(strings , Integer.valueOf(43)); // safeAdd(strings , Integer.valueOf(43)); //2. 取出的时候就会发生类型转换异常 String s = strings.get(0); //java.lang.ClassCastException } //unsafeAdd 方法的参数是一个原生类型List。所以有风险，应该用参数化类型List来代替 private static void unsafeAdd(List list, Object o) { list.add(o); } private static void safeAdd(List&lt;String&gt; list, String o) { // 加上参数化类型之后，这边编译就会报错 list.add(o); } Item27 消除非受检的警告Item28 list优先于数组为什么呢？ 数组是协变的（covariant），泛型是非协变的。 1234567//1. 下面编译不报错，启动才报错// 由于Long是Object的子类，所以 Long[] 也是 Object[]的子类 Object[] objectArray = new Long[1]; objectArray[0] = &quot;abc&quot;;// java.lang.ArrayStoreException //2. 用list编译就报错了 List&lt;Object&gt; list = new ArrayList&lt;Long&gt;(); 数组是具体化的（reified），也就是说在运行的时候数组是知道其元素类型的，并且会强制执行类型限制。相反，泛型是通过泛型擦除来实现的，这就意味着只能在编译器强制执行类型限制，运行的时候是会擦除其元素信息的。类型擦除是为了允许泛型代码可一个之前没有泛型的老代码进行互用 当你在转换到数组类型时遇到泛型数组创建错误或者非受检转换异常的时候，最好的方法是使用集合类型List，而不是数组类型E[]。可能会牺牲一些性能，但是换回的却是更好的类型安全性和互用性。 12345678910111213141516171819202122232425// Chooser - a class badly in need of generics!// 调用这个类的choose方法时，返回的都是Object对象，需要手动进行类型准换public class Chooser { private final Object[] choiceArray; public Chooser(Collection choices) { choiceArray = choices.toArray(); } public Object choose() { Random rnd = ThreadLocalRandom.current(); return choiceArray[rnd.nextInt(choiceArray.length)]; }}public static void main(String[] args) { ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(1); list.add(2); list.add(3); Chooser chooser = new Chooser(list); Integer choose = (Integer) chooser.choose(); System.out.println(&quot;choose = &quot; + choose); } 为了避免进行可能出错的强制类型转换，引入泛型,但还是有点问题： 12345678910111213141516public class Chooser&lt;T&gt; { private final T[] choiceArray; public Chooser(Collection&lt;T&gt; choices) { //它不能确定在运行时的类型转换的安全性，因为程序不知道类型T代表的是什么 //泛型中的元素类型信息在运行时被擦除了 choiceArray = (T[]) choices.toArray(); } public Object choose() { Random rnd = ThreadLocalRandom.current(); return choiceArray[rnd.nextInt(choiceArray.length)]; }} 为了消除这个非受检转换警告，使用list来代替数组： 12345678910111213public class Chooser&lt;T&gt; { private final List&lt;T&gt; choiceArray; public Chooser(Collection&lt;T&gt; choices) { choiceArray = new ArrayList&lt;&gt;(choices); } public T choose() { Random rnd = ThreadLocalRandom.current(); return choiceArray.get(rnd.nextInt(choiceArray.size())); }} 数组和泛型有一些非常不同的类型规则，数组是协变和可具体化的；而泛型是非协变和擦除的。因此，数组可以提供运行时类型安全但是没有贬义是类型安全，而泛型却恰恰相反。一般来说，数组和泛型不能混用。如果你发现你自己混用了他们，并且出现了编译error或者warning。你的第一反应应该是用列表替换数组。 Item29 优先考虑泛型一个例子：未进行参数化的Stack 12345678910111213141516171819202122232425public class Stack { private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { elements = new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public Object pop() { if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} 将这个Stack使用泛型，同时还需要兼容之前的版本： 声明参数类型为E 将所有的Object改成E 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public class Stack&lt;E&gt; { private E[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { // 这里报错 。。 不能创建一个泛型数组。 elements = new E[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public E pop() { if (size == 0) throw new EmptyStackException(); E result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }}// 两种解决方案：1. elementst数组，再进行强制类型转换为E数组 public class Stack&lt;E&gt; { private E[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { // 这里报错 elements = (E[]) new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(E e) { ensureCapacity(); elements[size++] = e; } public E pop() { if (size == 0) throw new EmptyStackException(); E result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} 2. 将elements设为object数组，每次pop访问数组中的元素的时候进行强制类型转换 public class Stack&lt;E&gt; { private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { // 这里报错 elements = new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public E pop() { if (size == 0) throw new EmptyStackException(); E result = (E) elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} 第一种方法可读性更强：数组被声明为E[]，清晰地表示了它只包含E实例；并且更加简洁：在一个典型的泛型类中，你可以以在代码里看到很多次数组，第一中方法只需要进行一次转换（当数组创建的时候）；而第二种方法需要在每次读取数组元素的时候都进行转换。因此第一种方法要更受欢迎一些，在实际情况中，也用得更多些。但是第一种方法确实造成了”堆泄露“（详见Item32）：因为数组的运行时类型和编译时类型不一致（除非E刚刚也是Object）。有一些程序员对这点深恶痛绝，选择使用第二种方法。 Item30 优先考虑泛型方法基于参数化类型进行计算的静态工具方法通常来说都是泛型的。 1234//java.util.Collections#sort(java.util.List&lt;T&gt;, java.util.Comparator&lt;? super T&gt;)public static &lt;T&gt; void sort(List&lt;T&gt; list, Comparator&lt;? super T&gt; c) { list.sort(c); } 写泛型方法和泛型类型差不多： 1234567891011121314151617// 使用了原始类型，是有缺陷的。 // 虽然编译可以通过，但是有很多warning public static Set union(Set s1 , Set s2){ Set res = new HashSet(s1); res.addAll(s2); return res; }// 进行泛型的修改：//这三个set（包括输入参数和返回值）的类型都必须是完全一样的。你可以使用”有限制的通配符类型“来使得这个方法更加灵活public static &lt;E&gt; Set&lt;E&gt; union(Set&lt;E&gt; s1 , Set&lt;E&gt; s2){ Set&lt;E&gt; res = new HashSet&lt;E&gt;(s1); res.addAll(s2); return res; } Item31 使用有限制的通配符来提升API的灵活性参数化类型是非协变的 1234567891011121314151617181920212223public class Stack&lt;E&gt; { public Stack(); public void push(E e); public E pop(); public boolean isEmpty();}//1. 实现将一些元素全部放入到 stcak中。下面这样实现要求 Iterable 中的元素类型必须要和 Stack中的元素类型相同// pushAll method without wildcard type - deficient! public void pushAll(Iterator&lt;E&gt; src) { while (src.hasNext()){ push(src.next()); } }Stack&lt;Number&gt; numberStack = new Stack&lt;&gt;();List&lt;Integer&gt; list = Arrays.asList(1, 2, 3);Iterator&lt;Integer&gt; iterator = list.iterator();numberStack.pushAll(iterator); // Integer 虽然是 Number的子类型，但是这里还是编译不通过 Java语言提供了一种特殊的参数化类型，被称为”有限制通配符类型“，就是用来解决类似这种问题的。 pushAll方法的参数类型不应该是”E的Iterable接口“，而应该是“E的某个子类型的Iterable接口”，然后，这里有一个通配符类型，可以准确的表示这个意思： Iterable&lt;? extends E&gt;（其中extends关键字可能会造成一些误导，重申一下Item29里子类型的定义，每个类型都是自身的子类型，即使它没有继承它自己）。下面是使用这种类型修改后的pushAll方法： 123456// Wildcard type for a parameter that serves as an E producer public void pushAll(Iterator&lt;? extends E&gt; src) { while (src.hasNext()){ push(src.next()); } } 1234567891011121314151617// popAll method without wildcard type - deficient! public void popAll(Collection&lt;E&gt; dst) { while (!dst.isEmpty()) dst.add(pop()); }//同样的Collection&lt;Object&gt; objects = new ArrayList&lt;&gt;(); numberStack.popAll(objects); // 编译报错//可修改如下： super 表示传入的可以是 Collection的父类（包括它自己）public void popAll(Collection&lt;? super E &gt; dst) { while (!dst.isEmpty()) dst.add(pop()); } 这个结论很明显：为了最大化灵活度，对于代表生产者或者消费者的输入参数，使用通配符类型。如果这个输入参数即是生产者又是消费者，这个通配符类型就没什么用了。你需要的是准确的类型匹配，不需要使用任何的通配符。 这个助记符可以帮助你记住应该使用哪些通配符： PECS 表示producer-extends, consumer-super。 换句话说，如果这个参数化类型表示一个T生产者，使用&lt;? extends T&gt;；如果表示的是T消费者，使用 &lt;? super T&gt;。在我们的Stack的例子里，pushAll的src参数产生了E实例供Stack使用；所以src的合适类型是Iterable&lt;? extends T&gt;；popAll的dst参数消费每一个来自Stack的E实例，因此合适的类型是Collection&lt;? super E&gt;。PECS助记符刻画了直到通配符使用的基本原则。Naftalin和Wadler把它称之为“Get and Put Principle”。 12//前面的例子修改：public static &lt;E&gt; Set&lt;E&gt; union(Set&lt;? extends E&gt; s1, Set&lt;? extends E&gt; s2) 不要使用有限制的通配符类型作为返回类型 Item32 谨慎地并用泛型和可变参数Item33 考虑类型安全的异构容器","link":"/2021/07/29/JavaSE-%E6%B3%9B%E5%9E%8B/"},{"title":"JavaSE(三)类的设计和继承","text":"本节讲述了Java中的一个重要特性：继承。并以此来衍生了很多的概念和使用方法： 继承（类、父类、子类） Object类 泛型数组列表 对象包装器（自动装箱和拆箱） 可变参数的方法 枚举类 反射特性 继承的设计技巧 继承继承的意思是： 可以基于已存在的类构造一个新类。继承已存在的类就是复用(继承)这些类的方法和域。在此基础上，还可以添加一些新的方法和域，以满足新的需求。 比如Manager类继承了Employee类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public class Employee implements Comparable&lt;Employee&gt;, Cloneable { // instance fields private String name; private double salary; private LocalDate hireDay; public Employee(String name, double salary) { this.name = name; this.salary = salary; } public Employee(String name, double salary, int year , int mouth , int day) { this.name = name; this.salary = salary; this.hireDay = LocalDate.of(year,mouth,day); } public String getName() { return name; } public double getSalary() { return salary; } public LocalDate getHireDay() { return hireDay; } public void setHireDay(LocalDate hireDay) { this.hireDay = hireDay; } public void raiseSalary(double byPercent) { double raise = this.salary * byPercent / 100; this.salary += raise; } @Override public int compareTo(Employee o) { return Double.compare(salary, o.getSalary()); }}/** * @author shengbinbin * 经理类继承了员工类，获得了员工类的实例域和方法 * 1. 在子类中可以增加域、增加方法、覆盖父类的方法。但是绝不能删除继承的任何域或方法 */public class Manager extends Employee{ private double bonus; // 经理有自己的一个存储奖金信息的域 // 子类构造器 public Manager(String name, double salary, int year, int mouth, int day) { super(name, salary, year, mouth, day); // 通过super实现对超类构造器的调用，必须是子类构造器的第一个语句 bonus = 0; } // 覆盖父类中的返回薪水的方法 public double getSalary(){ double baseSalary = super.getSalary(); // super关键字代表的是父类 return baseSalary + bonus; } public void setBonus(double b){ bonus = b; }}// test Manager boss = new Manager(&quot;binshow&quot;,80000,1997,2,1); boss.setBonus(5000); //奖金5000 Employee[] staff = new Employee[3]; staff[0] = boss; staff[1] = new Employee(&quot;zkd&quot;,30000,1996,10,22); staff[2] = new Employee(&quot;aaa&quot;,10000,1997,7,18); for (Employee e : staff){ System.out.println(&quot;name= &quot; + e.getName() + &quot;, salary= &quot; + e.getSalary()); } 多态有一个用来判断是否应该设计为继承关系的简单规则，这就是“is-a” 规则 它表明子类的每个对象也是超类的对象。例如，每个经理都是雇员，因此，将Manager类设计为Employee类的子类是显而易见的，反之不然，并不是每一名雇员都是经理。 多态的意思是： 可以将一个子类的对象赋给超类变量。 1234567891011// 一个Employee变量即可以引用一个Employee类对象，也可以引用任何一个Employee的子类的对象 Employee e; e = new Employee(&quot;zkd&quot;,30000,1996,10,22); e = new Manager(&quot;binshow&quot;,20000,1996,1,2); Manager boss = new Manager(&quot;binshow&quot;,80000,1997,2,1); Employee[] staff = new Employee[3]; staff[0] = boss; // staff[0] 和 boss 引用的是同一个对象，但是编译器认为 staff[0] 是一个 Employee对象 boss.setBonus(5000); // 可以 // staff[0].setBonus(); // Error， 父类对象不能调用子类的方法。 理解方法调用下面假设要调用x.f(args),隐式参数x声明为类C的一个对象。下面是调用过程的详细过程: 编译器查看对象的声明类型和方法名。假设调用x.f(param)， 且隐式参数x声明为C类的对象。需要注意的是:有可能存在多个名字为f，但参数类型不一样的方法。例如，可能存在方法f(int) 和方法f(String)。 编译器将会一一列举 所有C类中名为f的方法和其超类中访问属性为public且名为f的方法(超类的私有方法不可访问)。至此，编译器已获得所有可能被调用的候选方法。 **接下来，编译器将查看调用方法时提供的参数类型。如果在所有名为f的方法中存在一个与提供的参数类型完全匹配，就选择这个方法。这个过程被称为重载解析( overloadingresolution)**。例如，对于调用x.f(“Hello” )来说，编译器将会挑选f(String), 而不是f(int)。由于允许类型转换，这个过程可能很复杂。如果找到和参数类型相匹配的方法，编译器就获得了需要调用的方法名字和参数类型。 如果是private方法、static 方法、final方法(有关final修饰符的含义将在下一节讲述)或者构造器，那么编译器将可以准确地知道应该调用哪个方法，我们将这种调用方式称为静态绑定( static binding)。与此对应的是，调用的方法依赖于隐式参数的实际类型，并且在运行时实现动态绑定。在我们列举的示例中，编译器采用动态绑定的方式生成一条调用f(String)的指令。 当程序运行，并且采用动态绑定调用方法时，虚拟机一定调用与x所引用对象的实际类型最合适的那个类的方法。假设x的实际类型是D，它是C类的子类。如果D类定义了方法f(String)，就直接调用它;否则，将在D类的超类中寻找f(String)，以此类推。 ps: 每次调用方法都要进行搜索，时间开销相当大。因此，**虚拟机预先为每个类创建了一个方法表( method table)**，其中列出了所有方法的签名和实际调用的方法。这样一来，在真正调用方法的时候只需要查找这个表就行了 12345678910111213141516171819202122232425262728293031323334// test Manager boss = new Manager(&quot;binshow&quot;,80000,1997,2,1); boss.setBonus(5000); //奖金5000 Employee[] staff = new Employee[3]; staff[0] = boss; staff[1] = new Employee(&quot;zkd&quot;,30000,1996,10,22); staff[2] = new Employee(&quot;aaa&quot;,10000,1997,7,18); for (Employee e : staff){ System.out.println(&quot;name= &quot; + e.getName() + &quot;, salary= &quot; + e.getSalary()); }// 以这里的e.getSalary() 为例：1. e是一个 Employee 类型，这个类只有一个getSalary方法且没有参数。所以不会有重载的解析。2. getSalary 不是 private、static、final方法，所以会采用动态绑定3. 虚拟机为 Employee类和Manager类 生成方法表:(省略了Object方法) Employee: getName() -&gt; Employee . getName() getSalary() -&gt; Employee . getSalary() getHireDay() -&gt; Employee . getHireDay() raiseSalary(double) -&gt; Emp1oyee. raiseSalary(doub1e) Manager: getName() -&gt; Employee.getName() getSalary() -&gt; Manager . getSalaryO getHireDay() -&gt; Emp1oyee.getHi reDay( raiseSalary(double) -&gt; Employee. raiseSalary(double) setBonus(double) -&gt; Manager . setBonus (doub1e) 4. 虚拟机提取e的实际类型的方法表，有可能是Employee、Manager的方法表，也可能是Employee的其他子类的方法表5. 最后搜索定义了getSalary 的类，调用该方法 如何阻止继承：final类和方法将类修饰成final表示该类不允许继承。同理将方法修饰成final表示在该类的子类中不允许重写该方法。 在早期的Java中，有些程序员为了避免动态绑定带来的系统开销而使用final 关键字。如果-一个方法没有被覆盖并且很短，编译器就能够对它进行优化处理，这个过程为称为内联( inlining)。例如，内联调用e.getName( )将被替换为访问e.name域。这是一项很有意义的改进，这是由于CPU在处理调用方法的指令时，使用的分支转移会扰乱预取指令的策略，所以，这被视为不受欢迎的。然而，如果getName在另外-一个类中被覆盖，那么编译器就无法:知道覆盖的代码将会做什么操作，因此也就不能对它进行内联处理了。 幸运的是，虚拟机中的即时编译器比传统编译器的处理能力强得多。这种编译器可以准确地知道类之间的继承关系，并能够检测出类中是否真正地存在覆盖给定的方法。如果方法很简短、被频繁调用且没有真正地被覆盖，那么即时编译器就会将这个方法进行内联处理。如果虚拟机加载了另外一个子类，而在这个子类中包含了对内联方法的覆盖，那么将会发生什么情况呢?优化器将取消对覆盖方法的内联。这个过程很慢，但却很少发生。 强制类型转换进行类型转换的 唯一原因是：暂时忽略对象的实际类型之后使用对象的全部功能 12345678910111213141516171819202122232425double x = 3.14; int n = (int) x; // 强制类型转换，将x的值转换成整数类型，舍弃了小数部分。 Manager manager = (Manager) staff[0]; // 将Employee对象 强制转换成 Manager对象。 // staff[0].setBonus(); // Error manager.setBonus(2000); // 暂时忽略对象的实际类型之后使用对象的全部功能 /** * 将一个值存入变量时，编译器将会检查是否允许这个操作。 * 1. 如果是将子类的引用赋给一个超类变量（前面所说的多态），编译器是运行的。 * 2. 但是如果是将一个超类的引用赋给一个子类的变量，就必须进行类型转换了，这样会进行运行时的检查 * 3. 最好在将超类转换成子类之前，使用instanceOf进行检查 */ Employee a = new Employee(&quot;zkd&quot;,30000,1996,10,22); Manager b = new Manager(&quot;binshow&quot;,20000,1996,1,2); a = b; //将子类的引用赋给一个超类变量 b = (Manager) a; //将一个超类的引用赋给一个子类的变量 a.getSalary(); b.getSalary(); //两个类的对象都可以正确的调用 getSalary方法。由于动态绑定 // 只有在使用Manager 类中的特有方法才需要进行类型转换。 b.setBonus(2); 抽象类在继承层次中，位于上层的类更具有通用性和抽象性。比如一个Employee、一个Student都是一个Person类。都有一个name域，就可以将这个name放在Person这个继承层次较高的通用超类中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107/** * @author shengbinbin * 抽象类Person */public abstract class Person { // 实例域 private String name; //构造器 public Person(String name){ this.name = name; } //对外提供的域访问器 public String getName(){ return name; } // 抽象方法，提供给子类去扩展 public abstract String getDescription(); }public class Employee extends Person implements Comparable&lt;Employee&gt;, Cloneable { // instance fields private double salary; private LocalDate hireDay; public Employee(String name, double salary) { super(name); this.salary = salary; } public Employee(String name, double salary, int year , int mouth , int day) { super(name); this.salary = salary; this.hireDay = LocalDate.of(year,mouth,day); } @Override // 必须要重写抽象父类中的抽象方法 public String getDescription() { return String.format(&quot;an employee with a salary of $%.2f&quot;,salary); } public double getSalary() { return salary; } public LocalDate getHireDay() { return hireDay; } public void setHireDay(LocalDate hireDay) { this.hireDay = hireDay; } public void raiseSalary(double byPercent) { double raise = this.salary * byPercent / 100; this.salary += raise; } @Override public int compareTo(Employee o) { return Double.compare(salary, o.getSalary()); }}public class Student extends Person{ private String major; public Student(String name , String major) { super(name); this.major = major; } @Override public String getDescription() { return &quot;a student majoring in &quot; + major; }}public class PersonTest { public static void main(String[] args) { // Person[] people = new Person[2]; people[0] = new Employee(&quot;binshow&quot;,2000,1995,1,2); people[1] = new Student(&quot;zkd&quot; , &quot;math&quot;); // 由于不能构造Person类的对象，所有变量P肯定指的是Person的子类 for (Person p : people){ System.out.println(p.getName() + &quot; , &quot; + p.getDescription()); } // binshow , an employee with a salary of $2000.00 // zkd , a student majoring in math }} Object：所有类的超类Object类是Java中所有类的始祖，在Java中每个类都是由它扩展而来的。 12345678910public static void main(String[] args) { // 1. 可以用Object类型的变量仅作为任意类型的指代符 Object obj = new Employee(); // 2. 但是如果要用到具体某个类型自身的方法，还是需要强制类型转换 Employee e = (Employee) obj; //3. 在Java中只有原始数据类型（数字，字母，boolean值）不能用object指代。数组对象无论是对象数组，还是原始数据类型数组都是Object的子类 Employee[] staff = new Employee[10]; obj = staff; //对象数组 obj = new int[10]; //原始数据类型数组 } equals方法 如果两个对象引用相等，这两个对象就相等。 123456java.lang.Object#equals //比较的是两个对象的引用是否一致。 public boolean equals(Object obj) { return (this == obj); } 在实际使用中经常需要基于状态来比较两个对象是否相等:比如下面比较两个员工对象是否相等 12345678910111213141516171819202122232425public class Employee { private String name; private double salary; private LocalDate hireDay; @Override public boolean equals(Object o) { if (this == o) return true; //1. 如果两个对象所属的类不同，肯定也不相等 if (o == null || getClass() != o.getClass()) return false; Employee employee = (Employee) o; //到这时o是一个不为空的 Employee 对象了 //2. 只有满足两个员工对象的姓名、薪水和入职日期是一样的，才能认为是同一个对象 // PS: 在这里防止 name 和 hireDay 为null，所以需要用equals方法进行比较。equals方法中如果两个参数都为null返回true。其中一个为null返回false //return name.equals(employee.name) &amp;&amp; salary == employee.salary &amp;&amp; hireDay.equals(employee.hireDay); return Objects.equals(name , employee.name) &amp;&amp; salary == employee.salary &amp;&amp; Objects.equals(hireDay,employee.hireDay); } @Override public int hashCode() { return Objects.hash(salary, hireDay); }} 在子类中如果定义equals方法，则首先调用父类的equals方法比较引用地址，如果检测失败，则两个对象肯定不相等。如果父类中的字段都相等，再比较子类中的实例字段。 12345678910111213141516171819public class Manager extends Employee{ private double bonus; @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; //1. 首先调用父类中的equals方法 if (!super.equals(o)) return false; Manager manager = (Manager) o; return Double.compare(manager.bonus, bonus) == 0; } @Override public int hashCode() { return Objects.hash(super.hashCode(), bonus); }} Java语言规范要求equals方法具有一下特性： 自反性： 对称性 传递性 一致性 对于任意非空引用x，x.equals(null)肯定返回false equals方法中继承关系的处理前面的例子中，如果发现两个对象所属的类不匹配，就返回false。但有的情况会这么编写equals方法： 12345678910111213141516@Override public boolean equals(Object o) { if (this == o) return true; if (o == null ) return false; //这种方式就允许了o属于它的子类，但是这样编写会有一些问题 if (!(o instanceof Employee)) return false; Employee employee = (Employee) o; return Objects.equals(name , employee.name) &amp;&amp; salary == employee.salary &amp;&amp; Objects.equals(hireDay,employee.hireDay); }问题： e.equals(m) //其中 e是Employee对象。而 m是 Manager对象。两个对象有相同的姓名，薪水和雇佣日期，此时这个equals方法返回true 但是根据对称性原则 m.equals(e) 必须也要返回true，这样的话就忽略了Manager独有的字段比较 java.util.AbstractSet#equals,有两个子类TreeSet和HashSet 123456789101112131415161718public boolean equals(Object o) { if (o == this) return true; if (!(o instanceof Set)) return false; Collection&lt;?&gt; c = (Collection&lt;?&gt;) o; if (c.size() != size()) return false; try { return containsAll(c); } catch (ClassCastException unused) { return false; } catch (NullPointerException unused) { return false; } } 结论： 如果子类可以有自己的相等性概念，则对称性需求将强制使用getClass检测 如果由父类决定相等性概念，那么可以使用instanceOf来检测，可以在不同的子类对象中进行相等性比较。 编写equals方法的正确步骤： 显示参数为otherObject 检查this和otherObject是否相等。（比较地址比比较字段开销要小很多） 如果otherObject为null，直接返回false 比较this和otherObject的两个类，如果equals的语义可以在子类中改变。则使用getClass进行检测；如果所有的子类都有相同的相等性语义，则使用instanceOf检测。 将otherObject强制转换成this的类 进行字段比较，基础数据类型用==，其他字段使用equals。如果在子类中重新定义equals，则需要包含一个super.equals（other）的调用。 hashCode方法Object默认的hashCode方法会根据对象的存储地址来计算出散列码。 123456789101112131415 String s = &quot;Ok&quot;; StringBuilder sb = new StringBuilder(s); System.out.println(&quot;s的哈希码是：&quot; + s.hashCode()); System.out.println(&quot;sb的哈希码是：&quot; + sb.hashCode()); String t = new String(&quot;Ok&quot;); StringBuilder tb = new StringBuilder(t); System.out.println(&quot;t的哈希码是：&quot; + t.hashCode()); System.out.println(&quot;tb的哈希码是：&quot; + tb.hashCode());// s的哈希码是：2556// sb的哈希码是：685325104// t的哈希码是：2556// tb的哈希码是：460141958 // s 和 t 的哈希码是一样的，是因为String的hashCode是从字符串内容导出的 // sb 和 tb 的哈希码是不一样的，是因为 StringBuilder 的hashCode 是默认的Object的hashCode,也就是从内存地址导出的 12345678910111213java.lang.String#hashCodepublic int hashCode() { int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) { char val[] = value; for (int i = 0; i &lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h; } 如果重新定义了hashCode方法，也必须要重写equals方法。因为如果x.equals(y)为true，那么x.hashcode()和y.hashcode()必须也要相同 toString方法toString方法用来返回表示对象值的字符串。比如： 12345678@Override public String toString() { return &quot;Employee{&quot; + &quot;name='&quot; + name + '\\'' + &quot;, salary=&quot; + salary + &quot;, hireDay=&quot; + hireDay + '}'; } 设计子类的toString方法需要加入子类的字段 123456@Override public String toString() { return &quot;Manager{&quot; + &quot;bonus=&quot; + bonus + &quot;} &quot; + super.toString(); } 如果x是一个任意对象，那么调用System.out.println(x); 就会调用x的toString方法。 123456//数组继承了Object类的toString方法 .int[] nums = {1,2,3,4}; System.out.println(nums); // [I@4554617c(前缀[I表示是一个数组) String numsString = &quot;&quot; + nums; System.out.println(numsString); //[I@4554617c System.out.println(Arrays.toString(nums));//[1, 2, 3, 4] 泛型数组列表ArrayList在C/C++中，必须在编译的时候就要确定整个数组的大小。而在Java中提供了动态数组ArrayList，如果调用add方法而内部数组已经满了，数组就会自动创建一个更大的数组，并将所有对象从较小的数组拷贝到较大的数组中。 ArrayList是一个采用类型参数的泛型类。为了指定数组中保存的元素类型，使用尖括号&lt;&gt;来指定存储的元素类型 1234567891011121314public static void main(String[] args) { // 构造了一个保存Employee对象的数组列表 ArrayList&lt;Employee&gt; staff = new ArrayList&lt;&gt;(); //1. 使用add方法将元素加入到数组列表中 staff.add(new Employee(&quot;binshow&quot;,10000,1997,1,2)); //2. 如果调用add方法且内部数组已经满了，list将自动创建一个更大的数组 //3. 如果已经清楚list数组中可能存储的元素数量，可以在填充数组之前用下面的 方法： staff.ensureCapacity(100); // 直接分配一个可以包含100个对象的内部数组，不用重新分配空间 // ArrayList&lt;Employee&gt; staff = new ArrayList&lt;&gt;(100); 或者构造的时候就传递容量进去 //4. 获取list中第一个位置的元素 Employee employee = staff.get(0); //5. 移除list中第一个位置的元素 Employee remove = staff.remove(0); } 对象包装器和自动装箱 背景：有时候需要将基本类型转换成对象，所以所有的基本对象都有一个与之对应的类，比如int对应于Integer。 定义一个整型数组列表，尖括号的类型参数不允许是基本数据类型，所有就要用到包装类。 12new ArrayList&lt;int&gt;(); //会发生编译错误new ArrayList&lt;Integer&gt;(); // 这样才可以 自动装箱和拆箱： 12345list.add(3); // 等价于 list.add(Integer.valueOf(3)); 自动将基本数据类型转换成包装类型 int n = list.get(0); // 等价于 int i = list.get(0).intValue(); 自动将包装类型转换成基本数据类型 Integer n = 3; n++; //编译器会自动插入一个对象拆箱的指令再进行自增运算 包装类型对象有一个常量缓冲池，比如Integer的缓冲范围为 -128 到 127 123456789101112131415161718192021Integer a = -129; Integer b = -129; System.out.println(a == b); //false Integer a = -128; Integer b = -128; System.out.println(a == b); //true ，都是指向常量池中的对象 Integer a = 100; Integer b = 100; System.out.println(a == b); //true Integer a = 127; Integer b = 127; System.out.println(a == b); //true Integer a = 128; Integer b = 128; System.out.println(a == b); //false 包装类型引用可以为null，所有自动拆箱可能会爆空指针。 12Integer n = null; System.out.println(2 * n); //NullPointerException 自动装箱和拆箱是编译器认可的，也就是说编译器在生成类的字节码时会插入必要的方法调用。虚拟机只是执行这些字节码。 变参方法现在的Java支持用可变的参数数量调用的方法。 print方法 12345// ...省略号表示这个方法可以接受任意数量的对象// 这里接受的是Object数组，来保存所有的参数。public PrintStream printf(Locale l, String format, Object ... args) { return format(l, format, args); } 枚举类 举例: Size枚举类只有4个实例。不可能构造新的对象了，因此在比较两个枚举类型的值时，并不需要equals方法，直接用==就可以了。 123456public enum Size { SMALL, MEDIUM, LARGE, EXTRE_LARGE} 枚举类可以增加构造器、方法和字段。构造器只是在构造枚举常量的时候调用 1234567891011121314151617public enum Size { SMALL(&quot;S&quot;), MEDIUM(&quot;M&quot;), LARGE(&quot;L&quot;), EXTRE_LARGE(&quot;XL&quot;); private String abbreviation; //缩写 //枚举类的构造器总是私有的，因为不可能再构造新的对象了 private Size(String abbreviation){ this.abbreviation = abbreviation; } public String getAbbreviation(){ return abbreviation; }} 每个枚举类型都有一个静态values方法，可以返回包含全部枚举值的数组。 123456789101112131415System.out.println(Arrays.toString(Size.values())); //[SMALL, MEDIUM, LARGE, EXTRE_LARGE]public class EnumTest { public static void main(String[] args) { Scanner scanner = new Scanner(System.in); System.out.println(&quot;enter a size:(SMALL , MEDIUM , LARGE , EXTRE_LARGE)&quot;); String input = scanner.next().toUpperCase(); Size size = Enum.valueOf(Size.class, input); //反向，通过字符串返回枚举类型 System.out.println(&quot;size = &quot; + size); System.out.println(&quot;abbreviation =:&quot; + size.getAbbreviation()); if (size == Size.EXTRE_LARGE) System.out.println(&quot;Good Job!&quot;); }} 反射可以利用反射编写能动态操作Java代码的程序。 在运行时分析类的能力 在运行时查看对象 实现通用的数组操作代码 利用Method对象。 Class类：在程序运行期间，java运行时系统始终为所有的对象维护一个运行时的类型标识。这个信息跟踪着每个对象所属的类，虚拟机可以利用运行时的类型信息选择相应的方法去执行。保存这些信息的类成为Class。Object类中的getClass对象就会返回一个Class类型的实例。 获取Class类对象的三种方式： 1234567891011121314151617181920212223public class Test { // Java运行时系统始终为每个对象维护一个运行时类型标识 ，这个信息保存为Class类。 public static void main(String[] args) throws ClassNotFoundException { Employee e = new Employee(&quot;binshow&quot;,27000,1997,1,8); //1. 通过getClass() 获得 Class 对象会描述一个特定类的属性 Class&lt;? extends Employee&gt; cl = e.getClass(); System.out.println(&quot;cl = &quot; + cl.getName()); //cl = chapter5.Employee //2. 通过Class.forName 来获取类名对于的Class对象 String className = &quot;java.util.Random&quot;; Class&lt;?&gt; aClass = Class.forName(className); System.out.println(&quot;aClass = &quot; + aClass.getName()); //aClass = java.util.Random //3. 直接通过后缀名获取 Class&lt;Random&gt; randomClass = Random.class; System.out.println(&quot;randomClass = &quot; + randomClass.getName()); //randomClass = java.util.Random Class&lt;Integer&gt; integerClass = int.class; System.out.println(&quot;integerClass = &quot; + integerClass.getName()); // integerClass = int Class&lt;int[]&gt; aClass1 = int[].class; System.out.println(&quot;aClass1 = &quot; + aClass1.getName()); //aClass1 = [I 数组类型 }} 通过Class类对象获取新的 对象 1234String className = &quot;java.util.Random&quot;; Class&lt;?&gt; aClass = Class.forName(className); Object o = aClass.getConstructor().newInstance(); //获取Class类的无参构造，再创建一个新的实例。如果没有无参构造则报错 System.out.println(o); //java.util.Random@1b6d3586 通过放射来分析类的能力：Field、Method、Constructors 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import java.lang.reflect.Constructor;import java.lang.reflect.Field;import java.lang.reflect.Method;import java.lang.reflect.Modifier;import java.util.Scanner;/** * 通过反射来分析类的能力 */public class ReflectionTest { public static void main(String[] args) { //java.lang.Double Scanner scanner = new Scanner(System.in); System.out.println(&quot;enter the class name:&quot;); String name = scanner.next(); try{ Class&lt;?&gt; cl = Class.forName(name); Class&lt;?&gt; sc = cl.getSuperclass(); String modifiers = Modifier.toString(cl.getModifiers()); if (modifiers.length() &gt; 0) System.out.println(&quot;modifiers = &quot; + modifiers); System.out.print(&quot;className = &quot; + cl.getName()); if (sc != null &amp;&amp; sc != Object.class) System.out.print(&quot; extends &quot; + sc.getName()); System.out.print(&quot;\\n{\\n&quot;); printConstructor(cl); printMethod(cl); printFields(cl); } catch (ClassNotFoundException e) { e.printStackTrace(); } } // 打印所有的构造器方法及参数类型 public static void printConstructor(Class cl){ Constructor[] constructors = cl.getDeclaredConstructors(); //getDeclared 表示所有的，没有的话就返回公有的 for (Constructor c : constructors){ String name = c.getName(); System.out.print(&quot; &quot;); String modifiers = Modifier.toString(c.getModifiers()); System.out.print(name + &quot;(&quot;); //打印参数类型 Class[] parameterTypes = c.getParameterTypes(); for (int i = 0; i &lt; parameterTypes.length; i++) { if (i &gt; 0) System.out.print(&quot;,&quot;); System.out.print(parameterTypes[i].getName()); } System.out.println(&quot;);&quot;); } } //打印类的所有方法 public static void printMethod(Class cl){ Method[] methods = cl.getDeclaredMethods(); for (Method m : methods){ Class&lt;?&gt; returnType = m.getReturnType(); //返回参数类型 String name = m.getName(); System.out.print(&quot; &quot;); String modifiers = Modifier.toString(m.getModifiers()); if (modifiers.length() &gt; 0) System.out.print(modifiers + &quot; &quot;); System.out.print(returnType.getName() + &quot; &quot; + name + &quot;(&quot;); //打印参数类型 Class&lt;?&gt;[] parameterTypes = m.getParameterTypes(); for (int i = 0; i &lt; parameterTypes.length; i++) { if (i &gt; 0) System.out.print(&quot;, &quot;); System.out.print(parameterTypes[i].getName()); } System.out.println(&quot;);&quot;); } } //打印类的所有字段 public static void printFields(Class cl){ Field[] fields = cl.getDeclaredFields(); for (Field f : fields){ Class type = f.getType(); String name = f.getName(); System.out.print(&quot; &quot;); String modifies = Modifier.toString(f.getModifiers()); if (modifies.length() &gt; 0) System.out.print(modifies + &quot; &quot;); System.out.println(type.getName() + &quot; &quot; + name + &quot;;&quot;); } }} 调用任意方法:和利用Field类的get方法查看对象域的过程类似，Method类中也有一个invoke方法，可以调用包装 1 5.8 继承的设计技巧 将公共操作和字段放在超类里面。比如姓名应该放在Person类中而不是Employee和Student中。 不要使用受保护的字段。 使用继承来实现“is a”的关系。 除非所有继承的方法都有意义，否则就不要使用继承。 在覆盖方法时，不要改变预期的行为。 使用多态而不要使用类型信息。 不要滥用放射。因为编译器无法帮助你查找编程错误，只有在运行时才会发生错误。","link":"/2021/08/02/JavaSE-%E4%B8%89-%E7%B1%BB%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%92%8C%E7%BB%A7%E6%89%BF/"},{"title":"JavaSE(六)泛型、枚举和注解","text":"本节主要讲述的是泛型、枚举和注解的使用方法和设计原则—来源于Java核心卷1和EffectiveJava 泛型程序设计为什么要使用泛型泛型（generic programming）意味着编写的代码可以对多种不同类型的对象重用。比如说一个ArrayList就可以收集任何类的对象。. 在java5使用泛型前，ArrayList是内部维护了一个Object引用的数组，add方法都是add一个Object类型的元素。这种方式存在两个问题： 获取一个值时必须进行类型转换。 可以向数组列表中添加任何类型的值不会编译错误。 引入类型参数后： 12345之前：ArrayList list = new ArrayList();之后：ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); // 这样一看就知道这个数组列表是String对象。get的时候返回的也是String类型//编译器还会检查你add的时候是不是传入的String类型的参数。 定义简单的范型类123456789101112131415161718192021222324252627282930313233343536373839404142public class Pair&lt;T&gt; { //类型变量T在整个类中可以用于指定方法的返回类型和字段、局部变量的类型。 private T first; private T second; public Pair(){ first = null; second = null; } public Pair(T first , T second){ this.first = first; this.second = second; } public T getFirst(){return first;} public T getSecond(){return second;} public void setFirst(T newValue){ first = newValue; } public void setSecond(T newValue){ second = newValue; }}//可以定义多个类型变量public class Pair&lt;T,U&gt; {}//可以用具体的类型实例化类型变量，比如Pair&lt;String&gt;Pair&lt;String&gt;(String,String) 方法： String getFirst(); String getSecond(); void setFirst(String); void setSecond(String);也就是说：泛型类相当于普通类的工厂。 使用上面定义的泛型类： 123456789101112131415161718192021222324public class PairTest { public static void main(String[] args) { String[] words = {&quot;Mary&quot; , &quot;had&quot; , &quot;a&quot; , &quot;little&quot; , &quot;lamb&quot;}; Pair&lt;String&gt; minax = ArrayAlg.minax(words); System.out.println(&quot;min = &quot; + minax.getFirst()); System.out.println(&quot;max = &quot; + minax.getSecond()); }}class ArrayAlg{ //返回String数组中的最大值和最小值，用Pair对象来进行包装 public static Pair&lt;String&gt; minax(String[] a){ if (a == null || a.length == 0) return null; String min = a[0]; String max = a[0]; for (int i = 1; i &lt; a.length; i++) { if (min.compareTo(a[i]) &gt; 0) min = a[i]; if (max.compareTo(a[i]) &lt; 0) max = a[i]; } return new Pair&lt;&gt;(min , max); }} 泛型方法12345678910111213class ArrayAlg{ //返回中间的元素 public static&lt;T&gt; T getMiddle(T...a){ return a[a.length/2]; }}测试：String mid = ArrayAlg.getMiddle(&quot;a&quot; , &quot;b&quot; , &quot;String&quot;,&quot;c&quot;,&quot;d&quot;);String middle = ArrayAlg.getMiddle(mid);System.out.println(&quot;middle = &quot; + middle); 类型变量的限定12345678910111213141516171819202122232425// 由于T是实现了Comparable接口的类，所以才可以调用compareTo方法。 public static &lt;T extends Comparable&gt; T min(T[] a){ if (a == null || a.length == 0) return null; T smallest = a[0]; for (int i = 1; i &lt; a.length; i++) { if (smallest.compareTo(a[i]) &gt; 0) smallest = a[i]; } return smallest; }一个类型变量可以有多个限定，比如 T extends Comparable &amp; Serializable //测试： LocalDate[] birthdays = { LocalDate.of(1991,12,9), LocalDate.of(1997,1,8), LocalDate.of(1996,10,22), LocalDate.of(2001,7,29), }; LocalDate min1 = ArrayAlg.min(birthdays); System.out.println(&quot;min1 = &quot; + min1.toString()); 泛型代码和虚拟机类型擦除定义的泛型类型，虚拟机都在编译的时候擦除类型参数，提供一个相应的原始类型。 比如：上文的Pair类的原始类型如下（没有限定的类型参数转换为Object） 1234567891011121314151617181920212223242526public class Pair { //类型变量T在整个类中可以用于指定方法的返回类型和字段、局部变量的类型。 private Object first; private Object second; public Pair(){ first = null; second = null; } public Pair(Object first , Object second){ this.first = first; this.second = second; } public Object getFirst(){return first;} public Object getSecond(){return second;} public void setFirst(Object newValue){ first = newValue; } public void setSecond(Object newValue){ second = newValue; }} 原始类型用第一个限定类替换类型变量。 转换泛型表达式当一个泛型方法调用时，如果擦除了返回类型，编译器会插入强制类型转换。 123456Pair&lt;Employee&gt; staff = new Pair&lt;&gt;(); // getFirst方法编译完之后返回的类型是Object，编译器会自动插入Employee的强制类型转换 // 也就是说：编译器将这个方法转换成了两条虚拟机指令 // 1. 对原始方法 Pair.getFirst的调用 // 2. 将放回的 Object类型强制转换为Employee Employee first = staff.getFirst(); 转换泛型方法调用遗留代码限制和局限性大多数限制都是由于类型擦除而引起的 不能用基本类型实例化类型参数。只有Pair，没有Pair. 运行时的类型查询只适用于原始类型。 不能创建参数化类型的数组。 Varargs警告 不能实例化类型变量 不能构造泛型数组。 泛型类的静态上下文中类型变量无效 不能抛出或捕获泛型类的实例 可以取消对检查型异常的检查 范型类型的继承规则通配符类型反射和泛型EffectiveJava中的泛型泛型的需求：在拥有泛型之前，你必须对每个从集合中读出来的对象进行类型转换。如果有人不小心插入了一个错误类型的对象，这个类型转换就会再运行时失败。使用泛型，你就可以告诉编译器，那些类型的对象允许加入这个集合。编译器会自动地为你进行转换，然后当你要插入一个错误类型的时候，在编译地时候就会报错 Item26 不要使用原生类型每一个泛型都包括一组参数类型，其构成方式如下：首先是类或者接口的名字，然后紧跟着用&lt;&gt;括起来的实际类型参数，实际类型参数和泛型中的形式类型参数对应 原生类型指的是没有任何类型参数的泛型类型的名称。 1234567891011121314151617181920212223242526272829// 只是为了兼容之前的代码,不建议直接使用原生类型SetSet set = new Set;// 错误的插入了不同类型的 数据类型，编译时不会报错set.add(1);set.add(&quot;String&quot;);set.add('a');同理：public static void main(String[] args) { List&lt;String&gt; strings = new ArrayList&lt;&gt;(); //1. 添加的时候不会报错，编译也能通过 unsafeAdd(strings , Integer.valueOf(43)); // safeAdd(strings , Integer.valueOf(43)); //2. 取出的时候就会发生类型转换异常 String s = strings.get(0); //java.lang.ClassCastException } //unsafeAdd 方法的参数是一个原生类型List。所以有风险，应该用参数化类型List来代替 private static void unsafeAdd(List list, Object o) { list.add(o); } private static void safeAdd(List&lt;String&gt; list, String o) { // 加上参数化类型之后，这边编译就会报错 list.add(o); } Item27 消除非受检的警告Item28 list优先于数组为什么呢？ 数组是协变的（covariant），泛型是非协变的。 1234567//1. 下面编译不报错，启动才报错// 由于Long是Object的子类，所以 Long[] 也是 Object[]的子类 Object[] objectArray = new Long[1]; objectArray[0] = &quot;abc&quot;;// java.lang.ArrayStoreException //2. 用list编译就报错了 List&lt;Object&gt; list = new ArrayList&lt;Long&gt;(); 数组是具体化的（reified），也就是说在运行的时候数组是知道其元素类型的，并且会强制执行类型限制。相反，泛型是通过泛型擦除来实现的，这就意味着只能在编译器强制执行类型限制，运行的时候是会擦除其元素信息的。类型擦除是为了允许泛型代码可一个之前没有泛型的老代码进行互用 当你在转换到数组类型时遇到泛型数组创建错误或者非受检转换异常的时候，最好的方法是使用集合类型List，而不是数组类型E[]。可能会牺牲一些性能，但是换回的却是更好的类型安全性和互用性。 12345678910111213141516171819202122232425// Chooser - a class badly in need of generics!// 调用这个类的choose方法时，返回的都是Object对象，需要手动进行类型准换public class Chooser { private final Object[] choiceArray; public Chooser(Collection choices) { choiceArray = choices.toArray(); } public Object choose() { Random rnd = ThreadLocalRandom.current(); return choiceArray[rnd.nextInt(choiceArray.length)]; }}public static void main(String[] args) { ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(1); list.add(2); list.add(3); Chooser chooser = new Chooser(list); Integer choose = (Integer) chooser.choose(); System.out.println(&quot;choose = &quot; + choose); } 为了避免进行可能出错的强制类型转换，引入泛型,但还是有点问题： 12345678910111213141516public class Chooser&lt;T&gt; { private final T[] choiceArray; public Chooser(Collection&lt;T&gt; choices) { //它不能确定在运行时的类型转换的安全性，因为程序不知道类型T代表的是什么 //泛型中的元素类型信息在运行时被擦除了 choiceArray = (T[]) choices.toArray(); } public Object choose() { Random rnd = ThreadLocalRandom.current(); return choiceArray[rnd.nextInt(choiceArray.length)]; }} 为了消除这个非受检转换警告，使用list来代替数组： 12345678910111213public class Chooser&lt;T&gt; { private final List&lt;T&gt; choiceArray; public Chooser(Collection&lt;T&gt; choices) { choiceArray = new ArrayList&lt;&gt;(choices); } public T choose() { Random rnd = ThreadLocalRandom.current(); return choiceArray.get(rnd.nextInt(choiceArray.size())); }} 数组和泛型有一些非常不同的类型规则，数组是协变和可具体化的；而泛型是非协变和擦除的。因此，数组可以提供运行时类型安全但是没有贬义是类型安全，而泛型却恰恰相反。一般来说，数组和泛型不能混用。如果你发现你自己混用了他们，并且出现了编译error或者warning。你的第一反应应该是用列表替换数组。 Item29 优先考虑泛型一个例子：未进行参数化的Stack 12345678910111213141516171819202122232425public class Stack { private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { elements = new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public Object pop() { if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} 将这个Stack使用泛型，同时还需要兼容之前的版本： 声明参数类型为E 将所有的Object改成E 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public class Stack&lt;E&gt; { private E[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { // 这里报错 。。 不能创建一个泛型数组。 elements = new E[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public E pop() { if (size == 0) throw new EmptyStackException(); E result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }}// 两种解决方案：1. elementst数组，再进行强制类型转换为E数组 public class Stack&lt;E&gt; { private E[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { // 这里报错 elements = (E[]) new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(E e) { ensureCapacity(); elements[size++] = e; } public E pop() { if (size == 0) throw new EmptyStackException(); E result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} 2. 将elements设为object数组，每次pop访问数组中的元素的时候进行强制类型转换 public class Stack&lt;E&gt; { private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { // 这里报错 elements = new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public E pop() { if (size == 0) throw new EmptyStackException(); E result = (E) elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} 第一种方法可读性更强：数组被声明为E[]，清晰地表示了它只包含E实例；并且更加简洁：在一个典型的泛型类中，你可以以在代码里看到很多次数组，第一中方法只需要进行一次转换（当数组创建的时候）；而第二种方法需要在每次读取数组元素的时候都进行转换。因此第一种方法要更受欢迎一些，在实际情况中，也用得更多些。但是第一种方法确实造成了”堆泄露“（详见Item32）：因为数组的运行时类型和编译时类型不一致（除非E刚刚也是Object）。有一些程序员对这点深恶痛绝，选择使用第二种方法。 Item30 优先考虑泛型方法基于参数化类型进行计算的静态工具方法通常来说都是泛型的。 1234//java.util.Collections#sort(java.util.List&lt;T&gt;, java.util.Comparator&lt;? super T&gt;)public static &lt;T&gt; void sort(List&lt;T&gt; list, Comparator&lt;? super T&gt; c) { list.sort(c); } 写泛型方法和泛型类型差不多： 1234567891011121314151617// 使用了原始类型，是有缺陷的。 // 虽然编译可以通过，但是有很多warning public static Set union(Set s1 , Set s2){ Set res = new HashSet(s1); res.addAll(s2); return res; }// 进行泛型的修改：//这三个set（包括输入参数和返回值）的类型都必须是完全一样的。你可以使用”有限制的通配符类型“来使得这个方法更加灵活public static &lt;E&gt; Set&lt;E&gt; union(Set&lt;E&gt; s1 , Set&lt;E&gt; s2){ Set&lt;E&gt; res = new HashSet&lt;E&gt;(s1); res.addAll(s2); return res; } Item31 使用有限制的通配符来提升API的灵活性参数化类型是非协变的 1234567891011121314151617181920212223public class Stack&lt;E&gt; { public Stack(); public void push(E e); public E pop(); public boolean isEmpty();}//1. 实现将一些元素全部放入到 stcak中。下面这样实现要求 Iterable 中的元素类型必须要和 Stack中的元素类型相同// pushAll method without wildcard type - deficient! public void pushAll(Iterator&lt;E&gt; src) { while (src.hasNext()){ push(src.next()); } }Stack&lt;Number&gt; numberStack = new Stack&lt;&gt;();List&lt;Integer&gt; list = Arrays.asList(1, 2, 3);Iterator&lt;Integer&gt; iterator = list.iterator();numberStack.pushAll(iterator); // Integer 虽然是 Number的子类型，但是这里还是编译不通过 Java语言提供了一种特殊的参数化类型，被称为”有限制通配符类型“，就是用来解决类似这种问题的。 pushAll方法的参数类型不应该是”E的Iterable接口“，而应该是“E的某个子类型的Iterable接口”，然后，这里有一个通配符类型，可以准确的表示这个意思： Iterable&lt;? extends E&gt;（其中extends关键字可能会造成一些误导，重申一下Item29里子类型的定义，每个类型都是自身的子类型，即使它没有继承它自己）。下面是使用这种类型修改后的pushAll方法： 123456// Wildcard type for a parameter that serves as an E producer public void pushAll(Iterator&lt;? extends E&gt; src) { while (src.hasNext()){ push(src.next()); } } 1234567891011121314151617// popAll method without wildcard type - deficient! public void popAll(Collection&lt;E&gt; dst) { while (!dst.isEmpty()) dst.add(pop()); }//同样的Collection&lt;Object&gt; objects = new ArrayList&lt;&gt;(); numberStack.popAll(objects); // 编译报错//可修改如下： super 表示传入的可以是 Collection的父类（包括它自己）public void popAll(Collection&lt;? super E &gt; dst) { while (!dst.isEmpty()) dst.add(pop()); } 这个结论很明显：为了最大化灵活度，对于代表生产者或者消费者的输入参数，使用通配符类型。如果这个输入参数即是生产者又是消费者，这个通配符类型就没什么用了。你需要的是准确的类型匹配，不需要使用任何的通配符。 这个助记符可以帮助你记住应该使用哪些通配符： PECS 表示producer-extends, consumer-super。 换句话说，如果这个参数化类型表示一个T生产者，使用&lt;? extends T&gt;；如果表示的是T消费者，使用 &lt;? super T&gt;。在我们的Stack的例子里，pushAll的src参数产生了E实例供Stack使用；所以src的合适类型是Iterable&lt;? extends T&gt;；popAll的dst参数消费每一个来自Stack的E实例，因此合适的类型是Collection&lt;? super E&gt;。PECS助记符刻画了直到通配符使用的基本原则。Naftalin和Wadler把它称之为“Get and Put Principle”。 12//前面的例子修改：public static &lt;E&gt; Set&lt;E&gt; union(Set&lt;? extends E&gt; s1, Set&lt;? extends E&gt; s2) 不要使用有限制的通配符类型作为返回类型 Item32 谨慎地并用泛型和可变参数Item33 考虑类型安全的异构容器枚举编写一个最简单的枚举类： 123456789101112131415161718192021222324252627/** * 1. 最简单的枚举类，有4个实例，不可能再构造新的对象。 * 2. 比较两个枚举值时，可以直接用 == * 3. 枚举类可以增加构造器、方法和字段。 */public enum Size { SMALL(&quot;S&quot;), MEDIUM(&quot;M&quot;), LARGE(&quot;L&quot;), EXTRA_LARGE(&quot;XL&quot;); private String abbreviation; //构造器总是私有的 private Size(String abbreviation){ this.abbreviation = abbreviation; } public String getAbbreviation(){return abbreviation;}} System.out.println(Size.EXTRA_LARGE.toString()); //EXTRA_LARGE System.out.println(Size.EXTRA_LARGE.getAbbreviation()); //XL System.out.println(Enum.valueOf(Size.class, &quot;SMALL&quot;)); //SMALL枚举类的代表 Size[] values = Size.values(); //返回一个包含 全部枚举值的数组 关于枚举的设计原则使用枚举代替int常量一个枚举类型是指有一组固定的常量组成的合法值的类型，比如一年的季节，太阳系的星星，一副牌的花色。在enum类型被添加到java语言中之前，通常使用一组命名的int常量来表示枚举类型，每一个int值表示一个枚举类型的成员。代码如下： 12345678// The int enum pattern - severely deficient! public static final int APPLE_FUJI = 0; public static final int APPLE_PIPPIN = 1; public static final int APPLE_GRANNY_SMITH = 2; public static final int ORANGE_NAVEL = 0; public static final int ORANGE_TEMPLE = 1; public static final int ORANGE_BLOOD = 2; 这种技术，称为”int枚举模式“，有跟多的缺点。 它完全没有提供类型安全的保证，表达能力也不强。当你把一个apple传递给一个需要orange的方法时，编译器也不会生成警告， 还可以使用==来对apple和orange进行比较，甚至更糟糕： 12// Tasty citrus flavored applesauce!int i = (APPLE_FUJI - ORANGE_TEMPLE) / APPLE_PIPPIN; 使用这种int枚举的程序是非常脆弱的，因为int枚举是编译时常量，他们的int值会被编译到使用他们的程序中。如果一个int枚举相关的值被修改了，它的客户端也必须重新编译。如果客户端没有重新编译而继续执行的话，客户端的行为就是不正确的. java提供了枚举类来解决这些问题： 12public enum Apple { FUJI, PIPPIN, GRANNY_SMITH }public enum Orange { NAVEL, TEMPLE, BLOOD } Java的Enum类型的基本想法很简单：他们就是使用公有静态final域为每一个枚举常量导出一个实例的类。Enum类型由于没有可以访问的构造器，Enum类型是真正的final类。由于客户端既不能穿件enum类型的实例，也不能继承enum类，因为除了它声明的枚举实例外，不会有其他的实例。也就是说，枚举类型是实例受控的（详见Item1）。它们是一组单例的集合（Item3），单例本质上是每个元素的枚举。 Enum类型提供了编译时的类型安全。只要你声明了一个参数的类型是Apple，那么久可以保证传递给这个参数的任何非空的对象引用，一定是这三个有效的Apple值之一。试图传递不同错误类型的值，编译时会出现error，试图把一个类型的表达式赋值给另外一个类型的变量，或者使用==操作符来比较两个不同的enum类型的时候，都会出现编译时错误。 除了完善了enums的不足之处以外，enum类型还允许添加任意多的方法和域，允许实现任意多的接口。Enum还提供了所有的Object方法的高效实现（第三章），还实现了Comparable接口（Item14）和Serializable接口（第12章），以及它的序列化形式可以承受大多数的enum类型的转换形式。 给枚举类添加方法和域 那么我们为什么妖王enum类型里添加方法或者域呢？一开始，可能是想把数据和这些实例关联起来。比如我们的Apple和Orange类型，添加一个可以返回水果颜色的方法，或者返回图片的方法，就很有必要。你可以给enum类型增加任何一个看起来合适的方法。一个Enum类型一开始可能就是一组枚举常量的集合，随着时间推进，就进化成了一个功能齐全的抽象了。 举一个功能齐全的enum类型的例子，比如太阳系里的八大行星。每一个行星都质量和半径，然后根据质量和半径可以计算其表面重力值。然后给定一个对象额质量，就可以计算其在该行星表面所受的重力。下面是这个Enum的代码。每个枚举后面的圆括号内的数字，是要传递给构造器的参数。在这个例子中，就是行星的质量和半径。 要编写一个像Planet这样丰富的Enum类型并不难。为了将枚举常量和数据关联起来，需要声明实例数据域，然后写一个构造器，使用这些数据，并把他们保存在数据域里。Enum实例天生就是不可变的，因此所有的域都应该是final的（Item17）。域可以是公开的，但是最好还是将域设为私有的，然后提供公有的访问方法（Item16）。在Planet这个例子中，构造器还计算并保存了surfaceGravity的值，这仅仅是一个优化。这个surfaceGravity的值也可以在，每次调用surfaceWeight方法时，使用mass和radius重新计算。surfaceWeight方法参数为一个对象的质量，返回这个对象在这个常量代表的星星上的重量。 123456789101112131415161718192021222324252627282930313233343536373839/** * 包含方法和域的枚举类 */public enum Planet { /** * 每个行星都是一个实例 */ //八大行星。每一个行星的质量和半径 MERCURY(3.302e+23, 2.439e6), VENUS (4.869e+24, 6.052e6), EARTH (5.975e+24, 6.378e6), MARS (6.419e+23, 3.393e6), JUPITER(1.899e+27, 7.149e7), SATURN (5.685e+26, 6.027e7), URANUS (8.683e+25, 2.556e7), NEPTUNE(1.024e+26, 2.477e7); // 枚举类的所有域都应该用final来修饰 private final double mass; // In kilograms private final double radius; // In meters private final double surfaceGravity; // In m / s^2 // Universal gravitational constant in m^3 / kg s^2 private static final double G = 6.67300E-11; // 常量 // Constructor 枚举类的构造器，必须和上面罗列的实例参数复合 private Planet(double mass, double radius) { this.mass = mass; this.radius = radius; surfaceGravity = G * mass / (radius * radius); } // 枚举类的域是私有的，对外提供访问的方法 public double mass() { return mass; } public double radius() { return radius; } public double surfaceGravity() { return surfaceGravity; } public double surfaceWeight(double mass) { return mass * surfaceGravity; // F = ma }} 12345678910// 根据某个物体在地球上的重量（任何单位），答应出一个超级棒的表格，显示该物体在8个行星上的重量（相同单位public class WeightTable { public static void main(String[] args) { double earthWeight = Double.parseDouble(args[0]); double mass = earthWeight / Planet.EARTH.surfaceGravity(); //这个Planet，以及所有的枚举，都有一个静态的values方法，可以返回一个枚举值的数组 for (Planet p : Planet.values()) System.out.printf(&quot;Weight on %s is %f%n&quot;, p, p.surfaceWeight(mass)); } } 枚举常量的一些行为可能只需要在定义该枚举的类或者方法中使用，这种行为最好是用私有或者包级私有的方法来实现。每个常量就会有一组隐藏的行为，使得包含该枚举的类型或包在使用常量的时候，可以运作得很好。和其他的类一样，除非有让人难以拒绝的原因，不要把枚举方法暴露给它的客户端，将其声明为私有的，或者有需要的话，声明为包级私有的（Item15）。 如果一个枚举类型有普遍适用性，它就应该是一个顶级类；如果它只是在某个顶级类中使用，那么它应该是这个顶级类的成员类。比如，java.math.RoundingMode枚举类表示十进制小数的舍入模式（rounding mode）。这个舍入模式只在Bigdecimal类里使用，但是RoundingMode提供了一个有用的抽象，不仅仅局限于BigDecimal类。因此类库设计者通过把RoundingMode做成一个顶级类，以鼓励其他需要舍入模式的程序员重用这个枚举，从而增加API之间的一致性。 给枚举类中的每个实例关联不同的行为 有时候，你可能还需要每个实例关联不同的行为。比如，假如你在写一个表示基本四则运算的枚举类型，你想提供一个方法来执行每个实例表示的算数操作。下面是一种通过在枚举的值上使用switch来实现的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// Enum type that switches on its own value - questionablepublic enum Operation { PLUS, MINUS, TIMES, DIVIDE; // Do the arithmetic operation represented by this constant public double apply(double x, double y) { switch(this) { case PLUS: return x + y; case MINUS: return x - y; case TIMES: return x * y; case DIVIDE: return x / y; } throw new AssertionError(&quot;Unknown op: &quot; + this); }}// 问题：新增加一个枚举常量的时候，还需要变动switch中的case，变动较大。// 变体1：在enum类中声明一个抽象的apply方法，然后在“特定于常量的类主体”中，使用具体的方法来覆盖每个常量的抽象方法。这种方法被称为“特定于常量的方法实现”// Enum type with constant-specific method implementationspublic enum Operation { PLUS {public double apply(double x,double y){return x + y;}}, MINUS {public double apply(double x, double y){return x - y;}}, TIMES {public double apply(double x, double y){return x * y;}}, DIVIDE {public double apply(double x, double y){return x / y;}}; // 抽象方法 public abstract double apply(double x, double y);}// 变体2：特定于常量的方法实现可以和特定于常量的数据一起使用// Enum type with constant-specific class bodies and datapublic enum Operation { PLUS(&quot;+&quot;) { public double apply(double x, double y) { return x + y; } }, MINUS(&quot;-&quot;) { public double apply(double x, double y) { return x - y; } }, TIMES(&quot;*&quot;) { public double apply(double x, double y) { return x * y; } }, DIVIDE(&quot;/&quot;) { public double apply(double x, double y) { return x / y; } }; private final String symbol; Operation(String symbol) { this.symbol = symbol; } @Override public String toString() { return symbol; } public abstract double apply(double x, double y);} 这个toString方法的实现使得答应数学表达式变得更容易，比如下面这一小段代码： 1234567891011public static void main(String[] args) { double x = Double.parseDouble(args[0]); double y = Double.parseDouble(args[1]); for (Operation op : Operation.values()) System.out.printf(&quot;%f %s %f = %f%n&quot;,x, op, y, op.apply(x, y));} 2.000000 + 4.000000 = 6.000000 2.000000 - 4.000000 = -2.000000 2.000000 * 4.000000 = 8.000000 2.000000 / 4.000000 = 0.500000 那么什么时候应该使用枚举类型呢？当你需要一组固定的常量集合，并且在编译时就知道其成员的时候，就应该使用枚举。当然，这就包括一些“自然枚举类型”，比如行星们，一周的天数以及棋子的数目。当然还包括一些其他在编译时就知道其所有可能的值的集合，比如，菜单的选项，运算符，和命令行标志。并不需要枚举类型中的常量集合一直保持不变。专门设计的枚举类型的特性可以允许枚举类型随时间演变。 总的来说，枚举类型相对于int常量的优势是让人难以拒绝的。枚举类型可读性更好，更安全，也更加强大。一些枚举类型不需要显示的构造器和成员，也有一些枚举可以从每个常量关联的数据，以及提供的行为受数据影响的方法中获得好处。少数的枚举需要将多个行为和一个方法关联，在这种比较少的情况下，特定于常量的方法实现，比在枚举值上进行switch，要好得多。当有多个（但不是全部）使用需要共用相同的行为的时候，可以考虑使用策略枚举模式。 Item35 用实例代替序数12345678910111213141516171819202122// Abuse of ordinal to derive an associated value - DON'T DO THIS public enum Ensemble { SOLO, DUET, TRIO, QUARTET, QUINTET, SEXTET, SEPTET, OCTET, NONET, DECTET; public int numberOfMusicians() { return ordinal() + 1; } }//不要从枚举的序数（ordinal）中获取关联的值；应该把它保存在一个实例域中// 也就是说，枚举类中的每个实例最好编写一个序号保存起来public enum Ensemble { SOLO(1), DUET(2), TRIO(3), QUARTET(4), QUINTET(5), SEXTET(6), SEPTET(7), OCTET(8), DOUBLE_QUARTET(8), NONET(9), DECTET(10), TRIPLE_QUARTET(12); private final int numberOfMusicians; Ensemble(int size) { this.numberOfMusicians = size; } public int numberOfMusicians(){ return numberOfMusicians; }} Item36 用EnumSet代替位域如果一个枚举类型的元素主要是用在集合中，一般会用int枚举模式 123456789101112// Bit field enumeration constants - OBSOLETE! public class Text { public static final int STYLE_BOLD = 1 &lt;&lt; 0; //1 public static final int STYLE_ITALIC = 1 &lt;&lt; 1; //2 public static final int STYLE_UNDERLINE = 1 &lt;&lt; 2; //4 public static final int STYLE_STRIKETHROUGH = 1 &lt;&lt; 3; // 8 // Parameter is bitwise OR of zero or more STYLE_ constants public void applyStyles(int styles) { ... } } java,util包里提供了EnumSet类来高效的表示从单个枚举类型中取出的一组值的集合。这个类实现了Set接口，提供了丰富的功能、类型安全、以及可以和任何其他Set实现的互用性 12345678910111213//前面的例子使用Enum和EnumSer表示的代码。它更短，更简洁，更安全：// EnumSet - a modern replacement for bit fieldspublic class Text {public enum Style { BOLD, ITALIC, UNDERLINE, STRIKETHROUGH } // Any Set could be passed in, but EnumSet is clearly best public void applyStyles(Set&lt;Style&gt; styles) { ... }}使用： text.applyStyles(EnumSet.of(Style.BOLD, Style.ITALIC)); EnumSet&lt;Planet&gt; neptune = EnumSet.of(Planet.NEPTUNE, Planet.MERCURY, Planet.EARTH); Item37 使用EnumMap代替序数索引12345678910111213141516public class Plant { enum LifeCycle { ANNUAL, PERENNIAL, BIENNIAL } final String name; final LifeCycle lifeCycle; Plant(String name, LifeCycle lifeCycle) { this.name = name; this.lifeCycle = lifeCycle; } @Override public String toString() { return name; }} 12345678910111213141516171819public static void main(String[] args) { Plant a = new Plant(&quot;a&quot;, Plant.LifeCycle.ANNUAL); Plant b = new Plant(&quot;b&quot;, Plant.LifeCycle.ANNUAL); Plant c = new Plant(&quot;c&quot;, Plant.LifeCycle.BIENNIAL); Plant d = new Plant(&quot;d&quot;, Plant.LifeCycle.PERENNIAL); Plant e = new Plant(&quot;e&quot;, Plant.LifeCycle.BIENNIAL); List&lt;Plant&gt; garden = Arrays.asList(a, b, c, d, e); //需求：将这五种植物按照 Plant.LifeCycle 分类： Map&lt;Plant.LifeCycle, Set&lt;Plant&gt;&gt; plantsByLifeCycle = new EnumMap&lt;&gt;(Plant.LifeCycle.class); System.out.println(plantsByLifeCycle); //{} for (Plant.LifeCycle lc : Plant.LifeCycle.values()) plantsByLifeCycle.put(lc, new HashSet&lt;&gt;()); for (Plant p : garden) plantsByLifeCycle.get(p.lifeCycle).add(p); System.out.println(plantsByLifeCycle); //{ANNUAL=[b, a], PERENNIAL=[d], BIENNIAL=[c, e]} } Item38 用接口模拟可以扩展的枚举注解自定义一个注解： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * 自定义注解 MyTest */@Retention(RetentionPolicy.RUNTIME) // 表示这个注解运行时应该被保留@Target(ElementType.METHOD) //表示只有在方法声明上使用时合法的，它不能被用在类声明，域声明，或者其他的程序元素上public @interface MyTest {}// 测试注解的用例// @MyTest被称为”标记注解“，因为它没有参数，只是简单的给被注解元素做了个标记// 注解不会改变被注解代码的语义，但是可以使这段代码可以被一些工具特殊处理public class Sample { @MyTest public static void m1() { } // Test should pass public static void m2() { } @MyTest public static void m3() { // Test should fail ，抛出了一个运行时异常 throw new RuntimeException(&quot;Boom&quot;); } public static void m4() { } @MyTest // 不是静态方法，不能通过放射之后 类名.方法名调用 public void m5() { } // INVALID USE: nonstatic method public static void m6() { } @MyTest public static void m7() { // Test should fail throw new RuntimeException(&quot;Crash&quot;); } public static void m8() { }}// 通过反射的方式来测试public class RunTest { public static void main(String[] args) throws ClassNotFoundException { int tests = 0; int passed = 0; Class&lt;?&gt; testClass = Class.forName(&quot;effectiveJava.chapter4.Sample&quot;); Method[] methods = testClass.getDeclaredMethods(); for (Method m : methods){ // 1. 如果方法 存在 @MyTest 注解 if (m.isAnnotationPresent(MyTest.class)) { tests++; try { //2. 运行这个方法 m.invoke(null); passed++; } catch (InvocationTargetException wrappedExc) { //3. 如果这个方法发生了异常。反射机制会将这个异常包装在一个InvocationTargetException里。 // 然后测试工具会捕获这个异常，并打印一个失败报告 Throwable exc = wrappedExc.getCause(); System.out.println(m + &quot; failed: &quot; + exc); } catch (Exception exc) { //4. 反射调用的方法抛出了不是InvocationTargetException的异常，那就表明这是一个Test注解在编译时没有发现的非法的使用。 System.out.println(&quot;Invalid @Test: &quot; + m); //Invalid @Test: public void effectiveJava.chapter4.Sample.m5() } } } System.out.printf(&quot;Passed: %d, Failed: %d%n&quot;,passed, tests - passed); // Passed: 1, Failed: 3 }} 注解需求改变：只有抛出指定的异常才能通过测试: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface ExceptionTest { // 允许使用这个注解的用户指定一个异常或错误类型 Class&lt;? extends Throwable&gt; value();}public class Sample2 { @ExceptionTest(ArithmeticException.class) public static void m1(){ // Test should pass int i = 0; i = i / i; } @ExceptionTest(ArithmeticException.class) public static void m2(){ // Should fail (wrong exception) int[] a = new int[0]; int i = a[1]; } @ExceptionTest(ArithmeticException.class) public static void m3(){ // Should fail (no exception) }}public static void main(String[] args) throws ClassNotFoundException { int tests = 0; int passed = 0; Class&lt;?&gt; testClass = Class.forName(&quot;effectiveJava.chapter4.Sample2&quot;); Method[] methods = testClass.getDeclaredMethods(); for (Method m : methods) { if (m.isAnnotationPresent(ExceptionTest.class)) { tests++; try { m.invoke(null); System.out.printf(&quot;Test %s failed: no exception%n&quot;, m); //Test public static void effectiveJava.chapter4.Sample2.m3() failed: no exception } catch (InvocationTargetException wrappedEx) { Throwable exc = wrappedEx.getCause(); Class&lt;? extends Throwable&gt; excType = m.getAnnotation(ExceptionTest.class).value(); if (excType.isInstance(exc)) { passed++; } else { //Test public static void effectiveJava.chapter4.Sample2.m2() failed: expected java.lang.ArithmeticException, got java.lang.ArrayIndexOutOfBoundsException: 1 System.out.printf( &quot;Test %s failed: expected %s, got %s%n&quot;, m, excType.getName(), exc); } } catch (Exception exc) { System.out.println(&quot;Invalid @Test: &quot; + m); } } } System.out.printf(&quot;Passed: %d, Failed: %d%n&quot;,passed, tests - passed); // Passed: 1, Failed: 3 } 注解需求改变：当抛出几个特定类型中的任何一个的时候，测试通过 12345678910111213141516171819202122232425262728293031323334353637// Annotation type with an array parameter @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface ExceptionTest { Class&lt;? extends Exception&gt;[] value(); }// Code containing an annotation with an array parameter @ExceptionTest({ IndexOutOfBoundsException.class,NullPointerException.class }) public static void doublyBad() { List&lt;String&gt; list = new ArrayList&lt;&gt;(); // The spec permits this method to throw either // IndexOutOfBoundsException or NullPointerException list.addAll(5, null);}// testif (m.isAnnotationPresent(ExceptionTest.class)) { tests++; try { m.invoke(null); System.out.printf(&quot;Test %s failed: no exception%n&quot;, m); } catch (Throwable wrappedExc) { Throwable exc = wrappedExc.getCause(); int oldPassed = passed; Class&lt;? extends Exception&gt;[] excTypes = m.getAnnotation(ExceptionTest.class).value(); for (Class&lt;? extends Exception&gt; excType : excTypes) { if (excType.isInstance(exc)) { passed++; break; } } if (passed == oldPassed) System.out.printf(&quot;Test %s failed: %s %n&quot;, m, exc); }} 在Java8中，还有一种方法可以处理多值注解：**@Repeatable 来表示这个注解可以在一个单个元素上多次应用** 12345678910111213141516171819202122232425262728293031323334353637383940414243// Repeatable annotation type @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) @Repeatable(ExceptionTestContainer.class) //Repeatable 这个元注解有一个参数，该参数是一个包含注解类型的class对象，这个包含注解类型的唯一的参数就是，被@Repeatable所注解的注解类型 数组 public @interface ExceptionTest { Class&lt;? extends Exception&gt; value(); } @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface ExceptionTestContainer { ExceptionTest[] value(); }// Code containing a repeated annotation @ExceptionTest(IndexOutOfBoundsException.class) @ExceptionTest(NullPointerException.class) public static void doublyBad() { ... }// Processing repeatable annotations if (m.isAnnotationPresent(ExceptionTest.class) || m.isAnnotationPresent(ExceptionTestContainer.class)) { tests++; try { m.invoke(null); System.out.printf(&quot;Test %s failed: no exception%n&quot;, m); } catch (Throwable wrappedExc) { Throwable exc = wrappedExc.getCause(); int oldPassed = passed; ExceptionTest[] excTests = m.getAnnotationsByType(ExceptionTest.class); for (ExceptionTest excTest : excTests) { if (excTest.value().isInstance(exc)) { passed++; break; } } if (passed == oldPassed) System.out.printf(&quot;Test %s failed: %s %n&quot;, m, exc); } } Item40 坚持使用Override注解","link":"/2021/08/03/JavaSE-%E5%85%AD-%E6%B3%9B%E5%9E%8B%E3%80%81%E6%9E%9A%E4%B8%BE%E5%92%8C%E6%B3%A8%E8%A7%A3/"},{"title":"JavaSE-七-集合框架","text":"","link":"/2021/08/01/JavaSE-%E4%B8%83-%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6/"},{"title":"Java并发（一）内存模型和锁","text":"主要讲述了 并发编程的发展历程 解决可见性和有序性 解决原子性 死锁 等待通知 并发编程的发展历程并发编程问题的由来随着CPU 、 内存 、IO设备的不断发展，三者的速度差异一直是存在的（CPU一天 ， 内存一年 ， IO设备十年） 为了合理利用 CPU 的高性能，平衡这三者的速度差异，计算机体系机构、操作系统、编译程序都做出了贡献，主要体现为： CPU 增加了缓存，以均衡与内存的速度差异； 操作系统增加了进程、线程，以分时复用 CPU，进而均衡 CPU 与 I/O 设备的速度差异； 编译程序优化指令执行次序，使得缓存能够得到更加合理地利用 1.1 缓存导致的可见性问题 一个线程对共享变量的修改，另外一个线程能够立刻看到，我们称为可见性。 单核时代，电脑只有1个CPU，所有的线程操作的是同一个CPU的缓存，也就不存在可见性问题。 多核时代，每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了，当多个线程在不同的 CPU 上执行时，这些线程操作的是不同的 CPU 缓存，如下图线程A对变量V的操作线程B就看不到了。 比如下面这个代码：两个线程同时对变量count 进行2000次++操作。最后count的值一定&lt;=4000（由于线程操作的不可见性带来的操作覆盖） 1234567891011121314151617181920212223242526272829303132public class HelloWorld { private int count = 0; private void add() { int idx = 0; while(idx++ &lt; 2000) { count += 1; } } public static int calc() throws Exception { final HelloWorld test = new HelloWorld(); Thread th1 = new Thread(()-&gt;{ test.add(); }); Thread th2 = new Thread(()-&gt;{ test.add(); }); th1.start(); th2.start(); // 等待两个线程执行结束 th1.join(); th2.join(); return test.count; } public static void main(String[] args) throws Exception { long c =calc(); System.out.println(c); }} 1.2 线程切换带来的原子性问题 一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性 在一个时间片内，如果一个进程进行一个 IO 操作，例如读个文件，这个时候该进程可以把自己标记为“休眠状态”并出让 CPU 的使用权，待文件读进内存，操作系统会把这个休眠的进程唤醒，唤醒后的进程就有机会重新获得 CPU 的使用权了。 早期的操作系统基于进程来调度 CPU，不同进程间是不共享内存空间的，所以进程要做任务切换就要切换内存映射地址，而一个进程创建的所有线程，都是共享一个内存空间的，所以线程做任务切换成本就很低了。现代的操作系统都基于更轻量的线程来调度，现在我们提到的“任务切换”都是指“线程切换”。 Java并发程序都是基于多线程的，也就会涉及到任务切换。一般来说任务切换的时机大多数都是在一个时间片结束的时候，但是在Java这种高级语言中，一个语句往往需要多个CPU指令来执行。比如上面代码中的count += 1，至少需要三条 CPU 指令。 指令 1：首先，需要把变量 count 从内存加载到 CPU 的寄存器； 指令 2：之后，在寄存器中执行 +1 操作； 指令 3：最后，将结果写入内存（缓存机制导致可能写入的是 CPU 缓存而不是内存） 操作系统进行任务切换的时候，发生在任何一个CPU指令执行完之后，而不是可见的Java语言后。如下这种情况线程A就会把线程B执行的count+=1覆盖了。 编译优化带来的有序性问题有序性指的是程序按照代码的先后顺序执行。编译器为了优化性能，有时候会进行指令重排序。 重排序分3种类型。 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术(Instruction-Level Parallelism，ILP)来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应 机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上 去可能是在乱序执行 例如程序中：“a=6；b=7；”编译器优化后可能变成“b=7；a=6；”，在这个例子中，编译器调整了语句的顺序，但是不影响程序的最终结果，但是有时会出现问题。 比如单例模式中的双重检验模式： 1234567891011121314151617181920public class Singleton { static Singleton instance; public Singleton getInstance(){ if (instance == null){ // a synchronized (Singleton.class){ if (instance == null) instance = new Singleton(); //在CPU指令上并不是一步操作 } } return instance; }}/**解释一下这个双重校验模式：假设有两个线程 A、B 同时调用 getInstance() 方法，他们会同时发现 instance == null (同时执行到a处代码) ，于是同时对 Singleton.class 加锁，此时 JVM 保证只有一个线程能够加锁成功（假设是线程 A），另外一个线程则会处于等待状态（假设是线程 B）； 线程 A 会创建一个Singleton 实例，之后释放锁，锁释放后，线程 B 被唤醒，线程 B 再次尝试加锁，此时是可以加锁成功的，加锁成功后，线程 B 检查 instance == null 时会发现，已经创建过Singleton 实例了，所以线程 B 不会再创建一个 Singleton 实例。 看起来是没问题，但实际上还存在瑕疵，因为new一个新的对象分为如下几步（先在内存中初始化对象再赋值给变量）： 分配一块内存 M； 在内存 M 上初始化 Singleton 对象； 然后 M 的地址赋值给 instance 变量。 经过指令重排序变成了下面这种情况（先将内存赋值给变量再进行初始化）： 分配一块内存 M； 将 M 的地址赋值给 instance 变量； 最后在内存 M 上初始化 Singleton 对象。 不安全的情况：假设线程 A 先执行 getInstance() 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance() 方法，那么线程 B 在执行第一个判断时会发现 instance != null ，所以直接返回 instance，而此时的 instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。 解决可见性和有序性导致可见性的原因是缓存，导致有序性的原因是编译优化，那解决可见性、有序性最直接的办法就是禁用缓存和编译优化。虽然可以解决问题，但是程序的性能会受到很大影响，因此，最合理的方式应该是按需禁用缓存以及编译优化。何谓按需，按照程序的要求来进行禁用。 使用volatilevolatile在C语言中也存在，它的原始意义就是禁用CPU缓存。 声明一个 volatile 变量 volatile int x = 0，它表达的是：告诉编译器，对这个变量的读写，不能使用 CPU 缓存，必须从内存中读取或者写入。 Happens-Before 规则它想表达的是：前面一个操作的结果对后续的操作是可见的。Happens-Before 约束了编译器的优化行为，虽允许编译器优化，但是要求编译器优化后一定遵守 Happens\u0002Before 规则。 1234567891011121314class VolatileExample { int x = 0; volatile boolean v = false; public void writer() { x = 42; // a v = true; // b } public void reader() { if (v == true) { // c // 这里 x 会是多少呢？ } }} 假设线程 A 执行 writer() 方法，按照 volatile 语义，会把变量“v=true” 写入内存；假设线程 B 执行 reader() 方法，同样按照 volatile 语义，线程 B会从内存中读取变量 v，如果线程 B 看到 “v == true” 时，那么线程 B 看到的变量 x 是多少呢？ 同一线程的顺序性规则指在一个线程中，按照程序顺序，前面的操作 Happens-Before 于后续的任意操作。比如上面的代码 x = 42 Happens-Before 于代码 “v = true;”. volatile 变量规则对一个 volatile 变量的写操作， Happens-Before 于后续对这个 volatile 变量的读操作。 传递性指如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens\u0002-Before C。将这个规则和上面的volatile规则结合就能得到问题的答案： a 对b 可见 ， b是对volatile变量的写操作，所有b对c可见。 — 也就是 a 对c可见，所以 读出来是42。 管程中锁的规则指对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。 管程是一种通用的同步原语，在Java 中指的就是 synchronized，synchronized 是 Java 里对管程的实现。 线程 start() 规则主线程A启动子线程B后，子线程B可以看到主线程在启动子线程B之前的操作。 12345678910Thread B = new Thread(()-&gt;{ // 主线程调用 B.start() 之前 // 所有对共享变量的修改，此处皆可见 // 此例中，var==77 }); // 此处对共享变量 var 修改 var = 77; // 主线程启动子线程 B.start(); 线程Join()规则指主线程 A 等待子线程 B 完成（主线程 A 通过调用子线程 B的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），主线程能够看到子线程的操作。当然所谓的“看到”，指的是对共享变量的操作。 换句话说：如果在线程 A 中，调用线程 B 的 join() 并成功返回，那么线程 B 中的任意操作 Happens-Before 于该 join() 操作的返回。 12345678910111213Thread B = new Thread(()-&gt;{ // 此处对共享变量 var 修改 var = 66;}); // 例如此处对共享变量修改， // 则这个修改结果对线程 B 可见 // 主线程启动子线程 B.start(); B.join() // 子线程所有对共享变量的修改 // 在主线程调用 B.join() 之后皆可见 // 此例中，var==66 使用final关键字final 修饰变量时，初衷是告诉编译器：这个变量生而不变，可以可劲儿优化。Java 编译器在 1.5 以前的版本的确优化得很努力，以至于都优化错了。 解决原子性原子性问题的源头是线程切换，如果能够禁用线程切换那不就能解决这个问题了吗？而操作系统做线程切换是依赖 CPU 中断的，所以禁止 CPU 发生中断就能够禁止线程切换。在早期单核 CPU 时代，这个方案的确是可行的。但是在多核场景下，同一时刻，有可能有两个线程同时在执行，一个线程执行在 CPU-1上，一个线程执行在 CPU-2 上，此时禁止 CPU 中断，只能保证 CPU 上的线程连续执行，并不能保证同一时刻只有一个线程执行，如果这两个线程同时写 long 型变量高 32 位的话，那就有可能出现我们开头提及的诡异 Bug 了。 同一时刻只有一个线程执行”这个条件非常重要，我们称之为互斥 简易锁模型： 改进后：锁是用来保护资源的： synchronizedsynchronized既可以修饰方法，也可以修饰代码块。Java 编译器会在 synchronized 修饰的方法或代码块前后自动加上加锁 lock() 和解锁 unlock()，这样做的好处就是加锁 lock() 和解锁 unlock() 一定是成对出现的。 当修饰静态方法的时候，锁定的是当前类的 Class 对象，在上面的例子中就是 Class X； 当修饰非静态方法的时候，锁定的是当前实例对象 this。 修饰代码块： 1234567891011121314151617181920class X { // 修饰非静态方法 synchronized void foo() { // 临界区 } // 修饰静态方法 synchronized static void bar() { // 临界区 } // 修饰代码块 Object obj = new Object()； void baz() { synchronized(obj) { // 临界区 } }} 被 synchronized 修饰后，无论是单核CPU 还是多核 CPU，只有一个线程能够执行addOne方法,也就说如果有 1000 个线程执行 addOne() 方法，最终结果一定是 value 的值增加了 1000。 1234567891011public class HelloWorld { long value = 0L; long get(){ return value; } synchronized void addOne(){ value +=1; }} 执行 addOne() 方法后，value 的值对 get() 方法是可见的吗？这个可见性是没法保证的。管程中锁的规则，是只保证后续对这个锁的加锁的可见性，而 get() 方法并没有加锁操作，所以可见性没法保证。那如何解决呢？很简单，就是 get() 方法也 synchronized 一下， 1234567891011public class HelloWorld { long value = 0L; synchronized long get(){ return value; } synchronized void addOne(){ value +=1; }} 这样的话 get 和addone方法也是互斥的。 这个模型更像现实世界里面球赛门票的管理，一个座位只允许一个人使用，这个座位就是“受保护资源”，球场的入口就是 Java 类里的方法，而门票就是用来保护资源的“锁”，Java 里的检票工作是由 synchronized 解决的。 锁和受保护资源的关系受保护资源和锁之间的关联关系是 N:1 的关系.可以用一把锁来保护多个资源，但是不能用多把锁来保护一个资源 1234567891011public class HelloWorld { static long value = 0L; synchronized long get(){ return value; } synchronized static void addOne(){ value +=1; }} 这样的话就是两个锁锁的是同一个资源：两个锁分别是 this 和 SafeCalc.class。由于临界区 get() 和 addOne() 是用两个锁保护的，因此这两个临界区没有互斥 关系，临界区 addOne() 对 value 的修改对临界区 get() 也没有可见性保证，这就导致并发问题了。 锁，一定有一个要锁定的对象，至于这个锁定的对象要保护的资源以及在哪里加锁 / 解锁，就属于设计层面的事情了。 如何用一把锁来保护多个资源保护没有关联关系的多个资源例如，银行业务中有针对账户余额（余额是一种资源）的取款操作，也有针对账户密码（密码也是一种资源）的更改操作，我们可以为账户余额和账户密码分配不同的锁来解决并发问题： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * @author shengbinbin * @description: 保护没有关联关系的多个资源 */public class Account { // 锁：保护账户余额 private final Object balLock = new Object(); //账户余额 private Integer balance; //锁：保护账户密码 private final Object pwLock = new Object(); // 账户密码 private String password; //取款 void withdraw(Integer amt){ synchronized (balLock){ if (this.balance &gt; amt) { this.balance -= amt; } } } //查看余额 Integer getBalance(){ synchronized (balLock){ return balance; } } //更改密码 void updatePassword(String pw){ synchronized (pwLock){ this.password = pw; } } //查看密码 String getPassword(){ synchronized (pwLock){ return password; } } } 我们也可以用一把互斥锁来保护多个资源，例如我们可以用 this 这一把锁来管理账户类里所有的资源：账户余额和用户密码.就是性能太差，会导致取款、查看余额、修改密码、查看密码这四个操作都是串行的。 我们用两把锁，取款和修改密码是可以并行的。用不同的锁对受保护资源进行精细化管理，能够提升性能。这种锁还有个名字，叫细粒度锁。 保护有关联关系的多个资源例如银行业务里面的转账操作，账户 A 减少 100 元，账户 B 增加 100 元。这两个账户就是有关联关系的。那对于像转账这种有关联关系的操作，我们应该怎么去解决呢？ 1234567891011public class Account { private int balance; //怎么保证转账操作 transfer() 没有并发问题呢 void transfer(Account target , int amt){ if (this.balance &gt; amt){ this.balance -= amt; target.balance += amt; } }} 如果简单的用synchronize来修饰这个方法并不能解决并发问题，为什么呢？ 临界区内有两个资源，分别是转出账户的余额 this.balance 和转入账户的余额 target.balance，并且用的是一把锁 this。问题是这把锁可以保护自己的 余额，怎么保护别人的余额呢？ 下面我们具体分析一下，假设有 A、B、C 三个账户，余额都是 200 元，我们用两个线程分别执行两个转账操作： 账户 A 转给账户 B 100 元，账户 B 转给账户 C 100 元， 最后我们期望的结果应该是账户 A 的余额是 100 元，账户 B 的余额是 200 元， 账户 C 的余额是300元。 我们假设线程 1 执行账户 A 转账户 B 的操作，线程 2 执行账户 B 转账户 C 的操作。这两个线程分别在两颗 CPU 上同时执行，那它们是互斥的吗？ 我们期望是，但实际上并不是。 因为线程 1 锁定的是账户 A 的实例（A.this），而线程 2 锁定的是账户 B 的实例（B.this），所以这两个线程可以同时进入临界区 transfer()。同时进入临界区的结果是什么呢？ 线程 1 和线程 2 都会读到账户 B 的余额为 200，导致最终账户 B 的余额可能是300（线程 1 后于线程 2 写 B.balance，线程 2 写的 B.balance 值被线程 1 覆盖）， 可能是 100（线程 1 先于线程 2 写 B.balance，线程 1 写的 B.balance 值被线程 2 覆盖），就是不可能是 200。 如何解决这个问题呢？ 只要我们的锁能覆盖所有受保护资源就可以了 this 是对象级别的锁，所以 A 对象和 B 对象都有自己的锁，如何让 A 对象和 B 对象共享一把锁呢？ 方案1：把 Account 默认构造函数变为 private，同时增加一个带 Object lock 参数的构造函数，创建 Account 对象时，传入相同的 lock，这样所有的 Account 对象都会共享这个lock 了。 12345678910111213141516171819class Account { private Object lock； private int balance; private Account(); // 创建 Account 时传入同一个 lock 对象 public Account(Object lock) { this.lock = lock;} // 转账 void transfer(Account target, int amt){ // 此处检查所有对象共享的锁 synchronized(lock) { if (this.balance &gt; amt) { this.balance -= amt; target.balance += amt;}}}} 它要求在创建 Account 对象的时候必须传入同一个对象，如果创建 Account 对象时，传入的 lock 不是同一个对象，会出现锁自家门来保护他家资产的荒唐事 方案2：用Account.class 作为共享的锁。Account.class 是所有 Account 对象共享的，而且这个对象是 Java 虚拟机在加载 Account 类的时候创建的，所以我们不用担心它的唯一性。使用Account.class 作为共享的锁，我们就无需在创建 Account 对象时传入了，代码更简单 1234567891011121314public class Account { private int balance; //怎么保证转账操作 transfer() 没有并发问题呢 void transfer(Account target , int amt){ synchronized (Account.class){ if (this.balance &gt; amt){ this.balance -= amt; target.balance += amt; } } }} 缺点：会导致所有转账都是串行的。 总结：对如何保护多个资源已经很有心得了，关键是要分析多个资源之间的关系。如果资源之间没有关系，很好处理，每个资源一把锁就可以了。如果资源之间有关联关系，就要选择一个粒度更大的锁，这个锁应该能够覆盖所有相关的资源。 死锁在古代的时候，账户的存在形式真的就是一个账本，而且每个账户都有一个账本，这些账本都统一存放在文件架上。在给我们做转账时，要去文件架上把转出账本和转入账本都拿到手，然后做转账。这个柜员在拿账本的时候可能遇到以下三种情况： 文件架上恰好有转出账本和转入账本，那就同时拿走； 如果文件架上只有转出账本和转入账本之一，那这个柜员就先把文件架上有的账本拿到手，同时等着其他柜员把另外一个账本送回来； 转出账本和转入账本都没有，那这个柜员就等着两个账本都被送回来。 类比到现在的模型，其实就是两把锁：转出账本 和 转入账本。在 transfer() 方法内部，我们首先尝试锁定转出账户 this（先把转出账本拿到手），然后尝试锁定转入账户 target（再把转入账本拿到手），只有当两者都成功时，才执行转账操作。 代码实现： 123456789101112131415161718192021/** * @author shengbinbin * @description: 转账 */public class Account { private int balance; void transfer(Account target , int amt){ //1. 先锁定转出账号 synchronized (this){ //① //2. 再锁定转入的账号 synchronized (target){ //② if (this.balance &gt; amt){ this.balance -= amt; target.balance += amt; } } } }} 这样的话，我们只需要锁定两个账户就行了，而不是锁定Account.class。锁的粒度更小了，并发度更高了。但是我们需要警惕死锁的发生： 出现死锁如果有客户找柜员张三做个转账业务：账户 A 转账户 B 100 元 此时另一个客户找柜员李四也做个转账业务：账户 B转账户 A 100 元， 于是张三和李四同时都去文件架上拿账本，这时候有可能凑巧张三拿到了账本 A，李四拿到了账本 B。张三拿到账本 A 后就等着账本 B（账本 B 已经被李四拿走），而李四拿到账本 B 后就等着账本 A（账本 A 已经被张三拿走），他们要等多久呢？ 他们会永远等待下去…因为张三不会把账本 A 送回去，李四也不会把账本 B 送回去。这就是死锁。 死锁的一个比较专业的定义是：一组互相竞争资源的线程因互相等待，导致“永久”阻塞的现象。 比较上面的代码可知： 当 T1 和 T2 同时执行完①处的代码时，T1 获得了账户 A 的锁（对于 T1，this 是账户A），而 T2 获得了账户 B 的锁（对于 T2，this 是账户 B）。之后 T1 和 T2 在执行②处的代码时，T1 试图获取账户 B 的锁时，发现账户 B 已经被锁定（被 T2 锁定），所以 T1 开始等待；T2 则试图获取账户 A 的锁时，发现账户 A 已经被锁定（被 T1 锁定），所以 T2也开始等待。于是 T1 和 T2 会无期限地等待下去，也就是我们所说的死锁了。 死锁的条件及预防死锁发生的四个条件： 互斥，共享资源 X 和 Y 只能被一个线程占用； 占有且等待，线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X； 不可抢占，其他线程不能强行抢占线程 T1 占有的资源； 循环等待，线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等待。 破坏这4个条件： 对于“占用且等待”这个条件，我们可以一次性申请所有的资源，这样就不存在等待了。 对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。 对于“循环等待”这个条件，可以靠按序申请资源来预防。所谓按序申请，是指资源是有线性顺序的 如何实践： 可以增加一个账本管理员，只有账本 A 和 B 都在的时候才会给张三，也就是一次性申请所有的资源。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * @author shengbinbin * @description: 账本管理员 * @date 2021/8/292:39 下午 */public class Allocator { private List&lt;Object&gt; als = new ArrayList&lt;&gt;(); // 一次性申请所有的资源 synchronized boolean apply(Object from , Object to){ if (als.contains(from) || als.contains(to)){ return false; }else { als.add(from); als.add(to); } return true; } // 归还资源 synchronized void free(Object from , Object to){ als.remove(from); als.remove(to); }}public class Account { private int balance; private Allocator allocator; // 必须为单例 // 转账 void transfer(Account target , int amt){ while(!allocator.apply(target,this)){ // 一次性申请转出账户和转入账户，直到成功 // 死循环 ; } try { //1. 先锁定转出账号 synchronized (this){ //2. 再锁定转入的账号 synchronized (target){ if (this.balance &gt; amt){ this.balance -= amt; target.balance += amt; } } } }finally { allocator.free(this,target); } }} 破坏不可抢占条件: 这一点synchronized 是做不到的。原因是 synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。在java.util.concurrent 这个包下面提供的 Lock 是可以轻松解决这个问题的。 破坏循环等待条件：破坏这个条件，需要对资源进行排序，然后按序申请资源。申请的时候，我们可以按照从小到大的顺序来申请: 12345678910111213141516171819202122232425public class Account { private int id; private int balance; // 转账 void transfer(Account target , int amt){ Account left = this; Account right = target; if (this.id &gt; target.id){ left = target; right = this; } // 先锁序号小的账户 synchronized (left){ // 再锁序号大的账户 synchronized (right){ if (this.balance &gt; amt){ this.balance -= amt; target.balance += amt; } } } }} 例如上面转账那个例子，我们破坏占用且等待条件的成本就比破坏循环等待条件的成本高，破坏占用且等待条件，我们也是锁了所有的账户，而且还是用了死循环 while(!actr.apply(this, target));方法，不过好在 apply() 这个方法基本不耗时。 在转账这个例子中，破坏循环等待条件就是成本最低的一个方案。 等待通知机制现实中的等待通知例子去医院就诊： 患者先去挂号，然后到就诊门口分诊，等待叫号。这一步就是线程要去获取互斥锁，叫到号意味着线程已经获得锁了。 就诊过程中，大夫可能会让患者去做检查。这一步类似的就是线程的条件没有得到满足。 患者去做检查。这一步就是线程进入等待状态。 大夫叫下一个患者。这一步就是上一个线程释放了互斥锁。 患者做完检查。 这一步就是线程的条件重新满足了。 患者拿到检查报告重新去分诊等待较好。这一步就是这个线程要重新去获取锁。 综合一下，就可以得出一个完整的等待 - 通知机制：线程首先获取互斥锁，当线程要求的条件不满足时，释放互斥锁，进入等待状态；当要求的条件 满足时，通知等待的线程，重新获取互斥锁。 用synchronized来实现等待通知 这个等待队列和互斥锁是一对一的关系，每个互斥锁都有自己独立的等待队列。 因为notify() 只能保证在通知时间点，条件是满足的。而被通知线程的执行时间点和通知的时间点基本上不会重合，所以当线程执行的时候，很可能条件已经不满足了（保不齐有其他线程插队）。 12345678910111213141516171819202122232425262728293031323334353637public class Allocator { private List&lt;Object&gt; als = new ArrayList&lt;&gt;(); // 一次性申请所有的资源 synchronized void apply(Object from , Object to){ // 这里用while可以避免虚假唤醒 while (als.contains(from) || als.contains(to)){ try { wait(); } catch (InterruptedException e) { e.printStackTrace(); } } als.add(from); als.add(to); } // 归还资源 synchronized void free(Object from , Object to){ als.remove(from); als.remove(to); /** * 注意这里尽量用notifyAll 唤醒线程而不是notify * 因为notify会随机的通知等待队列中的一个线程，这样的话会蕴含一些风险：可能导致某些线程永远不会被通知到。 * * 假设我们有资源 A、B、C、D，线程 1 申请到了 AB，线程 2 申请到了 CD，此时线程 3 申 * 请 AB，会进入等待队列（AB 分配给线程 1，线程 3 要求的条件不满足），线程 4 申请 * CD 也会进入等待队列。我们再假设之后线程 1 归还了资源 AB，如果使用 notify() 来通知 * 等待队列中的线程，有可能被通知的是线程 4，但线程 4 申请的是 CD，所以此时线程 4 还 * 是会继续等待，而真正该唤醒的线程 3 就再也没有机会被唤醒了。 */ notifyAll(); }}","link":"/2021/08/28/Java%E5%B9%B6%E5%8F%91%EF%BC%88%E4%B8%80%EF%BC%89%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%94%81/"},{"title":"Java并发（二）管程和线程","text":"什么是管程 线程 线程数的设置 管程什么是管程Java 采用的是管程技术，synchronized 关键字及 wait()、notify()、notifyAll() 这三个方法都是管程的组成部分。而管程和信号量是等价的，所谓等价指的是用管程能够实现信号量，也能用信号量实现管程。但是管程更容易使用，所以 Java 选择了管程。 管程，对应的英文是 Monitor，指的是管理共享变量以及对共享变量的操作过程，让他们支持并发。翻译为Java 领域的语言，就是管理类的成员变量和成员方法，让这个类是线程安全的。 在并发编程领域，有两大核心问题： 一个是互斥，即同一时刻只允许一个线程访问共享资源； 另一个是同步，即线程之间如何通信、协作。这两大问题，管程都是能够解决的。 如何解决互斥管程解决互斥问题的思路很简单，就是将共享变量及其对共享变量的操作统一封装起来。 比如：管程 X 将共享变量 queue 这个队列和相关的操作入队 enq()、出队 deq() 都封装起来了；线程 A 和线程 B 如果想访问共享变量 queue，只能通过调用管程提供的 enq()、deq() 方法来实现；enq()、deq() 保证互斥性，只允许一个线程进入管程。管程的模型和面向对象的思想高度契合。 如何解决同步在管程模型里，共享变量和对共享变量的操作是被封装起来的，图中最外层的框就代表封装的意思。框的上面只有一个入口，并且在入口旁边还有一个入口等待队列。当多个线程同时试图进入管程内部时，只允许一个线程进入，其他线程则在入口等待队列中等待。这个过程类似就医流程的分诊，只允许一个患者就诊，其他患者都在门口等待。 管程里还引入了条件变量的概念，而且每个条件变量都对应有一个等待队列，如下图，条件变量 A 和条件变量 B 分别都有自己的等待队列。 假设有个线程 T1 执行出队操作，不过需要注意的是执行出队操作，有个前提条件，就是队列不能是空的，而队列不空这个前提条件就是管程里的条件变量。 如果线程 T1 进入管程后恰好发现队列是空的，那怎么办呢？等待啊，去哪里等呢？就去条件变量对应的等待队列里面等。此时线程 T1 就去“队列不空”这个条件变量的等待队列中等待。这个过程类似于大夫发现你要去验个血，于是给你开了个验血的单子，你呢就去验血的队伍里排队。线程T1 进入条件变量的等待队列后，是允许其他线程进入管程的。这和你去验血的时候，医生可以给其他患者诊治，道理都是一样的。 再假设之后另外一个线程 T2 执行入队操作，入队操作执行成功之后，“队列不空”这个条件对于线程 T1 来说已经满足了，此时线程 T2 要通知 T1，告诉它需要的条件已经满足了。当线程 T1 得到通知后，会从等待队列里面出来，但是出来之后不是马上执行，而是重新进入到入口等待队列里面。这个过程类似你验血完，回来找大夫，需要重新分诊。 Java 参考了 MESA 模型，语言内置的管程（synchronized）对 MESA 模型进行了精简。MESA 模型中，条件变量可以有多个，Java 语言内置的管程里只有一个条件变量。 线程Java 语言里的线程本质上就是操作系统的线程，它们是一一对应的。 通用的线程生命周期 初始状态，指的是线程已经被创建，但是还不允许分配 CPU 执行。这个状态属于编程语言特有的，不过这里所谓的被创建，仅仅是在编程语言层面被创建，而在操作系统层面，真正的线程还没有创建。 可运行状态，指的是线程可以分配 CPU 执行。在这种状态下，真正的操作系统线程已经被成功创建了，所以可以分配 CPU 执行。 当有空闲的 CPU 时，操作系统会将其分配给一个处于可运行状态的线程，被分配到CPU 的线程的状态就转换成了运行状态。 运行状态的线程如果调用一个阻塞的 API（例如以阻塞方式读文件）或者等待某个事件（例如条件变量），那么线程的状态就会转换到休眠状态，同时释放 CPU 使用权，休眠状态的线程永远没有机会获得 CPU 使用权。当等待的事件出现了，线程就会从休眠状态转换到可运行状态。 线程执行完或者出现异常就会进入终止状态，终止状态的线程不会切换到其他任何状态，进入终止状态也就意味着线程的生命周期结束了。 Java 语言里则把可运行状态和运行状态合并了，这两个状态在操作系统调度层面有用，而 JVM 层面不关心这两个状态，因为 JVM 把线程调度交给操作系统处理了。 Java中线程的生命周期六种状态： NEW（初始化状态） RUNNABLE（可运行 / 运行状态） BLOCKED（阻塞状态） WAITING（无时限等待） TIMED_WAITING（有时限等待） TERMINATED（终止状态） 状态转换： RUNNABLE 与 BLOCKED 的状态转换：只有一种场景，就是就是线程等待 synchronized 的隐式锁。synchronized 修饰的方法、代码块同一时刻只允许一个线程执行，其他线程只能等待，这种情况下，等待的线程就会从 RUNNABLE 转换到 BLOCKED 状态。而当等待的线程获得 synchronized 隐式锁时，就又会从 BLOCKED 转换到 RUNNABLE 状态。 如果你熟悉操作系统线程的生命周期的话，可能会有个疑问：线程调用阻塞式 API 时，是否会转换到 BLOCKED 状态呢？在操作系统层面，线程是会转换到休眠状态的，但是在JVM 层面，Java 线程的状态不会发生变化，也就是说 Java 线程的状态会依然保持RUNNABLE 状态。JVM 层面并不关心操作系统调度相关的状态，因为在 JVM 看来，等待CPU 使用权（操作系统层面此时处于可执行状态）与等待 I/O（操作系统层面此时处于休眠状态）没有区别，都是在等待某个资源，所以都归入了 RUNNABLE 状态。 RUNNABLE 与 WAITING 的状态转换: 第一种场景，获得 synchronized 隐式锁的线程，调用无参数的 Object.wait() 方法。 第二种场景，调用无参数的 Thread.join() 方法。其中的 join() 是一种线程同步方法，例如有一个线程对象 thread A，当调用 A.join() 的时候，执行这条语句的线程会等待 thread A执行完，而等待中的这个线程，其状态会从 RUNNABLE 转换到 WAITING。当线程thread A 执行完，原来等待它的线程又会从 WAITING 状态转换到 RUNNABLE。 第三种场景，调用 LockSupport.park() 方法。其中的 LockSupport 对象，也许你有点陌生，其实 Java 并发包中的锁，都是基于它实现的。调用 LockSupport.park() 方法，当前线程会阻塞，线程的状态会从 RUNNABLE 转换到 WAITING。调用LockSupport.unpark(Thread thread) 可唤醒目标线程，目标线程的状态又会从WAITING 状态转换到 RUNNABLE。 RUNNABLE 与 TIMED_WAITING 的状态转换: 调用带超时参数的 Thread.sleep(long millis) 方法； 获得 synchronized 隐式锁的线程，调用带超时参数的 Object.wait(long timeout) 方法； 调用带超时参数的 Thread.join(long millis) 方法； 调用带超时参数的 LockSupport.parkNanos(Object blocker, long deadline) 方法； 调用带超时参数的 LockSupport.parkUntil(long deadline) 方法。 TIMED_WAITING 和 WAITING 状态的区别，仅仅是触发条件多了超时参数。 从 NEW 到 RUNNABLE 状态:只要调用线程对象的start() 方法就可以了 从 RUNNABLE 到 TERMINATED 状态:线程执行完 run() 方法后，会自动转换到 TERMINATED 状态，当然如果执行 run() 方法的时候异常抛出，也会导致线程终止。 stop() 和 interrupt() 方法的区别 stop() 方法会真的杀死线程，不给线程喘息的机会，如果线程持有 ReentrantLock 锁，被stop() 的线程并不会自动调用 ReentrantLock 的 unlock() 去释放锁，那其他线程就再也没机会获得 ReentrantLock 锁，这实在是太危险了。所以该方法就不建议使用了，类似的方法还有 suspend() 和 resume() 方法，这两个方法同样也都不建议使用了。 interrupt() 方法仅仅是通知线程，线程有机会执行一些后续操作，同时也可以无视这个通知。被 interrupt 的线程，是怎么收到通知的呢？一种是异常，另一种是主动检测。 当线程 A 处于 WAITING、TIMED_WAITING 状态时，如果其他线程调用线程 A 的interrupt() 方法，会使线程 A 返回到 RUNNABLE 状态，同时线程 A 的代码会触发InterruptedException 异常。上面我们提到转换到 WAITING、TIMED_WAITING 状态的触发条件，都是调用了类似 wait()、join()、sleep() 这样的方法，我们看这些方法的签名，发现都会 throws InterruptedException 这个异常。这个异常的触发条件就是：其他线程调用了该线程的 interrupt() 方法。 当线程 A 处于 RUNNABLE 状态时，并且阻塞在 java.nio.channels.InterruptibleChannel上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 会触发java.nio.channels.ClosedByInterruptException 这个异常；而阻塞在java.nio.channels.Selector 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A的 java.nio.channels.Selector 会立即返回。 上面这两种情况属于被中断的线程通过异常的方式获得了通知。 还有一种是主动检测，如果线程处于 RUNNABLE 状态，并且没有阻塞在某个 I/O 操作上，例如中断计算圆周率的线程 A，这时就得依赖线程 A 主动检测中断状态了。如果其他线程调用线程 A 的 interrupt()方法，那么线程 A 可以通过 isInterrupted() 方法，检测是不是自己被中断了。 线程数的设置多少才是合理的为什么要用多线程本质上是为了提升性能。那么如何衡量性能的好坏呢？主要有下面两个指标： 延迟：指的是发出请求到收到响应这个过程的时间 吞吐量：指的是在单位时间内能处理请求的数量；吞吐量越大，意味着程序能处理的请求越多，性能也就越好。 那么如何降低延迟，增加吞吐量呢？ 优化算法。 发挥硬件的性能：主要就是IO和CPU两个方面的利用率。 创建多少线程合适呢我们的程序一般都是CPU计算和IO操作交叉执行的。由于IO设备的速度肯定要比CPU计算的速度慢的多，所以不部分情况下，IO操作执行的时间都要比CPU计算的时间长，这种场景我们成为IO密集型。相对应的如果大部分是纯CPU计算的场景我们称为CPU密集型。 对于IO密集型的场景，本线程本质上就是提升多核CPU的效率，最佳的线程数是与程序中的IO操作和CPU计算的耗时比相关的：线程数 = 1 + （IO耗时 / CPU耗时）。这样的话当线程 A 执行 IO 操作时，另外 R 个线程正好执行完各自的 CPU 计算。这样 CPU 的利用率就达到了 100%。至于多核 CPU，也很简单，只需要等比扩大就可以了。 对于 CPU 密集型的计算场景，理论上“线程的数量 =CPU 核数”就是最合适的。不过在工程上，线程的数量一般会设置为“CPU 核数 +1”，这样的话，当线程因为偶尔的内存页失效或其他原因导致阻塞时，这个额外的线程可以顶上，从而保证CPU 的利用率。 为什么局部变量是安全的局部变量是不存在数据竞争的，但是至于原因嘛，就说不清楚了。那它背后的原因到底是怎样的呢？要弄清楚这个，你需要一点编译原理的知识。你知道在 CPU 层面，是没有方法概念的，CPU 的眼里，只有一条条的指令。编译程序，负责把高级 语言里的方法转换成一条条的指令。所以你可以站在编译器实现者的角度来思考“怎么完成 方法到指令的转换”。 方法是如何被执行的代码如下： 123int a = 7；int[] b = fibonacci(a);int[] c = b; 调用fibonacci方法时，CPU会先找到这个方法的地址，然后跳转到这个地址再去执行代码，最后执行完这个方法之后，要能够返回到话首先需要找到调用方法的下一个语句的地址，再跳转到这个地址继续去执行。 那么CPU是如何找到调用方法的参数和返回地址呢？ —- 通过CPU的堆栈寄存器 比如A调用B，B调用C。那么在运行的时候就会出现下面这样的调用栈，每个方法在调用栈中都有自己的独立空间，称为栈帧。而每个栈帧中都有对应方法需要的参数和返回地址。而局部变量就是存放在栈帧中，随着方法同生共死。 所以每个线程都有自己的调用栈，局部变量存放在线程各自的调用栈里，不会共享，也就不会有线程安全的问题了。","link":"/2021/08/24/Java%E5%B9%B6%E5%8F%91%EF%BC%88%E4%BA%8C%EF%BC%89%E7%AE%A1%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/"},{"title":"Java并发（三）并发工具类","text":"并发工具类 Lock和Condition前面说过了管程的概念，Java关键字synchronized本身就是管程的体现之一。初次之外，JavaSDK并发包中还提供了Lock和Condition两个接口来实现管程，Lock来解决互斥问题，而Condition用来解决同步问题。 为什么不直接用synchronized呢在预防死锁时，有一个方案叫”破坏不可抢占的条件“。而synchronized是没办法实现的，因为synchronized去申请资源时，如果没有申请到，线程就直接阻塞了，啥也干不了，也释放不了已经占有的资源了。 对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。 如何解决synchronized的这个弊端呢？ 响应中断：synchronized的问题就是一旦发生死锁，就没有机会去唤醒阻塞的线程了。但如果阻塞的线程可以响应中断信号，这样就有机会释放它自己占用的资源了，这样就可以避免死锁。 支持超时：如果线程在一段时间内没有获取到锁，不是进入到阻塞状态，而是返回一个错误。这样也可以避免死锁。 非阻塞的获取锁：如果线程获取锁失败，并不是进入阻塞状态，而是直接返回。 以上的三种方法对应的就是Lock接口下的三个方法： 12345678// 支持中断的 APIvoid lockInterruptibly() throws InterruptedException;// 支持超时的 APIboolean tryLock(long time, TimeUnit unit) throws InterruptedException;// 支持非阻塞获取锁的 APIboolean tryLock(); 如何保证可见性的原理简单说就是利用了volatile相关的happens-before规则。比如说ReentrantLock类中有一个抽象类Sync继承了AbstractQueuedSynchronizer，内部持有一个volatile的成员变量state，每次获取锁和解锁的时候都回去读写state中的值。 也就是说在执行lock 和unlock中间的业务代码之前和之后都会读写volatile变量state。根据happens-before的相关规则： 顺序性规则：线程T1，执行业务代码 happens-before 释放锁的操作 unlock。 volatile变量规则：由于state的读取，线程T1的unlock操作 happens-before 线程T2的lock操作。 传递性规则：线程T1的业务代码 happens-before 线程T2的lock操作。 这样的话，后续线程T2可以看到业务代码执行之后的正确结果。 什么是可重入锁ReentrantLock的意思就是可重入的锁。顾名思义，指的就是线程可以重复的获取同一把锁。例如下面的代码： 12345678910111213141516171819202122232425262728/** * @author shengbinbin * @description: 可重入锁测试 */public class ReentrantTest { private final Lock lock = new ReentrantLock(); int value; public int getValue(){ lock.lock(); try { return value; }finally { lock.unlock(); } } public void addOne(){ lock.lock(); try { value = 1 + getValue(); // 线程执行到这里时调用getValue方法时会再次加锁 }finally { lock.unlock(); // 保证锁可释放 } }} 公平锁和非公平锁实现原理：锁都对应着一个等待队列，如果一个线程没有获得锁就会进入等待队列。当有线程释放锁的时候，就需要从等待队列中唤醒一个等待的线程。这时如果是公平锁，唤醒的策略就是谁等的时间长就唤醒谁。而非公平就不能保证这个。 一些实践建议出自 《Java 并发编程：设计原则与模式》 永远只在更新对象的成员变量时加锁 永远只在访问可变的成员变量时加锁 永远不在调用其他对象的方法时加锁 Condition支持了多个条件变量在很多并发场景下，支持多个条件变量能够让我们的并发程序可读性更好，实现起来也更容易。例如，实现一个阻塞队列，就需要两个条件变量。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * @author shengbinbin * @description: 实现一个阻塞队列 */public class BlockedQueue&lt;T&gt; { final Lock lock = new ReentrantLock(); // 条件变量：队列不满/不空 final Condition notFull = lock.newCondition(); final Condition notEmpty = lock.newCondition(); // 入队 void enq(T x){ lock.lock(); try { while (队列已满){ notFull.await(); } //省略入队操作. // 入队后, 通知可出队 notEmpty.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } // 出队 void deq(){ lock.lock(); try { while (队列已空){ // 等待队列不空 notEmpty.await(); } // 省略出队操作... // 出队后，通知可入队 notFull.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } }} 同步和异步同步和异步的区别到底是什么呢？ 通俗点来讲就是调用方是否需要等待结果，如果需要等待结果，就是同步；如果不需要等待结果，就是异步。 如果想让你的程序支持异步，有两种方法： 调用方创建一个子线程，在子线程中执行方法调用，这种方法称为异步调用。 方法实现的时候，创建一个新的线程来执行主要逻辑，主线程直接return。这种方法称为异步方法。 在 TCP 协议层面，发送完 RPC 请求后，线程是不会等待 RPC 的响应结果的。但是我们平时工作中的RPC调用大多数是同步的 这说明一定有人做了异步转同步的事情，本来发送请求是异步的，但是调用线程却阻塞了，说明Dubbo 帮我们做了异步转同步的事情。通过调用栈，你能看到线程是阻塞在DefaultFuture.get() 方法上，所以可以推断：Dubbo 异步转同步的功能应该是通过DefaultFuture 这个类实现的。 需求是：当 RPC 返回结果之前，阻塞调用线程，让调用线程等待；当 RPC 返回结果后，唤醒调用线程，让调用线程重新执行 这不就是一个经典的等待通知机制吗？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import javax.xml.ws.Response;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * @author shengbinbin * @description: TODO * @date 2021/9/510:07 下午 */public class AsyncToSyn { private final Lock lock = new ReentrantLock(); private final Condition done = lock.newCondition(); // 调用方通过该方法等待结果 Object get(int timeout) throws TimeoutException { long start = System.nanoTime(); lock.lock(); try { while (!isDone()){ done.await(timeout , TimeUnit.SECONDS); long cur = System.nanoTime(); if (isDone() || cur - start &gt; timeout){ break; } } } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } if (!isDone()){ throw new TimeoutException(); } return response; } //判断结果是否返回 private boolean isDone() { return response != null; } // RPC 结果返回时调用这个方法 private void doReceived(Response response){ lock.unlock(); try { response = res; if (done != null){ done.signal(); } }finally { lock.unlock(); } }} 调用线程通过调用 get() 方法等待 RPC 返回结果，这个方法里面，你看到的都是熟悉的“面孔”：调用 lock() 获取锁，在 finally 里面调用 unlock() 释放锁；获取锁后，通过经典的在循环中调用 await() 方法来实现等待。 当 RPC 结果返回时，会调用 doReceived() 方法，这个方法里面，调用 lock() 获取锁，在finally 里面调用 unlock() 释放锁，获取锁后通过调用 signal() 来通知调用线程，结果已经返回，不用继续等待了。 Semaphore信号量用来控制同时访问某个特定资源的操作数量，或者同时执行某个指定操作的数量。信号量还可以用来实现某种资源池，或者对容器施加边界。 Semaphore管理着一组许可（permit）,许可的初始数量可以通过构造函数设定，操作时首先要获取到许可，才能进行操作，操作完成后需要释放许可。如果没有获取许可，则阻塞到有许可被释放。如果初始化了一个许可为1的Semaphore，那么就相当于一个不可重入的互斥锁（Mutex）。 核心方法： Semaphore(int premits,boolean fair)： 构造器方法。permits为信号量初始化数量，第二个参数fair可以设置是否需要公平策略，如果传入true，那么Semaphore会把等待的线程放入FIFO队列中，以便许可证被释放后，可以分配给等待时间最长的线程； acquire()： 试图获取许可证，如果当前没有可用的，就会进入阻塞等待状态； tryAcquire()： 试图获取许可证，如果是否能够获取，都不会进入阻塞。 tryAcquire(long timeout, TimeUnit unit)： 和tryAcquire一样，只是多了一个超时时间，等待指定时间还获取不到许可证，就会停止等待； availablePermits: 获取可用许可证数量； release()： 释放一个许可证； release(int permits)： 释放指定数量的许可证。 使用案例： 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.util.concurrent.Executor;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Semaphore;/** * @author shengbinbin * @description: 保证同一时间内只有3个线程可以拿到许可证执行业务代码 * @date 2021/9/510:28 下午 */public class SemaphoreDemo { static Semaphore semaphore = new Semaphore(3,true); public static void main(String[] args) { ExecutorService service = Executors.newFixedThreadPool(5); for (int i = 0; i &lt; 100; i++) { service.submit(new Runnable() { @Override public void run() { try { semaphore.acquire(1); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot;：获取到许可证&quot;); try { // 执行业务代码 Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot;：释放许可证&quot;); semaphore.release(); } }); } service.shutdown(); }} 注意事项： 获取和释放的许可证数量必须一致，否则随着时间的推移，最后许可证数量不够用，会导致线程卡死。 Semaphore设置是否公平性时，一般设置为true比较合理，因为Semaphore使用场景就是用在耗时较长的操作，如果被反复插队，线程就会持续陷入等待。 获取和释放的许可证不要求为同一个线程，只要满足我们业务需要，可以由A线程获取许可证，让B线程来释放。 ReadWriteLock读写锁： 允许多个线程同时读共享变量 只允许一个线程写共享变量 如果一个线程正在写共享变量，此时会禁止别的线程读取共享变量 StampedLockCountDownLatchCyclicBarrier","link":"/2021/08/24/Java%E5%B9%B6%E5%8F%91%EF%BC%88%E4%B8%89%EF%BC%89%E5%B9%B6%E5%8F%91%E5%B7%A5%E5%85%B7%E7%B1%BB/"},{"title":"Java并发（五）原子类","text":"","link":"/2021/08/24/Java%E5%B9%B6%E5%8F%91%EF%BC%88%E4%BA%94%EF%BC%89%E5%8E%9F%E5%AD%90%E7%B1%BB/"},{"title":"Java并发（六）线程池","text":"本节主要围绕下面几个问题来讲述： 为什么要使用线程池 如何正确的创建线程池 如何获取线程池的执行结果 正确的使用CompletableFuture 正确的使用CompletionService 为什么要使用线程池第一：降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 第二：提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 第三：提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。但是，要做到合理利用线程池，必须对其实现原理了如指掌。 如何正确的创建线程池线程池的结果获取FutureThreadPoolExecutor的execute方法并没有返回值，那我们如何获取任务的执行结果呢？ 123456789101112131415161718192021222324//java.util.concurrent.ThreadPoolExecutor#executepublic void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); //1. 如果正在运行的线程小于 corePoolSize ，那么就尝试启动一个新的线程使用给定的命令作为第一个任务 if (workerCountOf(c) &lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } //2. 如果任务可以成功的排队，我们仍然需要检测是否需要一个新的线程（可能自从上次检测后线程就死亡了或者线程池关闭了） if (isRunning(c) &amp;&amp; workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } //3. 如果不能成功的排队任务，我们需要添加一个新的线程。如果添加失败就拒绝这个任务。 else if (!addWorker(command, false)) reject(command); } ExecutorService提供了三个重载submit方法，返回值都是Future接口 123456789//这个方法的参数是一个 Runnable 接口，Runnable 接口的 run() 方法是没有返回值的，所以 `submit(Runnable task)` 这个方法返回的 Future 仅可以用来断言任务已经结束了，类似于 Thread.join()。Future&lt;?&gt; submit(Runnable task);//这个方法的参数是一个 Callable 接口，它只有一个 call() 方法，并且这个方法是有返回值的，所以这个方法返回的 Future 对象可以通过调用其 get() 方法来获取任务的执行结果。&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task);//这个方法很有意思，假设这个方法返回的 Future 对象是 f，f.get() 的返回值就是传给 submit() 方法的参数 result&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); Future接口有5个方法： 123456789101112// 取消任务boolean cancel()boolean mayInterruptIfRunning);// 判断任务是否已取消 boolean isCancelled();// 判断任务是否已结束boolean isDone();// 获得任务执行结果get();// 获得任务执行结果，支持超时get(long timeout, TimeUnit unit); FutureTask工具类FutureTask 实现了 Runnable 和 Future 接口： 12345678910111213// 有两个构造函数public FutureTask(Callable&lt;V&gt; callable) { if (callable == null) throw new NullPointerException(); this.callable = callable; this.state = NEW; // ensure visibility of callable }public FutureTask(Runnable runnable, V result) { this.callable = Executors.callable(runnable, result); this.state = NEW; // ensure visibility of callable } 由于实现了 Runnable 接口，所以可以将 FutureTask 对象作为任务提交给 ThreadPoolExecutor 去执行，也可以直接被 Thread 执行； 又因为实现了 Future 接口，所以也能用来获得任务的执行结果。 123456789101112131415public static void main(String[] args) throws ExecutionException, InterruptedException { // 创建 FutureTask FutureTask futureTask = new FutureTask&lt;&gt;(() -&gt; 1+2); // 创建线程池 ExecutorService threadPool = newCachedThreadPool(); // 提交 FutureTask threadPool.submit(futureTask); //通过 FutureTask的get方法获取结果 System.out.println(futureTask.get()); //3 // 也可以直接将FutureTask 作为线程构造函数的参数 new Thread(futureTask).start(); System.out.println(futureTask.get()); } 利用 FutureTask 对象可以很容易获取子线程的执行结果！ 用多线程实现烧水泡茶 用两个线程 T1 和 T2 来完成烧水泡茶程序，T1 负责洗水壶、烧开水、泡茶这三道工序，T2 负责洗茶壶、洗茶杯、拿茶叶三道工序，其中 T1 在执行泡茶这道工序时需要等待 T2 完成拿茶叶的工序。对于 T1 的这个等待动作，你应该可以想出很多种办法，例如 Thread.join()、CountDownLatch，甚至阻塞队列都可以解决，不过今天我们用 Future 特性来实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;import java.util.concurrent.TimeUnit;/** * @author shengbinbin * @description: 使用FutureTask 来实现烧水泡茶 * @date 2021/8/2510:40 下午 */public class TestFuture { public static void main(String[] args) throws ExecutionException, InterruptedException { // 创建任务 T2 的 FutureTask FutureTask&lt;String&gt; ft2 = new FutureTask&lt;&gt;(new T2task()); // 创建任务 T1 的 FutureTask FutureTask&lt;String&gt; ft1 = new FutureTask&lt;&gt;(new T1task(ft2)); // 线程 T1 执行任务 ft1 Thread T1 = new Thread(ft1); T1.start(); // 线程 T2 执行任务 ft2 Thread T2 = new Thread(ft2); T2.start(); // 等待线程 T1 执行结果 System.out.println(ft1.get()); } // T1Task 需要执行的任务： // 洗水壶、烧开水、泡茶 static class T1task implements Callable&lt;String&gt;{ FutureTask&lt;String&gt; ft2; // T1 任务需要 T2 任务的 FutureTask T1task(FutureTask&lt;String&gt; ft2){ this.ft2 = ft2; } @Override public String call() throws Exception { System.out.println(&quot;T1: 洗水壶..&quot;); TimeUnit.SECONDS.sleep(1); // 需要1分钟 System.out.println(&quot;T1: 烧开水...&quot;); TimeUnit.SECONDS.sleep(15); // 获取 T2 线程的茶叶,需要有线程的交互 String tf = ft2.get(); System.out.println(&quot;T1: 拿到茶叶:&quot;+tf); System.out.println(&quot;T1: 泡茶...&quot;); return &quot; 上茶:&quot; + tf; } } // T2Task 需要执行的任务: // 洗茶壶、洗茶杯、拿茶叶 static class T2task implements Callable&lt;String&gt;{ @Override public String call() throws Exception { System.out.println(&quot;T2: 洗茶壶...&quot;); TimeUnit.SECONDS.sleep(1); System.out.println(&quot;T2: 洗茶杯...&quot;); TimeUnit.SECONDS.sleep(2); System.out.println(&quot;T2: 拿茶叶...&quot;); TimeUnit.SECONDS.sleep(1); return &quot; 龙井 &quot;; } }} 利用 Java 并发包提供的 Future 可以很容易获得异步任务的执行结果，无论异步任务是通过线程池 ThreadPoolExecutor 执行的，还是通过手工创建子线程来执行的。Future 可以类比为现实世界里的提货单，比如去蛋糕店订生日蛋糕，蛋糕店都是先给你一张提货单，你拿到提货单之后，没有必要一直在店里等着，可以先去干点其他事，比如看场电影；等看完电影后，基本上蛋糕也做好了，然后你就可以凭提货单领蛋糕了。 利用多线程可以快速将一些串行的任务并行化，从而提高性能；如果任务之间有依赖关系，比如当前任务依赖前一个任务的执行结果，这种问题基本上都可以用 Future 来解决。在分析这种问题的过程中，建议你用有向图描述一下任务之间的依赖关系，同时将线程的分工也做好，类似于烧水泡茶最优分工方案那幅图。对照图来写代码，好处是更形象，且不易出错。 CompletableFuture用多线程优化性能，其实不过就是将串行操作变成并行操作: 12345678910// 以下两个方法都是耗时操作doBizA();doBizB();//异步化：主线程无需等待 doBizA() 和 doBizB() 的执行结果，也就是说 doBizA() 和 doBizB() 两个操作已经被异步化了。new Thread(()-&gt;doBizA()) .start();new Thread(()-&gt;doBizB()) .start(); 用CompletableFuture实现烧水泡茶123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.concurrent.CompletableFuture;import java.util.concurrent.TimeUnit;/** * @author shengbinbin * @description: CompletableFuture 使用 * @date 2021/8/2510:52 下午 */public class TestCompletableFuture { public static void main(String[] args) { // 任务 1：洗水壶 -&gt; 烧开水 CompletableFuture&lt;Void&gt; f1 = CompletableFuture.runAsync(()-&gt;{ System.out.println(&quot;T1: 洗水壶...&quot;); sleep(1, TimeUnit.SECONDS); System.out.println(&quot;T1: 烧开水...&quot;); sleep(15, TimeUnit.SECONDS); }); // 任务 2：洗茶壶 -&gt; 洗茶杯 -&gt; 拿茶叶 CompletableFuture&lt;String&gt; f2 = CompletableFuture.supplyAsync(()-&gt;{ System.out.println(&quot;T2: 洗茶壶...&quot;); sleep(1, TimeUnit.SECONDS); System.out.println(&quot;T2: 洗茶杯...&quot;); sleep(2, TimeUnit.SECONDS); System.out.println(&quot;T2: 拿茶叶...&quot;); sleep(1, TimeUnit.SECONDS); return &quot; 龙井 &quot;; }); // 任务 3：任务 1 和任务 2 完成后执行：泡茶 CompletableFuture&lt;String&gt; f3 = f1.thenCombine(f2, (__, tf)-&gt;{ System.out.println(&quot;T1: 拿到茶叶:&quot; + tf); System.out.println(&quot;T1: 泡茶...&quot;); return &quot; 上茶:&quot; + tf; }); // 等待任务 3 执行结果 System.out.println(f3.join()); } static void sleep(int t, TimeUnit u) { try { u.sleep(t); }catch(InterruptedException e){} }} 创建 CompletableFuture 对象主要靠下面4个静态方法： 1234567// 使用默认线程池static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable) //Runnable 接口的 run() 方法没有返回值static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier) //Supplier 接口的 get() 方法是有返回值的。// 可以指定线程池 static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable, Executor executor)static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier, Executor executor) 默认情况下 CompletableFuture 会使用公共的 ForkJoinPool 线程池，这个线程池默认创建的线程数是 CPU 的核数（也可以通过 JVM option:-Djava.util.concurrent.ForkJoinPool.common.parallelism 来设置 ForkJoinPool 线程池的线程数）。如果所有 CompletableFuture 共享一个线程池，那么一旦有任务执行一些很慢的 I/O 操作，就会导致线程池中所有线程都阻塞在 I/O 操作上，从而造成线程饥饿，进而影响整个系统的性能。所以，强烈建议你要根据不同的业务类型创建不同的线程池，以避免互相干扰。 创建完 CompletableFuture 对象之后，会自动地异步执行 runnable.run() 方法或者 supplier.get() 方法。 对于一个异步操作，你需要关注两个问题： 一个是异步操作什么时候结束， 另一个是如何获取异步操作的执行结果。 因为 CompletableFuture 类实现了 Future 接口，所以这两个问题你都可以通过 Future 接口来解决。另外，CompletableFuture 类还实现了 CompletionStage 接口， CompletionStage 接口任务是有时序关系的，比如有串行关系、并行关系、汇聚关系等。这样说可能有点抽象，这里还举前面烧水泡茶的例子，其中洗水壶和烧开水就是串行关系，洗水壶、烧开水和洗茶壶、洗茶杯这两组任务之间就是并行关系，而烧开水、拿茶叶和泡茶就是汇聚关系。 例如前面提到的 f3 = f1.thenCombine(f2, ()-&gt;{}) 描述的就是一种汇聚关系。烧水泡茶程序中的汇聚关系是一种 AND 聚合关系，这里的 AND 指的是所有依赖的任务（烧开水和拿茶叶）都完成后才开始执行当前任务（泡茶）。既然有 AND 聚合关系，那就一定还有 OR 聚合关系，所谓 OR 指的是依赖的任务只要有一个完成就可以执行当前任务。 描述串行关系主要是 thenApply、thenAccept、thenRun 和 thenCompose 这四个系列的接口。 1234567public &lt;U&gt; CompletionStage&lt;U&gt; thenApply(Function&lt;? super T,? extends U&gt; fn);public CompletionStage&lt;Void&gt; thenAccept(Consumer&lt;? super T&gt; action);public CompletionStage&lt;Void&gt; thenRun(Runnable action);public &lt;U&gt; CompletionStage&lt;U&gt; thenCompose(Function&lt;? super T, ? extends CompletionStage&lt;U&gt;&gt; fn); thenApply 系列函数里参数 fn 的类型是接口 Function&lt;T, R&gt;，这个接口里与 CompletionStage 相关的方法是 R apply(T t)，这个方法既能接收参数也支持返回值，所以 thenApply 系列方法返回的是CompletionStage&lt;R&gt;。 而 thenAccept 系列方法里参数 consumer 的类型是接口Consumer&lt;T&gt;，这个接口里与 CompletionStage 相关的方法是 void accept(T t)，这个方法虽然支持参数，但却不支持回值，所以 thenAccept 系列方法返回的是CompletionStage&lt;Void&gt;。 thenRun 系列方法里 action 的参数是 Runnable，所以 action 既不能接收参数也不支持返回值，所以 thenRun 系列方法返回的也是CompletionStage&lt;Void&gt;。 这些方法里面 Async 代表的是异步执行 fn、consumer 或者 action。其中，需要你注意的是 thenCompose 系列方法，这个系列的方法会新创建出一个子流程，最终结果和 thenApply 系列是相同的。 12345678CompletionStage&lt;R&gt; thenApply(fn);CompletionStage&lt;R&gt; thenApplyAsync(fn);CompletionStage&lt;Void&gt; thenAccept(consumer);CompletionStage&lt;Void&gt; thenAcceptAsync(consumer);CompletionStage&lt;Void&gt; thenRun(action);CompletionStage&lt;Void&gt; thenRunAsync(action);CompletionStage&lt;R&gt; thenCompose(fn);CompletionStage&lt;R&gt; thenComposeAsync(fn); thenApply的使用： 12345678public static void main(String[] args) throws ExecutionException, InterruptedException { CompletableFuture&lt;String&gt; completableFuture = CompletableFuture .supplyAsync(() -&gt; &quot;Hello,World&quot;) // 1 是一个异步流程 .thenApply(s -&gt; s + &quot;QQ&quot;) // 2 .thenApply(String :: toUpperCase); //3 和 2是一个串行的 // join方法的作用是使主线程等待 completableFuture 的任务执行完 System.out.println(completableFuture.join()); } 描述 AND 汇聚关系主要是 thenCombine、thenAcceptBoth 和 runAfterBoth 系列的接口，这些接口的区别也是源自 fn、consumer、action 这三个核心参数不同。它们的使用你可以参考上面烧水泡茶的实现程序，这里就不赘述了 123456CompletionStage&lt;R&gt; thenCombine(other, fn);CompletionStage&lt;R&gt; thenCombineAsync(other, fn);CompletionStage&lt;Void&gt; thenAcceptBoth(other, consumer);CompletionStage&lt;Void&gt; thenAcceptBothAsync(other, consumer);CompletionStage&lt;Void&gt; runAfterBoth(other, action);CompletionStage&lt;Void&gt; runAfterBothAsync(other, action); 描述 OR 汇聚关系主要是 applyToEither、acceptEither 和 runAfterEither 系列的接口，这些接口的区别也是源自 fn、consumer、action 这三个核心参数不同。 123456CompletionStage applyToEither(other, fn);CompletionStage applyToEitherAsync(other, fn);CompletionStage acceptEither(other, consumer);CompletionStage acceptEitherAsync(other, consumer);CompletionStage runAfterEither(other, action);CompletionStage runAfterEitherAsync(other, action); 使用： 123456789101112131415161718CompletableFuture&lt;String&gt; f1 = CompletableFuture.supplyAsync(()-&gt;{ int t = getRandom(5, 10); sleep(t, TimeUnit.SECONDS); return String.valueOf(t);}); CompletableFuture&lt;String&gt; f2 = CompletableFuture.supplyAsync(()-&gt;{ int t = getRandom(5, 10); sleep(t, TimeUnit.SECONDS); return String.valueOf(t);}); CompletableFuture&lt;String&gt; f3 = f1.applyToEither(f2,s -&gt; s); System.out.println(f3.join()); 异常处理虽然上面我们提到的 fn、consumer、action 它们的核心方法都不允许抛出可检查异常，但是却无法限制它们抛出运行时异常，例如下面的代码，执行 7/0 就会出现除零错误这个运行时异常。非异步编程里面，我们可以使用 try{}catch{}来捕获并处理异常，那在异步编程里面，异常该如何处理呢？ 12345CompletableFuture&lt;Integer&gt; f0 = CompletableFuture. .supplyAsync(()-&gt;(7/0)) .thenApply(r-&gt;r*10);System.out.println(f0.join()); CompletionStage 接口给我们提供的方案非常简单，比 try{}catch{}还要简单，下面是相关的方法，使用这些方法进行异常处理和串行操作是一样的，都支持链式编程方式。 12345CompletionStage exceptionally(fn);CompletionStage&lt;R&gt; whenComplete(consumer);CompletionStage&lt;R&gt; whenCompleteAsync(consumer);CompletionStage&lt;R&gt; handle(fn);CompletionStage&lt;R&gt; handleAsync(fn); 下面的示例代码展示了如何使用 exceptionally() 方法来处理异常，exceptionally() 的使用非常类似于 try{}catch{}中的 catch{}，但是由于支持链式编程方式，所以相对更简单。既然有 try{}catch{}，那就一定还有 try{}finally{}，whenComplete() 和 handle() 系列方法就类似于 try{}finally{}中的 finally{}，无论是否发生异常都会执行 whenComplete() 中的回调函数 consumer 和 handle() 中的回调函数 fn。whenComplete() 和 handle() 的区别在于 whenComplete() 不支持返回结果，而 handle() 是支持返回结果的。 12345678public static void main(String[] args) throws ExecutionException, InterruptedException { CompletableFuture&lt;Integer&gt; completableFuture = CompletableFuture .supplyAsync(() -&gt; 7/0) // 1 是一个异步流程 .thenApply(r -&gt; r * 10) // 2 .exceptionally(e -&gt; 0); // 如果发生运行时异常就这样 // join方法的作用是使主线程等待 completableFuture 的任务执行完 System.out.println(completableFuture.join()); }","link":"/2021/08/24/Java%E5%B9%B6%E5%8F%91%EF%BC%88%E5%85%AD%EF%BC%89%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"title":"Java并发理论基础-下","text":"Java并发理论基础下章主要描述了以下内容 Lock接口实现的锁 常见的并发容器 常见的并发工具类 线程池 Lock接口synchronized在1.6之后做了很多的优化，效率提高了很多，但是还有很多问题是synchronized无法解决的，因此Lock接口及其实现方法就出现了： 能够响应中断。synchronized 的问题是，持有锁 A 后，如果尝试获取锁 B 失败，那么线程就进入阻塞状态，一旦发生死锁，就没有任何机会来唤醒阻塞的线程。但如果阻塞状态的线程能够响应中断信号，也就是说当我们给阻塞的线程发送中断信号的时候，能够唤醒它，那它就有机会释放曾经持有的锁 A。这样就破坏了不可抢占条件了。 支持超时。如果线程在一段时间之内没有获取到锁，不是进入阻塞状态，而是返回一个错误，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。 非阻塞地获取锁。如果尝试获取锁失败，并不进入阻塞状态，而是直接返回，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。 API接口方法123456void lock(); //获取锁，调用该方法的线程会获得锁，获得锁之后从该方法返回void lockInterruptibly() throws InterruptedException; //可中断的获得锁boolean tryLock(); //尝试非阻塞的获取锁，调用该方法后立即返回，如果能获取返回true，否则返回falseboolean tryLock(long time, TimeUnit unit) throws InterruptedException; //超时的获取锁void unlock(); //释放锁Condition newCondition(); //获取等待通知组件，该组件和当前锁绑定，当前线程获得了锁之后才能调用组件的wait方法释放锁 Lock的一般使用实例1234567Lock lock = new ReentrantLock(); lock.lock(); try { //业务逻辑 }finally { lock.unlock();//在finally块中释放锁，目的是保证在获取到锁之后，最终能够被释放。 } synchronized和ReentrantLock的区别 synchronized是JVM内建的同步机制，是一个关键字，ReentrantLock是一个类。 ReentrantLock可以实现公平锁，可以自定义条件，可以定义超时时间，需要显式的释放锁，而synchronized只能是非公平锁。 每一个lock操作，为了保证锁的释放，最好在finally中显式的unlock lock只适用于代码块，而synchronized可以用来修饰方法，代码块 在Java6之前，synchronized完全依靠操作系统的互斥锁来实现，需要进行用户态和内核态的切换，所以开销较大，但随着一系列的锁优化，synchronized的性能也越来越好了 队列同步器AQSAQS的全称是AbstractQueuedSynchronizer，它的定位是为Java中几乎所有的锁和同步器提供一个基础框架。 AQS是基于FIFO的队列实现的，并且内部维护了一个volatile修饰的状态变量state，通过原子更新这个状态变量state即可以实现加锁解锁操作。 AQS的源码解析 主要内部类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647static final class Node { //初始化两个节点引用 static final Node SHARED = new Node(); static final Node EXCLUSIVE = null; static final int CANCELLED = 1; // 标识线程已取消 static final int SIGNAL = -1; // 标识后继节点需要唤醒 static final int CONDITION = -2; // 标识线程等待在一个条件上 static final int PROPAGATE = -3; // 标识后面的共享锁需要无条件的传播（共享锁需要连续唤醒读的线程） volatile int waitStatus; //// 当前节点保存的线程对应的等待状态 volatile Node prev; volatile Node next; volatile Thread thread; // 当前节点保存的线程 Node nextWaiter; final boolean isShared() { return nextWaiter == SHARED; } final Node predecessor() throws NullPointerException { Node p = prev; if (p == null) throw new NullPointerException(); else return p; } Node() { // Used to establish initial head or SHARED marker } Node(Thread thread, Node mode) { // Used by addWaiter this.nextWaiter = mode; this.thread = thread; } Node(Thread thread, int waitStatus) { // Used by Condition this.waitStatus = waitStatus; this.thread = thread; } } 主要属性 123456789101112131415161718 private transient volatile Node head; //维护一个头节点和尾节点的引用 private transient volatile Node tail; private volatile int state; //同步状态，用volatile修饰 //获取当前同步状态 protected final intgetState() { return state; } //设置新的同步状态 protected final void setState(int newState) { state = newState;} //通过unsafe类的CAS修改同步状态 protected final boolean compareAndSetState(int expect, int update) { // See below for intrinsics setup to support this return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 子类需要实现的方法–模版方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495//提供给子类重写，独占式的获取同步状态，实现该方法需要查询当前状态并判断是否符合预期，然后用CAS来设置同步状态 protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException(); } //提供给子类重写，独占式的释放同步状态，等待的线程将有机会获取同步状态 protected boolean tryRelease(int arg) { throw new UnsupportedOperationException(); } //共享式的获取同步状态，返回值大于0表示成功 protected int tryAcquireShared(int arg) { throw new UnsupportedOperationException(); } //共享式的释放同步状态 protected boolean tryReleaseShared(int arg) { throw new UnsupportedOperationException(); } //当前同步器是否在独占模式下被线程占用 protected boolean isHeldExclusively() { throw new UnsupportedOperationException(); } //独占式获取同步状态，获取成功则返回，否则进入同步队列等待 public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } //和上面这个方法相同，但是响应中断 public final void acquireInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); if (!tryAcquire(arg)) doAcquireInterruptibly(arg); } //在上面的方法中增加了时间限制 public final boolean tryAcquireNanos(int arg, long nanosTimeout) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); return tryAcquire(arg) || doAcquireNanos(arg, nanosTimeout); } //独占式释放同步状态，释放后唤醒同步队列中的第一个节点 public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } //共享式的获取同步状态，主要区别是同一时间可以有多个线程获取到同步状态 public final void acquireShared(int arg) { if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg); } //和上面相同，响应中断 public final void acquireSharedInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg); } public final boolean tryAcquireSharedNanos(int arg, long nanosTimeout) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); return tryAcquireShared(arg) &gt;= 0 || doAcquireSharedNanos(arg, nanosTimeout); } //共享式的释放同步状态 public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } 节点加入等待队列流程同步器将节点加入到同步队列的过程：加入队列的过程必须要保证线程安全，因此同步器提供了一个基于CAS的设置尾节点的方法:**compareAndSetTail(Node expect,Node update)**，它需要传递当前线程“认为”的尾节点和当前节点，只有设置成功后，当前节点才正式 与之前的尾节点建立关联。 1234private final boolean compareAndSetTail(Node expect, Node update) { return unsafe.compareAndSwapObject(this, tailOffset, expect, update);} 设置首节点的过程设置首节点的过程：同步队列遵循FIFO，首节点是获取同步状态成功的节点，首节点的线程在释放同步状态时，将会唤醒后继节点，而后继节点将会在获取同步状态成功时将自己设置为首节点。 设置首节点是通过获取同步状态成功的线程来完成的，由于只有一个线程能够成功获取到同步状态，因此设置头节点的方法并不需要使用CAS来保证，它只需要将首节 点设置成为原首节点的后继节点并断开原首节点的next引用即可。 acquire流程分析123456AQS ----&gt; acquire()public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 123456789101112131415161718192021222324252627282930313233343536373839404142tryAcquire 方法针对公平锁和非公平锁有着不同的实现，总的来说是保证线程安全的获取同步状态 //Fair version of tryAcquire. Don't grant access unless recursive call or no waiters or is first. protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (!hasQueuedPredecessors() &amp;&amp; //hasQueuedPredecessors 是公平锁和非公平锁的区别 compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } //可重入锁的实现 else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } 如果tryAcquire不能获取锁： 123456789101112131415161718192021222324252627282930313233343536373839404142434445//构造新的尾节点，通过CAS来放入队列尾部//Creates and enqueues node for current thread and given mode.Params://mode – Node.EXCLUSIVE for exclusive, Node.SHARED for sharedReturns://the new nodeprivate Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node); ////如果多个线程获取同步状态失败，并发的添加到list，也许会顺序混乱，通过CAS变 得“串行化”了 return node; }/*Inserts node into queue, initializing if necessary. See picture above.Params:node – the node to insertReturns:node's predecessor*/private Node enq(final Node node) { for (;;) { ////通过“死循环”来保证节点的正确添加 Node t = tail; if (t == null) { // Must initialize if (compareAndSetHead(new Node())) tail = head; } else { node.prev = t; //只有通过CAS将节点设置成为尾节点之后，当前线程才能从该方法返回，否则，当前线 程不断地尝试设置 if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } 12345678910111213141516171819202122//acquireQueued --- 节点进入同步队列之后，就进入了一个自旋的过程，每个节点(或者说每个线程)都在自 省地观察，当条件满足，获取到了同步状态，就可以从这个自旋过程中退出，否则依旧留在这 个自旋过程中(并会阻塞节点的线程):final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { //死循环自旋的过程 final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return interrupted; } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } acquire方法调用流程： 前驱节点为头节点且能够获取同步状态的判断条件和线程进入等待状态是获 取同步状态的自旋过程。当同步状态获取成功之后，当前线程从acquire(int arg)方法返回，如果 对于锁这种并发组件而言，代表着当前线程获取了锁。 当前线程获取同步状态并执行了相应逻辑之后，就需要释放同步状态，使得后续节点能 够继续获取同步状态。通过调用同步器的release(int arg)方法可以释放同步状态，该方法在释 放了同步状态之后，会唤醒其后继节点(进而使后继节点重新尝试获取同步状态)。 12345678910@ReservedStackAccess public final boolean release(int arg) { if (tryRelease(arg)) { //tryRelease针对公平锁和非公平锁也有不同的实现 Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } 重入锁重进入是指任意线程在获取到锁之后能够再次获取该锁而不会被锁所阻塞，该特性的实现需要解决以下两个问题。 线程再次获取锁。锁需要去识别获取锁的线程是否为当前占据锁的线程，如果是，则再次成功获取。 锁的最终释放。线程重复n次获取了锁，随后在第n次释放该锁后，其他线程能够获取到该锁。锁的最终释放要求锁对于获取进行计数自增，计数表示当前锁被重复获取的次数，而锁被释放时，计数自减，当计数等于0时表示锁已经成功释放。 12345678910111213141516171819protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { //如果当前线程就是拥有锁的线程 int nextc = c + acquires; //则共享变量++ if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } 读写锁读写锁，并不是 Java 语言特有的，而是一个广为使用的通用技术，所有的读写锁都遵守以下三条基本原则： 允许多个线程同时读共享变量； 只允许一个线程写共享变量； 如果一个写线程正在执行写操作，此时禁止读线程读共享变量。 读写锁在同一时刻可以允许多个读线程访问，但是在写线程访问时，所有的读线程和其他写线程均被阻塞。读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。 LockSupport类LockSupport定义了一组以park开头的方法用来阻塞当前线程，以及unpark(Thread thread) 方法来唤醒一个被阻塞的线程： 1234567891011public static void park(Object blocker) { //阻塞当前线程 Thread t = Thread.currentThread(); setBlocker(t, blocker); UNSAFE.park(false, 0L); setBlocker(t, null); }public static void unpark(Thread thread) { //唤醒当前线程 if (thread != null) UNSAFE.unpark(thread); } Condition接口等待通知模式：任意一个Java对象，都拥有一组监视器方法（定义在java.lang.Object上），主要包括wait()、 wait(long timeout)、notify()以及notifyAll()方法，这些方法与synchronized同步关键字配合，可以实现等待/通知模式 Condition接口也提供了类似Object的监视器方法，与Lock配合可以实现等待/通知模式 新版生产者和消费者一般都会将Condition对象作为成员变量。当调用await()方法后，当前线程会释放锁并在此等待，而其他线程调用Condition对象的signal()方法，通知当前线程后，当前线程才从await()方法返回，并且在返回前已经获取了锁 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class SharedDate{ //共享资源类 private int num = 0; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void increment(){ lock.lock(); try { while (num != 0) condition.await(); //1. 判断释放满足条件，注意用while num++; //2. 业务逻辑 System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); condition.signal(); //3. 唤醒其他线程 } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } public void decrement(){ lock.lock(); try { while (num == 0) condition.await(); num--; System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); condition.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } }}/** 需求：两个线程操作一个初始值为0的变量，一个线程操作变量+1，另一个线程操作变量-1。操作10次后变量依旧为0 * */public class ConditionDemo { public static void main(String[] args) { // 线程操作资源类 SharedDate sharedDate = new SharedDate(); Thread a = new Thread(()-&gt;{ for (int i = 0; i &lt; 5; i++) { sharedDate.increment(); } },&quot;A&quot;); Thread b = new Thread(()-&gt;{ for (int i = 0; i &lt; 5; i++) { sharedDate.decrement(); } },&quot;B&quot;); a.start(); b.start(); } } 精确通知不同的等待者12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485class SharedDate{ //共享资源类 private int num = 1; //1 A ，2 B ， 3 C private Lock lock = new ReentrantLock(); private Condition c1 = lock.newCondition(); //多个条件实现精确通知 private Condition c2 = lock.newCondition(); private Condition c3 = lock.newCondition(); public void print2(int a){ lock.lock(); try { while (num != 1) c1.await(); for (int i = 0; i &lt; a; i++) { System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); } num = 2; //要修改状态位，以此来唤醒不同的线程 c2.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } public void print4(int a){ lock.lock(); try { while (num != 2) c2.await(); for (int i = 0; i &lt; a; i++) { System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); } num = 3;//要修改状态位，以此来唤醒不同的线程 c3.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } public void print8(int a){ lock.lock(); try { while (num != 3) c3.await(); for (int i = 0; i &lt; a; i++) { System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); } num = 1;//要修改状态位，以此来唤醒不同的线程 c1.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } }}/** 需求：三个线程依次打印1，2，3。其中A线程打印2次，B线程打印4次，C线程打印6次** */public class ConditionDemo { public static void main(String[] args) { // 线程操作资源类 SharedDate sharedDate = new SharedDate(); Thread a = new Thread(()-&gt;{ sharedDate.print2(2); },&quot;A&quot;); Thread b = new Thread(()-&gt;{ sharedDate.print4(4); },&quot;B&quot;); Thread c = new Thread(()-&gt;{ sharedDate.print8(8); },&quot;C&quot;); a.start(); b.start(); c.start(); }} Java并发容器CopyOnWriteArrayListCopyOnWrite，顾名思义就是写的时候会将共享变量新复制一份出来，这样做的好处是读操作完全无锁 CopyOnWriteArrayList 内部维护了一个数组，成员变量 array 就指向这个内部数组，所有的读操作都是基于 array 进行的。 如果在遍历 array 的同时，还有一个写操作，例如增加元素，CopyOnWriteArrayList 是如何处理的呢？CopyOnWriteArrayList 会将 array 复制一份，然后在新复制处理的数组上执行增加元素的操作，执行完之后再将 array 指向这个新的数组。通过下图你可以看到，读写是可以并行的，遍历操作一直都是基于原 array 执行，而写操作则是基于新 array 进行。 ConcurrentHashMap1.1 为什么要使用ConcurrentHashMap hashMap线程不安全，hashtable效率低下 ConcurrentHashMap的锁分段技术可有效提升并发访问率，首先将数据分成一段一段地存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问 1.2 结构![image-20210502103840847](/Users/shengbinbin/Library/Application Support/typora-user-images/image-20210502103840847.png) ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁（ReentrantLock），在ConcurrentHashMap里扮演锁的角色；HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组。Segment的结构和HashMap类似，是一种数组和链表结构。一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得与它对应的Segment锁 1.3 初始化1.4 定位segment1.5 常用的方法操作2. ConcurrentLinkedQueue3. Java中的阻塞队列3.1 什么是阻塞队列阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作支持阻塞的插入和移除方法。 1）支持阻塞的插入方法：意思是当队列满时，队列会阻塞插入元素的线程，直到队列不满。 2）支持阻塞的移除方法：意思是在队列为空时，获取元素的线程会等待队列变为非空。 阻塞队列常用于生产者和消费者的场景，生产者是向队列里添加元素的线程，消费者是从队列里取元素的线程。阻塞队列就是生产者用来存放元素、消费者用来获取元素的容器 3.2 常见的阻塞队列种类JDK 7提供了7个阻塞队列，如下。 ·ArrayBlockingQueue：一个由数组结构组成的有界阻塞队列。 ·LinkedBlockingQueue：一个由链表结构组成的有界阻塞队列。 ·PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列。 ·DelayQueue：一个使用优先级队列实现的无界阻塞队列。 ·SynchronousQueue：一个不存储元素的阻塞队列。 ·LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。 ·LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。 3.3 阻塞队列的实现原理ArrayBlockingQueue使用了Condition来实现 1234567891011121314151617181920212223242526272829303132333435363738394041/** Condition for waiting takes */private fnal Condition notEmpty;/** Condition for waiting puts */private fnal Condition notFull;public ArrayBlockingQueue(int capacity, boolean fair) { if (capacity &lt;= 0) throw new IllegalArgumentException(); this.items = new Object[capacity]; lock = new ReentrantLock(fair); notEmpty = lock.newCondition(); notFull = lock.newCondition(); }//put方法public void put(E e) throws InterruptedException { checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { while (count == items.length) notFull.await(); enqueue(e); } finally { lock.unlock(); } }//take public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { while (count == 0) notEmpty.await(); return dequeue(); } finally { lock.unlock(); } } 第七部分、13个原子操作类1. 原子更新基本类型3个·AtomicBoolean：原子更新布尔类型。 ·AtomicInteger：原子更新整型。 ·AtomicLong：原子更新长整型。 1.1 实现原理基于CAS（compare-and-swap）技术来实现的，所谓CAS，表征的是一些列操作的集合，获取当前数值，进行一些运算，利用CAS指令试图进行更新。如果当前数值未变，代表没有其他线程进行并发修改，则成功更新。否则，可能出现不同的选择，要么进行重试，要么就返回一个成功或者失败的结果。 1234567891011121314151617181920public class AtomicInteger extends Number implements java.io.Serializable { private static final long serialVersionUID = 6214790243416807050L; // setup to use Unsafe.compareAndSwapInt for updates private static final Unsafe unsafe = Unsafe.getUnsafe(); //使用了unsafe类 private static final long valueOffset; private volatile int value; //volatile修饰的变量 static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(&quot;value&quot;)); } catch (Exception ex) { throw new Error(ex); } } public AtomicInteger(int initialValue) { value = initialValue; } public AtomicInteger() { } 12345678910111213141516public final int getAndIncrement() { return unsafe.getAndAddInt(this, valueOffset, 1); //调用的是unsafe类中的方法实现原子自增的 }public final int getAndAddInt(Object o, long offset, int delta) { int v; do { v = getIntVolatile(o, offset); } while (!compareAndSwapInt(o, offset, v, v + delta)); return v; }//cas底层是unsafe类中的本地方法，依赖于CPU提供的特定指令public final native boolean compareAndSwapInt(Object o, long offset, int expected, int x); 1.2 CAS的问题 CAS也并不是没有副作用，试想，其常用的失败重试机制，隐含着一个假设，即竞争情况是短暂的。大多数应用场景中，确实大部分重试只会发生一次就获得了成功，但是总是有意外情况，所以在有需要的时候，还是要考虑限制自旋的次数，以免过度消耗CPU。 另外一个就是著名的ABA问题，这是通常只在lock-free算法下暴露的问题。我前面说过CAS是在更新时比较前值，如果对方只是恰好相同，例如期间发生了 A -&gt; B -&gt; A的更新，仅仅判断数值是A，可能导致不合理的修改操作。针对这种情况，Java提供了AtomicStampedReference工具类，通过为引用建立类似版本号（stamp）的方式，来保证CAS的正确性 2. 原子更新数组·AtomicIntegerArray：原子更新整型数组里的元素。 ·AtomicLongArray：原子更新长整型数组里的元素。 ·AtomicReferenceArray：原子更新引用类型数组里的元素。 ·AtomicIntegerArray类主要是提供原子的方式更新数组里的整型，其常用方法如下。 3.原子更新引用类型4. 原子更新字段类第八部分、并发工具类1.倒计时器CountDownLatchCountDownLatch允许一个或多个线程等待其他线程完成操作。 运动员进行跑步比赛时，假设有 6 个运动员参与比赛，裁判员在终点会为这 6 个运动员分别计时，可以想象没当一个运动员到达终点的时候，对于裁判员来说就少了一个计时任务。直到所有运动员都到达终点了，裁判员的任务也才完成。这 6 个运动员可以类比成 6 个线程，当线程调用 CountDownLatch.countDown 方法时就会对计数器的值减一，直到计数器的值为 0 的时候，裁判员（调用 await 方法的线程）才能继续往下执行。 1.1 方法解析12345//整型数 N，之后调用 CountDownLatch 的countDown方法会对 N 减一，知道 N 减到 0 的时候，当前调用await方法的线程继续执行。public CountDownLatch(int count) { if (count &lt; 0) throw new IllegalArgumentException(&quot;count &lt; 0&quot;); this.sync = new Sync(count); } public void await() throws InterruptedException：调用await()方法的线程会被挂起，等待直到count值为0再继续执行。 public boolean await(long timeout, TimeUnit unit) throws InterruptedException：同await()，若等待timeout时长后，count值还是没有变为0，不再等待，继续执行。时间单位如下常用的毫秒、天、小时、微秒、分钟、纳秒、秒。 public void countDown()： count值递减1. public long getCount()：获取当前count值。 public String toString()：重写了toString()方法，多打印了count值，具体参考源码。 1.2 使用实例 创建CountDownLatch并设置计数器值。 启动多线程并且调用CountDownLatch实例的countDown()方法。 主线程调用 await() 方法，这样主线程的操作就会在这个方法上阻塞，直到其他线程完成各自的任务，count值为0，停止阻塞，主线程继续执行。 123456789101112131415161718192021222324252627282930313233343536373839404142public class CountDownLatchDemo { //线程数 private static int N = 10; // 单位：min private static int countDownLatchTimeout = 5; public static void main(String[] args) { //创建CountDownLatch并设置计数值，该count值可以根据线程数的需要设置 CountDownLatch countDownLatch = new CountDownLatch(N); //创建线程池 ExecutorService cachedThreadPool = Executors.newCachedThreadPool(); for (int i = 0; i &lt; N; i++) { cachedThreadPool.execute(() -&gt; { try { System.out.println(Thread.currentThread().getName() + &quot; do something!&quot;); } catch (Exception e) { System.out.println(&quot;Exception: do something exception&quot;); } finally { //该线程执行完毕-1 countDownLatch.countDown(); } }); } System.out.println(&quot;main thread do something-1&quot;); try { countDownLatch.await(countDownLatchTimeout, TimeUnit.MINUTES); } catch (InterruptedException e) { System.out.println(&quot;Exception: await interrupted exception&quot;); } finally { System.out.println(&quot;countDownLatch: &quot; + countDownLatch.toString()); } System.out.println(&quot;main thread do something-2&quot;); //若需要停止线程池可关闭;// cachedThreadPool.shutdown(); }} 2.循环栅栏：CyclicBarrier 开运动会时，会有跑步这一项运动，我们来模拟下运动员入场时的情况，假设有 6 条跑道，在比赛开始时，就需要 6 个运动员在比赛开始的时候都站在起点了，裁判员吹哨后才能开始跑步。跑道起点就相当于“barrier”，是临界点，而这 6 个运动员就类比成线程的话，就是这 6 个线程都必须到达指定点了，意味着凑齐了一波，然后才能继续执行，否则每个线程都得阻塞等待，直至凑齐一波即可。cyclic 是循环的意思，也就是说 CyclicBarrier 当多个线程凑齐了一波之后，仍然有效，可以继续凑齐下一波 2.1 常用方法12345678910//等到所有的线程都到达指定的临界点await() throws InterruptedException, BrokenBarrierException//与上面的await方法功能基本一致，只不过这里有超时限制，阻塞等待直至到达超时时间为止await(long timeout, TimeUnit unit) throws InterruptedException,BrokenBarrierException, TimeoutException//获取当前有多少个线程阻塞等待在临界点上int getNumberWaiting()//用于查询阻塞等待的线程是否被中断boolean isBroken() 2.2 使用实例123456789101112131415161718192021222324public class CyclicBarrierDemo { //指定必须有6个运动员到达才行 private static CyclicBarrier barrier = new CyclicBarrier(6, () -&gt; { System.out.println(&quot;所有运动员入场，裁判员一声令下！！！！！&quot;); }); public static void main(String[] args) { System.out.println(&quot;运动员准备进场，全场欢呼............&quot;); ExecutorService service = Executors.newFixedThreadPool(6); for (int i = 0; i &amp;lt; 6; i++) { service.execute(() -&amp;gt; { try { System.out.println(Thread.currentThread().getName() + &quot; 运动员，进场&quot;); barrier.await(); System.out.println(Thread.currentThread().getName() + &quot; 运动员出发&quot;); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } }); }}} 2.3 CyclicBarrier 和CountDownLatch 的区别 CountDownLatch是不可以重置的，所以无法重用；而CyclicBarrier则没有这种限制，可以重用。 CountDownLatch的基本操作组合是countDown/await。调用await的线程阻塞等待countDown足够的次数，不管你是在一个线程还是多个线程里countDown，只要次数足够即可。所以就像Brain Goetz说过的，CountDownLatch操作的是事件。 CyclicBarrier的基本操作组合，则就是await，当所有的伙伴（parties）都调用了await，才会继续进行任务，并自动进行重置。注意，正常情况下，CyclicBarrier的重置都是自动发生的，如果我们调用reset方法，但还有线程在等待，就会导致等待线程被打扰，抛出BrokenBarrierException异常。CyclicBarrier侧重点是线程，而不是调用事件，它的典型应用场景是用来等待并发线程结束。 3.控制并发线程数的SemaphoreSemaphore（信号量）是用来控制同时访问特定资源的线程数量，它通过协调各个线程，以保证合理的使用公共资源 第九部分、线程池1. 为什么要使用线程池第一：降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 第二：提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 第三：提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。但是，要做到合理利用线程池，必须对其实现原理了如指掌。 2. 线程池的实现原理2.1五种不同的标准线程池Executors目前提供了5种不同的线程池创建配置： newCachedThreadPool()，它是一种用来处理大量短时间工作任务的线程池，具有几个鲜明特点：它会试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；如果线程闲置的时间超过60秒，则被终止并移出缓存；长时间闲置时，这种线程池，不会消耗什么资源。其内部使用SynchronousQueue作为工作队列。 newFixedThreadPool(int nThreads)，重用指定数目（nThreads）的线程，其背后使用的是无界的工作队列，任何时候最多有nThreads个工作线程是活动的。这意味着，如果任务数量超过了活动队列数目，将在工作队列中等待空闲线程出现；如果有工作线程退出，将会有新的工作线程被创建，以补足指定的数目nThreads。 newSingleThreadExecutor()，它的特点在于工作线程数目被限制为1，操作一个无界的工作队列，所以它保证了所有任务的都是被顺序执行，最多会有一个任务处于活动状态，并且不允许使用者改动线程池实例，因此可以避免其改变线程数目。 newSingleThreadScheduledExecutor()和newScheduledThreadPool(int corePoolSize)，创建的是个ScheduledExecutorService，可以进行定时或周期性的工作调度，区别在于单一工作线程还是多个工作线程。 newWorkStealingPool(int parallelism)，这是一个经常被人忽略的线程池，Java 8才加入这个创建方法，其内部会构建ForkJoinPool，利用Work-Stealing算法，并行地处理任务，不保证处理顺序。 2.2 创建七大参数 corePoolSize：核心线程数 maximumPoolSize：最大线程数 keepAliveTime：线程存活时间 TimeUnit：存活时间的单位 workQueue：工作队列，必须是阻塞队列 ThreadFactory ：创建线程的工厂 RejectedExecutionHandler：拒绝执行的策略 2.3 四大拒绝策略·AbortPolicy：直接抛出异常。 ·CallerRunsPolicy：只用调用者所在线程来运行任务。 ·DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务。 ·DiscardPolicy：不处理，丢弃掉。 Ps：可以根据应用场景需要来实现RejectedExecutionHandler接口自定义策略。如记录日志或持久化存储不能处理的任务。 2.4 任务处理流程![image-20210502114404422](/Users/shengbinbin/Library/Application Support/typora-user-images/image-20210502114404422.png) 2.5 提交任务submit和execute execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功。 execute()方法输入的任务是一个Runnable类的实例。 submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 2.6 线程池的大小选择策略 如果是计算型任务，说明CPU是一种稀缺的资源，线程太多会导致上下文切换，所以线程数一般为按照CPU核的数目N或者N+1； 如果是IO密集型任务，线程数 = CPU核数 × （1 + 平均等待时间/平均工作时间） 第十部分、Executor框架1. Executor框架简介1.1Executor框架的两级调度模型 在HotSpot VM的线程模型中，Java线程（java.lang.Thread）被一对一映射为本地操作系统线 程。Java线程启动时会创建一个本地操作系统线程；当该Java线程终止时，这个操作系统线程也会被回收。操作系统会调度所有线程并将它们分配给可用的CPU。 在上层，Java多线程程序通常把应用分解为若干个任务，然后使用用户级的调度器（Executor框架）将这些任务映射为固定数量的线程；在底层，操作系统内核将这些线程映射到硬件处理器上: ![image-20210503091113831](/Users/shengbinbin/Library/Application Support/typora-user-images/image-20210503091113831.png) 1.2 三大组成部分 任务。包括被执行任务需要实现的接口：Runnable接口或Callable接口。 任务的执行。包括任务执行机制的核心接口Executor，以及继承自Executor的 ExecutorService接口。Executor框架有两个关键类实现了ExecutorService接口 （ThreadPoolExecutor和ScheduledThreadPoolExecutor）。 异步计算的结果。包括接口Future和实现Future接口的FutureTask类。 ![image-20210503091241378](/Users/shengbinbin/Library/Application Support/typora-user-images/image-20210503091241378.png) 2. ThreadPoolExecutor详解Executor框架最核心的类是ThreadPoolExecutor，它是线程池的实现类，主要由下列4个组件构成。 ·corePool：核心线程池的大小。 ·maximumPool：最大线程池的大小。 ·BlockingQueue：用来暂时保存任务的工作队列。 ·RejectedExecutionHandler：当ThreadPoolExecutor已经关闭或ThreadPoolExecutor已经饱和 时（达到了最大线程池大小且工作队列已满），execute()方法将要调用的Handler。 2.1 FixedThreadPoolFixedThreadPool的corePoolSize和maximumPoolSize都被设置为创建FixedThreadPool时指定的参数nThreads。 123456public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory); } keepAliveTime设置为0L，意味着多余的空闲线程会被立即终止 FixedThreadPool使用无界队列LinkedBlockingQueue作为线程池的工作队列（队列的容量为 Integer.MAX_VALUE）。使用无界队列作为工作队列会对线程池带来如下影响: 1）当线程池中的线程数达到corePoolSize后，新任务将在无界队列中等待，因此线程池中 的线程数不会超过corePoolSize。 2）由于1，使用无界队列时maximumPoolSize将是一个无效参数。 3）由于1和2，使用无界队列时keepAliveTime将是一个无效参数。 4）由于使用无界队列，运行中的FixedThreadPool（未执行方法shutdown()或 shutdownNow()）不会拒绝任务（不会调用RejectedExecutionHandler.rejectedExecution方法）。 2.2 SingleThreadExecutor123456public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); } SingleThreadExecutor的corePoolSize和maximumPoolSize被设置为1。其他参数与 FixedThreadPool相同。SingleThreadExecutor使用无界队列LinkedBlockingQueue作为线程池的工 作队列（队列的容量为Integer.MAX_VALUE）。SingleThreadExecutor使用无界队列作为工作队列 对线程池带来的影响与FixedThreadPool相同. 2.3 CachedThreadPool12345public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); } CachedThreadPool的corePoolSize被设置为0，即corePool为空；maximumPoolSize被设置为 Integer.MAX_VALUE，即maximumPool是无界的。这里把keepAliveTime设置为60L，意味着 CachedThreadPool中的空闲线程等待新任务的最长时间为60秒，空闲线程超过60秒后将会被 终止。 如果主线程提交任务的速度高于 maximumPool中线程处理任务的速度时，CachedThreadPool会不断创建新线程。极端情况下， CachedThreadPool会因为创建过多线程而耗尽CPU和内存资源。","link":"/2021/05/07/Java%E5%B9%B6%E5%8F%91%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%B8%8B/"},{"title":"Java并发（四）并发集合","text":"","link":"/2021/08/24/Java%E5%B9%B6%E5%8F%91%EF%BC%88%E5%9B%9B%EF%BC%89%E5%B9%B6%E5%8F%91%E9%9B%86%E5%90%88/"},{"title":"Java并发理论基础-上","text":"并发编程领域可以抽象成三个核心问题：分配任务、相互协作和互斥： 分配任务：将一个大的任务交给不同的进程/线程来做 相互协作：线程间的协作，比如一个线程执行完了一个任务，如何通知执行后续任务的线程开工 互斥：要实现在同一时刻内只有一个线程访问共享变量 第一部分、并发编程的发展1. 并发编程问题的由来随着CPU 、 内存 、IO设备的不断发展，三者的速度差异一直是存在的（CPU一天 ， 内存一年 ， IO设备十年） 为了合理利用 CPU 的高性能，平衡这三者的速度差异，计算机体系机构、操作系统、编译程序都做出了贡献，主要体现为： CPU 增加了缓存，以均衡与内存的速度差异； 操作系统增加了进程、线程，以分时复用 CPU，进而均衡 CPU 与 I/O 设备的速度差异； 编译程序优化指令执行次序，使得缓存能够得到更加合理地利用 1.1 缓存导致的可见性问题 一个线程对共享变量的修改，另外一个线程能够立刻看到，我们称为可见性。 单核时代，电脑只有1个CPU，所有的线程操作的是同一个CPU的缓存，也就不存在可见性问题。 多核时代，每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了，当多个线程在不同的 CPU 上执行时，这些线程操作的是不同的 CPU 缓存，如下图 1.2 线程切换带来的原子性问题 一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性 在一个时间片内，如果一个进程进行一个 IO 操作，例如读个文件，这个时候该进程可以把自己标记为“休眠状态”并出让 CPU 的使用权，待文件读进内存，操作系统会把这个休眠的进程唤醒，唤醒后的进程就有机会重新获得 CPU 的使用权了。 早期的操作系统基于进程来调度 CPU，不同进程间是不共享内存空间的，所以进程要做任务切换就要切换内存映射地址，而一个进程创建的所有线程，都是共享一个内存空间的，所以线程做任务切换成本就很低了。现代的操作系统都基于更轻量的线程来调度，现在我们提到的“任务切换”都是指“线程切换”。 Java 并发程序都是基于多线程的，自然也会涉及到任务切换，也许你想不到，任务切换竟然也是并发编程里诡异 Bug 的源头之一。任务切换的时机大多数是在时间片结束的时候，我们现在基本都使用高级语言编程，高级语言里一条语句往往需要多条 CPU 指令完成，例如上面代码中的count += 1，至少需要三条 CPU 指令。 指令 1：首先，需要把变量 count 从内存加载到 CPU 的寄存器； 指令 2：之后，在寄存器中执行 +1 操作； 指令 3：最后，将结果写入内存（缓存机制导致可能写入的是 CPU 缓存而不是内存） 1.3 编译优化带来的有序性问题有序性指的是程序按照代码的先后顺序执行。编译器为了优化性能，有时候会进行指令重排序。 重排序分3种类型。 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术(Instruction-Level Parallelism，ILP)来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应 机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上 去可能是在乱序执行 例如程序中：“a=6；b=7；”编译器优化后可能变成“b=7；a=6；”，在这个例子中，编译器调整了语句的顺序，但是不影响程序的最终结果，但是有时会出现问题。 比如单例模式中的双重检验模式： 1234567891011121314public class Singleton { static Singleton instance; public Singleton getInstance(){ if (instance == null){ synchronized (Singleton.class){ if (instance == null) instance = new Singleton(); //在CPU指令上并不是一步操作 } } return instance; }} new一个新的对象分为如下几步（先在内存中初始化对象再赋值给变量）： 分配一块内存 M； 在内存 M 上初始化 Singleton 对象； 然后 M 的地址赋值给 instance 变量。 经过指令重排序变成了下面这种情况（先将内存赋值给变量再进行初始化）： 分配一块内存 M； 将 M 的地址赋值给 instance 变量； 最后在内存 M 上初始化 Singleton 对象。 不安全的情况：假设线程 A 先执行 getInstance() 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance() 方法，那么线程 B 在执行第一个判断时会发现 instance != null ，所以直接返回 instance，而此时的 instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。 2. 并发编程面临的挑战并发编程的目的是为了让程序运行得更快，但是，并不是启动更多的线程就能让程序最大限度地并发执行。 2.1上下文切换所谓的多线程并发执行是通过CPU给每个线程分配CPU时间片来实现的，因为时间片非常短（一般是几十毫秒ms），所以CPU通过不停地切 换线程执行，让我们感觉多个线程是同时执行的。 上下文切换是指：当前任务执行完一个时间片后需要切换到下一个任务，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这 个任务的状态。所以任务从保存到再加载的过程就是一次上下文切换 多线程一定快吗？不一定，因为线程有创建和上下文切换的开销。 如何减少上下文切换： 无锁并发编程。多线程竞争锁时，会引起上下文切换，所以多线程处理数据时，可以用一 些办法来避免使用锁，如将数据的ID按照Hash算法取模分段，不同的线程处理不同段的数据。 CAS算法。Java的Atomic包使用CAS算法来更新数据，而不需要加锁。 使用最少线程。避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样会造成大量线程都处于等待状态。 协程:在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。 2.2 死锁及避免办法 避免一个线程同时获取多个锁。 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 尝试使用定时锁，使用lock.tryLock(timeout)来替代使用内部锁机制。 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。 3. 什么是线程安全？线程安全需要保证几个基本特性： 原子性，简单说就是相关操作不会中途被其他线程干扰，一般通过同步机制实现。 可见性，是一个线程修改了某个共享变量，其状态能够立即被其他线程知晓，通常被解释为将线程本地状态反映到主内存上，volatile就是负责保证可见性的。 有序性，是保证线程内串行语义，避免指令重排等。 第二部分、Java中如何实现并发安全由上一部分可知：解决可见性、有序性最直接的办法就是禁用缓存和编译优化，但是性能上会带来问题。因此需要做到按需禁用。站在程序员的视角看就是 Java 内存模型规范了 JVM 如何提供按需禁用缓存和编译优化的方法。具体来说，这些方法包括 volatile、synchronized 和 final 三个关键字，以及六项 Happens-Before 规则， 1. volatile保证可见性volatile是轻量级的 synchronized，它在多处理器开发中保证了共享变量的“可见性”,不会引起线程上下文的切换和调度。 1volatile int x = 0; //告诉编译器，对这个变量的读写，不能使用 CPU 缓存，必须从内存中读取或者写入 1.1 如何实现的可见性有volatile变量修饰的共享变量进行写操作的时候会多出第二行汇编代码: 0x01a3de1d: movb 0×0,0×1104800(0×0,0×1104800(%esi); 0x01a3de24: lock addl 0×0,0×1104800(0×0,(%esp); Lock前缀的指令： 1)将当前处理器缓存行的数据写回到系统内存。 2)这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。 为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存(L1，L2或其他)后再进行操作，但操作完不知道何时会写到内存。如果对声明了volatile的 变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据 写回到系统内存。但是，就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操 作就会有问题。所以，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一 致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当 处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状 态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存 里。 1.2 volatile的使用优化追加64字节能够提高并发编程的效率？ 处理器的L1、L2或L3缓存的高速缓存行是64个字节宽，如果队列的头节点和尾节点都不足64字节的话，处理器会将 它们都读到同一个高速缓存行中，在多处理器下每个处理器都会缓存同样的头、尾节点，当一 个处理器试图修改头节点时，会将整个缓存行锁定，那么在缓存一致性机制的作用下，会导致 其他处理器不能访问自己高速缓存中的尾节点，而队列的入队和出队操作则需要不停修改头 节点和尾节点，所以在多处理器的情况下将会严重影响到队列的入队和出队效率。Doug lea使 用追加到64字节的方式来填满高速缓冲区的缓存行，避免头节点和尾节点加载到同一个缓存 行，使头、尾节点在修改时不会互相锁定。 2. synchronized保证原子性2.1 应用方法·对于普通同步方法，锁是当前实例对象。 ·对于静态同步方法，锁是当前类的Class对象。 ·对于同步方法块，锁是Synchonized括号里配置的对象。 123456789101112131415161718class X { // 修饰非静态方法,锁定的是当前实例对象 this synchronized void foo() { // 临界区 } // 修饰静态方法,锁定的是当前类的 Class 对象 synchronized static void bar() { // 临界区 } // 修饰代码块 //当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。 Object obj = new Object()； void baz() { synchronized(obj) { // 临界区 } }} 2.2 实现原理 在Java6之前，synchronized完全依靠操作系统的互斥锁来实现，需要进行用户态和内核态的切换，所以开销较大，但随着一系列的锁优化，synchronized的性能也越来越好了 JVM基于进入和退出Monitor对象来实现方法同步和代码块同步，但两者的实现细节不一样。 代码块同步是使用monitorenter 和monitorexit指令实现的，而方法同步是使用另外一种方式实现的 monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。 任何对象都有 一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态。线程执行到monitorenter 指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。 源代码获取： 首先，synchronized的行为是JVM runtime的一部分，所以我们需要先找到Runtime相关的功能实现。通过在代码中查询类似“monitor_enter”或“Monitor Enter”，很直观的就 可以定位到： sharedRuntime.cpp/hpp，它是解释器和编译器运行时的基类。 synchronizer.cpp/hpp，JVM同步相关的各种基础逻辑。 在sharedRuntime.cpp中，下面代码体现了synchronized的主要逻辑。 1234567Handle h_obj(THREAD, obj); if (UseBiasedLocking) { //检查是否开启了偏向锁 // Retry fas entry if bias is revoked to avoid unnecessary infation ObjectSynchronizer::fast_enter(h_obj, lock, true, CHECK); //完整的流程 } else { ObjectSynchronizer::slow_enter(h_obj, lock, CHECK); //直接进入轻量级锁获取逻辑 } 偏斜锁并不适合所有应用场景，撤销操作（revoke）是比较重的行为，只有当存在较多不会真正竞争的synchronized块儿时，才能体现出明显改善。实践中对于偏斜锁的一直是有 争议的，有人甚至认为，当你需要大量使用并发类库时，往往意味着你不需要偏斜锁。从具体选择来看，我还是建议需要在实践中进行测试，根据结果再决定是否使用。 还有一方面是，偏斜锁会延缓JIT 预热的进程，所以很多性能测试中会显式地关闭偏斜锁， -XX:-UseBiasedLocking 12345678910111213141516171819void ObjectSynchronizer::fas_enter(Handle obj, BasicLock* lock, bool attempt_rebias, TRAPS) { if (UseBiasedLocking) { if (!SafepointSynchronize::is_at_safepoint()) { //revoke_and_rebias是获取偏斜锁的入口方法 BiasedLocking::Condition cond = BiasedLocking::revoke_and_rebias(obj, attempt_rebias, THREAD); if (cond == BiasedLocking::BIAS_REVOKED_AND_REBIASED) { return; } } else { assert(!attempt_rebias, &quot;can not rebias toward VM thread&quot;); //revoke_at_safepoint则定义了当检测到安全点时的处理逻辑 BiasedLocking::revoke_at_safepoint(obj); } assert(!obj-&gt;mark()-&gt;has_bias_pattern(), &quot;biases should be revoked by now&quot;); } slow_enter(obj, lock, THREAD);} 1234567891011121314151617181920212223void ObjectSynchronizer::slow_enter(Handle obj, BasicLock* lock, TRAPS) { markOop mark = obj-&gt;mark(); if (mark-&gt;is_neutral()) { // 将目前的Mark Word复制到Displaced Header上 lock-&gt;set_displaced_header(mark); // 利用CAS设置对象的Mark Word if (mark == obj()-&gt;cas_set_mark((markOop) lock, mark)) { TEVENT(slow_enter: release sacklock); return; } // 检查存在竞争 } else if (mark-&gt;has_locker() &amp;&amp; THREAD-&gt;is_lock_owned((address)mark-&gt;locker())) { // 清除 lock-&gt;set_displaced_header(NULL); return; } // 重置Displaced Header lock-&gt;set_displaced_header(markOopDesc::unused_mark()); ObjectSynchronizer::infate(THREAD, obj(), infate_cause_monitor_enter)-&gt;enter(THREAD);} 2.3 锁的优化过程synchronized用的锁是存在Java对象头里的。如果对象是数组类型，还会存数组类型： 锁一共有4种状态，级别从低到高依次是: 无锁状态、 偏向锁状态、 轻量级锁状态、 重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级 2.3.1 偏向锁经验：大多数情况下，锁都是由同一个线程多次获得。 偏向锁的加锁： 当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出 同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否 存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁。如果测试失败，则需 要再测试一下Mark Word中偏向锁的标识是否设置成1(表示当前是偏向锁):如果没有设置，则 使用CAS竞争锁;如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。 偏向锁的撤销： 偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时， 持有偏向锁的线程才会释放锁。偏向锁的撤销，需要等待全局安全点(在这个时间点上没有正 在执行的字节码)。它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着， 如果线程不处于活动状态，则将对象头设置成无锁状态;如果线程仍然活着，拥有偏向锁的栈 会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他 线程，要么恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。 偏向锁的启用： 偏向锁在Java 6和Java 7里是默认启用的，但是它在应用程序启动几秒钟之后才激活 2.3.2 轻量级锁加锁过程： 线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并 将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用 CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁 解锁过程： 轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成 功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁 因为自旋会消耗CPU，为了避免无用的自旋(比如获得锁的线程被阻塞住了)，一旦锁升级 成重量级锁，就不会再恢复到轻量级锁状态。当锁处于这个状态下，其他线程试图获取锁时， 都会被阻塞住，当持有锁的线程释放锁之后会唤醒这些线程，被唤醒的线程就会进行新一轮 的夺锁之争 2.3.3 原子操作的实现原理处理器如何实现： 锁总线：多个处理器同时从各自的缓存中读取变量i，分别进行加1操作，然后分别写入 系统内存中。那么，想要保证读改写共享变量的操作是原子的，就必须保证CPU1读改写共享 变量的时候，CPU2不能操作缓存了该共享变量内存地址的缓存。 处理器使用总线锁就是来解决这个问题的。所谓总线锁就是使用处理器提供的一个 LOCK#信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住，那么该 处理器可以独占共享内存。 锁缓存：总线锁定把CPU和内存之间的通信锁住了，这使得锁定期间，其他处 理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，目前处理器在某些场合下 使用缓存锁定代替总线锁定来进行优化。缓存锁定”是指内存区域如果被缓存在处理器的缓存 行中，并且在Lock操作期间被锁定，那么当它执行锁操作回写到内存时，处理器不在总线上声 言LOCK#信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子 性，因为缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据，当其他处 理器回写已被锁定的缓存行的数据时，会使缓存行无效 Java如何实现： 使用循环CAS实现原子操作：JVM中的CAS操作正是利用了处理器提供的CMPXCHG指令实现的。 从Java 1.5开始，JDK的并发包里提供了一些类来支持原子操作，如AtomicBoolean(用原子 方式更新的boolean值)、AtomicInteger(用原子方式更新的int值)和AtomicLong(用原子方式更 新的long值)。这些原子包装类还提供了有用的工具方法，比如以原子的方式将当前值自增1和 自减1。 CAS实现原子操作的三大问题： 1)ABA问题 2)循环时间长开销大 3)只能保证一个共享变量的原子操作 (3)使用锁机制实现原子操作 2.4 转账为例分析锁保护没有关联关系的多个资源，例如，银行业务中有针对账户余额（余额是一种资源）的取款操作，也有针对账户密码（密码也是一种资源）的更改操作，我们可以为账户余额和账户密码分配不同的锁来解决并发问题，这个还是很简单的。 1234567891011121314151617181920212223242526272829303132333435363738394041//不同的资源用不同的锁保护class Account { // 锁：保护账户余额 private final Object balLock = new Object(); // 账户余额 private Integer balance; // 锁：保护账户密码 private final Object pwLock = new Object(); // 账户密码 private String password; // 取款 void withdraw(Integer amt) { synchronized(balLock) { if (this.balance &gt; amt){ this.balance -= amt; } } } // 查看余额 Integer getBalance() { synchronized(balLock) { return balance; } } // 更改密码 void updatePassword(String pw){ synchronized(pwLock) { this.password = pw; } } // 查看密码 String getPassword() { synchronized(pwLock) { return password; } }} 当然，我们也可以用一把互斥锁来保护多个资源，例如我们可以用 this 这一把锁来管理账户类里所有的资源：账户余额和用户密码。具体实现很简单，示例程序中所有的方法都增加同步关键字 synchronized 就可以了. 但是用一把锁有个问题，就是性能太差，会导致取款、查看余额、修改密码、查看密码这四个操作都是串行的。而我们用两把锁，取款和修改密码是可以并行的。用不同的锁对受保护资源进行精细化管理，能够提升性能。这种锁还有个名字，叫细粒度锁。 保护有关联关系的多个资源 例如银行业务里面的转账操作，账户 A 减少 100 元，账户 B 增加 100 元。这两个账户就是有关联关系的。那对于像转账这种有关联关系的操作，我们应该怎么去解决呢？ 12345678910111213class Account { private int balance; // 转账 //临界区内有两个资源，分别是转出账户的余额 this.balance 和转入账户的余额 target.balance，并且用的是一把锁 this //问题就出在 this 这把锁上，this 这把锁可以保护自己的余额 this.balance，却保护不了别人的余额 target.balance，就像你不能用自家的锁来保护别人家的资产 synchronized void transfer( Account target, int amt){ if (this.balance &gt; amt) { this.balance -= amt; target.balance += amt; } } } ​ 下面我们具体分析一下，假设有 A、B、C 三个账户，余额都是 200 元，我们用两个线程分别执行两个转账操作：账户 A 转给账户 B 100 元，账户 B 转给账户 C 100 元，最后我们期望的结果应该是账户 A 的余额是 100 元，账户 B 的余额是 200 元， 账户 C 的余额是 300 元。 我们假设线程 1 执行账户 A 转账户 B 的操作，线程 2 执行账户 B 转账户 C 的操作。这两个线程分别在两颗 CPU 上同时执行，那它们是互斥的吗？我们期望是，但实际上并不是。因为线程 1 锁定的是账户 A 的实例（A.this），而线程 2 锁定的是账户 B 的实例（B.this），所以这两个线程可以同时进入临界区 transfer()。同时进入临界区的结果是什么呢？线程 1 和线程 2 都会读到账户 B 的余额为 200，导致最终账户 B 的余额可能是 300（线程 1 后于线程 2 写 B.balance，线程 2 写的 B.balance 值被线程 1 覆盖），可能是 100（线程 1 先于线程 2 写 B.balance，线程 1 写的 B.balance 值被线程 2 覆盖），就是不可能是 200。 使用锁的正确姿势 很简单，只要我们的锁能覆盖所有受保护资源就可以了。在上面的例子中，this 是对象级别的锁，所以 A 对象和 B 对象都有自己的锁，如何让 A 对象和 B 对象共享一把锁呢？ 用 Account.class 作为共享的锁, 缺点就是转账操作都成串行了 123456789101112class Account { private int balance; // 转账 void transfer(Account target, int amt){ synchronized(Account.class) { if (this.balance &gt; amt) { this.balance -= amt; target.balance += amt; } } } } 现实世界里，账户转账操作是支持并发的，而且绝对是真正的并行，银行所有的窗口都可以做转账操作。只要我们能仿照现实世界做转账操作，串行的问题就解决了。 我们试想在古代，没有信息化，账户的存在形式真的就是一个账本，而且每个账户都有一个账本，这些账本都统一存放在文件架上。银行柜员在给我们做转账时，要去文件架上把转出账本和转入账本都拿到手，然后做转账。这个柜员在拿账本的时候可能遇到以下三种情况： 文件架上恰好有转出账本和转入账本，那就同时拿走； 如果文件架上只有转出账本和转入账本之一，那这个柜员就先把文件架上有的账本拿到手，同时等着其他柜员把另外一个账本送回来； 转出账本和转入账本都没有，那这个柜员就等着两个账本都被送回来。 上面这个过程在编程的世界里怎么实现呢？其实用两把锁就实现了，转出账本一把，转入账本另一把。在 transfer() 方法内部，我们首先尝试锁定转出账户 this（先把转出账本拿到手），然后尝试锁定转入账户 target（再把转入账本拿到手），只有当两者都成功时，才执行转账操作。 12345678910111213141516class Account { private int balance; // 转账 void transfer(Account target, int amt){ // 锁定转出账户 synchronized(this) { // 锁定转入账户 synchronized(target) { if (this.balance &gt; amt) { this.balance -= amt; target.balance += amt; } } } } } 使用细粒度锁可以提高并行度，是性能优化的一个重要手段,但是会出现死锁的情况，比如下面这种情况： 账户 A 转账户 B 100 元，此时另一个客户找柜员李四也做个转账业务：账户 B 转账户 A 100 元，于是张三和李四同时都去文件架上拿账本，这时候有可能凑巧张三拿到了账本 A，李四拿到了账本 B。张三拿到账本 A 后就等着账本 B（账本 B 已经被李四拿走），而李四拿到账本 B 后就等着账本 A（账本 A 已经被张三拿走），他们要等多久呢？他们会永远等待下去…因为张三不会把账本 A 送回去，李四也不会把账本 B 送回去。我们姑且称为死等吧。 解决死锁在第四节讲。 3. Happens-Before 规则前面一个操作的结果对后续操作是可见的,Happens-Before 约束了编译器的优化行为，虽允许编译器优化，但是要求编译器优化后一定遵守 Happens-Before 规则 3.1 程序的顺序性规则在一个线程中，按照程序顺序，前面的操作 Happens-Before 于后续的任意操作 12345678910111213class VolatileExample { int x = 0; volatile boolean v = false; public void writer() { x = 42; //先发生 v = true; //后发生 } public void reader() { if (v == true) { // 这里 x 会是多少呢？ } }} 3.2 volatile 变量规则对一个 volatile 变量的写操作， Happens-Before 于后续对这个 volatile 变量的读操作 3.3 传递性规则如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens-Before C 3.4 管程中锁的规则对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。 管程是一种通用的同步原语，在 Java 中指的就是 synchronized，synchronized 是 Java 里对管程的实现。 123456synchronized (this) { // 此处自动加锁 // x 是共享变量, 初始值 =10 if (this.x &lt; 12) { this.x = 12; } } // 此处自动解锁 假设 x 的初始值是 10，线程 A 执行完代码块后 x 的值会变成 12（执行完自动释放锁），线程 B 进入代码块时，能够看到线程 A 对 x 的写操作，也就是线程 B 能够看到 x==12。这个也是符合我们直觉的 3.5 线程 start() 规则主线程 A 启动子线程 B 后，子线程 B 能够看到主线程在启动子线程 B 前的操作。 123456789Thread B = new Thread(()-&gt;{ // 主线程调用 B.start() 之前 // 所有对共享变量的修改，此处皆可见 // 此例中，var==77});// 此处对共享变量 var 修改var = 77;// 主线程启动子线程B.start(); 3.6 线程 join() 规则主线程 A 等待子线程 B 完成（主线程 A 通过调用子线程 B 的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），主线程能够看到子线程的操作。当然所谓的“看到”，指的是对共享变量的操作. 换句话说就是，如果在线程 A 中，调用线程 B 的 join() 并成功返回，那么线程 B 中的任意操作 Happens-Before 于该 join() 操作的返回。 123456789101112Thread B = new Thread(()-&gt;{ // 此处对共享变量 var 修改 var = 66;});// 例如此处对共享变量修改，// 则这个修改结果对线程 B 可见// 主线程启动子线程B.start();B.join()// 子线程所有对共享变量的修改// 在主线程调用 B.join() 之后皆可见// 此例中，var==66 4. 如何预防死锁4.1 死锁发生的四个条件 互斥，共享资源 X 和 Y 只能被一个线程占用； 占有且等待，线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X； 不可抢占，其他线程不能强行抢占线程 T1 占有的资源； 循环等待，线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等待。 4.2 破坏死锁条件 对于“占用且等待”这个条件，我们可以一次性申请所有的资源，这样就不存在等待了。 对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。 对于“循环等待”这个条件，可以靠按序申请资源来预防。所谓按序申请，是指资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的，这样线性化后自然就不存在循环了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//利用上面转账的例子class Allocator { private List&lt;Object&gt; als = new ArrayList&lt;&gt;(); // 一次性申请所有资源 synchronized boolean apply( Object from, Object to){ if(als.contains(from) || als.contains(to)){ return false; } else { als.add(from); als.add(to); } return true; } // 归还资源 synchronized void free( Object from, Object to){ als.remove(from); als.remove(to); }} class Account { // actr 应该为单例 private Allocator actr; private int balance; // 转账 void transfer(Account target, int amt){ // 一次性申请转出账户和转入账户，直到成功 while(!actr.apply(this, target))； //while 死循环 try{ // 锁定转出账户 synchronized(this){ // 锁定转入账户 synchronized(target){ if (this.balance &gt; amt){ this.balance -= amt; target.balance += amt; } } } } finally { actr.free(this, target) } } } 破坏不可抢占条件看上去很简单，核心是要能够主动释放它占有的资源，这一点 synchronized 是做不到的。原因是 synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。 1234567891011121314151617181920212223class Account { private int id; private int balance; // 转账 void transfer(Account target, int amt){ Account left = this ① Account right = target; ② if (this.id &gt; target.id) { ③ left = target; ④ right = this; ⑤ } ⑥ // 锁定序号小的账户 synchronized(left){ // 锁定序号大的账户 synchronized(right){ if (this.balance &gt; amt){ this.balance -= amt; target.balance += amt; } } } } } 5. 等待通知模式优化循环等待最好的方案应该是：如果线程要求的条件（转出账本和转入账本同在文件架上）不满足，则线程阻塞自己，进入等待状态；当线程要求的条件（转出账本和转入账本同在文件架上）满足后，通知等待的线程重新执行。其中，使用线程阻塞的方式就能避免循环等待消耗 CPU 的问题。 类比就医环节： 就医流程基本上是这样： 患者先去挂号，然后到就诊门口分诊，等待叫号； 当叫到自己的号时，患者就可以找大夫就诊了； 就诊过程中，大夫可能会让患者去做检查，同时叫下一位患者； 当患者做完检查后，拿检测报告重新分诊，等待叫号； 当大夫再次叫到自己的号时，患者再去找大夫就诊。 下面我们来对比看一下前面都忽视了哪些细节。 患者到就诊门口分诊，类似于线程要去获取互斥锁；当患者被叫到时，类似线程已经获取到锁了。 大夫让患者去做检查（缺乏检测报告不能诊断病因），类似于线程要求的条件没有满足。 患者去做检查，类似于线程进入等待状态；然后大夫叫下一个患者，这个步骤我们在前面的等待 - 通知机制中忽视了，这个步骤对应到程序里，本质是线程释放持有的互斥锁。 患者做完检查，类似于线程要求的条件已经满足；患者拿检测报告重新分诊，类似于线程需要重新获取互斥锁，这个步骤我们在前面的等待 - 通知机制中也忽视了。 5.1 synchronized 实现等待 - 通知机制wait方法原理（会释放锁）： notify方法原理： 为什么说是曾经满足过呢？因为notify() 只能保证在通知时间点，条件是满足的。而被通知线程的执行时间点和通知的时间点基本上不会重合，所以当线程执行的时候，很可能条件已经不满足了（保不齐有其他线程插队）。 上面我们一直强调 wait()、notify()、notifyAll() 方法操作的等待队列是互斥锁的等待队列，所以如果 synchronized 锁定的是 this，那么对应的一定是 this.wait()、this.notify()、this.notifyAll()；如果 synchronized 锁定的是 target，那么对应的一定是 target.wait()、target.notify()、target.notifyAll() 。而且 wait()、notify()、notifyAll() 这三个方法能够被调用的前提是已经获取了相应的互斥锁，所以我们会发现 wait()、notify()、notifyAll() 都是在 synchronized{}内部被调用的。如果在 synchronized{}外部调用，或者锁定的 this，而用 target.wait() 调用的话，JVM 会抛出一个运行时异常：java.lang.IllegalMonitorStateException。 等待 - 通知机制的基本原理搞清楚后，我们就来看看它如何解决一次性申请转出账户和转入账户的问题吧。在这个等待 - 通知机制中，我们需要考虑以下四个要素。 互斥锁：上一篇文章我们提到 Allocator 需要是单例的，所以我们可以用 this 作为互斥锁。 线程要求的条件：转出账户和转入账户都没有被分配过。 何时等待：线程要求的条件不满足就等待。 何时通知：当有线程释放账户时就通知。 ps：因为当 wait() 返回时，有可能条件已经发生变化了，曾经条件满足，但是现在已经不满足了，所以要重新检验条件是否满足。 12345678910111213141516171819202122class Allocator { private List&lt;Object&gt; als; // 一次性申请所有资源 synchronized void apply(Object from, Object to){ // 经典写法 while(als.contains(from) || als.contains(to)){ try{ wait(); //不满足就wait }catch(Exception e){ } } als.add(from); als.add(to); } // 归还资源 synchronized void free( Object from, Object to){ als.remove(from); als.remove(to); notifyAll(); }} notify() 是会随机地通知等待队列中的一个线程，而 notifyAll() 会通知等待队列中的所有线程。 尽量使用 notifyAll()，因为使用notify可能会造成有的线程再也不能被唤醒了 6. 管程Java 采用的是管程技术，synchronized 关键字及 wait()、notify()、notifyAll() 这三个方法都是管程的组成部分。而管程和信号量是等价的，所谓等价指的是用管程能够实现信号量，也能用信号量实现管程。但是管程更容易使用，所以 Java 选择了管程。 管程，对应的英文是 Monitor，很多 Java 领域的同学都喜欢将其翻译成“监视器“。所谓管程，指的是管理共享变量以及对共享变量的操作过程，让他们支持并发。翻译为 Java 领域的语言，就是管理类的成员变量和成员方法，让这个类是线程安全的。那管程是怎么管的呢？ 6.1 MESA 模型在管程的发展史上，先后出现过三种不同的管程模型，分别是：Hasen 模型、Hoare 模型和 MESA 模型。其中，现在广泛应用的是 MESA 模型，并且 Java 管程的实现参考的也是 MESA 模型。所以今天我们重点介绍一下 MESA 模型。 在并发编程领域，有两大核心问题：一个是互斥，即同一时刻只允许一个线程访问共享资源；另一个是同步，即线程之间如何通信、协作。这两大问题，管程都是能够解决的。 6.1.1 解决互斥管程解决互斥问题的思路很简单，就是将共享变量及其对共享变量的操作统一封装起来。 在下图中，管程 X 将共享变量 queue 这个队列和相关的操作入队 enq()、出队 deq() 都封装起来了； 线程 A 和线程 B 如果想访问共享变量 queue，只能通过调用管程提供的 enq()、deq() 方法来实现； enq()、deq() 保证互斥性，只允许一个线程进入管程。不知你有没有发现，管程模型和面向对象高度契合的。 6.1.2 解决同步在管程模型里，共享变量和对共享变量的操作是被封装起来的，图中最外层的框就代表封装的意思。框的上面只有一个入口，并且在入口旁边还有一个入口等待队列。当多个线程同时试图进入管程内部时，只允许一个线程进入，其他线程则在入口等待队列中等待。这个过程类似就医流程的分诊，只允许一个患者就诊，其他患者都在门口等待。 管程里还引入了条件变量的概念，而且每个条件变量都对应有一个等待队列，如下图，条件变量 A 和条件变量 B 分别都有自己的等待队列。 那条件变量和等待队列的作用是什么呢？其实就是解决线程同步问题。你也可以结合上面提到的入队出队例子加深一下理解。 假设有个线程 T1 执行数据出队操作，不过需要注意的是执行出队操作，有个前提条件，就是队列中的数据不能是空的，而队列不空这个前提条件就是管程里的条件变量。 如果线程 T1 进入管程后恰好发现队列是空的，那怎么办呢？等待啊，去哪里等呢？就去条件变量对应的等待队列里面等。此时线程 T1 就去“队列不空”这个条件变量的等待队列中等待。这个过程类似于大夫发现你要去验个血，于是给你开了个验血的单子，你呢就去验血的队伍里排队。线程 T1 进入条件变量的等待队列后，是允许其他线程进入管程的。这和你去验血的时候，医生可以给其他患者诊治，道理都是一样的。 再假设之后另外一个线程 T2 执行数据入队操作，入队操作执行成功之后，“队列不空”这个条件对于线程 T1 来说已经满足了，此时线程 T2 要通知 T1，告诉它需要的条件已经满足了。当线程 T1 得到通知后，会从等待队列里面出来，但是出来之后不是马上执行，而是重新进入到入口等待队列里面。这个过程类似你验血完，回来找大夫，需要重新分诊。 条件变量及其等待队列我们讲清楚了，下面再说说 wait()、notify()、notifyAll() 这三个操作。前面提到线程 T1 发现“队列不空”这个条件不满足，需要进到对应的等待队列里等待。这个过程就是通过调用 wait() 来实现的。如果我们用对象 A 代表“队列不空”这个条件，那么线程 T1 需要调用 A.wait()。同理当“队列不空”这个条件满足时，线程 T2 需要调用 A.notify() 来通知 A 等待队列中的一个线程，此时这个队列里面只有线程 T1。至于 notifyAll() 这个方法，它可以通知等待队列中的所有线程。 这里我还是来一段代码再次说明一下吧。下面的代码实现的是一个阻塞队列，阻塞队列有两个操作分别是入队和出队，这两个方法都是先获取互斥锁，类比管程模型中的入口。 对于入队操作，如果队列已满，就需要等待直到队列不满，所以这里用了notFull.await();。 对于出队操作，如果队列为空，就需要等待直到队列不空，所以就用了notEmpty.await();。 如果入队成功，那么队列就不空了，就需要通知条件变量：队列不空notEmpty对应的等待队列。 如果出队成功，那就队列就不满了，就需要通知条件变量：队列不满notFull对应的等待队列。 123456789101112131415161718192021222324252627282930313233343536373839//实现的是一个阻塞队列，阻塞队列有两个操作分别是入队和出队，这两个方法都是先获取互斥锁public class BlockedQueue&lt;T&gt;{ final Lock lock = new ReentrantLock(); // 条件变量：队列不满 final Condition notFull = lock.newCondition(); // 条件变量：队列不空 final Condition notEmpty = lock.newCondition(); // 入队 void enq(T x) { lock.lock(); try { while (队列已满){ // 等待队列不满 notFull.await(); } // 省略入队操作... // 入队后, 通知可出队 notEmpty.signal(); }finally { lock.unlock(); } } // 出队 void deq(){ lock.lock(); try { while (队列已空){ // 等待队列不空 notEmpty.await(); } // 省略出队操作... // 出队后，通知可入队 notFull.signal(); }finally { lock.unlock(); } }} 6.1.3 wait() 的正确姿势1234//编程范式：用if会造成虚假唤醒while(条件不满足) { wait();} Hasen 模型、Hoare 模型和 MESA 模型的一个核心区别就是当条件满足后，如何通知相关线程。管程要求同一时刻只允许一个线程执行，那当线程 T2 的操作使线程 T1 等待的条件满足时，T1 和 T2 究竟谁可以执行呢？ Hasen 模型里面，要求 notify() 放在代码的最后，这样 T2 通知完 T1 后，T2 就结束了，然后 T1 再执行，这样就能保证同一时刻只有一个线程执行。hasen 是执行完，再去唤醒另外一个线程，能够保证线程的执行。 Hoare 模型里面，T2 通知完 T1 后，T2 阻塞，T1 马上执行；等 T1 执行完，再唤醒 T2，也能保证同一时刻只有一个线程执行。但是相比 Hasen 模型，T2 多了一次阻塞唤醒操作。hoare，是中断当前线程，唤醒另外一个线程，执行玩再去唤醒，也能够保证完成。 MESA 管程里面，T2 通知完 T1 后，T2 还是会接着执行，T1 并不立即执行，仅仅是从条件变量的等待队列进到入口等待队列里面。这样做的好处是 notify() 不用放到代码的最后，T2 也没有多余的阻塞唤醒操作。但是也有个副作用，就是当 T1 再次执行的时候，可能曾经满足的条件，现在已经不满足了，所以需要以循环方式检验条件变量。 6.1.4 notify什么时候使用 所有等待线程拥有相同的等待条件； 所有等待线程被唤醒后，执行相同的操作； 只需要唤醒一个线程。 wait和sleep的区别 相同点： 都是让线程阻塞 都可以接受到中断通知 不同点： 在同步代码块中，sleep不会释放锁，wait会释放锁。所以wait方法必须在synchronized 保护的代码中使用，而sleep没有这个要求。 sleep方法必须定义一个时间，时间到期后自动恢复。而wait可以不设置参数，意味着永久等待 wait是Object类的方法，sleep是Thread的方法。 第三部分、Java中的线程现代操作系统调度的最小单元是线程，也叫轻量级进程(Light Weight Process)，在一个进程里可以创建多个线程，这些线程都拥有各自的计数器、堆栈和局部变量等属性，并且能够访问共享的内存变量。处理器在这些线程上高速切换，让使用者感觉到这些线程在同时执行。 1. 线程的发展路程1.1 操作系统的发展操作系统的发展经历了三个阶段： 手工操作： 单道批处理系统：输入机与主机之间增加了一个存储设备磁带(盘)，单道批处理系统是将作业一个一个加入内存的，那么某一个作业因为等待磁带（盘）或者其他I/O操作而暂停时，那计算机就只能一直阻塞，直到该I/O完成。对于CPU操作密集型的程序，I/O操作相对较少，因此浪费的时间也很少。但是对于I/O操作较多的场景来说，CPU的资源是属于严重浪费的。 多道批处理系统： 为了解决单道批处理系统因为输入/输出（I/O）请求后，导致计算机等待I/O完成而造成的计算机的资源的浪费。接下来又出现了多道批处理系统。多道批处理系统与单道批处理系统的主要区别是在内存中允许一个或多个作业，当一个作业在等待I/O处理时，多批处理系统会通过相应调度算法调度另外一个作业让计算机执行。从而使CPU的利用率得到更大的提高 1.2 进程的由来在多道批处理系统中引申出了一个非常重要的模式，即允许多个作业进入内存并运行。由于在内存中存储了多个作业，那么多个作业如何进行区分？当某个作业因为等待I/O暂停时，怎么恢复到之前的运行状态呢？ 所以这个时候，人们就发明了进程这一概念，用进程来保存每个作业的数据与运行状态，同时对每个进程划分对应的内存地址空间（代码、数据、进程空间、打开的文件），并且要求进程只能使用它自己的内存空间。那么就可以达到作业的区分及恢复。 1.3 线程的由来因为一个进程在一个时间段内只能做一件事情。如果某个程序有多个任务，只能逐个执行这些任务。同时进程中存储了大量信息（数据，进程运行状态信息等）。那么当计算机进行进程切换的时候，必然存在着很大的时间与空间消耗（因为每个进程对应不同内存地址空间，进程的切换，实际是处理器处理不同的地址空间） 为了实现一个进程中任务的切换同时又避免地址空间的切换：发明了线程这一概念，用线程表示进程中的不同任务，同时又将计算机实际调度的单元转到线程。这样就避免了进程的内存地址空间的切换，也达到了多任务的并发执行。 1.4 进程和线程的区别 进程是CPU分配系统资源的基本单位，线程是CPU调度和执行的基本单位。 一个进程可以包含多个线程，进程拥有自己独立的地址空间，而进程中的不同线程共享该进程的地址空间 进程的切换会涉及到虚拟地址空间的切换，开销比较大，线程的切换开销比较小 1.5 为什么要使用多线程 更多的处理器核心：一个 单线程程序在运行时只能使用一个处理器核心，那么再多的处理器核心加入也无法显著提升 该程序的执行效率。相反，如果该程序使用多线程技术，将计算逻辑分配到多个处理器核心 上，就会显著减少程序的处理时间，并且随着更多处理器核心的加入而变得更有效率 更快的响应时间：一笔订单的创建，它包括插入订单数据、生成订单快照、发送邮件通知卖家和记录 货品销售数量等。可以使用多线程技术，即将数据一致性不强的操作派发给其他线程处 理(也可以使用消息队列)，如生成订单快照、发送邮件等。这样做的好处是响应用户请求的线 程能够尽可能快地处理完成，缩短了响应时间，提升了用户体验 更好的编程模型 1.6 线程的优先级现代操作系统基本采用时分的形式调度运行的线程，操作系统会分出一个个时间片，线程会分配到若干时间片，当线程的时间片用完了就会发生线程调度，并等待着下次分配。线程分配到的时间片多少也就决定了线程使用处理器资源的多少，而线程优先级就是决定线程需要多或者少分配一些处理器资源的线程属性。 在Java线程中，通过一个整型成员变量priority来控制优先级，优先级的范围从1~10，在线程构建的时候可以通过setPriority(int)方法来修改优先级，默认优先级是5，优先级高的线程分配时间片的数量要多于优先级低的线程。 1.7 线程的6种状态 NEW（初始化状态） RUNNABLE（可运行 / 运行状态） BLOCKED（阻塞状态） WAITING（无时限等待） TIMED_WAITING（有时限等待） TERMINATED（终止状态） 1.8 Daemon线程Daemon线程是一种支持型线程，因为它主要被用作程序中后台调度以及支持性工作。这 意味着，当一个Java虚拟机中不存在非Daemon线程的时候，Java虚拟机将会退出。可以通过调 用Thread.setDaemon(true)将线程设置为Daemon线程。 2. 启动和终止线程调用线程的start()方法进行启动，随着run()方法的执行完毕，线程也随之终止 2.1 构造线程的三种方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package server.doc.thread;import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;public class ThreadTest { public static void main(String[] args) throws ExecutionException, InterruptedException { A a = new A(); Thread threadA = new Thread(a); threadA.start(); B b = new B(); Thread threadB = new Thread(b); threadB.start(); C c = new C(); FutureTask&lt;Integer&gt; integerFutureTask = new FutureTask&lt;&gt;(c); //FutureTask&lt;V&gt;()是Runnable的实现类 Thread threadC = new Thread(integerFutureTask); threadC.start(); System.out.println(integerFutureTask.get());//可通过get方法获得返回值 }}class A extends Thread{ @Override public void run() { System.out.println(&quot;=======继承Thread类创建线程====&quot;); }}class B implements Runnable{ @Override public void run() { System.out.println(&quot;=======实现runnable接口创建线程====&quot;); }}//实现Callable接口创建线程,Integer就是返回值class C implements Callable&lt;Integer&gt; { @Override public Integer call() throws Exception { System.out.println(&quot;=======实现Callable接口创建线程====&quot;); return 2; }} 2.2 启动线程start源码1234567891011121314151617181920212223242526272829// 该方法可以创建一个新的线程出来public synchronized void start() { // 如果没有初始化，抛异常 if (threadStatus != 0) throw new IllegalThreadStateException(); group.add(this); // started 是个标识符，我们在做一些事情的时候，经常这么写 // 动作发生之前标识符是 false，发生完成之后变成 true boolean started = false; try {// 这里会创建一个新的线程，执行完成之后，新的线程已经在运行了，既 target 的内容已经在运行了 start0(); // 这里执行的还是主线程 started = true; } finally { try { // 如果失败，把线程从线程组中删除 if (!started) { group.threadStartFailed(this); } // Throwable 可以捕捉一些 Exception 捕捉不到的异常，比如说子线程抛出的异常 } catch (Throwable ignore) { /* do nothing. If start0 threw a Throwable then it will be passed up the call stack */ } }}// 开启新线程使用的是 native 方法private native void start0(); 2.3 正确的停止线程中断可以理解为线程的一个标识位属性，它表示一个运行中的线程是否被其他线程进行了中断操作。中断好比其他线程对该线程打了个招呼，其他线程通过调用该线程的interrupt() 方法对其进行中断操作。 从原理上讲应该用 interrupt 来请求中断，而不是强制停止，因为这样可以避免数据错乱，也可以让线程有时间结束收尾工作。 123while (!Thread.currentThread().islnterrupted() &amp;&amp; more work to do) { do more work} 我们一旦调用某个线程的 interrupt() 之后，这个线程的中断标记位就会被设置成 true。每个线程都有这样的标记位，当线程执行时，应该定期检查这个标记位，如果标记位被设置成 true，就说明有程序想终止该线程。回到源码，可以看到在 while 循环体判断语句中，首先通过 Thread.currentThread().isInterrupt() 判断线程是否被中断，随后检查是否还有工作要做。 被 interrupt 的线程，是怎么收到通知的呢？一种是异常，另一种是主动检测。 异常： 当线程 A 处于 WAITING、TIMED_WAITING 状态时，如果其他线程调用线程 A 的 interrupt() 方法，会使线程 A 返回到 RUNNABLE 状态，同时线程 A 的代码会触发 InterruptedException 异常。上面我们提到转换到 WAITING、TIMED_WAITING 状态的触发条件，都是调用了类似 wait()、join()、sleep() 这样的方法，我们看这些方法的签名，发现都会 throws InterruptedException 这个异常。这个异常的触发条件就是：其他线程调用了该线程的 interrupt() 方法。 当线程 A 处于 RUNNABLE 状态时，并且阻塞在 java.nio.channels.InterruptibleChannel 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 会触发 java.nio.channels.ClosedByInterruptException 这个异常；而阻塞在 java.nio.channels.Selector 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 的 java.nio.channels.Selector 会立即返回。 主动监测： 线程通过检查自身是否被中断来进行响应，线程通过方法isInterrupted()来进行判断是否 被中断，也可以调用静态方法Thread.interrupted()对当前线程的中断标识位进行复位。 sleep情况下能否感召到打断位？ 如果 sleep、wait 等可以让线程进入阻塞的方法使线程休眠了，而处于休眠中的线程被中断，那么线程是可以感受到中断信号的，并且会抛出一个 InterruptedException 异常，同时清除中断信号，将中断标记位设置成 false。这样一来就不用担心长时间休眠中线程感受不到中断了，因为即便线程还在休眠，仍然能够响应中断通知，并抛出异常。 3.线程间通信的几种方式 volatile和synchronized关键字 对于同步块的实现使用了monitorenter和monitorexit指令，而同步方法则 是依靠方法修饰符上的ACC_SYNCHRONIZED来完成的。无论采用哪种方式，其本质是对一 个对象的监视器(monitor)进行获取，而这个获取过程是排他的，也就是同一时刻只能有一个 线程获取到由synchronized所保护对象的监视器。 任意一个对象都拥有自己的监视器，当这个对象由同步块或者这个对象的同步方法调用 时，执行方法的线程必须先获取到该对象的监视器才能进入同步块或者同步方法，而没有获 取到监视器(执行该方法)的线程将会被阻塞在同步块和同步方法的入口处，进入BLOCKED 状态。 等待/通知机制（wait / notify） 等待/通知机制，是指一个线程A调用了对象O的wait()方法进入等待状态，而另一个线程B 调用了对象O的notify()或者notifyAll()方法，线程A收到通知后从对象O的wait()方法返回，进而 执行后续操作。上述两个线程通过对象O来完成交互，而对象上的wait()和notify/notifyAll()的 关系就如同开关信号一样，用来完成等待方和通知方之间的交互工作。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package _2不同的生产者消费者模式;/** * 线程之间的通信问题：两个线程交替执行A B操作同一个变量+1，-1* */public class A { public static void main(String[] args) { Data data = new Data(); new Thread(()-&gt;{ for (int i = 0; i &lt; 10; i++) { try { data.increment(); } catch (InterruptedException e) { e.printStackTrace(); } } },&quot;A&quot;).start(); new Thread(()-&gt;{ for (int i = 0; i &lt; 10; i++) { try { data.decrement(); } catch (InterruptedException e) { e.printStackTrace(); } } },&quot;B&quot;).start(); new Thread(()-&gt;{ for (int i = 0; i &lt; 10; i++) { try { data.increment(); } catch (InterruptedException e) { e.printStackTrace(); } } },&quot;C&quot;).start(); new Thread(()-&gt;{ for (int i = 0; i &lt; 10; i++) { try { data.decrement(); } catch (InterruptedException e) { e.printStackTrace(); } } },&quot;D&quot;).start(); }}//1.判断是否需要等待//2.执行业务//3.通知其他线程class Data{ //数字，资源类 private int num = 0; //+1 public synchronized void increment() throws InterruptedException { while (num != 0) { //用if会出现虚假唤醒现象 this.wait(); } num++; System.out.println(Thread.currentThread().getName()+&quot;=====&quot;+num); //通知其他线程，+1完毕 this.notifyAll(); } //-1 public synchronized void decrement() throws InterruptedException { while (num == 0) { this.wait(); } num--; System.out.println(Thread.currentThread().getName()+&quot;=====&quot;+num); this.notifyAll(); }}复制代码 Thread.join()方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package server.doc.thread;public class JoinTest implements Runnable { @Override public void run() { System.out.println(&quot;join thread demo&quot;); } public static void main(String[] args) throws InterruptedException { System.out.println(&quot;main thread start...&quot;); JoinTest joinTest = new JoinTest(); Thread thread = new Thread(joinTest); thread.setName(&quot;joinTest thread&quot;); thread.start(); thread.join(); System.out.println(&quot;main thread end&quot;); }}//没有join的时候：main thread start...main thread endjoin thread demo//有join的时候main thread start...join thread demomain thread end也就是说：当main线程去调用t.join()是，会将自己当前线程阻塞，等到t线程执行完成到达完结状态，main线程才可以继续执行复制代码//join 源码public final synchronized void join(long millis) throws InterruptedException { long base = System.currentTimeMillis(); long now = 0; // 首先校验参数是否合法 if (millis &lt; 0) { throw new IllegalArgumentException(&quot;timeout value is negative&quot;); } // 如果join方法没有参数，则相当于直接调用wait方法 if (millis == 0) { while (isAlive()) { wait(0); } } else { while (isAlive()) {//判断当前的线程是否处于活动状态。什么是活动状态呢？活动状态就是线程已经启动且尚未终止 long delay = millis - now; if (delay &lt;= 0) { break; } wait(delay); now = System.currentTimeMillis() - base; } } }复制代码 ThreadLocal（后续讲解）","link":"/2021/05/06/Java%E5%B9%B6%E5%8F%91%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%B8%8A/"},{"title":"MySQL（三）文件和表结构","text":"本节主要描述了以下两个方面： 在MySQL数据库中存在了很多不同类型的文件 MySQL中的文件参数文件当MySQL实例启动时，数据库会先去读取一个配置参数文件，用于寻找数据库的各种文件的所在位置以及指定一些初始化的参数。 这些参数文件是以文本的方式来进行存储的。 参数我们可以通过常用的命令来进行查询： 1234567891011121314151617mysql&gt; show variables like 'innodb_buffer%';+-------------------------------------+----------------+| Variable_name | Value |+-------------------------------------+----------------+| innodb_buffer_pool_chunk_size | 134217728 || innodb_buffer_pool_dump_at_shutdown | ON || innodb_buffer_pool_dump_now | OFF || innodb_buffer_pool_dump_pct | 25 || innodb_buffer_pool_filename | ib_buffer_pool || innodb_buffer_pool_in_core_file | ON || innodb_buffer_pool_instances | 1 || innodb_buffer_pool_load_abort | OFF || innodb_buffer_pool_load_at_startup | ON || innodb_buffer_pool_load_now | OFF || innodb_buffer_pool_size | 134217728 |+-------------------------------------+----------------+11 rows in set (0.01 sec) 日志文件错误日志error log可以通过上述的命令找到 错误日志所在的位置。 1234567mysql&gt; show variables like 'log_error';+---------------+----------------------------------------+| Variable_name | Value |+---------------+----------------------------------------+| log_error | /usr/local/mysql/data/mysqld.local.err |+---------------+----------------------------------------+1 row in set (0.01 sec) 可以通过cat的命令查看一下这个日志： 12345678# shengbinbin @ 192 in ~ [22:15:11] C:1$ sudo cat /usr/local/mysql/data/mysqld.local.err2021-05-01T06:58:39.829973Z 0 [System] [MY-010116] [Server] /usr/local/mysql/bin/mysqld (mysqld 8.0.23) starting as process 89912021-05-01T06:58:39.832273Z 0 [Warning] [MY-010159] [Server] Setting lower_case_table_names=2 because file system for /usr/local/mysql/data/ is case insensitive2021-05-01T06:58:40.028554Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.2021-05-01T06:58:40.140717Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.2021-05-01T06:58:40.200394Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: /tmp/mysqlx.sock 错误日志对MySQL的启动、运行和关闭过程进行了记录。 慢查询日志slow query log可以帮助定位到可能存在问题的SQL语句，从而进行SQL语句层面的优化。 我们可以设置一个阈值，这样运行时间超过这个值的SQL语句都会被记录到慢查询日志文件中。这个阈值默认是10秒。 123456789101112131415mysql&gt; show variables like 'long_query_time';+-----------------+-----------+| Variable_name | Value |+-----------------+-----------+| long_query_time | 10.000000 |+-----------------+-----------+1 row in set (0.01 sec)mysql&gt; show variables like 'slow_query_log'; //慢查询日志默认是关闭的状态+----------------+-------+| Variable_name | Value |+----------------+-------+| slow_query_log | OFF |+----------------+-------+1 row in set (0.00 sec) 另外一个参数是log_queries_not_using_indexes，打开之后如果运行的SQL没有使用索引，则会记录下来。 1234567mysql&gt; show variables like 'log_queries_not_using_indexes';+-------------------------------+-------+| Variable_name | Value |+-------------------------------+-------+| log_queries_not_using_indexes | OFF |+-------------------------------+-------+1 row in set (0.00 sec) 查看slow_log这个表的结构： 123456789101112131415161718mysql&gt; show create table mysql.slow_log;| slow_log | CREATE TABLE `slow_log` ( `start_time` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6), `user_host` mediumtext NOT NULL, `query_time` time(6) NOT NULL, `lock_time` time(6) NOT NULL, `rows_sent` int NOT NULL, `rows_examined` int NOT NULL, `db` varchar(512) NOT NULL, `last_insert_id` int NOT NULL, `insert_id` int NOT NULL, `server_id` int unsigned NOT NULL, `sql_text` mediumblob NOT NULL, `thread_id` bigint unsigned NOT NULL) ENGINE=CSV DEFAULT CHARSET=utf8 COMMENT='Slow log' |1 row in set (0.00 sec) 查询日志log记录了所有对MySQL数据库的请求信息，无论这些请求是否得到了正确的执行。 二进制日志binlogbinary log 记录了对MySQL数据库执行更改的所有操作，不包含select和show等操作。 需要注意的是如果操作本身没有让数据库发生变化，该操作也会写入二进制日志中。 123456789101112131415161718192021mysql&gt; update user set username = 'binshow' where id = 1;Query OK, 0 rows affected (0.00 sec)Rows matched: 1 Changed: 0 Warnings: 0mysql&gt; show master status;+---------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+---------------+----------+--------------+------------------+-------------------+| binlog.000032 | 156 | | | |+---------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec)mysql&gt; show binlog events in 'binlog.000032';+---------------+-----+----------------+-----------+-------------+-----------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+---------------+-----+----------------+-----------+-------------+-----------------------------------+| binlog.000032 | 4 | Format_desc | 1 | 125 | Server ver: 8.0.23, Binlog ver: 4 || binlog.000032 | 125 | Previous_gtids | 1 | 156 | |+---------------+-----+----------------+-----------+-------------+-----------------------------------+2 rows in set (0.00 sec) binlog日志的作用如下： 恢复：用户可以通过binlog日志进行point-in-time的恢复。 复制：通过复制和执行二进制日志使别的MySQL数据库和当前的MySQL数据库进行实时同步。 审计：用户可以通过二进制日志中的信息来进行审计，判断是否有对数据库的注入共计。 表结构本章的核心点在于：数据是如何在表中组织和存放的。表是关于特定实体的数据集合。 索引组织表 在InnoDB中，表都是根据主键的顺序组织存放的，这种存储方式的表称为索引组织表。 每个表都会有一个主键Primary Key，如果创建这个表的时候没有显示的定义这个主键，那么就会按照一下的方式来选择or创建主键： 首先判断表中是否有非空的唯一索引，如果有，该列就是主键。（如果有多个，会选择建表的第一个非空唯一索引） 如果没有，INNODB存储引擎就会自动创建一个6字节大小的指针。 12345678910111213141516171819202122232425262728293031mysql&gt; create table z( -&gt; a int not null, -&gt; b int null, -&gt; c int not null, -&gt; d int not null, -&gt; unique key(b), -&gt; unique key(d), -&gt; unique key(c));Query OK, 0 rows affected (0.02 sec)mysql&gt; insert into z select 1,2,3,4;Query OK, 1 row affected (0.01 sec)Records: 1 Duplicates: 0 Warnings: 0mysql&gt; insert into z select 5,6,7,8;Query OK, 1 row affected (0.01 sec)Records: 1 Duplicates: 0 Warnings: 0mysql&gt; insert into z select 9,10,11,12;Query OK, 1 row affected (0.00 sec)Records: 1 Duplicates: 0 Warnings: 0mysql&gt; select a,b,c,d,_rowid from z;+---+------+----+----+--------+| a | b | c | d | _rowid |+---+------+----+----+--------+| 1 | 2 | 3 | 4 | 4 || 5 | 6 | 7 | 8 | 8 || 9 | 10 | 11 | 12 | 12 |+---+------+----+----+--------+3 rows in set (0.00 sec) _rowid 可以显示出表的主键，也就是说在这里d被视为主键了。（因为b列是允许null值的，所以被忽略了） ps：_rowid 只能用于查看单个列是主键的情况，如果就多个列组成的主键就不行了。","link":"/2021/09/06/MySQL%EF%BC%88%E4%B8%89%EF%BC%89%E6%96%87%E4%BB%B6%E5%92%8C%E8%A1%A8%E7%BB%93%E6%9E%84/"},{"title":"MapStruct使用详解","text":"MapStruct的使用需求： 随着模块的划分越来越细，尤其是现在的分布式系统中，应用与应用之间，还有单独的应用细分模块之后，DO（domain） 一般不会让外部依赖，这时候需要在提供对外接口的模块里放 DTO 用于对象传输，也即是 DO 对象对内，DTO对象对外，DTO 可以根据业务需要变更，并不需要映射 DO 的全部属性。 是什么MapStruct 就是一个属性映射工具，只需要定义一个 Mapper 接口，MapStruct 就会自动实现这个映射接口，避免了复杂繁琐的映射实现。 MapStruct官网地址： http://mapstruct.org/ 那么为啥不用 BeanUtils 的 copyProperties 方法呢？不也照样可以实现属性的映射么？BeanUtils 只能同属性映射，或者在属性相同的情况下，允许被映射的对象属性少；但当遇到被映射的属性数据类型被修改或者被映射的字段名被修改，则会导致映射失败。 怎么用需求描述： 有这么个场景：从数据库查询出来了一个user对象（包含id，用户名，密码，手机号，邮箱，角色这些字段）和一个对应的角色对象role（包含id，角色名，角色描述这些字段），现在在controller需要用到user对象的id，用户名，和角色对象的角色名三个属性。 一种方式是直接把两个对象传递到controller层，但是这样会多出很多没用的属性。 更通用的方式是需要用到的属性封装成一个类(DTO)，通过传输这个类的实例来完成数据传输。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * @author shengbinbin * @description: User DO 实体类 * @date 2021/8/1110:29 下午 */@Data@Builder@NoArgsConstructor@AllArgsConstructorpublic class User { private Long id; private String username; private String password; private String phoneNum; private String email; private Role role;}/** * @author shengbinbin * @description: Role实体类 * @date 2021/8/1110:36 下午 */@Data@Builder@NoArgsConstructor@AllArgsConstructorpublic class Role { private Long id; private String roleName; private String description;}/** * @author shengbinbin * @description: UserRoleDTO 传输对象实体类 * @date 2021/8/1110:36 下午 */@Data@Builder@NoArgsConstructor@AllArgsConstructorpublic class UserRoleDTO { /**用户id*/ private Long userId; /**用户名*/ private String name; /**角色名*/ private String roleName;} 模拟转换类： 1234567891011121314151617181920212223242526public class ConvertTest { User user = null; /** * 模拟从数据库中查出user对象 */ @Before public void before() { Role role = new Role(2L, &quot;binshow&quot;, &quot;超级管理员&quot;); user = new User(1L, &quot;zkd&quot;, &quot;12345&quot;, &quot;8888888&quot;, &quot;123456@qq.com&quot;, role); } /** * 模拟把user对象转换成UserRoleDto对象 */ @Test public void test1() { UserRoleDTO userRoleDto = new UserRoleDTO(); userRoleDto.setUserId(user.getId()); userRoleDto.setName(user.getUsername()); userRoleDto.setRoleName(user.getRole().getRoleName()); System.out.println(userRoleDto); }} 使用mapstruct之后： 引入依赖： 1234567891011121314151617181920212223242526&lt;dependencies&gt; &lt;!--记录小坑：lombok 1.18.16 之后必须使用 lombok-mapstruct-binding 插件--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;!-- jdk8以下就使用mapstruct --&gt; &lt;artifactId&gt;mapstruct-jdk8&lt;/artifactId&gt; &lt;version&gt;1.2.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct-processor&lt;/artifactId&gt; &lt;version&gt;1.2.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.22&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 编写一个Mapper接口（测试类也可）：定义映射规则 1234567891011121314151617181920212223242526272829303132/** * @author shengbinbin * @description: MapStruct的映射工具类 * @date 2021/8/1110:44 下午 * * @Mapper 定义这是一个MapStruct对象属性转换接口，在这个类里面规定转换规则 * * 在项目构建时，会自动生成改接口的实现类，这个实现类将实现对象属性值复制 * */@Mapperpublic interface UserRoleMapper { /** * 获取该类自动生成的实现类的实例 * 接口中的属性都是 public static final 的 方法都是public abstract的 */ UserRoleMapper INSTANCES = Mappers.getMapper(UserRoleMapper.class); /** * 这个方法就是用于实现对象属性复制的方法 * * @Mapping 用来定义属性复制规则 source 指定源对象属性 target指定目标对象属性 * * @param user 这个参数就是源对象，也就是需要被复制的对象 * @return 返回的是目标对象，就是最终的结果对象 */ @Mappings({ @Mapping(source = &quot;id&quot;, target = &quot;userId&quot;), @Mapping(source = &quot;username&quot;, target = &quot;name&quot;), @Mapping(source = &quot;role.roleName&quot;, target = &quot;roleName&quot;) }) UserRoleDTO toUserRoleDto(User user);} Build一下项目，就会发现target编译之后的Class有这个接口的实现类： 1234567891011121314151617181920212223242526272829303132333435// MapStruct 接口自动生成public class UserRoleMapperImpl implements UserRoleMapper { public UserRoleMapperImpl() { } public UserRoleDTO toUserRoleDto(User user) { if (user == null) { return null; } else { UserRoleDTO userRoleDTO = new UserRoleDTO(); userRoleDTO.setName(user.getUsername()); String roleName = this.userRoleRoleName(user); if (roleName != null) { userRoleDTO.setRoleName(roleName); } userRoleDTO.setUserId(user.getId()); return userRoleDTO; } } private String userRoleRoleName(User user) { if (user == null) { return null; } else { Role role = user.getRole(); if (role == null) { return null; } else { String roleName = role.getRoleName(); return roleName == null ? null : roleName; } } }} 测试类中测试： 12345678910111213141516171819202122232425262728/** * @author shengbinbin * @description: 测试将 DO 实体类转换成 DTO 传输对象 * @date 2021/8/1110:38 下午 */public class ConvertTest { User user = null; /** * 模拟从数据库中查出user对象 */ @Before public void before() { Role role = new Role(2L, &quot;manager&quot;, &quot;超级管理员&quot;); user = new User(1L, &quot;zkd&quot;, &quot;12345&quot;, &quot;8888888&quot;, &quot;123456@qq.com&quot;, role); } /** * 模拟把user对象转换成UserRoleDto对象 */ @Test public void test1() { UserRoleDTO userRoleDTO = UserRoleMapper.INSTANCES.toUserRoleDto(user); System.out.println(&quot;userRoleDTO = &quot; + userRoleDTO); }} 可以在接口和测试类中提供默认方法来进行其他的操作。 可以绑定多个对象的属性到模板对象中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * @author shengbinbin * @description: MapStruct的映射工具类 * @date 2021/8/1110:44 下午 * * @Mapper 定义这是一个MapStruct对象属性转换接口，在这个类里面规定转换规则 * * 在项目构建时，会自动生成改接口的实现类，这个实现类将实现对象属性值复制 * */@Mapperpublic interface UserRoleMapper { /** * 获取该类自动生成的实现类的实例 * 接口中的属性都是 public static final 的 方法都是public abstract的 */ UserRoleMapper INSTANCES = Mappers.getMapper(UserRoleMapper.class); /** * 这个方法就是用于实现对象属性复制的方法 * * @Mapping 用来定义属性复制规则 source 指定源对象属性 target指定目标对象属性 * * @param user 这个参数就是源对象，也就是需要被复制的对象 * @return 返回的是目标对象，就是最终的结果对象 */ @Mappings({ @Mapping(source = &quot;id&quot;, target = &quot;userId&quot;), @Mapping(source = &quot;username&quot;, target = &quot;name&quot;), @Mapping(source = &quot;role.roleName&quot;, target = &quot;roleName&quot;) }) UserRoleDTO toUserRoleDto(User user); /** * 多个参数中的值绑定 * @param user 源1 * @param role 源2 * @return 从源1、2中提取出的结果 */ @Mappings({ @Mapping(source = &quot;user.id&quot;, target = &quot;userId&quot;), // 把user中的id绑定到目标对象的userId属性中 @Mapping(source = &quot;user.username&quot;, target = &quot;name&quot;), // 把user中的username绑定到目标对象的name属性中 @Mapping(source = &quot;role.roleName&quot;, target = &quot;roleName&quot;) // 把role对象的roleName属性值绑定到目标对象的roleName中 }) UserRoleDTO toUserRoleDto(User user, Role role); /** * 直接使用参数作为值 * @param user * @param myRoleName * @return */ @Mappings({ @Mapping(source = &quot;user.id&quot;, target = &quot;userId&quot;), // 把user中的id绑定到目标对象的userId属性中 @Mapping(source = &quot;user.username&quot;, target = &quot;name&quot;), // 把user中的username绑定到目标对象的name属性中 @Mapping(source = &quot;myRoleName&quot;, target = &quot;roleName&quot;) // 把role对象的roleName属性值绑定到目标对象的roleName中 }) UserRoleDTO useParameter(User user, String myRoleName);} 生成的实现类： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class UserRoleMapperImpl implements UserRoleMapper { public UserRoleMapperImpl() { } public UserRoleDTO toUserRoleDto(User user) { if (user == null) { return null; } else { UserRoleDTO userRoleDTO = new UserRoleDTO(); userRoleDTO.setName(user.getUsername()); String roleName = this.userRoleRoleName(user); if (roleName != null) { userRoleDTO.setRoleName(roleName); } userRoleDTO.setUserId(user.getId()); return userRoleDTO; } } public UserRoleDTO toUserRoleDto(User user, Role role) { if (user == null &amp;&amp; role == null) { return null; } else { UserRoleDTO userRoleDTO = new UserRoleDTO(); if (user != null) { userRoleDTO.setName(user.getUsername()); userRoleDTO.setUserId(user.getId()); } if (role != null) { userRoleDTO.setRoleName(role.getRoleName()); } return userRoleDTO; } } public UserRoleDTO useParameter(User user, String myRoleName) { if (user == null &amp;&amp; myRoleName == null) { return null; } else { UserRoleDTO userRoleDTO = new UserRoleDTO(); if (user != null) { userRoleDTO.setName(user.getUsername()); userRoleDTO.setUserId(user.getId()); } if (myRoleName != null) { userRoleDTO.setRoleName(myRoleName); } return userRoleDTO; } } private String userRoleRoleName(User user) { if (user == null) { return null; } else { Role role = user.getRole(); if (role == null) { return null; } else { String roleName = role.getRoleName(); return roleName == null ? null : roleName; } } }} 注解关键词123456789101112@Mapper 只有在接口加上这个注解， MapStruct 才会去实现该接口 @Mapper 里有个 componentModel 属性，主要是指定实现类的类型，一般用到两个 default：默认，可以通过 Mappers.getMapper(Class) 方式获取实例对象 spring：在接口的实现类上自动添加注解 @Component，可通过 @Autowired 方式注入@Mapping：属性映射，若源对象属性与目标对象名字一致，会自动映射对应属性 source：源属性 target：目标属性 dateFormat：String 到 Date 日期之间相互转换，通过 SimpleDateFormat，该值为 SimpleDateFormat 的日期格式 ignore: 忽略这个字段@Mappings：配置多个@Mapping@MappingTarget 用于更新已有对象@InheritConfiguration 用于继承配置 原理解析相比于反射获取对象进行拷贝的方法，这种更贴近于原生 get/set 方法的框架显得更为高效。这个文件是通过在 mapper 中的注解，使用生成映射器的注解处理器从而自动生成了这段代码","link":"/2021/08/11/MapStruct%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/"},{"title":"MySQL（一）语法和数据类型","text":"DDL，英文叫做 Data Definition Language，也就是数据定义语言，它用来定义我们的数据库对象，包括数据库、数据表和列。通过使用 DDL，我们可以创建，删除和修改数据库和表结构。 DML，英文叫做 Data Manipulation Language，数据操作语言，我们用它操作和数据库相关的记录，比如增加、删除、修改数据表中的记录。 DCL，英文叫做 Data Control Language，数据控制语言，我们用它来定义访问权限和安全级别。 DQL，英文叫做 Data Query Language，数据查询语言，我们用它查询想要的记录，它是 SQL 语言的重中之重。 DDL数据定义语言123456789101112131415shengbinbin@192 ~ % mysql -uroot -p //登陆数据库Enter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 8Server version: 8.0.23 MySQL Community Server - GPLCopyright (c) 2000, 2021, Oracle and/or its affiliates.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; 12345678910111213141516171819mysql&gt; CREATE DATABASE test; //创建数据库Query OK, 1 row affected (0.00 sec)mysql&gt; CREATE DATABASE test;ERROR 1007 (HY000): Can't create database 'test'; database existsmysql&gt; show databases; //查看所有数据库+--------------------+| Database |+--------------------+| binshow || information_schema | // 存储了系统中的一些数据库对象信息，比如用户表信息，列信息，权限信息等等| mybatis || mysql | // mysql 存储了系统的用户权限信息| performance_schema || sys || test |+--------------------+7 rows in set (0.02 sec) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253mysql&gt; use test //选择数据库Database changedmysql&gt; show tables; //展示数据库中的表Empty set (0.00 sec)mysql&gt; use mysql;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+------------------------------------------------------+| Tables_in_mysql |+------------------------------------------------------+| columns_priv || component || db || default_roles || engine_cost || func || general_log || global_grants || gtid_executed || help_category || help_keyword || help_relation || help_topic || innodb_index_stats || innodb_table_stats || password_history || plugin || procs_priv || proxies_priv || replication_asynchronous_connection_failover || replication_asynchronous_connection_failover_managed || role_edges || server_cost || servers || slave_master_info || slave_relay_log_info || slave_worker_info || slow_log || tables_priv || time_zone || time_zone_leap_second || time_zone_name || time_zone_transition || time_zone_transition_type || user |+------------------------------------------------------+35 rows in set (0.01 sec)mysql&gt; 1234567891011121314151617mysql&gt; drop database test; //删除数据库Query OK, 0 rows affected (0.01 sec)mysql&gt; show databases;+--------------------+| Database |+--------------------+| binshow || information_schema || mybatis || mysql || performance_schema || sys |+--------------------+6 rows in set (0.01 sec)mysql&gt; 12345678910111213141516mysql&gt; create table emp(ename varchar(10),hiredate date,sal decimal(10,2),deptno int(2)); //创建表Query OK, 0 rows affected, 1 warning (0.01 sec)mysql&gt; desc emp; //查看表的结构+----------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------------+------+-----+---------+-------+| ename | varchar(10) | YES | | NULL | || hiredate | date | YES | | NULL | || sal | decimal(10,2) | YES | | NULL | || deptno | int | YES | | NULL | |+----------+---------------+------+-----+---------+-------+4 rows in set (0.01 sec)mysql&gt; drop table emp; //删除表Query OK, 0 rows affected (0.01 sec) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788mysql&gt; desc emp;+----------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------------+------+-----+---------+-------+| ename | varchar(10) | YES | | NULL | || hiredate | date | YES | | NULL | || sal | decimal(10,2) | YES | | NULL | || deptno | int | YES | | NULL | |+----------+---------------+------+-----+---------+-------+4 rows in set (0.00 sec)mysql&gt; alter table emp modify ename varchar(20); //修改表的结构类型Query OK, 0 rows affected (0.01 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc emp;+----------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------------+------+-----+---------+-------+| ename | varchar(20) | YES | | NULL | || hiredate | date | YES | | NULL | || sal | decimal(10,2) | YES | | NULL | || deptno | int | YES | | NULL | |+----------+---------------+------+-----+---------+-------+4 rows in set (0.01 sec)mysql&gt; alter table emp add column age int(3); //增加字段Query OK, 0 rows affected, 1 warning (0.01 sec)Records: 0 Duplicates: 0 Warnings: 1mysql&gt; desc emp;+----------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------------+------+-----+---------+-------+| ename | varchar(20) | YES | | NULL | || hiredate | date | YES | | NULL | || sal | decimal(10,2) | YES | | NULL | || deptno | int | YES | | NULL | || age | int | YES | | NULL | |+----------+---------------+------+-----+---------+-------+5 rows in set (0.00 sec)mysql&gt; alter table emp drop column age; //删除字段Query OK, 0 rows affected (0.01 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc emp;+----------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------------+------+-----+---------+-------+| ename | varchar(20) | YES | | NULL | || hiredate | date | YES | | NULL | || sal | decimal(10,2) | YES | | NULL | || deptno | int | YES | | NULL | |+----------+---------------+------+-----+---------+-------+4 rows in set (0.01 sec)mysql&gt; alter table emp add column age int(3);Query OK, 0 rows affected, 1 warning (0.01 sec)Records: 0 Duplicates: 0 Warnings: 1mysql&gt; alter table emp change age age1 int(4); //字段改名Query OK, 0 rows affected, 1 warning (0.01 sec)Records: 0 Duplicates: 0 Warnings: 1mysql&gt; desc emp;+----------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------------+------+-----+---------+-------+| ename | varchar(20) | YES | | NULL | || hiredate | date | YES | | NULL | || sal | decimal(10,2) | YES | | NULL | || deptno | int | YES | | NULL | || age1 | int | YES | | NULL | |+----------+---------------+------+-----+---------+-------+5 rows in set (0.00 sec)mysql&gt; alter table emp rename emp1; //修改表名称Query OK, 0 rows affected (0.01 sec)mysql&gt; show tables;+-------------------+| Tables_in_binshow |+-------------------+| emp1 |+-------------------+1 row in set (0.00 sec) DML数据操控语言插入数据1234567891011121314151617181920212223242526272829303132mysql&gt; insert into emp(ename , hiredate , sal ,deptno) values('binshow','2020-02-02','2000',2); //插入语句Query OK, 1 row affected (0.01 sec)mysql&gt; insert into emp(ename,sal) values('wb','1000'); //部分列显示插入数据Query OK, 1 row affected (0.00 sec)mysql&gt; select * from emp;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 2000.00 | 2 | NULL || wb | NULL | 1000.00 | NULL | NULL |+---------+------------+---------+--------+------+2 rows in set (0.00 sec)mysql&gt; insert into emp values('zkd','2020-02-04','3000',3,4); //不加要插入的列名称，但是要一一对应Query OK, 1 row affected (0.00 sec)mysql&gt; insert into emp values('zkd','2020-02-04','3000',3,4),('aaa','2020-01-04','4000',3,4); //直接插入多个数据Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; select * from emp;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 2000.00 | 2 | NULL || wb | NULL | 1000.00 | NULL | NULL || zkd | 2020-02-04 | 3000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 || aaa | 2020-01-04 | 4000.00 | 3 | 4 |+---------+------------+---------+--------+------+5 rows in set (0.00 sec)mysql&gt; 更新数据1234567891011121314151617181920212223242526272829mysql&gt; select * from emp;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 2000.00 | 2 | NULL || wb | NULL | 1000.00 | NULL | NULL || zkd | 2020-02-04 | 3000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 || aaa | 2020-01-04 | 4000.00 | 3 | 4 |+---------+------------+---------+--------+------+5 rows in set (0.00 sec)mysql&gt; update emp set sal = 5000 where ename = 'binshow'; //更新语句Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select * from emp;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 5000.00 | 2 | NULL || wb | NULL | 1000.00 | NULL | NULL || zkd | 2020-02-04 | 3000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 || aaa | 2020-01-04 | 4000.00 | 3 | 4 |+---------+------------+---------+--------+------+5 rows in set (0.00 sec)mysql&gt; 123456789101112131415161718192021222324mysql&gt; create table dept(deptno int, deptname varchar(10));Query OK, 0 rows affected (0.01 sec)mysql&gt; desc dept;+----------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+-------------+------+-----+---------+-------+| deptno | int | YES | | NULL | || deptname | varchar(10) | YES | | NULL | |+----------+-------------+------+-----+---------+-------+2 rows in set (0.00 sec)mysql&gt; insert into dept values(1,'tech'),(2,'sale'),(5,'fin');Query OK, 3 rows affected (0.01 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; select * from dept;+--------+----------+| deptno | deptname |+--------+----------+| 1 | tech || 2 | sale || 5 | fin |+--------+----------+3 rows in set (0.00 sec)mysql&gt; 删除数据123456789101112131415mysql&gt; delete from emp where ename = 'wb';Query OK, 1 row affected (0.00 sec)mysql&gt; select * from emp;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 5000.00 | 2 | NULL || zkd | 2020-02-04 | 3000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 || aaa | 2020-01-04 | 4000.00 | 3 | 4 |+---------+------------+---------+--------+------+4 rows in set (0.00 sec)mysql&gt; 查询数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859mysql&gt; select distinct deptno from emp; //查询不重复的数据+--------+ | deptno |+--------+| 2 || 3 |+--------+2 rows in set (0.00 sec)mysql&gt; select * from emp where sal = '5000'; //条件查询+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 5000.00 | 2 | NULL |+---------+------------+---------+--------+------+1 row in set (0.00 sec)mysql&gt; select * from emp order by sal; +---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| zkd | 2020-02-04 | 3000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 || aaa | 2020-01-04 | 4000.00 | 3 | 4 || binshow | 2020-02-02 | 5000.00 | 2 | NULL |+---------+------------+---------+--------+------+4 rows in set (0.00 sec)mysql&gt; select * from emp order by deptno,sal desc;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 5000.00 | 2 | NULL || aaa | 2020-01-04 | 4000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 |+---------+------------+---------+--------+------+4 rows in set (0.00 sec)mysql&gt; select * from emp order by deptno,sal desc limit 2;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 5000.00 | 2 | NULL || aaa | 2020-01-04 | 4000.00 | 3 | 4 |+---------+------------+---------+--------+------+2 rows in set (0.00 sec)mysql&gt; select * from emp order by deptno,sal desc limit 1,2;+-------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+-------+------------+---------+--------+------+| aaa | 2020-01-04 | 4000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 |+-------+------------+---------+--------+------+2 rows in set (0.00 sec)mysql&gt; 聚合操作的顺序问题1select * from table a where id = 1 group by age having xxx; 一条完整的 SELECT 语句内部的执行顺序是这样的： FROM 子句组装数据（包括通过 ON 进行连接）； WHERE 子句进行条件筛选； GROUP BY 分组 ； 使用聚集函数进行计算； HAVING 筛选分组； 计算所有的表达式； SELECT 的字段； ORDER BY 排序； LIMIT 筛选。 having 和 where的区别：having是对聚合后的结果进行条件的过滤，而where是聚合前就对记录过滤 12345678910111213141516171819202122232425262728293031323334mysql&gt; select count(1) from emp;+----------+| count(1) |+----------+| 4 |+----------+1 row in set (0.01 sec)mysql&gt; select deptno,count(1) from emp group by deptno;+--------+----------+| deptno | count(1) |+--------+----------+| 2 | 2 || 3 | 1 || 1 | 1 |+--------+----------+3 rows in set (0.00 sec)mysql&gt; select deptno,count(1) from emp group by deptno having count(1)&gt;1; //统计人数大于1的部门+--------+----------+| deptno | count(1) |+--------+----------+| 2 | 2 |+--------+----------+1 row in set (0.00 sec)mysql&gt; select sum(sal),max(sal),min(sal) from emp; //统计薪水总和，最大薪水，最小薪水+----------+----------+----------+| sum(sal) | max(sal) | min(sal) |+----------+----------+----------+| 16000.00 | 6000.00 | 1000.00 |+----------+----------+----------+1 row in set (0.00 sec)mysql&gt; 表连接 内连接和外连接的区别：内连接仅仅选出两个表中相互匹配的记录。 左连接和右连接的区别： 左连接指包含所有左表中的记录甚至是右边表中没有和她匹配的记录。 12345678910111213141516171819202122232425262728293031323334mysql&gt; select * from emp;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 5000.00 | 2 | NULL || aaa | 2020-01-04 | 4000.00 | 3 | 4 || sbb | 2018-02-01 | 6000.00 | 1 | 18 || bbb | 2018-04-11 | 1000.00 | 2 | 36 |+---------+------------+---------+--------+------+4 rows in set (0.00 sec)mysql&gt; select * from dept;+--------+----------+| deptno | deptname |+--------+----------+| 1 | tech || 2 | sale || 5 | fin || 3 | hr |+--------+----------+4 rows in set (0.00 sec)mysql&gt; select ename ,deptname from emp , dept where emp.deptno = dept.deptno;+---------+----------+| ename | deptname |+---------+----------+| sbb | tech || bbb | sale || binshow | sale || aaa | hr |+---------+----------+4 rows in set (0.00 sec)mysql&gt; 12345678910111213141516171819202122232425262728mysql&gt; insert into emp values('ccc','2010-02-03','10000',4,45);Query OK, 1 row affected (0.00 sec) //查询emp中所有用户名和所在部门名称mysql&gt; select ename , deptname from emp left join dept on emp.deptno = dept.deptno;+---------+----------+| ename | deptname |+---------+----------+| binshow | sale || aaa | hr || sbb | tech || bbb | sale || ccc | NULL |+---------+----------+5 rows in set (0.01 sec)mysql&gt; select ename , deptname from dept right join emp on emp.deptno = dept.deptno; //和上面相同+---------+----------+| ename | deptname |+---------+----------+| binshow | sale || aaa | hr || sbb | tech || bbb | sale || ccc | NULL |+---------+----------+5 rows in set (0.00 sec)mysql&gt; 子查询123456789101112mysql&gt; select * from emp where deptno in(select deptno from dept);+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 5000.00 | 2 | NULL || aaa | 2020-01-04 | 4000.00 | 3 | 4 || sbb | 2018-02-01 | 6000.00 | 1 | 18 || bbb | 2018-04-11 | 1000.00 | 2 | 36 |+---------+------------+---------+--------+------+4 rows in set (0.00 sec)mysql&gt; Union联合123456789101112131415161718192021222324252627282930313233mysql&gt; select deptno from emp -&gt; union -&gt; select deptno from dept; //去重了+--------+| deptno |+--------+| 2 || 3 || 1 || 4 || 5 |+--------+5 rows in set (0.00 sec)mysql&gt; select deptno from emp -&gt; union all -&gt; select deptno from dept;+--------+| deptno |+--------+| 2 || 3 || 1 || 2 || 4 || 1 || 2 || 5 || 3 |+--------+9 rows in set (0.00 sec)mysql&gt; DCL数据控制语言暂无 数据类型/运算符/常用函数待补充","link":"/2021/05/11/MySQL%EF%BC%88%E4%B8%80%EF%BC%89%E8%AF%AD%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"},{"title":"MySQL（二）存储引擎","text":"本篇讲述了MySQL的逻辑分层和存储引擎InnoDB MySQL的逻辑分层插件式的存储引擎架构将查询处理和其它的系统任务以及数据的存储提取相分离。 这种架构可以根据业务的需求和实际需要选择合适的存储引擎。 连接层：最上层是一些客户端和连接服务。主要完成一些类似于连接处理、授权认证、及相关的安全方案。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。 服务层：第二层服务层，主要完成大部分的核心服务功能， 包括查询解析、分析、优化、缓存、以及所有的内置函数，所有跨存储引擎的功能也都在这一层实现，包括触发器、存储过程、视图等 引擎层：第三层存储引擎层，存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API与存储引擎进行通信。不同的存储引擎具有的功能不同，这样我们可以根据自己的实际需要进行选取 存储层：第四层为数据存储层，主要是将数据存储在运行于该设备的文件系统之上，并完成与存储引擎的交互 一条SQL语句的执行流程 客户端请求 连接器（验证用户身份，给予权限） 查询缓存（存在缓存则直接返回，不存在则执行后续操作，mysql8.0之后取消了缓存） 分析器（对SQL进行词法分析和语法分析操作 优化器（主要对执行的sql优化选择最优的执行方案方法） 执行器（执行时会先看用户是否有执行权限，有才去使用这个引擎提供的接口） 去引擎层获取数据返回（如果开启查询缓存则会缓存查询结果） InnoDB存储引擎InnoDB 现在是 MySQL 默认的存储引擎，支持事务、行锁设计、MVCC、外键等特点。 InnoDB体系结构InnoDB存储引擎中有很多内存块，可以认为这些内存块组成了一个大的内存池，负责如下工作： 维护所有进程/线程需要访问的多个内部数据结构。 缓存磁盘上的数据，方便快速读取。对磁盘文件修改之前也会在这里做缓存。 重做日志redo log 缓冲 ​ 后台线程的主要作用有两个： 是负责刷新内存池中的数据，保证缓冲池中的内存缓存是最近的数据。 将已经修改后的数据文件刷新到磁盘中去（持久化），同时保证在发生异常情况下可以恢复到正常运行状态。 后台线程的分类INNODB存储引擎的模型是多线程的，不同线程负责不同的任务： Master线程一个非常核心的后台线程，主要负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性。包括脏页的刷新、合并插入缓存、UNDO页的回收等等 IO线程使用大量的AIO来处理写的IO请求，提高数据库的性能。IO线程的作用就是负责这些IO请求的回调，共有四种IO线程： insert buffer thread log thread read thread ， 4个 write thread，4个 12345678910111213141516171819mysql&gt; show engine innodb status; //可以查看InnoDB状态FILE I/O--------I/O thread 0 state: waiting for i/o request (insert buffer thread)I/O thread 1 state: waiting for i/o request (log thread)I/O thread 2 state: waiting for i/o request (read thread)I/O thread 3 state: waiting for i/o request (read thread)I/O thread 4 state: waiting for i/o request (read thread)I/O thread 5 state: waiting for i/o request (read thread)I/O thread 6 state: waiting for i/o request (write thread)I/O thread 7 state: waiting for i/o request (write thread)I/O thread 8 state: waiting for i/o request (write thread)I/O thread 9 state: waiting for i/o request (write thread)Pending normal aio reads: [0, 0, 0, 0] , aio writes: [0, 0, 0, 0] , ibuf aio reads:, log i/o's:, sync i/o's:Pending flushes (fsync) log: 0; buffer pool: 0900 OS file reads, 1594 OS file writes, 930 OS fsyncs0.00 reads/s, 0 avg bytes/read, 0.00 writes/s, 0.00 fsyncs/s Purge（清除） Thread事务被提交之后，其使用的undolog可能不再需要了，因此purge thread会回收已经使用并分配的undo页,默认有4个purgeThread。 123456mysql&gt; show variables like 'innodb_purge_threads'\\G*************************** 1. row ***************************Variable_name: innodb_purge_threads Value: 41 row in set (0.02 sec) Page Cleaner Thread将脏页中的刷新操作放入到单独的线程中来完成，减轻Master Thread的工作对用户查询线程的阻塞。 内存缓冲池缓冲池简单来说就是一块内存区域，通过内存的速度来弥补CPU和磁盘之间速度的差异。 在数据库中读取页的操作，首先将从磁盘读到的页存放在缓冲池中，下次再读相同的页时，首先判断该页是否在缓冲池中，如果在则命中，否则从磁盘中进行读取。 在数据库中修改页的操作，首先修改在缓冲池中的页，然后再以一定的频率刷新到磁盘上（**通过一种checkpoint到机制触发，并不是每次修改都会刷新）。 1234567891011121314mysql&gt; select version();+-----------+| version() |+-----------+| 8.0.23 |+-----------+1 row in set (0.00 sec)mysql&gt; show variables like 'innodb_buffer_pool_size';+-------------------------+-----------+| Variable_name | Value |+-------------------------+-----------+| innodb_buffer_pool_size | 134217728 |+-------------------------+-----------+1 row in set (0.01 sec) 缓冲池中存放着各种类型的页（其中数据页和索引页占了大部分）： 可以允许多个缓冲池实例，每个页根据哈希值平均分配到不同的缓冲池实例中，这样的话可以减少数据库内部的资源竞争，增加并发处理能力。 12345mysql&gt; show variables like 'innodb_buffer_pool_instances'\\G*************************** 1. row ***************************Variable_name: innodb_buffer_pool_instances Value: 11 row in set (0.01 sec) LRU列表、Free列表、Flush列表缓冲池中这么多页是如何管理的呢？如果内存满了该如何处理呢？其中页的大小默认为16KB。 这么多页使用优化之后的LRU算法（加入了midPoint位置）来进行管理的： 新读取的页并不是直接放到LRU列表的首部，而是放到midPoint的位置。默认配置下这个位置在LRU列表的5/8处。将midPoint之后的列表称为old列表，之前的列表称为new列表。 1234567mysql&gt; show variables like 'innodb_old_blocks_pct'\\G*************************** 1. row ***************************Variable_name: innodb_old_blocks_pct Value: 371 row in set (0.01 sec)# 37表示新读取的页插入到LRU列表尾部的37%的位置。 为什么要加入midPoint技术呢？为什么不直接放到LRU列表的首部呢？ 因为如果直接将读取的页放到LRU列表的首部的话，某些SQL的操作可能会使得缓冲池中的页被刷新出来，影响缓冲池的效率。这些操作一般为索引或数据的扫描操作，需要访问表中的许多页，这些页通常来说仅仅在这次查询中用到。如果直接放到LRU列表的首页的话，可能会将真正的热点数据从LRU中移除。 为了解决这个问题，还用了另外一个参数来管理LRU列表： 12345mysql&gt; show variables like 'innodb_old_blocks_time'\\G*************************** 1. row ***************************Variable_name: innodb_old_blocks_time Value: 10001 row in set (0.01 sec) innodb_old_blocks_time 表示页读取到mid位置之后需要等待多久才会翻入到LRU列表的热端。 数据库刚启动时，LRU列表是空的，此时页都存放在free列表中。当需要从缓冲池中分页时，会先看free列表中查找是否有可用的空闲页，如果由就将该页从free列表中删除，放入到LRU列表中。否则就根据LRU淘汰末尾的页，将该内存空间分配给新的页。 123456789101112131415161718192021mysql&gt; show engine innodb status;----------------------BUFFER POOL AND MEMORY----------------------Total large memory allocated 136970240Dictionary memory allocated 500792Buffer pool size 8191 //8191个页，每个页16KB = 128GFree buffers 7079 // free列表中的页数Database pages 1107 // LRU列表中的页数Old database pages 423Modified db pages 0 //Flush 脏页列表的页数，FLUSH列表即为脏页列表。Pending reads 0Pending writes: LRU 0, flush list 0, single page 0Pages made young 1, not young 0 //Pages made young 表示LRU列表中页移动到前端的次数0.00 youngs/s, 0.00 non-youngs/s //表示每秒上面两个操作的次数Pages read 899, created 208, written 9350.00 reads/s, 0.00 creates/s, 0.00 writes/sNo buffer pool page gets since the last printoutPages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/sLRU len: 1107, unzip_LRU len: 0 //可以压缩页，unzip_LRU列表管理着非16KB的页I/O sum[0]:cur[0], unzip sum[0]:cur[0] LRU列表中的页被修改之后，称该页为脏页（缓冲池中的页和磁盘上的页出现了数据不一致）。 重做日志缓冲InnoDB 存储引擎的内存区域除了有缓冲池外，还有重做日志缓冲(redo log buffer)。InnoDB 存储引擎首先将重做日志信息先放入到这个缓冲区，然后按一定频率将其刷新到重做日志文件。重做日志缓冲一般不需要设置得很大，因为一般情况下每一秒钟会将重做日志缓冲刷新到日志文件，因此用户只需要保证每秒产生的事务量在这个缓冲大小之内即可。该值可由配置参数innodb_ log_ buffer_ size控制，默认为8MB: 123456789mysql&gt; show variables like 'innodb_log_buffer_size';+------------------------+----------+| Variable_name | Value |+------------------------+----------+| innodb_log_buffer_size | 16777216 |+------------------------+----------+1 row in set (0.00 sec)mysql&gt; 重做日志在下列三种情况下会将重做日志缓冲中的内容刷新到外部磁盘的重做日志文件中: MasterThread每一秒将重做日志缓冲刷新到重做日志文件; 每个事务提交时会将重做日志缓冲刷新到重做日志文件; 当重做日志缓冲池剩余空间小于1/2时，重做日志缓冲刷新到重做日志文件。 Checkpoint技术Checkpoint技术就是将缓存池中脏页在某个时间点刷回到磁盘的操作 一个DML语句，进行数据update或delete 操作时，此时改变了缓冲池页中的记录，此时因为缓冲池页的数据比磁盘的新，此时的页就叫做脏页。 不管怎样，总会后的内存页数据需要刷回到磁盘里，这里就涉及几个问题： 若每次一个页发生变化，就将新页的版本刷新到磁盘，那么这个开销是非常大的 若热点数据集中在某几个页中，那么数据库的性能将变得非常差 如果在从缓冲池将页的新版本刷新到磁盘时发生了宕机，那么数据就不能恢复了 — Write Ahead Log Write Ahead Log（预写式日志）WAL策略解决了刷新页数据到磁盘时发生宕机而导致数据丢失的问题，它是关系数据库系统中用于提供原子性和持久性（ACID 属性中的两个）的一系列技术。 WAL策略核心点就是redo log，每当有事务提交时，先写入 redo log（重做日志），再修改缓冲池数据页，这样当发生掉电之类的情况时系统可以在重启后继续操作。 按理说有了WAL策略，我们就可以高枕无忧了。但其问题点又出现在redo log上面： redo log 不可能是无限大的，不能没完没了的存储我们的数据等待一起刷新到磁盘 在数据库怠机恢复时，如果redo log 太大的话恢复的代价也是非常大的 所以为了解决脏页的刷新性能，脏页应该在什么时间、什么情况下进行脏页的刷新就用到了Checkpoint技术。 Checkpoint 的目的1、缩短数据库的恢复时间 当数据库怠机恢复时，不需要重做所有的日志信息。因为Checkpoint前的数据页已经刷回到磁盘了。只需要Checkpoint后的redo log进行恢复就好了。 2、缓冲池不够用时，将脏页刷新到磁盘 当缓冲池空间不足时，根据LRU算法会溢出最近最少使用的页，若此页为脏页，那么需要强制执行Checkpoint，将脏页也就是页的新版本刷回磁盘。 3、redo log不可用时，刷新脏页 如图redo log 的不可用是因为当前数据库对其设计都是循环使用的，所以其空间并不是无限大。 当redo log被写满, 因为此时系统不能接受更新, 所有更新语句都会被堵住。 此时必须强制产生Checkpoint需要将 write pos 向前推进，推进范围内的脏页都需要刷新到磁盘 Checkpoint 的种类Checkpoint发生的时间、条件及脏页的选择等都非常复杂。 Checkpoint 每次刷新多少脏页到磁盘？ Checkpoint每次从哪里取脏页？ Checkpoint 什么时间被触发？ 面对上面的问题，InnoDB存储引擎内部为我们提供了两种Checkpoint： Sharp Checkpoint：发生在数据库关闭时将所有的脏页都刷新回磁盘，这是默认的工作方式，参数innodb_fast_shutdown=1 Fuzzy Checkpoint：InnoDB存储引擎内部使用这种模式,只刷新一部分脏页，而不是刷新所有的脏页回磁盘 FuzzyCheckpoint模糊检查点发生的4种情况 Master Thread Checkpoint 差不多以每秒或每十秒的速度从缓冲池的脏页列表中刷新一定比例的页回磁盘。这个过程是异步的，即此时InnoDB存储引擎可以进行其他的操作，用户查询线程不会阻塞。 特点：周期性、写入的量比较小、异步，不影响业务、通过capacity能力告知进行刷盘控制 12345678910mysql&gt; show variables like '%io_cap%';+------------------------+-------+| Variable_name | Value |+------------------------+-------+| innodb_io_capacity | 200 || innodb_io_capacity_max | 2000 |+------------------------+-------+2 rows in set (0.01 sec)# 通过innodb的io能力告知控制对flush list刷脏页数量，io_capacity越高，每次刷盘写入脏页数越多； FLUSH_LRU_LIST Checkpoint 因为LRU列表要保证一定数量的空闲页可被使用，所以如果不够会从尾部移除页，如果移除的页有脏页，就会进行此Checkpoint。 5.6版本后，这个Checkpoint放在了一个单独的Page Cleaner线程中进行，并且用户可以通过参数innodb_lru_scan_depth控制LRU列表中可用页的数量，该值默认为1024 1234567mysql&gt; show variables like 'innodb_lru_scan_depth';+-----------------------+-------+| Variable_name | Value |+-----------------------+-------+| innodb_lru_scan_depth | 1024 |+-----------------------+-------+1 row in set (0.01 sec) 读取lru list，找到脏页，写入磁盘。此情况下触发，默认扫描1024个lru冷端数据页，将脏页写入磁盘(有10个就刷10，有100个就刷100个……) Async/Sync Flush Checkpoint 指的是redo log文件不可用的情况，这时需要强制将一些页刷新回磁盘，而此时脏页是从脏页列表中选取的。5.6版本后不会阻塞用户查询 Dirty Page too much Checkpoint 即脏页的数量太多，导致InnoDB存储引擎强制进行Checkpoint。其目的总的来说还是为了保证缓冲池中有足够可用的页。其可由参数innodb_max_dirty_pages_pct控制,比如该值为75，表示当缓冲池中脏页占据75%时，强制进行CheckPoint 12345mysql&gt; show variables like 'innodb_max_dirty_pages_pct'\\G*************************** 1. row ***************************Variable_name: innodb_max_dirty_pages_pct Value: 90.0000001 row in set (0.00 sec) 因为CPU和磁盘间的鸿沟的问题，从而出现缓冲池数据页来加快数据库DML操作 因为缓冲池数据页与磁盘数据一致性的问题，从而出现WAL策略（核心就是redo log） 因为缓冲池脏页的刷新性能问题，从而出现Checkpoint技术 InnoDB 为了提高执行效率，并不会每次DML操作都和磁盘交互进行持久化。而是通过Write Ahead Log 先策略写入redo log保证事物的持久化。 对于事物中修改的缓冲池脏页，会通过异步的方式刷盘，而内存空闲页和redo log的可用是通过Checkpoint技术来保证的。 不同存储引擎文件存储结构每个数据表都有一个对应的.frm文件，保存每个数据表的元数据信息，包括表结构的定义信息。 MyISAM 物理文件结构为： .frm文件：与表相关的元数据信息都存放在frm文件，包括表结构的定义信息等 .MYD (MYData) 文件：MyISAM 存储引擎专用，用于存储MyISAM 表的数据 .MYI (MYIndex)文件：MyISAM 存储引擎专用，用于存储MyISAM 表的索引相关信息 InnoDB 物理文件结构为： .frm 文件：与表相关的元数据信息都存放在frm文件，包括表结构的定义信息等 .ibd 文件或 .ibdata 文件： 这两种文件都是存放 InnoDB 数据的文件，之所以有两种文件形式存放 InnoDB 的数据，是因为 InnoDB 的数据存储方式能够通过配置来决定是使用共享表空间存放存储数据，还是用独享表空间存放存储数据。 独享表空间存储方式使用.ibd文件，并且每个表一个.ibd文件 共享表空间存储方式使用.ibdata文件，所有表共同使用一个.ibdata文件（或多个，可自己配置） InnoDB和MyISAM的区别（5点） InnoDB 支持事务，MyISAM 不支持事务。这是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一； InnoDB 支持外键，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转为 MYISAM 会失败； InnoDB 是聚簇索引，MyISAM 是非聚簇索引。聚簇索引的文件存放在主键索引的叶子节点上，因此 InnoDB 必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。而 MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 InnoDB 不保存表的具体行数，执行select count(*) from table 时需要全表扫描。而 MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快； InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁。一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。这也是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一； 一张表，里面有ID自增主键，当insert了17条记录之后，删除了第15,16,17条记录，再把Mysql重启，再insert一条记录，这条记录的ID是18还是15 ? 如果表的类型是MyISAM，那么是18。因为MyISAM表会把自增主键的最大ID 记录到数据文件中，重启MySQL自增主键的最大ID也不会丢失； 如果表的类型是InnoDB，那么是15。因为InnoDB 表只是把自增主键的最大ID记录到内存中，所以重启数据库或对表进行OPTION操作，都会导致最大ID丢失 InnoDB的逻辑存储结构记录是按照行来存储的，但是数据库的读取并不以行为单位，否则一次读取（也就是一次 I/O 操作）只能处理一行数据，效率会非常低。因此在数据库中，不论读一行，还是读多行，都是将这些行所在的页进行加载。也就是说，数据库管理存储空间的基本单位是页（Page）。 一个页中可以存储多个行记录（Row），同时在数据库中，还存在着区（Extent）、段（Segment）和表空间（Tablespace）。行、页、区、段、表空间的关系如下图所示： 从图中你能看到一个表空间包括了一个或多个段，一个段包括了一个或多个区，一个区包括了多个页，而一个页中可以有多行记录，这些概念我简单给你讲解下。 表空间表空间（Tablespace）是一个逻辑容器，表空间存储的对象是段，在一个表空间中可以有一个或多个段，但是一个段只能属于一个表空间。数据库由一个或多个表空间组成，表空间从管理上可以划分为系统表空间、用户表空间、撤销表空间、临时表空间等。 在 InnoDB 中存在两种表空间的类型：共享表空间和独立表空间。如果是共享表空间就意味着多张表共用一个表空间。如果是独立表空间，就意味着每张表有一个独立的表空间，也就是数据和索引信息都会保存在自己的表空间中。独立的表空间可以在不同的数据库之间进行迁移。 12345678910mysql&gt; show variables like 'innodb_file_per_table';+-----------------------+-------+| Variable_name | Value |+-----------------------+-------+| innodb_file_per_table | ON | //每张表都会单独保存为一个.ibd 文件。+-----------------------+-------+1 row in set (0.00 sec)mysql&gt; 段段（Segment）由一个或多个区组成，区在文件系统是一个连续分配的空间（在 InnoDB 中是连续的 64 个页），不过在段中不要求区与区之间是相邻的。段是数据库中的分配单位，不同类型的数据库对象以不同的段形式存在。当我们创建数据表、索引的时候，就会相应创建对应的段，比如创建一张表时会创建一个表段，创建一个索引时会创建一个索引段。 区区（Extent）是比页大一级的存储结构，在 InnoDB 存储引擎中，一个区会分配 64 个连续的页。因为 InnoDB 中的页大小默认是 16KB，所以一个区的大小是 64*16KB=1MB。 页内结构页（Page）如果按类型划分的话，常见的有数据页（保存 B+ 树节点）、系统页、Undo 页和事务数据页等。数据页是我们最常使用的页。在 MySQL 的 InnoDB 存储引擎中，默认页的大小是 16KB，我们可以通过下面的命令来进行查看： 123456789mysql&gt; show variables like '%innodb_page_size%';+------------------+-------+| Variable_name | Value |+------------------+-------+| innodb_page_size | 16384 |+------------------+-------+1 row in set (0.00 sec)mysql&gt; 数据库 I/O 操作的最小单位是页，与数据库相关的内容都会存储在页结构里。数据页包括七个部分，分别是文件头（File Header）、页头（Page Header）、最大最小记录（Infimum+supremum）、用户记录（User Records）、空闲空间（Free Space）、页目录（Page Directory）和文件尾（File Tailer）。 可以分为一下三个部分： 首先是文件通用部分，也就是文件头和文件尾。它们类似集装箱，将页的内容进行封装，通过文件头和文件尾校验的方式来确保页的传输是完整的。在文件头中有两个字段，分别是 FIL_PAGE_PREV 和 FIL_PAGE_NEXT，它们的作用相当于指针，分别指向上一个数据页和下一个数据页。连接起来的页相当于一个双向的链表 第二个部分是记录部分，页的主要作用是存储记录，所以“最小和最大记录”和“用户记录”部分占了页结构的主要空间。另外空闲空间是个灵活的部分，当有新的记录插入时，会从空闲空间中进行分配用于存储新记录， 第三部分是索引部分，这部分重点指的是页目录，它起到了记录的索引作用，因为在页中，记录是以单向链表的形式进行存储的。单向链表的特点就是插入、删除非常方便，但是检索效率不高，最差的情况下需要遍历链表上的所有节点才能完成检索，因此在页目录中提供了二分查找的方式，用来提高记录的检索效率。这个过程就好比是给记录创建了一个目录： 将所有的记录分成几个组，这些记录包括最小记录和最大记录，但不包括标记为“已删除”的记录。 第 1 组，也就是最小记录所在的分组只有 1 个记录；最后一组，就是最大记录所在的分组，会有 1-8 条记录；其余的组记录数量在 4-8 条之间。这样做的好处是，除了第 1 组（最小记录所在组）以外，其余组的记录数会尽量平分。 在每个组中最后一条记录的头信息中会存储该组一共有多少条记录，作为 n_owned 字段。 页目录用来存储每组最后一条记录的地址偏移量，这些地址偏移量会按照先后顺序存储起来，每组的地址偏移量也被称之为槽（slot），每个槽相当于指针指向了不同组的最后一个记录。如下图所示： 页目录存储的是槽，槽相当于分组记录的索引。我们通过槽查找记录，实际上就是在做二分查找。这里我以上面的图示进行举例，5 个槽的编号分别为 0，1，2，3，4，我想查找主键为 9 的用户记录，我们初始化查找的槽的下限编号，设置为 low=0，然后设置查找的槽的上限编号 high=4，然后采用二分查找法进行查找。 首先找到槽的中间位置 p=(low+high)/2=(0+4)/2=2，这时我们取编号为 2 的槽对应的分组记录中最大的记录，取出关键字为 8。因为 9 大于 8，所以应该会在槽编号为 (p,high] 的范围进行查找 接着重新计算中间位置 p’=(p+high)/2=(2+4)/2=3，我们查找编号为 3 的槽对应的分组记录中最大的记录，取出关键字为 12。因为 9 小于 12，所以应该在槽 3 中进行查找。 遍历槽 3 中的所有记录，找到关键字为 9 的记录，取出该条记录的信息即为我们想要查找的内容。 从数据页的角度看 B+ 树是如何进行查询的MySQL 的 InnoDB 存储引擎采用 B+ 树作为索引，而索引又可以分成聚集索引和非聚集索引（二级索引），这些索引都相当于一棵 B+ 树，如图所示。一棵 B+ 树按照节点类型可以分成两部分： 叶子节点，B+ 树最底层的节点，节点的高度为 0，存储行记录。 非叶子节点，节点的高度大于 0，存储索引键和页面指针，并不存储行记录本身。 在一棵 B+ 树中，每个节点都是一个页，每次新建节点的时候，就会申请一个页空间。同一层上的节点之间，通过页的结构构成一个双向的链表（页文件头中的两个指针字段）。非叶子节点，包括了多个索引行，每个索引行里存储索引键和指向下一层页面的页面指针。最后是叶子节点，它存储了关键字和行记录，在节点内部（也就是页结构的内部）记录之间是一个单向的链表，但是对记录进行查找，则可以通过页目录采用二分查找的方式来进行。 当我们从页结构来理解 B+ 树的结构的时候，可以帮我们理解一些通过索引进行检索的原理： 1.B+ 树是如何进行记录检索的？ 如果通过 B+ 树的索引查询行记录，首先是从 B+ 树的根开始，逐层检索，直到找到叶子节点，也就是找到对应的数据页为止，将数据页加载到内存中，页目录中的槽（slot）采用二分查找的方式先找到一个粗略的记录分组，然后再在分组中通过链表遍历的方式查找记录。 2. 普通索引和唯一索引在查询效率上有什么不同？ 我们创建索引的时候可以是普通索引，也可以是唯一索引，那么这两个索引在查询效率上有什么不同呢？ 唯一索引就是在普通索引上增加了约束性，也就是关键字唯一，找到了关键字就停止检索。而普通索引，可能会存在用户记录中的关键字相同的情况，根据页结构的原理，当我们读取一条记录的时候，不是单独将这条记录从磁盘中读出去，而是将这个记录所在的页加载到内存中进行读取。InnoDB 存储引擎的页大小为 16KB，在一个页中可能存储着上千个记录，因此在普通索引的字段上进行查找也就是在内存中多几次“判断下一条记录”的操作，对于 CPU 来说，这些操作所消耗的时间是可以忽略不计的。所以对一个索引字段进行检索，采用普通索引还是唯一索引在检索效率上基本上没有差别。 总结今天我们学习了数据库中的基本存储单位，也就是页（Page），磁盘 I/O 都是基于页来进行读取的，在页之上还有区、段和表空间，它们都是更大的存储单位。我们在分配空间的时候会按照页为单位来进行分配，同一棵树上同一层的页与页之间采用双向链表，而在页里面，记录之间采用的单向链表的方式。 链表这种数据结构的特点是增加、删除比较方便，所以在对记录进行删除的时候，有时候并不是真的删除了记录，而只是逻辑上的删除，也就是在标记为上标记为“已删除”。但链表还有个问题就是查找效率低，因此在页结构中还专门设计了页目录这个模块，专门给记录做一个目录，通过二分查找法的方式进行检索提升效率。","link":"/2021/05/11/MySQL%EF%BC%88%E4%BA%8C%EF%BC%89%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/"},{"title":"MySQL（五）调优性能和分区","text":"MySQL的调优介绍： 查询优化器的执行 MySQL调优的不同维度 分区/分表/分库的操作 查询优化器SQL的执行过程 一条 SQL 查询语句首先会经过分析器，进行语法分析和语义检查。 语法分析是检查 SQL 拼写和语法是否正确，语义检查是检查 SQL 中的访问对象是否存在。比如我们在写 SELECT 语句的时候，列名写错了，系统就会提示错误。语法检查和语义检查可以保证 SQL 语句没有错误，最终得到一棵语法分析树 查询优化器的目标是找到执行 SQL 查询的最佳执行计划，执行计划就是查询树，它由一系列物理操作符组成，这些操作符按照一定的运算关系组成查询的执行计划。在查询优化器中，可以分为逻辑查询优化阶段和物理查询优化阶段。 逻辑查询优化就是通过改变 SQL 语句的内容来使得 SQL 查询更高效，同时为物理查询优化提供更多的候选执行计划。通常采用的方式是对 SQL 语句进行等价变换，对查询进行重写，而查询重写的数学基础就是关系代数。对条件表达式进行等价谓词重写、条件简化，对视图进行重写，对子查询进行优化，对连接语义进行了外连接消除、嵌套连接消除等。 逻辑查询优化是基于关系代数进行的查询重写，而关系代数的每一步都对应着物理计算，这些物理计算往往存在多种算法，因此需要计算各种物理路径的代价，从中选择代价最小的作为执行计划。在这个阶段里，对于单表和多表连接的操作，需要高效地使用索引，提升查询效率。 两种优化器模式我们需要通过优化器来制定数据表的扫描方式、连接方式以及连接顺序，从而得到最佳的 SQL 执行计划 第一种是基于规则的优化器（RBO，Rule-Based Optimizer），规则就是人们以往的经验，或者是采用已经被证明是有效的方式。通过在优化器里面嵌入规则，来判断 SQL 查询符合哪种规则，就按照相应的规则来制定执行计划，同时采用启发式规则去掉明显不好的存取路径。 第二种是基于代价的优化器（CBO，Cost-Based Optimizer），这里会根据代价评估模型，计算每条可能的执行计划的代价，也就是 COST，从中选择代价最小的作为执行计划。相比于 RBO 来说，CBO 对数据更敏感，因为它会利用数据表中的统计信息来做判断，针对不同的数据表，查询得到的执行计划可能是不同的，因此制定出来的执行计划也更符合数据表的实际情况。 RBO 的方式更像是一个出租车老司机，凭借自己的经验来选择从 A 到 B 的路径。而 CBO 更像是手机导航，通过数据驱动，来选择最佳的执行路径。 CBO如何计算代价1234567891011121314mysql&gt; select * from mysql.server_cost;+------------------------------+------------+---------------------+---------+---------------+| cost_name | cost_value | last_update | comment | default_value |+------------------------------+------------+---------------------+---------+---------------+| disk_temptable_create_cost | NULL | 2021-05-01 14:58:35 | NULL | 20 | | disk_temptable_row_cost | NULL | 2021-05-01 14:58:35 | NULL | 0.5 || key_compare_cost | NULL | 2021-05-01 14:58:35 | NULL | 0.05 || memory_temptable_create_cost | NULL | 2021-05-01 14:58:35 | NULL | 1 || memory_temptable_row_cost | NULL | 2021-05-01 14:58:35 | NULL | 0.1 || row_evaluate_cost | NULL | 2021-05-01 14:58:35 | NULL | 0.1 |+------------------------------+------------+---------------------+---------+---------------+6 rows in set (0.00 sec)mysql&gt; disk_temptable_create_cost，表示临时表文件（MyISAM 或 InnoDB）的创建代价，默认值为 20 disk_temptable_row_cost，表示临时表文件（MyISAM 或 InnoDB）的行代价，默认值 0.5 key_compare_cost，表示键比较的代价。键比较的次数越多，这项的代价就越大，这是一个重要的指标，默认值 0.05。 memory_temptable_create_cost，表示内存中临时表的创建代价，默认值 1。 memory_temptable_row_cost，表示内存中临时表的行代价，默认值 0.1。 row_evaluate_cost，统计符合条件的行代价，如果符合条件的行数越多，那么这一项的代价就越大，因此这是个重要的指标，默认值 0.1。 12345678910mysql&gt; select * from mysql.engine_cost; //页加载的代价+-------------+-------------+------------------------+------------+---------------------+---------+---------------+| engine_name | device_type | cost_name | cost_value | last_update | comment | default_value |+-------------+-------------+------------------------+------------+---------------------+---------+---------------+| default | 0 | io_block_read_cost | NULL | 2021-05-01 14:58:35 | NULL | 1 || default | 0 | memory_block_read_cost | NULL | 2021-05-01 14:58:35 | NULL | 0.25 |+-------------+-------------+------------------------+------------+---------------------+---------+---------------+2 rows in set (0.00 sec)mysql&gt; io_block_read_cost，从磁盘中读取一页数据的代价，默认是 1。 memory_block_read_cost，从内存中读取一页数据的代价，默认是 0.25。 总代价 = I/O 代价 + CPU 代价 + 内存代价 + 远程代价 MySQL调优调优的目标数据库调优的目的就是要让数据库运行得更快，也就是说响应的时间更快，吞吐量更大。 不过随着用户量的不断增加，以及应用程序复杂度的提升，我们很难用“更快”去定义数据库调优的目标，因为用户在不同时间段访问服务器遇到的瓶颈不同，比如双十一促销的时候会带来大规模的并发访问；还有用户在进行不同业务操作的时候，数据库的事务处理和SQL 查询都会有所不同。因此我们还需要更加精细的定位，去确定调优的目标。 可以从哪些维度进行调优 选择适合的 DBMS 优化表设计 表结构要尽量遵循第三范式的原则（关于第三范式，我在后面章节会讲）。这样可以让数据结构更加清晰规范，减少冗余字段，同时也减少了在更新，插入和删除数据时等异常情况的发生。 如果分析查询应用比较多，尤其是需要进行多表联查的时候，可以采用反范式进行优化。反范式采用空间换时间的方式，通过增加冗余字段提高查询的效率。 表字段的数据类型选择，关系到了查询效率的高低以及存储空间的大小。一般来说，如果字段可以采用数值类型就不要采用字符类型；字符长度要尽可能设计得短一些。针对字符类型来说，当确定字符长度固定时，就可以采用 CHAR 类型；当长度不固定时，通常采用 VARCHAR 类型。 优化逻辑查询:是通过改变 SQL语句的内容让 SQL 执行效率更高效，采用的方式是对 SQL 语句进行等价变换，比如我们在讲解 EXISTS 子查询和 IN 子查询的时候，会根据小表驱动大表的原则选择适合的子查询。在 WHERE 子句中会尽量避免对字段进行函数运算，它们会让字段的索引失效。 优化物理查询:它的核心是高效地建立索引，并通过这些索引来做各种优化。 如果数据重复度高，就不需要创建索引。通常在重复度超过 10% 的情况下，可以不创建这个字段的索引。比如性别这个字段（取值为男和女）。 要注意索引列的位置对索引使用的影响。比如我们在 WHERE 子句中对索引字段进行了表达式的计算，会造成这个字段的索引失效。 要注意联合索引对索引使用的影响。我们在创建联合索引的时候会对多个字段创建索引，这时索引的顺序就很重要了。比如我们对字段 x, y, z 创建了索引，那么顺序是(x,y,z) 还是 (z,y,x)，在执行的时候就会存在差别。 要注意多个索引对索引使用的影响。索引不是越多越好，因为每个索引都需要存储空间，索引多也就意味着需要更多的存储空间。此外，过多的索引也会导致优化器在进行评估的时候增加了筛选出索引的计算时间，影响评估的效率。 查询优化器在对 SQL 语句进行等价变换之后，还需要根据数据表的索引情况和数据情况确定访问路径，这就决定了执行 SQL 时所需要消耗的资源。SQL 查询时需要对不同的数据表进行查询，因此在物理查询优化阶段也需要确定这些查询所采用的路径，具体的情况包括： \\1. 单表扫描：对于单表扫描来说，我们可以全表扫描所有的数据，也可以局部扫描。 \\2. 两张表的连接：常用的连接方式包括了嵌套循环连接、HASH 连接和合并连接。 \\3. 多张表的连接：多张数据表进行连接的时候，顺序很重要，因为不同的连接路径查询的 效率不同，搜索空间也会不同。我们在进行多表连接的时候，搜索空间可能会达到很高的数据量级，巨大的搜索空间显然会占用更多的资源，因此我们需要通过调整连接顺 序，将搜索空间调整在一个可接收的范围内。 物理查询优化是在确定了逻辑查询优化之后，采用物理优化技术（比如索引等），通过计算 代价模型对各种可能的访问路径进行估算，从而找到执行方式中代价最小的作为执行计划。 在这个部分中，我们需要掌握的重点是对索引的创建和使用。 使用 Redis 或 Memcached 作为缓存,如果我们将常用的数据直接放到内存中，就会大幅提升查询的效率。 库级优化:读写分离,对数据库分库分表。当数据量级达到亿级以上时，有时候我们需要把一个数据库切成多份，放到不同的数据库服务器上 什么情况下做垂直切分，什么情况下做水平切分呢？ 如果数据库中的数据表过多，可以采用垂直分库的方式，将关联的数据表部署在一个数据库上。 如果数据表中的列过多，可以采用垂直分表的方式，将数据表分拆成多张，把经常一起使用的列放到同一张表里。 如果数据表中的数据达到了亿级以上，可以考虑水平切分，将大的数据表分拆成不同的子表，每张表保持相同的表结构,采用水平拆分的方式，就是将单张数据量大的表按照某个属性维度分成不同的小表,比如说按照年份拆分 表设计的三范式范式的定义会使用到主键和候选键（因为主键和候选键可以唯一标识元组），数据库中的键（Key）由一个或者多个属性组成。我总结了下数据表中常用的几种键和属性的定义： 超键：能唯一标识元组的属性集叫做超键。 候选键：如果超键不包括多余的属性，那么这个超键就是候选键。 主键：用户可以从候选键中选择一个作为主键。 外键：如果数据表 R1 中的某属性集不是 R1 的主键，而是另一个数据表 R2 的主键，那么这个属性集就是数据表 R1 的外键。 主属性：包含在任一候选键中的属性称为主属性。 非主属性：与主属性相对，指的是不包含在任何一个候选键中的属性。 1NF 指的是数据库表中的任何属性都是原子性的，不可再分。 2NF 指的数据表里的非主属性都要和这个数据表的候选键有完全依赖关系 3NF 在满足 2NF 的同时，对任何非主属性都不传递依赖于候选键 什么是反范式SQL执行慢如何定位呢 观察服务器的状态是否存在周期性的波动。如果存在周期性波动，有可能是周期性节点的原因，比如双十一、促销活动等。这样的话，我们可以通过 A1 这一步骤解决，也就是加缓存，或者更改缓存失效策略。 如果缓存策略没有解决，或者不是周期性波动的原因，我们就需要进一步分析查询延迟和卡顿的原因。接下来进入 S2 这一步，我们需要开启慢查询。慢查询可以帮我们定位执行慢的 SQL 语句。我们可以通过设置 long_query_time 参数定义“慢”的阈值，如果 SQL 执行时间超过了 long_query_time，则会认为是慢查询。当收集上来这些慢查询之后，我们就可以通过分析工具对慢查询日志进行分析。 在 S3 这一步骤中，我们就知道了执行慢的 SQL，这样就可以针对性地用 EXPLAIN 查看对应 SQL 语句的执行计划，或者使用 show profile 查看 SQL 中每一个步骤的时间成本。这样我们就可以了解 SQL 查询慢是因为执行时间长，还是等待时间长。 如果是 SQL 等待时间长，我们进入 A2 步骤。在这一步骤中，我们可以调优服务器的参数，比如适当增加数据库缓冲池等。 如果是 SQL 执行时间长，就进入 A3 步骤，这一步中我们需要考虑是索引设计的问题？还是查询关联的数据表过多？还是因为数据表的字段设计问题导致了这一现象。然后在这些维度上进行对应的调整。 数据库自身的 SQL 查询性能是否已经达到了瓶颈（CPU饱和/磁盘IO/硬件瓶颈，查看系统状态） 如果已经达到了性能瓶颈，进入 A4 阶段，需要考虑增加服务器，采用读写分离的架构，或者考虑对数据库进行分库分表，比如垂直分库、垂直分表和水平分表等。 使用慢查询定位执行慢的 SQL1234mysql &gt; show variables like '%slow_query_log';mysql &gt; set global slow_query_log='ON'; //打开慢查询日志mysql &gt; show variables like '%long_query_time%';//看下慢查询的时间阈值设置mysql &gt; set global long_query_time = 3; 使用 EXPLAIN 查看执行计划使用 Explain 关键字可以模拟优化器执行SQL查询语句，从而知道 MySQL 是如何处理你的 SQL 语句的。分析你的查询语句或是表结构的性能瓶颈 能干吗： 表的读取顺序 数据读取操作的操作类型 哪些索引可以使用 哪些索引被实际使用 表之间的引用 每张表有多少行被优化器查询 怎么玩： Explain + SQL语句 执行计划包含的信息（如果有分区表的话还会有partitions） 各字段解释 id（select 查询的序列号，包含一组数字，表示查询中执行select子句或操作表的顺序） id相同，执行顺序从上往下 id全不同，如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行 id部分相同，执行顺序是先按照数字大的先执行，然后数字相同的按照从上往下的顺序执行 select_type（查询类型，用于区别普通查询、联合查询、子查询等复杂查询） SIMPLE ：简单的select查询，查询中不包含子查询或UNION PRIMARY：查询中若包含任何复杂的子部分，最外层查询被标记为PRIMARY SUBQUERY：在select或where列表中包含了子查询 DERIVED：在from列表中包含的子查询被标记为DERIVED，MySQL会递归执行这些子查询，把结果放在临时表里 UNION：若第二个select出现在UNION之后，则被标记为UNION，若UNION包含在from子句的子查询中，外层select将被标记为DERIVED UNION RESULT：从UNION表获取结果的select table（显示这一行的数据是关于哪张表的） type（显示查询使用了那种类型，从最好到最差依次排列 system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL ） 在这些情况里，all 是最坏的情况，因为采用了全表扫描的方式。index 和 all 差不多，只不过 index 对索引表进行全扫描，这样做的好处是不再需要对数据进行排序，但是开销依然很大。 如果我们在 extra 列中看到 Using index，说明采用了索引覆盖，也就是索引可以覆盖所需的 SELECT 字段，就不需要进行回表，这样就减少了数据查找的开销。 system：表只有一行记录（等于系统表），是 const 类型的特例，平时不会出现 const：表示通过索引一次就找到了，const 用于比较 primary key 或 unique 索引，因为只要匹配一行数据，所以很快，如将主键置于 where 列表中，mysql 就能将该查询转换为一个常量 eq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配，常见于主键或唯一索引扫描 ref：非唯一性索引扫描，范围匹配某个单独值得所有行。本质上也是一种索引访问，他返回所有匹配某个单独值的行，然而，它可能也会找到多个符合条件的行，多以他应该属于查找和扫描的混合体 range：只检索给定范围的行，使用一个索引来选择行。key列显示使用了哪个索引，一般就是在你的where语句中出现了between、&lt;、&gt;、in等的查询，这种范围扫描索引比全表扫描要好，因为它只需开始于索引的某一点，而结束于另一点，不用扫描全部索引 index：Full Index Scan，index于ALL区别为index类型只遍历索引树。通常比ALL快，因为索引文件通常比数据文件小。（也就是说虽然all和index都是读全表，但index是从索引中读取的，而all是从硬盘中读的） ALL：Full Table Scan，将遍历全表找到匹配的行 tip: 一般来说，得保证查询至少达到range级别，最好到达ref possible_keys（显示可能应用在这张表中的索引，一个或多个，查询涉及到的字段若存在索引，则该索引将被列出，但不一定被查询实际使用） key 实际使用的索引，如果为NULL，则没有使用索引 查询中若使用了覆盖索引，则该索引和查询的 select 字段重叠，仅出现在key列表中 key_len 表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度。在不损失精确性的情况下，长度越短越好 key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的 ref （显示索引的哪一列被使用了，如果可能的话，是一个常数。哪些列或常量被用于查找索引列上的值） rows （根据表统计信息及索引选用情况，大致估算找到所需的记录所需要读取的行数） Extra（包含不适合在其他列中显示但十分重要的额外信息） using filesort: 说明mysql会对数据使用一个外部的索引排序，不是按照表内的索引顺序进行读取。mysql中无法利用索引完成的排序操作称为“文件排序”。常见于order by和group by语句中 Using temporary：使用了临时表保存中间结果，mysql在对查询结果排序时使用临时表。常见于排序order by和分组查询group by。 using index：表示相应的select操作中使用了覆盖索引，避免访问了表的数据行，效率不错，如果同时出现using where，表明索引被用来执行索引键值的查找；否则索引被用来读取数据而非执行查找操作 using where：使用了where过滤 using join buffer：使用了连接缓存 impossible where：where子句的值总是false，不能用来获取任何元祖 select tables optimized away：在没有group by子句的情况下，基于索引优化操作或对于MyISAM存储引擎优化COUNT(*)操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化 distinct：优化distinct操作，在找到第一匹配的元祖后即停止找同样值的动作 case: 第一行（执行顺序4）：id列为1，表示是union里的第一个select，select_type列的primary表示该查询为外层查询，table列被标记为，表示查询结果来自一个衍生表，其中derived3中3代表该查询衍生自第三个select查询，即id为3的select。【select d1.name……】 第二行（执行顺序2）：id为3，是整个查询中第三个select的一部分。因查询包含在from中，所以为derived。【select id,name from t1 where other_column=’’】 第三行（执行顺序3）：select列表中的子查询select_type为subquery，为整个查询中的第二个select。【select id from t3】 第四行（执行顺序1）：select_type为union，说明第四个select是union里的第二个select，最先执行【select name,id from t2】 第五行（执行顺序5）：代表从union的临时表中读取行的阶段，table列的&lt;union1,4&gt;表示用第一个和第四个select的结果进行union操作。【两个结果union操作】 Show Profile 分析查询通过慢日志查询可以知道哪些 SQL 语句执行效率低下，通过 explain 我们可以得知 SQL 语句的具体执行情况，索引使用等，还可以结合Show Profile命令查看执行状态。 Show Profile 是 MySQL 提供可以用来分析当前会话中语句执行的资源消耗情况。可以用于SQL的调优的测量 默认情况下，参数处于关闭状态，并保存最近15次的运行结果 分析步骤 是否支持，看看当前的mysql版本是否支持 1mysql&gt;Show variables like 'profiling'; --默认是关闭，使用前需要开启 开启功能，默认是关闭，使用前需要开启 1mysql&gt;set profiling=1; 运行SQL 查看结果 诊断SQL，show profile cpu,block io for query id(上一步前面的问题SQL数字号码) 日常开发需要注意的结论 converting HEAP to MyISAM 查询结果太大，内存都不够用了往磁盘上搬了。 create tmp table 创建临时表，这个要注意 Copying to tmp table on disk 把内存临时表复制到磁盘 locked 性能优化索引优化 全值匹配我最爱 最佳左前缀法则，比如建立了一个联合索引(a,b,c)，那么其实我们可利用的索引就有(a), (a,b), (a,b,c) 不在索引列上做任何操作（计算、函数、(自动or手动)类型转换），会导致索引失效而转向全表扫描 存储引擎不能使用索引中范围条件右边的列 尽量使用覆盖索引(只访问索引的查询(索引列和查询列一致))，减少select is null ,is not null 也无法使用索引 like “xxxx%” 是可以用到索引的，like “%xxxx” 则不行(like “%xxx%” 同理)。like以通配符开头(‘%abc…’)索引失效会变成全表扫描的操作， 字符串不加单引号索引失效 少用or，用它来连接时会索引失效 &lt;，&lt;=，=，&gt;，&gt;=，BETWEEN，IN 可用到索引，&lt;&gt;，not in ，!= 则不行，会导致全表扫描 一般性建议 对于单键索引，尽量选择针对当前query过滤性更好的索引 在选择组合索引的时候，当前Query中过滤性最好的字段在索引字段顺序中，位置越靠前越好。 在选择组合索引的时候，尽量选择可以能够包含当前query中的where字句中更多字段的索引 尽可能通过分析统计信息和调整query的写法来达到选择合适索引的目的 少用Hint强制索引 查询优化永远小标驱动大表（小的数据集驱动大的数据集） 12345slect * from A where id in (select id from B)`等价于#等价于select id from Bselect * from A where A.id=B.id复制代码 当 B 表的数据集必须小于 A 表的数据集时，用 in 优于 exists 12345select * from A where exists (select 1 from B where B.id=A.id)#等价于select * from Aselect * from B where B.id = A.id`复制代码 当 A 表的数据集小于B表的数据集时，用 exists优于用 in 注意：A表与B表的ID字段应建立索引。 order by关键字优化 order by子句，尽量使用 Index 方式排序，避免使用 FileSort 方式排序 MySQL 支持两种方式的排序，FileSort 和 Index，Index效率高，它指 MySQL 扫描索引本身完成排序，FileSort 效率较低； ORDER BY 满足两种情况，会使用Index方式排序；①ORDER BY语句使用索引最左前列 ②使用where子句与ORDER BY子句条件列组合满足索引最左前列 尽可能在索引列上完成排序操作，遵照索引建的最佳最前缀 如果不在索引列上，filesort 有两种算法，mysql就要启动双路排序和单路排序 双路排序：MySQL 4.1之前是使用双路排序,字面意思就是两次扫描磁盘，最终得到数据 单路排序：从磁盘读取查询需要的所有列，按照order by 列在 buffer对它们进行排序，然后扫描排序后的列表进行输出，效率高于双路排序 优化策略 增大sort_buffer_size参数的设置 增大max_lencth_for_sort_data参数的设置 GROUP BY关键字优化 group by实质是先排序后进行分组，遵照索引建的最佳左前缀 当无法使用索引列，增大 max_length_for_sort_data 参数的设置，增大sort_buffer_size参数的设置 where高于having，能写在where限定的条件就不要去having限定了 数据类型优化MySQL 支持的数据类型非常多，选择正确的数据类型对于获取高性能至关重要。不管存储哪种类型的数据，下面几个简单的原则都有助于做出更好的选择。 更小的通常更好：一般情况下，应该尽量使用可以正确存储数据的最小数据类型。 简单就好：简单的数据类型通常需要更少的CPU周期。例如，整数比字符操作代价更低，因为字符集和校对规则（排序规则）使字符比较比整型比较复杂。 尽量避免NULL：通常情况下最好指定列为NOT NULL 分区、分表、分库MySQL分区一般情况下我们创建的表对应一组存储文件，使用MyISAM存储引擎时是一个.MYI和.MYD文件，使用Innodb存储引擎时是一个.ibd和.frm（表结构）文件。 当数据量较大时（一般千万条记录级别以上），MySQL的性能就会开始下降，这时我们就需要将数据分散到多组存储文件，保证其单个文件的执行效率 能干嘛 逻辑数据分割 提高单一的写和读应用速度 提高分区范围读查询的速度 分割数据能够有多个不同的物理文件路径 高效的保存历史数据 怎么玩 首先查看当前数据库是否支持分区 MySQL5.6以及之前版本： 1SHOW VARIABLES LIKE '%partition%'; MySQL5.6： 1show plugins; 分区类型及操作 RANGE分区：基于属于一个给定连续区间的列值，把多行分配给分区。mysql将会根据指定的拆分策略，,把数据放在不同的表文件上。相当于在文件上,被拆成了小块.但是,对外给客户的感觉还是一张表，透明的。 按照 range 来分，就是每个库一段连续的数据，这个一般是按比如时间范围来的，比如交易表啊，销售表啊等，可以根据年月来存放数据。可能会产生热点问题，大量的流量都打在最新的数据上了。 range 来分，好处在于说，扩容的时候很简单。 LIST分区：类似于按RANGE分区，每个分区必须明确定义。它们的主要区别在于，LIST分区中每个分区的定义和选择是基于某列的值从属于一个值列表集中的一个值，而RANGE分区是从属于一个连续区间值的集合。 HASH分区：基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。这个函数可以包含MySQL 中有效的、产生非负整数值的任何表达式。 hash 分发，好处在于说，可以平均分配每个库的数据量和请求压力；坏处在于说扩容起来比较麻烦，会有一个数据迁移的过程，之前的数据需要重新计算 hash 值重新分配到不同的库或表 KEY分区：类似于按HASH分区，区别在于KEY分区只支持计算一列或多列，且MySQL服务器提供其自身的哈希函数。必须有一列或多列包含整数值。 看上去分区表很帅气，为什么大部分互联网还是更多的选择自己分库分表来水平扩展咧？ 分区表，分区键设计不太灵活，如果不走分区键，很容易出现全表锁 一旦数据并发量上来，如果在分区表实施关联，就是一个灾难 自己分库分表，自己掌控业务场景与访问模式，可控。分区表，研发写了一个sql，都不确定mysql是怎么玩的，不太可控 随着业务的发展，业务越来越复杂，应用的模块越来越多，总的数据量很大，高并发读写操作均超过单个数据库服务器的处理能力怎么办？ 这个时候就出现了数据分片，数据分片指按照某个维度将存放在单一数据库中的数据分散地存放至多个数据库或表中。数据分片的有效手段就是对关系型数据库进行分库和分表。 区别于分区的是，分区一般都是放在单机里的，用的比较多的是时间范围分区，方便归档。只不过分库分表需要代码实现，分区则是mysql内部实现。分库分表和分区并不冲突，可以结合使用。 说说分库与分表的设计 MySQL分表分表有两种分割方式，一种垂直拆分，另一种水平拆分。 垂直拆分 垂直分表，通常是按照业务功能的使用频次，把主要的、热门的字段放在一起做为主要表。然后把不常用的，按照各自的业务属性进行聚集，拆分到不同的次要表中；主要表和次要表的关系一般都是一对一的。 水平拆分(数据分片) 单表的容量不超过500W，否则建议水平拆分。是把一个表复制成同样表结构的不同表，然后把数据按照一定的规则划分，分别存储到这些表中，从而保证单表的容量不会太大，提升性能；当然这些结构一样的表，可以放在一个或多个数据库中。 水平分割的几种方法： 使用MD5哈希，做法是对UID进行md5加密，然后取前几位（我们这里取前两位），然后就可以将不同的UID哈希到不同的用户表（user_xx）中了。 还可根据时间放入不同的表，比如：article_201601，article_201602。 按热度拆分，高点击率的词条生成各自的一张表，低热度的词条都放在一张大表里，待低热度的词条达到一定的贴数后，再把低热度的表单独拆分成一张表。 根据ID的值放入对应的表，第一个表user_0000，第二个100万的用户数据放在第二 个表user_0001中，随用户增加，直接添加用户表就行了。 什么情况下做垂直切分，什么情况下做水平切分呢？ 如果数据库中的数据表过多，可以采用垂直分库的方式，将关联的数据表部署在一个数据库上。 如果数据表中的列过多，可以采用垂直分表的方式，将数据表分拆成多张，把经常一起使用的列放到同一张表里。 如果数据表中的数据达到了亿级以上，可以考虑水平切分，将大的数据表分拆成不同的子表，每张表保持相同的表结构。比如你可以按照年份来划分，把不同年份的数据放到不同的数据表中。2017 年、2018 年和 2019 年的数据就可以分别放到三张数据表中。 采用垂直分表的形式，就是将一张数据表分拆成多张表，采用水平拆分的方式，就是将单张数据量大的表按照某个属性维度分成不同的小表。 MySQL分库 为什么要分库? 数据库集群环境后都是多台 slave，基本满足了读取操作; 但是写入或者说大数据、频繁的写入操作对master性能影响就比较大，这个时候，单库并不能解决大规模并发写入的问题，所以就会考虑分库。 分库是什么？ 一个库里表太多了，导致了海量数据，系统性能下降，把原本存储于一个库的表拆分存储到多个库上， 通常是将表按照功能模块、关系密切程度划分出来，部署到不同库上。 优点： 减少增量数据写入时的锁对查询的影响 由于单表数量下降，常见的查询操作由于减少了需要扫描的记录，使得单表单次查询所需的检索行数变少，减少了磁盘IO，时延变短 但是它无法解决单表数据量太大的问题 分库分表后的难题 分布式事务的问题，数据的完整性和一致性问题。 数据操作维度问题：用户、交易、订单各个不同的维度，用户查询维度、产品数据分析维度的不同对比分析角度。 跨库联合查询的问题，可能需要两次查询 跨节点的count、order by、group by以及聚合函数问题，可能需要分别在各个节点上得到结果后在应用程序端进行合并 额外的数据管理负担，如：访问数据表的导航定位 额外的数据运算压力，如：需要在多个节点执行，然后再合并计算程序编码开发难度提升，没有太好的框架解决，更多依赖业务看如何分，如何合，是个难题。 总结","link":"/2021/05/12/MySQL%EF%BC%88%E4%BA%94%EF%BC%89%E8%B0%83%E4%BC%98%E6%80%A7%E8%83%BD%E5%92%8C%E5%88%86%E5%8C%BA/"},{"title":"MySQL（六）高可用集群","text":"","link":"/2021/05/12/MySQL%EF%BC%88%E5%85%AD%EF%BC%89%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"title":"MySQL（四）索引","text":"索引是加速数据查找的一种数据结构！ 本文介绍了 索引介绍 从磁盘IO的角度来理解SQL查询 理想的索引 索引介绍什么是索引 索引是mysql为了高效获取数据的一种数据结构 索引本身也很大，不可能全部存储在内存中，一般以索引文件的形式存储在磁盘上 索引的优势和劣势优势： 提高数据检索效率，降低数据库IO成本 降低数据排序的成本，将随机IO变成顺序IO，降低CPU的消耗 劣势： 索引也是一张表，保存了主键和索引字段，并指向实体表的记录，所以也需要占用内存 虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。 因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段， 都会调整因为更新所带来的键值变化后的索引信息 索引的分类功能逻辑分类 普通索引：是基础的索引，没有任何约束，主要用于提高查询效率。 唯一索引：就是在普通索引的基础上增加了数据唯一性的约束，在一张数据表里可以有多个唯一索引。 主键索引：在唯一索引的基础上增加了不为空的约束，也就是 NOT NULL+UNIQUE，一张表里最多只有一个主键索引 全文索引：用的不多，MySQL 自带的全文索引只支持英文 在一张数据表中只能有一个主键索引，这是由主键索引的物理实现方式决定的，因为数据存储在文件中只能按照一种顺序进行存储。但可以有多个普通索引或者多个唯一索引。 InnoDB 表必须要有主键，并且推荐使用整型自增主键，索引与数据是共同存储的，不管是主键索引还是辅助索引，在查找时都是通过先查找到索引节点才能拿到相对应的数据，如果我们在设计表结构时没有显式指定索引列的话，MySQL 会从表中选择数据不重复的列建立索引，如果没有符合的列，则 MySQL 自动为 InnoDB 表生成一个隐含字段作为主键，并且这个字段长度为6个字节，类型为整型。 物理实现分类 聚集索引：按照主键来排序存储数据，页子节点上就是数据行。 非聚集索引（辅助索引）：系统会进行两次查找，第一次先找到索引，第二次找到索引对应的位置取出数据行 每一个表只能有一个聚集索引，因为数据行本身只能按一个顺序存储。 聚集索引与非聚集索引的原理不同，在使用上也有一些区别： 聚集索引的叶子节点存储的就是我们的数据记录，非聚集索引的叶子节点存储的是数据位置。非聚集索引不会影响数据表的物理存储顺序。 一个表只能有一个聚集索引，因为只能有一种排序存储的方式，但可以有多个非聚集索引，也就是多个索引目录提供数据检索。 使用聚集索引的时候，数据的查询效率高，但如果对数据进行插入，删除，更新等操作，效率会比非聚集索引低。 字段个数分类 单一索引：每个索引只包含单个列，一个表可以有多个单列索引 联合索引：复合索引指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用复合索引时遵循最左前缀集合 数据结构分类 B+树索引 Hash索引 Full-Text全文索引 R-Tree索引 MyISAM 和 InnoDB 存储引擎，都使用 B+Tree的数据结构，它相对与 B-Tree结构，所有的数据都存放在叶子节点上，且把叶子节点通过指针连接到一起，形成了一条数据链表，以加快相邻数据的检索效率。 B+树介绍B+树是为磁盘或者其他辅助存储设备设计的一种平衡查找树。在B+树中，所有的记录节点都是按照键值的大小顺序存放在同一层的叶子节点上，由各个叶子节点的指针进行连接。比如下面这个：高度为2，每页存放4个记录，扇出为5 B+树的插入操作按照下面的原则分别插入 28，70，95： 28直接插入即可。 70： 95： 无论如何变化，B+树总是会保持平衡的。但是可能会需要大量的拆分页的操作，拆分页的操作意味着对磁盘的操作，所以应该尽可能的减少页的拆分操作。因此，B+树提供了类似于平衡二叉树的旋转功能。 比如上述的插入70：会先检查左右兄弟节点是否已经满了，如果没有满就会将记录移动到所在页的兄弟节点上。 B+树的删除操作删除操作中会依赖于一个变量：填充因子 fill factory，50%是填充因子可以设置的最小值。 彻底理解B树和B+树B树是专门为了磁盘等外设存储设备设计的一种平衡查找树，B 树的英文是 Balance Tree，也就是平衡的多路搜索树，它的高度远小于平衡二叉树的高度。在文件系统和数据库系统中的索引结构经常采用 B 树来实现。 系统从磁盘读取数据到内存时是以磁盘块（block）为基本单位的，位于同一个磁盘块中的数据会被一次性读取出来，而不是需要什么取什么。 InnoDB 存储引擎中有页（Page）的概念，页是其磁盘管理的最小单位。InnoDB 存储引擎中默认每个页的大小为16KB，可通过参数 innodb_page_size 将页的大小设置为 4K、8K、16K，在 MySQL 中可通过如下命令查看页的大小：show variables like 'innodb_page_size'; 而系统一个磁盘块的存储空间往往没有这么大，因此 InnoDB 每次申请磁盘空间时都会是若干地址连续磁盘块来达到页的大小 16KB。InnoDB 在把磁盘数据读入到磁盘时会以页为基本单位，在查询数据时如果一个页中的每条数据都能有助于定位数据记录的位置，这将会减少磁盘I/O次数，提高查询效率。 B-Tree 结构的数据可以让系统高效的找到数据所在的磁盘块。为了描述 B-Tree，首先定义一条记录为一个二元组[key, data] ，key为记录的键值，对应表中的主键值，data 为一行记录中除主键外的数据。对于不同的记录，key值互不相同。B-Tree作为平衡的多路搜索树，它的每一个节点最多可以包括 M 个子节点，M 称为 B 树的阶。同时你能看到，每个磁盘块中包括了关键字和子节点的指针。如果一个磁盘块中包括了 x 个关键字，那么指针数就是 x+1。对于一个 100 阶的 B 树来说，如果有 3 层的话最多可以存储约 100 万的索引数据。对于大量的索引数据来说，采用 B 树的结构是非常适合的，因为树的高度要远小于二叉树的高度。 一个 M 阶的 B 树（M&gt;2）有以下的特性： 根节点的儿子数的范围是 [2,M]。 每个中间节点包含 k-1 个关键字和 k 个孩子，孩子的数量 = 关键字的数量 +1，k 的取值范围为 [ceil(M/2), M]。 叶子节点包括 k-1 个关键字（叶子节点没有孩子），k 的取值范围为 [ceil(M/2), M]。 假设中间节点节点的关键字为：Key[1], Key[2], …, Key[k-1]，且关键字按照升序排序，即 Key[i]&lt;Key[i+1]。此时 k-1 个关键字相当于划分了 k 个范围，也就是对应着 k 个指针，即为：P[1], P[2], …, P[k]，其中 P[1] 指向关键字小于 Key[1] 的子树，P[i] 指向关键字属于 (Key[i-1], Key[i]) 的子树，P[k] 指向关键字大于 Key[k-1] 的子树。 所有叶子节点位于同一层。 B-Tree 中的每个节点根据实际情况可以包含大量的关键字信息和分支，如下图所示为一个 3 阶的 B-Tree： 每个节点占用一个盘块的磁盘空间，一个节点上有两个升序排序的关键字和三个指向子树根节点的指针，指针存储的是子节点所在磁盘块的地址。两个关键词划分成的三个范围域对应三个指针指向的子树的数据的范围域。以根节点为例，关键字为17和35，P1指针指向的子树的数据范围为小于17，P2指针指向的子树的数据范围为17~35，P3指针指向的子树的数据范围为大于35。 模拟查找关键字29的过程： 根据根节点找到磁盘块1，读入内存。【磁盘I/O操作第1次】 比较关键字29在区间（17,35），找到磁盘块1的指针P2。 根据P2指针找到磁盘块3，读入内存。【磁盘I/O操作第2次】 比较关键字29在区间（26,30），找到磁盘块3的指针P2。 根据P2指针找到磁盘块8，读入内存。【磁盘I/O操作第3次】 在磁盘块8中的关键字列表中找到关键字29。 从上一节中的B-Tree结构图中可以看到每个节点中不仅包含数据的key值，还有data值。而每一个页的存储空间是有限的，如果data数据较大时将会导致每个节点（即一个页）能存储的key的数量很小，当存储的数据量很大时同样会导致B-Tree的深度较大，增大查询时的磁盘I/O次数，进而影响查询效率。因此出现了改进之后的B+Tree： 在B+Tree中，所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只存储key值信息，这样可以大大加大每个节点存储的key值数量，降低B+Tree的高度。 B+Tree相对于B-Tree有几点不同： 有 k 个孩子的节点就有 k 个关键字。也就是孩子数量 = 关键字数，而 B 树中，孩子数量 = 关键字数 +1。 非叶子节点的关键字也会同时存在在子节点中，并且是在子节点中所有关键字的最大（或最小）。 非叶子节点仅用于索引，不保存数据记录，跟记录有关的信息都放在叶子节点中。而 B 树中，非叶子节点既保存索引，也保存数据记录。 所有关键字都在叶子节点出现，叶子节点构成一个有序链表，而且叶子节点本身按照关键字的大小从小到大顺序链接。 为什么Mysql索引要用B+树不是B树？ 首先，B+ 树查询效率更稳定。因为 B+ 树每次只有访问到叶子节点才能找到对应的数据，而在 B 树中，非叶子节点也会存储数据，这样就会造成查询效率不稳定的情况，有时候访问到了非叶子节点就可以找到关键字，而有时需要访问到叶子节点才能找到关键字。 其次，B+ 树的查询效率更高，这是因为通常 B+ 树比 B 树更矮胖（阶数更大，深度更低），查询所需要的磁盘 I/O 也会更少。同样的磁盘页大小，B+ 树可以存储更多的节点关键字。 不仅是对单个关键字的查询上，在查询范围上，B+ 树的效率也比 B 树高。这是因为所有关键字都出现在 B+ 树的叶子节点中，并通过有序链表进行了链接。而在 B 树中则需要通过中序遍历才能完成查询范围的查找，效率要低很多。 B+树索引聚集索引INNODB存储引擎表是索引组织表，也就是说表中的数据是按照主键的顺序存放的。而聚集索引（clustered index）就是按照每张表的主键来构造出一棵B+树。同时叶子节点存放的就是整张表的行记录数据，也将聚集索引的叶子节点称为数据页。每个数据页通过一个双向链表来进行链接。 由于实际的数据页只能按照一棵B+树来进行排序，所以每张表只能拥有一个聚集索引。 从数据页的角度来看B+树在一棵 B+ 树中，每个节点都是一个页，每次新建节点的时候，就会申请一个页空间。同一层上的节点之间，通过页的结构构成一个双向的链表（页文件头中的两个指针字段）。非叶子节点，包括了多个索引行，每个索引行里存储索引键和指向下一层页面的页面指针。最后是叶子节点，它存储了关键字和行记录，在节点内部（也就是页结构的内部）记录之间是一个单向的链表，但是对记录进行查找，则可以通过页目录采用二分查找的方式来进行。 当我们从页结构来理解 B+ 树的结构的时候，可以帮我们理解一些通过索引进行检索的原理： 1.B+ 树是如何进行记录检索的？ 如果通过 B+ 树的索引查询行记录，首先是从 B+ 树的根开始，逐层检索，直到找到叶子节点，也就是找到对应的数据页为止，将数据页加载到内存中，页目录中的槽（slot）采用二分查找的方式先找到一个粗略的记录分组，然后再在分组中通过链表遍历的方式查找记录。 2. 普通索引和唯一索引在查询效率上有什么不同？ 我们创建索引的时候可以是普通索引，也可以是唯一索引，那么这两个索引在查询效率上有什么不同呢？ 唯一索引就是在普通索引上增加了约束性，也就是关键字唯一，找到了关键字就停止检索。而普通索引，可能会存在用户记录中的关键字相同的情况，根据页结构的原理，当我们读取一条记录的时候，不是单独将这条记录从磁盘中读出去，而是将这个记录所在的页加载到内存中进行读取。InnoDB 存储引擎的页大小为 16KB，在一个页中可能存储着上千个记录，因此在普通索引的字段上进行查找也就是在内存中多几次“判断下一条记录”的操作，对于 CPU 来说，这些操作所消耗的时间是可以忽略不计的。所以对一个索引字段进行检索，采用普通索引还是唯一索引在检索效率上基本上没有差别。 回表查询通过某一列建立辅助索引时，通过该列查找数据时，需要经历以下两个阶段： 在辅助索引上检索name，到达其叶子节点获取对应的主键； 使用主键在主索引上再进行对应的检索操作 覆盖索引覆盖索引（Covering Index）,或者叫索引覆盖， 也就是平时所说的不需要回表操作 就是select的数据列只用从索引中就能够取得，不必读取数据行，MySQL可以利用索引返回select列表中的字段，而不必根据索引再次读取数据文件，换句话说查询列要被所建的索引覆盖。 索引是高效找到行的一个方法，但是一般数据库也能使用索引找到一个列的数据，因此它不必读取整个行。毕竟索引叶子节点存储了它们索引的数据，当能通过读取索引就可以得到想要的数据，那就不需要读取行了。一个索引包含（覆盖）满足查询结果的数据就叫做覆盖索引。 判断标准 使用explain，可以通过输出的extra列来判断，对于一个索引覆盖查询，显示为using index，MySQL查询优化器在执行查询前会决定是否有索引覆盖查询 哈希索引主要就是通过Hash算法（常见的Hash算法有直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的Hash值，与这条数据的行指针一并存入Hash表的对应位置；如果发生Hash碰撞（两个不同关键字的Hash值相同），则在对应Hash键下以链表形式存储。 检索算法：在检索查询时，就再次对待查关键字再次执行相同的Hash算法，得到Hash值，到对应Hash表对应位置取出数据即可，如果发生Hash碰撞，则需要在取值时进行筛选。目前使用Hash索引的数据库并不多，主要有Memory等。 MySQL目前有Memory引擎和NDB引擎支持Hash索引。 索引的使用原则如何通过索引让SQL查询效率最大化？ 哪些情况下需要创建索引 有唯一性约束的字段，比如主键或唯一性字段，可以直接创建主键索引或者唯一性索引。 频繁作为 WHERE 查询条件的字段，尤其在数据表大的情况下 需要经常 GROUP BY 和 ORDER BY 的列 如果同时有GROUP BY 和 ORDER BY 的情况： 1&gt;SELECT user_id, count(*) as num FROM product_comment group by user_id order by comment_time desc limit 100 当我们对 user_id 和 comment_time 分别创建索引,效率反而不高 实际上多个单列索引在多条件查询时只会生效一个索引（MySQL 会选择其中一个限制最严格的作为索引），所以在多条件联合查询的时候最好创建联合索引。在这个例子中，我们创建联合索引 (user_id, comment_time)，再来看下查询的时间，查询时间为 0.775s，效率提升了很多。如果我们创建联合索引的顺序为 (comment_time, user_id) 呢？运行时间为 1.990s，同样比两个单列索引要快，但是会比顺序为 (user_id, comment_time) 的索引要慢一些。这是因为在进行 SELECT 查询的时候，先进行 GROUP BY，再对数据进行 ORDER BY 的操作，所以按照这个联合索引的顺序效率是最高的。 UPDATE、DELETE 的 WHERE 条件列，一般也需要创建索引 DISTINCT 字段需要创建索引 做多表 JOIN 连接操作时，创建索引需要注意以下的原则： 查询中与其他表关联的字段，外键关系建立索引 首先，连接表的数量尽量不要超过 3 张，因为每增加一张表就相当于增加了一次嵌套的循环，数量级增长会非常快，严重影响查询的效率。 其次，对 WHERE 条件创建索引，因为 WHERE 才是对数据条件的过滤。如果在数据量非常大的情况下，没有 WHERE 条件过滤是非常可怕的。 最后，对用于连接的字段创建索引，并且该字段在多张表中的类型必须一致。比如 user_id 在 product_comment 表和 user 表中都为 int(11) 类型，而不能一个为 int 另一个为 varchar 类型。 什么时候不需要创建索引 WHERE 条件（包括 GROUP BY、ORDER BY）里用不到的字段不需要创建索引，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的 如果表记录太少，比如少于 1000 个，那么是不需要创建索引的 字段中如果有大量重复数据，也不用创建索引，比如性别字段 频繁更新的字段不一定要创建索引。因为更新数据的时候，也需要更新索引，如果索引太多，在更新索引的时候也会造成负担，从而影响效率。 索引什么时候会失效 如果索引进行了表达式计算，则会失效 如果对索引使用函数，也会造成失效 在 WHERE 子句中，如果在 OR 前的条件列进行了索引，而在 OR 后的条件列没有进行索引，那么索引会失效。 当我们使用 LIKE 进行模糊查询的时候，前面不能是 % 索引列与 NULL 或者 NOT NULL 进行判断的时候也会失效。 我们在使用联合索引的时候要注意最左原则 从磁盘IO的角度来理解SQL查询数据库存储的基本单位是页，对于一棵 B+ 树的索引来说，是先从根节点找到叶子节点，也就是先查找数据行所在的页，再将页读入到内存中，在内存中对页的记录进行查找，从而得到想要数据。你看，虽然我们想要查找的，只是一行记录，但是对于磁盘 I/O 来说却需要加载一页的信息，因为页是最小的存储单位。 如果我们想要查找多行记录，查询时间是否会成倍地提升呢？其实数据库会采用缓冲池的方式提升页的查找效率。 数据库缓冲池缓冲池管理器会尽量将经常使用的数据保存起来，在数据库进行页面读操作的时候，首先会判断该页面是否在缓冲池中，如果存在就直接读取，如果不存在，就会通过内存或磁盘将页面存放到缓冲池中再进行读取。 123456789mysql&gt; show variables like 'innodb_buffer_pool_size';+-------------------------+-----------+| Variable_name | Value |+-------------------------+-----------+| innodb_buffer_pool_size | 134217728 |+-------------------------+-----------+1 row in set (0.01 sec)mysql&gt; 实际上，当我们对数据库中的记录进行修改的时候，首先会修改缓冲池中页里面的记录信息，然后数据库会以一定的频率刷新到磁盘上。注意并不是每次发生更新操作，都会立刻进行磁盘回写。缓冲池会采用一种叫做 checkpoint 的机制将数据回写到磁盘上，这样做的好处就是提升了数据库的整体性能。 比如，当缓冲池不够用时，需要释放掉一些不常用的页，就可以采用强行采用 checkpoint 的方式，将不常用的脏页回写到磁盘上，然后再从缓冲池中将这些页释放掉。这里脏页（dirty page）指的是缓冲池中被修改过的页，与磁盘上的数据页不一致。 数据页加载的三种方式 内存读取：如果该数据存在于内存中，基本上执行时间在 1ms 左右，效率还是很高的 随机读取：如果数据没有在内存中，就需要在磁盘上对该页进行查找，整体时间预估在 10ms 左右，这 10ms 中有 6ms 是磁盘的实际繁忙时间（包括了寻道和半圈旋转时间），有 3ms 是对可能发生的排队时间的估计值，另外还有 1ms 的传输时间，将页从磁盘服务器缓冲区传输到数据库缓冲区中。这 10ms 看起来很快，但实际上对于数据库来说消耗的时间已经非常长了，因为这还只是一个页的读取时间。 顺序读取其实是一种批量读取的方式，因为我们请求的数据在磁盘上往往都是相邻存储的，顺序读取可以帮我们批量读取页面，这样的话，一次性加载到缓冲池中就不需要再对其他页面单独进行磁盘 I/O 操作了。如果一个磁盘的吞吐量是 40MB/S，那么对于一个 16KB 大小的页来说，一次可以顺序读取 2560（40MB/16KB）个页，相当于一个页的读取时间为 0.4ms。采用批量读取的方式，即使是从磁盘上进行读取，效率也比从内存中只单独读取一个页的效率要高。 通过 last_query_cost 统计 SQL 语句的查询成本我们先前已经讲过，一条 SQL 查询语句在执行前需要确定查询计划，如果存在多种查询计划的话，MySQL 会计算每个查询计划所需要的成本，从中选择成本最小的一个作为最终执行的查询计划。 如果我们想要查看某条 SQL 语句的查询成本，可以在执行完这条 SQL 语句之后，通过查看当前会话中的 last_query_cost 变量值来得到当前查询的成本。这个查询成本对应的是 SQL 语句所需要读取的页的数量 123456789101112131415161718mysql&gt; select * from dept;+--------+----------+| deptno | deptname |+--------+----------+| 1 | tech || 2 | sale || 5 | fin || 3 | hr |+--------+----------+4 rows in set (0.00 sec)mysql&gt; show status like 'last_query_cost';+-----------------+----------+| Variable_name | Value |+-----------------+----------+| Last_query_cost | 0.549000 |+-----------------+----------+1 row in set (0.01 sec) 如果另外一个查询所读取的页的数量是刚才的 20 倍，但是查询的效率并没有明显的变化，实际上这两个 SQL 查询的时间基本上一样，就是因为采用了顺序读取的方式将页面一次性加载到缓冲池中，然后再进行查找。虽然页数量（last_query_cost）增加了不少，但是通过缓冲池的机制，并没有增加多少查询时间 理想的索引是什么样的索引片索引片就是 SQL 查询语句在执行中需要扫描的一个索引片段，我们会根据索引片中包含的匹配列的数量不同，将索引分成窄索引（比如包含索引列数为 1 或 2）和宽索引（包含的索引列数大于 2）。 如果索引片越宽，那么需要顺序扫描的索引页就越多；如果索引片越窄，就会减少索引访问的开销。 过滤因子谓词的选择性也等于满足这个条件列的记录数除以总记录数的比例。 这时如果我们创建一个联合的过滤条件（height, team_id），那么它的过滤能力是怎样的呢？ 联合过滤因子有更高的过滤能力，这里还需要注意一个条件，那就是条件列的关联性应该尽量相互独立，否则如果列与列之间具有相关性，联合过滤因子的能力就会下降很多。比如城市名称和电话区号就有强相关性，这两个列组合到一起不会加强过滤效果。 你能看到过滤因子决定了索引片的大小（注意这里不是窄索引和宽索引），过滤因子的条件过滤能力越强，满足条件的记录数就越少，SQL 查询需要扫描的索引片也就越小。同理，如果我们没有选择好索引片中的过滤因子，就会造成索引片中的记录数过多的情况。 三星索引三星索引具体指的是： 在 WHERE 条件语句中，找到所有等值谓词中的条件列，将它们作为索引片中的开始列； 将 GROUP BY 和 ORDER BY 中的列加入到索引中； 将 SELECT 字段中剩余的列加入到索引片中。 首先，如果我们要通过索引查找符合条件的记录，就需要将 WHERE 子句中的等值谓词列加入到索引片中，这样索引的过滤能力越强，最终扫描的数据行就越少。 另外，如果我们要对数据记录分组或者排序，都需要重新扫描数据记录。为了避免进行 file sort 排序，可以把 GROUP BY 和 ORDER BY 中涉及到的列加入到索引中，因为创建了索引就会按照索引的顺序来存储数据，这样再对这些数据按照某个字段进行分组或者排序的时候，就会提升效率。 有时候我们并不能需要完全遵循三星索引的原则，原因主要有以下两点： 采用三星索引会让索引片变宽，这样每个页能够存储的索引数据就会变少，从而增加了页加载的数量。从另一个角度来看，如果数据量很大，比如有 1000 万行数据，过多索引所需要的磁盘空间可能会成为一个问题，对缓冲池所需空间的压力也会增加。 增加了索引维护的成本。如果我们为所有的查询语句都设计理想的三星索引，就会让数据表中的索引个数过多，这样索引维护的成本也会增加。举个例子，当我们添加一条记录的时候，就需要在每一个索引上都添加相应的行（存储对应的主键值），假设添加一行记录的时间成本是 10ms（磁盘随机读取一个页的时间），那么如果我们创建了 10 个索引，添加一条记录的时间就可能变成 0.1s，如果是添加 10 条记录呢？就会花费近 1s 的时间。从索引维护的成本来看消耗还是很高的。当然对于数据库来说，数据的更新不一定马上回写到磁盘上，但即使不及时将脏页进行回写，也会造成缓冲池中的空间占用过多，脏页过多的情况。 针对一条 SQL 查询来说，三星索引是个理想的方式，但实际运行起来我们要考虑更多维护的成本，在索引效率和索引维护之间进行权衡。 三星索引会让索引变宽，好处就是不需要进行回表查询，减少了磁盘 I/O 的次数，弊端就是会造成频繁的页分裂和页合并，对于数据的插入和更新来说，效率会降低不少。 总结：","link":"/2021/05/12/MySQL%EF%BC%88%E5%9B%9B%EF%BC%89%E7%B4%A2%E5%BC%95/"},{"title":"MySQL（四）事务","text":"事务是由一组SQL语句组成的逻辑处理单元，而锁可以保证事务的隔离性。 MySQL中的事务事务的四个基本属性事务是由一组SQL语句组成的逻辑处理单元，具有4个属性，通常简称为事务的ACID属性。 A (Atomicity) 原子性：整个事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样 C (Consistency) 一致性：一致性指的就是数据库在进行事务操作后，会由原来的一致状态，变成另一种一致的状态。也就是说当事务提交后，或者当事务发生回滚后，数据库的完整性约束不能被破坏。 I (Isolation)隔离性：一个事务的执行不能其它事务干扰。即一个事务内部的操作及使用的数据对其它并发事务是隔离的，并发执行的各个事务之间不能互相干扰 D (Durability) 持久性：在事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚 并发事务会带来的问题更新丢失（Lost Update)： 事务A和事务B选择同一行，然后基于最初选定的值更新该行时，由于两个事务都不知道彼此的存在，就会发生丢失更新问题 **脏读(Dirty Reads)**：读到了其他事务还没有提交的数据。 **不可重复读（Non-Repeatable Reads)**：事务 A 多次读取同一数据，事务B在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果不一致。 幻读（Phantom Reads)：幻读与不可重复读类似。它发生在一个事务A读取了几行数据，接着另一个并发事务B插入了一些数据时。在随后的查询中，事务A就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 幻读和不可重复读的区别： 不可重复读的重点是修改：在同一事务中，同样的条件，第一次读的数据和第二次读的数据不一样。（因为中间有其他事务提交了修改） 幻读的重点在于新增或者删除：在同一事务中，同样的条件,，第一次和第二次读出来的记录数不一样。（因为中间有其他事务提交了插入/删除） 如何解决这些问题“更新丢失”通常是应该完全避免的。但防止更新丢失，并不能单靠数据库事务控制器来解决，需要应用程序对要更新的数据加必要的锁来解决，因此，防止更新丢失应该是应用的责任。 “脏读” 、 “不可重复读”和“幻读” ，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决： 一种是加锁：在读取数据前，对其加锁，阻止其他事务对数据进行修改。 另一种是数据多版本并发控制（MultiVersion Concurrency Control，简称 MVCC 或 MCC），也称为多版本数据库：不用加任何锁， 通过一定机制生成一个数据请求时间点的一致性数据快照 （Snapshot)， 并用这个快照来提供一定级别 （语句级或事务级） 的一致性读取。从用户的角度来看，好象是数据库可以提供同一数据的多个版本。 事务的四种隔离级别READ-UNCOMMITTED(读未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读 InnoDB 存储引擎在 REPEATABLE-READ（可重读）事务隔离级别下使用的是Next-Key Lock 算法，因此可以避免幻读的产生. 介绍一下MVCCMultiversion Concurrency Control，中文翻译过来就是多版本并发控制技术，MVCC 的实现是通过保存数据在某个时间点的快照来实现的。也就是说不管需要执行多长时间，每个事务看到的数据都是一致的。 MVCC在innodb中的实现InnoDB 的 MVCC，是通过在每行记录后面保存两个隐藏的列来实现： db_row_id：隐藏的行 ID，用来生成默认聚集索引。如果我们创建数据表的时候没有指定聚集索引，这时 InnoDB 就会用这个隐藏 ID 来创建聚集索引。采用聚集索引的方式可以提升数据的查找效率。 db_trx_id：操作这个数据的事务 ID，也就是最后一个对该数据进行插入或更新的事务 ID。 db_roll_ptr：回滚指针，也就是指向这个记录的 Undo Log 信息。 InnoDB 将行记录快照保存在了 Undo Log 里，我们可以在回滚段中找到它们： 从图中你能看到回滚指针将数据行的所有快照记录都通过链表的结构串联了起来，每个快照的记录都保存了当时的 db_trx_id，也是那个时间点操作这个数据的事务 ID。这样如果我们想要找历史快照，就可以通过遍历回滚指针的方式进行查找 如果一个事务想要查询这个行记录，需要读取哪个版本的行记录呢？这时就需要用到 Read View 了，它帮我们解决了行的可见性问题。Read View 保存了当前事务开启时所有活跃（还没有提交）的事务列表 在 Read VIew 中有几个重要的属性： trx_ids，系统当前正在活跃的事务 ID 集合。 low_limit_id，活跃的事务中最大的事务 ID。 up_limit_id，活跃的事务中最小的事务 ID。 creator_trx_id，创建这个 Read View 的事务 ID。 假设当前有事务 creator_trx_id 想要读取某个行记录，这个行记录的事务 ID 为 trx_id，那么会出现以下几种情况： 如果 trx_id &lt; 活跃的最小事务 ID（up_limit_id），也就是说这个行记录在这些活跃的事务创建之前就已经提交了，那么这个行记录对该事务是可见的。 如果 trx_id &gt; 活跃的最大事务 ID（low_limit_id），这说明该行记录在这些活跃的事务创建之后才创建，那么这个行记录对当前事务不可见。 如果 up_limit_id &lt; trx_id &lt; low_limit_id，说明该行记录所在的事务 trx_id 在目前 creator_trx_id 这个事务创建的时候，可能还处于活跃的状态，因此我们需要在 trx_ids 集合中进行遍历，如果 trx_id 存在于 trx_ids 集合中，证明这个事务 trx_id 还处于活跃状态，不可见。否则，如果 trx_id 不存在于 trx_ids 集合中，证明事务 trx_id 已经提交了，该行记录可见 总而言之，当我们查询一条记录的时候，系统如何通过多版本并发控制技术找到它： 首先获取事务自己的版本号，也就是事务 ID； 获取 Read View； 查询得到的数据，然后与 Read View 中的事务版本号进行比较； 如果不符合 ReadView 规则，就需要从 Undo Log 中获取历史快照； 最后返回符合规则的数据。 InnoDB 中，MVCC 是通过 Undo Log + Read View 进行数据读取，Undo Log 保存了历史快照，而 Read View 规则帮我们判断当前版本的数据是否可见。 在隔离级别为读已提交（Read Commit）时，一个事务中的每一次 SELECT 查询都会获取一次 Read View 当隔离级别为可重复读的时候，就避免了不可重复读，这是因为一个事务只在第一次 SELECT 的时候会获取一次 Read View，而后面所有的 SELECT 都会复用这个 Read View innodb如何解决幻读的问题在可重复读的情况下，InnoDB 可以通过 Next-Key 锁 +MVCC 来解决幻读问题。 事务日志InnoDB 使用日志来减少提交事务时的开销。因为日志中已经记录了事务，就无须在每个事务提交时把缓冲池的脏块刷新(flush)到磁盘中。 事务修改的数据和索引通常会映射到表空间的随机位置，所以刷新这些变更到磁盘需要很多随机 IO。 InnoDB 假设使用常规磁盘，随机IO比顺序IO昂贵得多，因为一个IO请求需要时间把磁头移到正确的位置，然后等待磁盘上读出需要的部分，再转到开始位置。 InnoDB 用日志把随机IO变成顺序IO。一旦日志安全写到磁盘，事务就持久化了，即使断电了，InnoDB可以重放日志并且恢复已经提交的事务。 InnoDB 使用一个后台线程智能地刷新这些变更到数据文件。这个线程可以批量组合写入，使得数据写入更顺序，以提高效率。 事务日志可以帮助提高事务效率： 使用事务日志，存储引擎在修改表的数据时只需要修改其内存拷贝，再把该修改行为记录到持久在硬盘上的事务日志中，而不用每次都将修改的数据本身持久到磁盘。 事务日志采用的是追加的方式，因此写日志的操作是磁盘上一小块区域内的顺序I/O，而不像随机I/O需要在磁盘的多个地方移动磁头，所以采用事务日志的方式相对来说要快得多。 事务日志持久以后，内存中被修改的数据在后台可以慢慢刷回到磁盘。 如果数据的修改已经记录到事务日志并持久化，但数据本身没有写回到磁盘，此时系统崩溃，存储引擎在重启时能够自动恢复这一部分修改的数据。 目前来说，大多数存储引擎都是这样实现的，我们通常称之为预写式日志（Write-Ahead Logging），修改数据需要写两次磁盘。 事务的ACID是如何保证实现的事务的隔离性是通过锁实现，而事务的原子性、一致性和持久性则是通过事务日志实现 。 事务日志包括：重做日志redo和回滚日志undo redo log（重做日志） 实现持久化和原子性 在innoDB的存储引擎中，事务日志通过重做(redo)日志和innoDB存储引擎的日志缓冲(InnoDB Log Buffer)实现。事务开启时，事务中的操作，都会先写入存储引擎的日志缓冲中，在事务提交之前，这些缓冲的日志都需要提前刷新到磁盘上持久化，这就是DBA们口中常说的“日志先行”(Write-Ahead Logging)。当事务提交之后，在Buffer Pool中映射的数据文件才会慢慢刷新到磁盘。此时如果数据库崩溃或者宕机，那么当系统重启进行恢复时，就可以根据redo log中记录的日志，把数据库恢复到崩溃前的一个状态。未完成的事务，可以继续提交，也可以选择回滚，这基于恢复的策略而定。 在系统启动的时候，就已经为redo log分配了一块连续的存储空间，以顺序追加的方式记录Redo Log，通过顺序IO来改善性能。所有的事务共享redo log的存储空间，它们的Redo Log按语句的执行顺序，依次交替的记录在一起。 undo log（回滚日志） 实现一致性 undo log 主要为事务的回滚服务。在事务执行的过程中，除了记录redo log，还会记录一定量的undo log。undo log记录了数据在每个操作前的状态，如果事务执行过程中需要回滚，就可以根据undo log进行回滚操作。单个事务的回滚，只会回滚当前事务做的操作，并不会影响到其他的事务做的操作。 Undo记录的是已部分完成并且写入硬盘的未完成的事务，默认情况下回滚日志是记录下表空间中的（共享表空间或者独享表空间） 二种日志均可以视为一种恢复操作，redo_log是恢复提交事务修改的页操作，而undo_log是回滚行记录到特定版本。二者记录的内容也不同，redo_log是物理日志，记录页的物理修改操作，而undo_log是逻辑日志，根据每行记录进行记录。 又引出个问题：你知道MySQL 有多少种日志吗？ 错误日志：记录出错信息，也记录一些警告信息或者正确的信息。 查询日志：记录所有对数据库请求的信息，不论这些请求是否得到了正确的执行。 慢查询日志：设置一个阈值，将运行时间超过该值的所有SQL语句都记录到慢查询的日志文件中。 二进制日志：记录对数据库执行更改的所有操作。 中继日志：中继日志也是二进制日志，用来给slave 库恢复 事务日志：重做日志redo和回滚日志undo MySQL对分布式事务的支持分布式事务的实现方式有很多，既可以采用 InnoDB 提供的原生的事务支持，也可以采用消息队列来实现分布式事务的最终一致性。这里我们主要聊一下 InnoDB 对分布式事务的支持。 MySQL 从 5.0.3 InnoDB 存储引擎开始支持XA协议的分布式事务。一个分布式事务会涉及多个行动，这些行动本身是事务性的。所有行动都必须一起成功完成，或者一起被回滚。 在MySQL中，使用分布式事务涉及一个或多个资源管理器和一个事务管理器。 如图，MySQL 的分布式事务模型。模型中分三块：应用程序（AP）、资源管理器（RM）、事务管理器（TM）: 应用程序：定义了事务的边界，指定需要做哪些事务； 资源管理器：提供了访问事务的方法，通常一个数据库就是一个资源管理器； 事务管理器：协调参与了全局事务中的各个事务。 分布式事务采用两段式提交（two-phase commit）的方式： 第一阶段所有的事务节点开始准备，告诉事务管理器ready。 第二阶段事务管理器告诉每个节点是commit还是rollback。如果有一个节点失败，就需要全局的节点全部rollback，以此保障事务的原子性。 MySQL中的锁锁的分类从对数据操作的粒度分类：为了尽可能提高数据库的并发度，每次锁定的数据范围越小越好，理论上每次只锁定当前操作的数据的方案会得到最大的并发度，但是管理锁是很耗资源的事情（涉及获取，检查，释放锁等动作），因此数据库系统需要在高并发响应和系统性能两方面进行平衡，这样就产生了“锁粒度（Lock granularity）”的概念。 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低（MyISAM 和 MEMORY 存储引擎采用的是表级锁）； 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高（InnoDB 存储引擎既支持行级锁也支持表级锁，但默认情况下是采用行级锁）； 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 适用：从锁的角度来说，表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如Web应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用，如一些在线事务处理（OLTP）系统。 从对数据操作的类型分类： 读锁（共享锁）：针对同一份数据，多个读操作可以同时进行，不会互相影响 写锁（排他锁）：当前写操作没有完成前，它会阻断其他写锁和读锁 1234567891011121314151617181920mysql&gt; lock table dept read; //加上读锁Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from dept;+--------+----------+| deptno | deptname |+--------+----------+| 1 | tech || 2 | sale || 5 | fin || 3 | hr |+--------+----------+4 rows in set (0.00 sec)mysql&gt; update dept set deptno = 6 where deptname = 'tech'; //不能修改了ERROR 1099 (HY000): Table 'dept' was locked with a READ lock and can't be updatedmysql&gt; unlock table; //解锁Query OK, 0 rows affected (0.00 sec) MyISAM 的表锁有两种模式： 表共享读锁 （Table Read Lock）：不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求； 表独占写锁 （Table Write Lock）：会阻塞其他用户对同一表的读和写操作； MyISAM 表的读操作与写操作之间，以及写操作之间是串行的。当一个线程获得对一个表的写锁后， 只有持有锁的线程可以对表进行更新操作。 其他线程的读、 写操作都会等待，直到锁被释放为止。 默认情况下，写锁比读锁具有更高的优先级：当一个锁释放时，这个锁会优先给写锁队列中等候的获取锁请求，然后再给读锁队列中等候的获取锁请求。 InnoDB 实现了以下两种类型的行锁： 共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。 排他锁（X）：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。 为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB 还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁： 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的 IS 锁。 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的 IX 锁。 索引失效会导致行锁变表锁。比如 vchar 查询不写单引号的情况。 解释一下悲观锁和乐观锁乐观锁与悲观锁是两种并发控制的思想，可用于解决丢失更新问题 乐观锁会“乐观地”假定大概率不会发生并发更新冲突，访问、处理数据过程中不加锁，只在更新数据时再根据版本号或时间戳判断是否有冲突，有则处理，无则提交事务。用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式 乐观锁的版本号机制在表中设计一个版本字段 version，第一次读的时候，会获取 version 字段的取值。然后对数据进行更新或删除操作时，会执行UPDATE ... SET version=version+1 WHERE version=version。此时如果已经有事务对这条数据进行了更改，修改就不会成功。 这种方式类似我们熟悉的 SVN、CVS 版本管理系统，当我们修改了代码进行提交时，首先会检查当前版本号与服务器上的版本号是否一致，如果一致就可以直接提交，如果不一致就需要更新服务器上的最新代码，然后再进行提交。 悲观锁会“悲观地”假定大概率会发生并发更新冲突，访问、处理数据前就加排他锁，在整个数据处理过程中锁定数据，事务提交或回滚后才释放锁。另外与乐观锁相对应的，悲观锁是由数据库自己实现了的，要用的时候，我们直接调用数据库的相关语句就可以了。 从这两种锁的设计思想中，你能看出乐观锁和悲观锁的适用场景： 乐观锁适合读操作多的场景，相对来说写的操作比较少。它的优点在于程序实现，不存在死锁问题，不过适用场景也会相对乐观，因为它阻止不了除了程序以外的数据库操作。 悲观锁适合写操作多的场景，因为写的操作具有排它性。采用悲观锁的方式，可以在数据库层面阻止其他事务对该数据的操作权限，防止读 - 写和写 - 写的冲突。 InnoDB锁的模式 InnoDB 三种行锁的方式 **记录锁(Record Locks)**： 单个行记录上的锁。对索引项加锁，锁定符合条件的行。其他事务不能修改和删除加锁项； 1SELECT * FROM table WHERE id = 1 FOR UPDATE; 它会在 id=1 的记录上加上记录锁，以阻止其他事务插入，更新，删除 id=1 这一行 在通过 主键索引 与 唯一索引 对数据行进行 UPDATE 操作时，也会对该行数据加记录锁： 1UPDATE SET age = 50 WHERE id = 1; 间隙锁（Gap Locks）： 当我们使用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁。对于键值在条件范围内但并不存在的记录，叫做“间隙”。 InnoDB 也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁。 对索引项之间的“间隙”加锁，锁定记录的范围（对第一条记录前的间隙或最后一条将记录后的间隙加锁），不包含索引项本身。其他事务不能在锁范围内插入数据，这样就防止了别的事务新增幻影行。 间隙锁基于非唯一索引，它锁定一段范围内的索引记录。间隙锁基于下面将会提到的Next-Key Locking 算法，请务必牢记：使用间隙锁锁住的是一个区间，而不仅仅是这个区间中的每一条数据。 1SELECT * FROM table WHERE id BETWEN 1 AND 10 FOR UPDATE; 即所有在（1，10）区间内的记录行都会被锁住，所有id 为 2、3、4、5、6、7、8、9 的数据行的插入会被阻塞，但是 1 和 10 两条记录行并不会被锁住。 GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况 临键锁(Next-key Locks)： 临键锁，是记录锁与间隙锁的组合，它的封锁范围，既包含索引记录，又包含索引区间。(临键锁的主要目的，也是为了避免幻读(Phantom Read)。如果把事务的隔离级别降级为RC，临键锁则也会失效。) Next-Key 可以理解为一种特殊的间隙锁，也可以理解为一种特殊的算法。通过临建锁可以解决幻读的问题。 每个数据行上的非唯一索引列上都会存在一把临键锁，当某个事务持有该数据行的临键锁时，会锁住一段左开右闭区间的数据。需要强调的一点是，InnoDB 中行级锁是基于索引实现的，临键锁只与非唯一索引列有关，在唯一索引列（包括主键列）上不存在临键锁。 对于行的查询，都是采用该方法，主要目的是解决幻读的问题 死锁该如何解决死锁产生： 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环 当事务试图以不同的顺序锁定资源时，就可能产生死锁。多个事务同时锁定同一个资源时也可能会产生死锁 锁的行为和顺序和存储引擎相关。以同样的顺序执行语句，有些存储引擎会产生死锁有些不会——死锁有双重原因：真正的数据冲突；存储引擎的实现方式。 检测死锁：数据库系统实现了各种死锁检测和死锁超时的机制。InnoDB存储引擎能检测到死锁的循环依赖并立即返回一个错误。 死锁恢复：死锁发生以后，只有部分或完全回滚其中一个事务，才能打破死锁，InnoDB目前处理死锁的方法是，将持有最少行级排他锁的事务进行回滚。所以事务型应用程序在设计时必须考虑如何处理死锁，多数情况下只需要重新执行因死锁回滚的事务即可。 外部锁的死锁检测：发生死锁后，InnoDB 一般都能自动检测到，并使一个事务释放锁并回退，另一个事务获得锁，继续完成事务。但在涉及外部锁，或涉及表锁的情况下，InnoDB 并不能完全自动检测到死锁， 这需要通过设置锁等待超时参数 innodb_lock_wait_timeout 来解决 死锁影响性能：死锁会影响性能而不是会产生严重错误，因为InnoDB会自动检测死锁状况并回滚其中一个受影响的事务。在高并发系统上，当许多线程等待同一个锁时，死锁检测可能导致速度变慢。 有时当发生死锁时，禁用死锁检测（使用innodb_deadlock_detect配置选项）可能会更有效，这时可以依赖innodb_lock_wait_timeout设置进行事务回滚。 MyISAM避免死锁： 在自动加锁的情况下，MyISAM 总是一次获得 SQL 语句所需要的全部锁，所以 MyISAM 表不会出现死锁。 InnoDB避免死锁： 为了在单个InnoDB表上执行多个并发写入操作时避免死锁，可以在事务开始时通过为预期要修改的每个元祖（行）使用SELECT ... FOR UPDATE语句来获取必要的锁，即使这些行的更改语句是在之后才执行的。 在事务中，如果要更新记录，应该直接申请足够级别的锁，即排他锁，而不应先申请共享锁、更新时再申请排他锁，因为这时候当用户再申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁 如果事务需要修改或锁定多个表，则应在每个事务中以相同的顺序使用加锁语句。 在应用中，如果不同的程序会并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会 通过SELECT ... LOCK IN SHARE MODE获取行的读锁后，如果当前事务再需要对该记录进行更新操作，则很有可能造成死锁。 改变事务隔离级别 如果出现死锁，可以用 show engine innodb status;命令来确定最后一个死锁产生的原因。返回结果中包括死锁相关事务的详细信息，如引发死锁的 SQL 语句，事务已经获得的锁，正在等待什么锁，以及被回滚的事务等。据此可以分析死锁产生的原因和改进措施。 我们都不希望出现死锁的情况，可以采取一些方法避免死锁的发生： 如果事务涉及多个表，操作比较复杂，那么可以尽量一次锁定所有的资源，而不是逐步来获取，这样可以减少死锁发生的概率； 如果事务需要更新数据表中的大部分数据，数据表又比较大，这时可以采用锁升级的方式，比如将行级锁升级为表级锁，从而减少死锁产生的概率； 不同事务并发读写多张数据表，可以约定访问表的顺序，采用相同的顺序降低死锁发生的概率。 总结","link":"/2021/05/12/MySQL%EF%BC%88%E5%9B%9B%EF%BC%89%E4%BA%8B%E5%8A%A1%E5%92%8C%E9%94%81/"},{"title":"Netty（一）前世今生","text":"传统的HTTP服务器原理 创建一个ServerSocket，监听并绑定一个端口 一系列客户端来请求这个端口 服务器使用Accept，获得一个来自客户端的Socket连接对象 启动一个新线程处理连接 4.1 读Socket，得到字节流 4.2. 解码协议，得到Http请求对象 4.3 处理Http请求，得到一个结果，封装成一个HttpResponse对象 4.4 编码协议，将结果序列化字节流 写Socket，将字节流发给客户端 继续循环步骤3 HTTP服务器之所以称为HTTP服务器，是因为编码解码协议是HTTP协议，如果协议是Redis协议，那它就成了Redis服务器，如果协议是WebSocket，那它就成了WebSocket服务器，等等。 使用Netty你就可以定制编解码协议，实现自己的特定协议的服务器。 NIO上面是一个传统处理http的服务器，但是在高并发的环境下，线程数量会比较多，System load也会比较高，于是就有了NIO。 他并不是Java独有的概念，NIO代表的一个词汇叫着IO多路复用。它是由操作系统提供的系统调用，早期这个操作系统调用的名字是select，但是性能低下，后来渐渐演化成了Linux下的epoll和Mac里的kqueue。我们一般就说是epoll，因为没有人拿苹果电脑作为服务器使用对外提供服务。而Netty就是基于Java NIO技术封装的一套框架。为什么要封装，因为原生的Java NIO使用起来没那么方便，而且还有臭名昭著的bug，Netty把它封装之后，提供了一个易于操作的使用模式和接口，用户使用起来也就便捷多了。 说NIO之前先说一下BIO（Blocking IO）,如何理解这个Blocking呢？ 客户端监听（Listen）时，Accept是阻塞的，只有新连接来了，Accept才会返回，主线程才能继 读写socket时，Read是阻塞的，只有请求消息来了，Read才能返回，子线程才能继续处理 读写socket时，Write是阻塞的，只有客户端把消息收了，Write才能返回，子线程才能继续读取下一个请求","link":"/2021/05/24/Netty%EF%BC%88%E4%B8%80%EF%BC%89%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/"},{"title":"Netty（七）案例编写和调优参数","text":"","link":"/2021/05/27/Netty%EF%BC%88%E4%B8%83%EF%BC%89%E6%A1%88%E4%BE%8B%E7%BC%96%E5%86%99%E5%92%8C%E8%B0%83%E4%BC%98%E5%8F%82%E6%95%B0/"},{"title":"Netty（三）TCP粘包和半包","text":"本篇讲述了什么是TCP的粘包和半包以及Netty是如何解决的 什么是粘包和半包代码演示123456789101112131415161718192021222324252627282930313233343536373839//服务端代码public class TimeServer { public void bind(int port) throws Exception { //初始化两个 loopGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { //服务端初始化的 bootstrap ServerBootstrap s = new ServerBootstrap(); //加载两个 loopGroup s.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 1024)//设置 NioServerSocketChannel 的 TCP 参数，设置 backlog 为 1024 .childHandler(new ChildChannelHandler()); //绑定 IO 事件处理类 ChildChannelHandler。 //异步监听端口，同步等待关闭 ChannelFuture f = s.bind(port).sync(); f.channel().closeFuture().sync(); }finally { //关闭两个 loopGroup bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { int port = 8080; new TimeServer().bind(8080); }}public class ChildChannelHandler extends ChannelInitializer&lt;SocketChannel&gt; { @Override protected void initChannel(SocketChannel ch) throws Exception { //获取 channel 的 pipeline，这里仅仅加进尾端 ch.pipeline().addLast(new TimeServerHandler()); }} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253//客户端代码public class TimeClient { public void connect(int port, String host) { // 创建客户端处理 IO 读写的 NioEventLoopGroup 线程组 EventLoopGroup group = new NioEventLoopGroup(); try { // 创建客户端辅助启动类 Bootstrap b = new Bootstrap(); b.group(group).channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new FirstClientHandler()); } }); //调用connect发起异步请求，调用同步方法等待成功 ChannelFuture f = b.connect(host, port).sync(); f.channel().closeFuture().sync(); }catch (Exception e ) { }finally { group.shutdownGracefully(); } } public static void main(String[] args) { int port = 8080; new TimeClient().connect(port, &quot;127.0.0.1&quot;); }}public class FirstClientHandler extends ChannelInboundHandlerAdapter { @Override public void channelActive(ChannelHandlerContext ctx) { for (int i = 0; i &lt; 1000; i++) { ByteBuf buffer = getByteBuf(ctx); ctx.channel().writeAndFlush(buffer); } } private ByteBuf getByteBuf(ChannelHandlerContext ctx) { byte[] bytes = &quot;你好，我的名字是binshow!&quot;.getBytes(Charset.forName(&quot;utf-8&quot;)); ByteBuf buffer = ctx.alloc().buffer(); buffer.writeBytes(bytes); return buffer; } } 从服务端的控制台输出可以看出，存在三种类型的输出 一种是正常的字符串输出。 一种是多个字符串“粘”在了一起，我们定义这种 ByteBuf 为粘包。 一种是一个字符串被“拆”开，形成一个破碎的包，我们定义这种 ByteBuf 为半包。 原因分析 由图可以看出：发送端的字节流都会先传入缓冲区，再通过网络传入到接收端的缓冲区中，最终由接收端获取。 对于操作系统来说，只认TCP协议，尽管我们的应用层是按照 ByteBuf 为 单位来发送数据，server按照Bytebuf读取，但是到了底层操作系统仍然是按照字节流发送数据，因此，数据到了服务端，也是按照字节流的方式读入，然后到了 Netty 应用层面，重新拼装成ByteBuf，而这里的 ByteBuf 与客户端按顺序发送的 ByteBuf 可能是不对等的。因此，我们需要在客户端根据自定义协议来组装我们应用层的数据包，然后在服务端根据我们的应用层的协议来组装数据包，这个过程通常在服务端称为拆包，而在客户端称为粘包。 粘包的主要原因： 发送方每次写入数据 &lt; 套接字缓冲区大小 接收方读取套接字缓冲区数据不够及时 半包的主要原因： 发送方写入数据 &gt; 套接字缓冲区大小 发送的数据大于协议的MTU（最大传输单元），必须拆包 根本原因：TCP是流式协议，消息是无边界的。而UDP虽然一次运输多个包裹，但是每个包裹是有边界的，所以没有粘包等现象。 Netty对三种常用封帧方式的支持在没有 Netty 的情况下，用户如果自己需要拆包，基本原理就是不断从 TCP 缓冲区中读取数据，每次读取完都需要判断是否是一个完整的数据包 如果当前读取的数据不足以拼接成一个完整的业务数据包，那就保留该数据，继续从 TCP 缓冲区中读取，直到得到一个完整的数据包。 如果当前读到的数据加上已经读取的数据足够拼接成一个数据包，那就将已经读取的数据拼接上本次读取的数据，构成一个完整的业务数据包传递到业务逻辑，多余的数据仍然保留，以便和下次读到的数据尝试拼接。 而在Netty中，已经造好了许多类型的拆包器，我们直接用就好： FixedLengthFrameDecoder固定长度的拆包器 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class TimeClient { public void connect(int port, String host) { // 创建客户端处理 IO 读写的 NioEventLoopGroup 线程组 EventLoopGroup group = new NioEventLoopGroup(); try { // 创建客户端辅助启动类 Bootstrap b = new Bootstrap(); b.group(group).channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new FirstClientHandler()); ch.pipeline().addLast(new FixedLengthFrameDecoder(32)); // } }); //调用connect发起异步请求，调用同步方法等待成功 ChannelFuture f = b.connect(host, port).sync(); f.channel().closeFuture().sync(); }catch (Exception e ) { }finally { group.shutdownGracefully(); } } public static void main(String[] args) { int port = 8080; new TimeClient().connect(port, &quot;127.0.0.1&quot;); }}public class TimeServer { public void bind(int port) throws Exception { //初始化两个 loopGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { //服务端初始化的 bookstrap ServerBootstrap s = new ServerBootstrap(); //加载两个 loopGroup s.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 1024)//设置 NioServerSocketChannel 的 TCP 参数，设置 backlog 为 1024 //.childHandler(new ChildChannelHandler()); //绑定 IO 事件处理类 ChildChannelHandler。 .childHandler(new ChannelInitializer() { @Override protected void initChannel(Channel channel) throws Exception { ChannelPipeline pipeline = channel.pipeline(); pipeline.addLast(new FixedLengthFrameDecoder(32)); pipeline.addLast(new ChildChannelHandler()); } }); //异步监听端口，同步等待关闭 ChannelFuture f = s.bind(port).sync(); f.channel().closeFuture().sync(); }finally { //关闭两个 loopGroup bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { int port = 8080; new TimeServer().bind(8080); }} LineBasedFrameDecoder每个数据包之间以换行符作为分隔符。 1234567891011121314151617181920212223242526272829303132public class TimeServer { public void bind(int port) throws Exception { //初始化两个 loopGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { //服务端初始化的 bootstrap ServerBootstrap s = new ServerBootstrap(); //加载两个 loopGroup s.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 1024) .childHandler(new ChildChannelHandler()); //异步监听端口，同步等待关闭 ChannelFuture f = s.bind(port).sync(); f.channel().closeFuture().sync(); }finally { //关闭两个 loopGroup bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { int port = 8080; new TimeServer().bind(8080); }} 12345678910111213public class ChildChannelHandler extends ChannelInitializer&lt;SocketChannel&gt; { @Override protected void initChannel(SocketChannel ch) throws Exception { // 以行为编解码基础的 LineBasedFrameDecoder，并设置最大行字节为 1024 ch.pipeline().addLast(new LineBasedFrameDecoder(1024)); //2使用 StringDecoder 将链条中的前结点编解码结果解码为字符串 ch.pipeline().addLast(new StringDecoder()); //3.根据链条中的前结点的编解码结果进行业务逻辑处理 ch.pipeline().addLast(new TimeServerHandler()); }} 12345678910111213141516171819202122232425262728293031323334public class TimeClient { public void connect(int port, String host) { // 创建客户端处理 IO 读写的 NioEventLoopGroup 线程组 EventLoopGroup group = new NioEventLoopGroup(); try { // 创建客户端辅助启动类 Bootstrap b = new Bootstrap(); b.group(group).channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new LineBasedFrameDecoder(1024)); ch.pipeline().addLast(new StringDecoder()); ch.pipeline().addLast(new TimeClientHandler()); } }); //调用connect发起异步请求，调用同步方法等待成功 ChannelFuture f = b.connect(host, port).sync(); f.channel().closeFuture().sync(); }catch (Exception e ) { }finally { group.shutdownGracefully(); } } public static void main(String[] args) { int port = 8080; new TimeClient().connect(port, &quot;127.0.0.1&quot;); }} 123456789101112131415161718192021222324252627282930313233343536373839public class TimeClientHandler extends ChannelInboundHandlerAdapter { //日志记录 private static final Logger logger = Logger.getLogger(TimeClientHandler.class.getName()); private int counter; private byte[] req; public TimeClientHandler() { req = (&quot;QUERY TIME ORDER&quot; + System.getProperty(&quot;line.separator&quot;)).getBytes(); } @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { ByteBuf message = null; for (int i=0;i&lt;100;i++) { message= Unpooled.buffer(req.length); message.writeBytes(req); ctx.writeAndFlush(message); } } //当客户端和服务端tcp链路建立成功之后，netty的nio线程会调用channelActive方法 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { String body = (String) msg; System.out.println(&quot;now is : &quot; + body + &quot; ; the counter is : &quot; + ++counter); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { logger.warning(&quot;unexpected exception from downstream: &quot; + cause.getMessage()); ctx.close(); }} DelimiterBasedFrameDecoder1234567891011121314151617181920212223242526272829303132333435363738394041public class EchoServer { public void bind(int port) throws Exception { EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { ServerBootstrap s = new ServerBootstrap(); s.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 1024) //添加日志 .handler(new LoggingHandler()) //处理 .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { protected void initChannel(SocketChannel ch) throws Exception { //根据 &quot;$_&quot; 作为分隔符，然后进行分割 ByteBuf delimitoer = Unpooled.copiedBuffer(&quot;$_&quot;.getBytes()); //根据指定分隔符来切割信息流的开始和结束 ch.pipeline().addLast(new DelimiterBasedFrameDecoder(1024, delimitoer)); //字符串解码 ch.pipeline().addLast(new StringDecoder()); //将字符传递给服务端处理器 ch.pipeline().addLast(new EchoServerHandler()); } }); ChannelFuture f = s.bind(port).sync(); f.channel().closeFuture().sync(); }catch (Exception e) { }finally { bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { new EchoServer().bind(8080); }} 12345678910111213141516171819public class EchoServerHandler extends ChannelInboundHandlerAdapter { int count = 0; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { String body = (String) msg; System.out.println(&quot;this is &quot;+ ++count + &quot; times receive client:[&quot; + body + &quot;]&quot; ); body += &quot;$_&quot;; ByteBuf echo = Unpooled.copiedBuffer(body.getBytes()); ctx.writeAndFlush(echo); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); ctx.close(); }} 123456789101112131415161718192021222324252627282930313233public class EchoClient { public void connect(int port, String host) { // 创建客户端处理 IO 读写的 NioEventLoopGroup 线程组 EventLoopGroup group = new NioEventLoopGroup(); try { // 创建客户端辅助启动类 Bootstrap b = new Bootstrap(); b.group(group).channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { protected void initChannel(SocketChannel ch) throws Exception { ByteBuf delimitoer = Unpooled.copiedBuffer(&quot;$_&quot;.getBytes()); ch.pipeline().addLast(new DelimiterBasedFrameDecoder(1024, delimitoer)); ch.pipeline().addLast(new StringDecoder()); ch.pipeline().addLast(new EchoClientHandler()); } }); //调用connect发起异步请求，调用同步方法等待成功 ChannelFuture f = b.connect(host, port).sync(); f.channel().closeFuture().sync(); }catch (Exception e ) { }finally { group.shutdownGracefully(); } } public static void main(String[] args) { new EchoClient().connect(8080, &quot;127.0.0.1&quot;); }} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class EchoClient { public void connect(int port, String host) { // 创建客户端处理 IO 读写的 NioEventLoopGroup 线程组 EventLoopGroup group = new NioEventLoopGroup(); try { // 创建客户端辅助启动类 Bootstrap b = new Bootstrap(); b.group(group).channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { protected void initChannel(SocketChannel ch) throws Exception { ByteBuf delimitoer = Unpooled.copiedBuffer(&quot;$_&quot;.getBytes()); ch.pipeline().addLast(new DelimiterBasedFrameDecoder(1024, delimitoer)); ch.pipeline().addLast(new StringDecoder()); ch.pipeline().addLast(new EchoClientHandler()); } }); //调用connect发起异步请求，调用同步方法等待成功 ChannelFuture f = b.connect(host, port).sync(); f.channel().closeFuture().sync(); }catch (Exception e ) { }finally { group.shutdownGracefully(); } } public static void main(String[] args) { new EchoClient().connect(8080, &quot;127.0.0.1&quot;); }}//而客户端处理器需要输出结果，查看是不是有分隔符没分割成功的问题public class EchoClientHandler extends ChannelInboundHandlerAdapter { private int counter; static final String ECHO_REQ = &quot;Hi, Lilinfeng. welcome to Netty.$_&quot;; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { for (int i=0;i &lt; 100;i++) { ctx.writeAndFlush(Unpooled.copiedBuffer(ECHO_REQ.getBytes())); } } @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { System.out.println(&quot;this is &quot;+ ++counter + &quot; times receive server :[&quot; + msg + &quot;]&quot;); } @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { ctx.flush(); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); ctx.close(); }} 源码解读","link":"/2021/05/24/Netty%EF%BC%88%E4%B8%89%EF%BC%89TCP%E7%B2%98%E5%8C%85%E5%92%8C%E5%8D%8A%E5%8C%85/"},{"title":"Netty（八）优化和安全","text":"","link":"/2021/05/27/Netty%EF%BC%88%E5%85%AB%EF%BC%89%E4%BC%98%E5%8C%96%E5%92%8C%E5%AE%89%E5%85%A8/"},{"title":"Netty（四）编解码技术","text":"本篇主要讲述了 二次编码 keepalive和idle检测 为什么需要二次编码我们把解决半包和粘包问题的常用三种解码器叫做一次解码器，这层解码的结果是字节。 我们在项目中使用的是对象，需要和字节进行相互转换，所以二次解码器就是将字节转换成对象。相对应的编码器就是将Java对象转换成字节流方便存储或传输。 一次解码器：ByteToMessageDecoder io.netty.buffer.ByteBuf(原始数据流,可能出现粘包或半包) –&gt; io.netty.buffer.ByteBuf(用户数据的字节数组) 二次解码器：MessageToMessageDecoder io.netty.buffer.ByteBuf(用户数据的字节数组) –&gt; java Object 常用的二次编解码方式 Java序列号 XML JSON ProtoBuf Marshaling 选择编解码方式的特点 压缩后的空间大小 编解码的时间速度 可读性 多语言的支持 Protobuf的简介和使用源码解读12345678public class ByteArrayDecoder extends MessageToMessageDecoder&lt;ByteBuf&gt; { @Override protected void decode(ChannelHandlerContext ctx, ByteBuf msg, List&lt;Object&gt; out) throws Exception { // copy the ByteBuf content to a byte array //将netty的ByteBuf 转换成JDK中的字节数组 out.add(ByteBufUtil.getBytes(msg)); }} Keepalive和Idle检测为什么需要keepalive生活场景类比： 订餐电话场景 服务器应用 电话线路 数据连接（TCP 连接） 交谈的话语 数据 通话双方 数据发送和接收方 对比\\场景 订餐电话场景 服务器应用 需要 keepalive 的场景 对方临时着急走开 对端异常“崩溃” 对方在，但是很忙，不知道什么时候忙完 对端在，但是处理不过来 电话线路故障 对端在，但是不可达 不做 keepalive 的后果 线路占用，耽误其他人订餐 连接已坏，但是还浪费资源维持，下次直接用会直接报错 如何设计keepalive？以TCP中为例TCP keepalive 核心参数： # sysctl -a|grep tcp_keepalive net.ipv4.tcp_keepalive_time = 7200 net.ipv4.tcp_keepalive_intvl = 75 net.ipv4.tcp_keepalive_probes = 9 当启用（默认关闭）keepalive 时，TCP 在连接没有数据通过的7200秒后发送 keepalive 消息，当探测没有确认时，按75秒的重试频率重发，一直发 9 个探测包都没有确认，就认定连接失效。 所以总耗时一般为：2 小时 11 分钟 (7200 秒 + 75 秒* 9 次) 为什么应用层还需要keepalive ? 协议分层，各层关注点不同：传输层关注是否“通”，应用层关注是否可服务？ 类比前面的电话订餐例子，电话能通，不代表有人接；服务器连接在，但是不定可以服务（例如服务不过来等）。 TCP 层的 keepalive 默认关闭，且经过路由等中转设备 keepalive 包可能会被丢弃。 TCP 层的 keepalive 时间太长：默认 &gt; 2 小时，虽然可改，但属于系统参数，改动影响所有应用。 HTTP 属于应用层协议，但是常常听到名词“ HTTP Keep-Alive ”指的是对长连接和短连接的选择： • Connection : Keep-Alive 长连接（HTTP/1.1 默认长连接，不需要带这个 header） • Connection : Close 短连接 idle检测是什么 Idle 监测，只是负责诊断，诊断后，做出不同的行为，决定 Idle 监测的最终用途： 发送 keepalive :一般用来配合 keepalive ，减少 keepalive 消息。 Keepalive 设计演进：V1 定时 keepalive 消息 -&gt; V2 空闲监测 + 判定为 Idle 时才发keepalive。 V1：keepalive 消息与服务器正常消息交换完全不关联，定时就发送； V2：有其他数据传输的时候，不发送 keepalive ，无数据传输超过一定时间，判定为 Idle，再发 keepalive 。 如何开启keepalive和idle1234567891011在server端： //构造引导器实例ServerBootstrap ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) //设置NIO的channel .option(ChannelOption.SO_BACKLOG, 100) .handler(new LoggingHandler(LogLevel.INFO)) //两种设置keepalive风格 .childOption(ChannelOption.SO_KEEPALIVE, true) .childOption(NioChannelOption.SO_KEEPALIVE, true)","link":"/2021/05/25/Netty%EF%BC%88%E5%9B%9B%EF%BC%89%E7%BC%96%E8%A7%A3%E7%A0%81%E6%8A%80%E6%9C%AF/"},{"title":"Netty（五）锁和内存使用","text":"本篇讲述了Netty中对锁的正确使用和对内存的分配原则 Netty中如何正确的使用锁锁的对象和范围–减少粒度1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950=======ServerBootstrap=======@Override void init(Channel channel) throws Exception { final Map&lt;ChannelOption&lt;?&gt;, Object&gt; options = options0();//无线程安全问题 synchronized (options) { //针对这两种属性来锁 setChannelOptions(channel, options, logger); } final Map&lt;AttributeKey&lt;?&gt;, Object&gt; attrs = attrs0(); synchronized (attrs) { for (Entry&lt;AttributeKey&lt;?&gt;, Object&gt; e: attrs.entrySet()) { @SuppressWarnings(&quot;unchecked&quot;) AttributeKey&lt;Object&gt; key = (AttributeKey&lt;Object&gt;) e.getKey(); channel.attr(key).set(e.getValue()); } } ChannelPipeline p = channel.pipeline(); final EventLoopGroup currentChildGroup = childGroup; final ChannelHandler currentChildHandler = childHandler; final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] currentChildOptions; final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] currentChildAttrs; synchronized (childOptions) { currentChildOptions = childOptions.entrySet().toArray(newOptionArray(0)); } synchronized (childAttrs) { currentChildAttrs = childAttrs.entrySet().toArray(newAttrArray(0)); } p.addLast(new ChannelInitializer&lt;Channel&gt;() { @Override public void initChannel(final Channel ch) throws Exception { final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); if (handler != null) { pipeline.addLast(handler); } ch.eventLoop().execute(new Runnable() { @Override public void run() { pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); } }); } }); } 锁的对象本身大小–减少空间占用12345678910111213141516171819=====ChannelOutboundBuffer===== private static final AtomicLongFieldUpdater&lt;ChannelOutboundBuffer&gt; TOTAL_PENDING_SIZE_UPDATER = AtomicLongFieldUpdater.newUpdater(ChannelOutboundBuffer.class, &quot;totalPendingSize&quot;); private volatile long totalPendingSize; //统计待发送的字节数 private static final AtomicIntegerFieldUpdater&lt;ChannelOutboundBuffer&gt; UNWRITABLE_UPDATER = AtomicIntegerFieldUpdater.newUpdater(ChannelOutboundBuffer.class, &quot;unwritable&quot;); private void incrementPendingOutboundBytes(long size, boolean invokeLater) { if (size == 0) { return; } long newWriteBufferSize = TOTAL_PENDING_SIZE_UPDATER.addAndGet(this, size); if (newWriteBufferSize &gt; channel.config().getWriteBufferHighWaterMark()) { setUnwritable(invokeLater); } } Atomic long VS long： 前者是一个对象，包含对象头（object header）以用来保存 hashcode、lock 等信息，32 位系统占用8字节；64 位系统占 16 字节，所以在 64 位系统情况下： volatile long = 8 bytes AtomicLong = 8 bytes （volatile long）+ 16bytes （对象头）+ 8 bytes (引用) = 32 bytes至少节约 24 字节! 结论：Atomic* objects -&gt; Volatile primary type + Static Atomic*FieldUpdater 锁的速度–提高并发性记录内存分配字节数等功能用到的 LongCounter（io.netty.util.internal.PlatformDependent#newLongCounter() ） 123456789101112131415161718=====PlatformDependent=====public static LongCounter newLongCounter() { if (javaVersion() &gt;= 8) { //判断JDK的版本 return new LongAdderCounter(); } else { return new AtomicLongCounter(); } }//继承JDK中的 LongAdder，在高并发下比较优秀final class LongAdderCounter extends LongAdder implements LongCounter { @Override public long value() { return longValue(); }} 不同场景选择不同的并发包例1：关闭和等待关闭事件执行器（Event Executor）： Object.wait/notify -&gt; CountDownLatch 1234567=====SingleThreadEventExecutor=====private volatile ThreadProperties threadProperties; private final Executor executor; private volatile boolean interrupted; private final CountDownLatch threadLock = new CountDownLatch(1); private final Set&lt;Runnable&gt; shutdownHooks = new LinkedHashSet&lt;Runnable&gt;(); 例2：Nio Event loop中负责存储task的Queue Jdk’s LinkedBlockingQueue (MPMC) -&gt; jctools’ MPSC 123456789io.netty.util.internal.PlatformDependent.Mpsc#newMpscQueue(int)：static &lt;T&gt; Queue&lt;T&gt; newMpscQueue(final int maxCapacity) { // Calculate the max capacity which can not be bigger then MAX_ALLOWED_MPSC_CAPACITY. // This is forced by the MpscChunkedArrayQueue implementation as will try to round it // up to the next power of two and so will overflow otherwise. final int capacity = max(min(maxCapacity, MAX_ALLOWED_MPSC_CAPACITY), MIN_MAX_MPSC_CAPACITY); return USE_MPSC_CHUNKED_ARRAY_QUEUE ? new MpscChunkedArrayQueue&lt;T&gt;(MPSC_CHUNK_SIZE, capacity) : new MpscGrowableAtomicArrayQueue&lt;T&gt;(MPSC_CHUNK_SIZE, capacity); } 避免用锁 Netty 应用场景下：局部串行 + 整体并行 &gt; 一个队列 + 多个线程模式: 降低用户开发难度、逻辑简单、提升处理性能 避免锁带来的上下文切换和并发保护等额外开销 避免用锁：用 ThreadLocal 来避免资源争用，例如 Netty 轻量级的线程池实现 Netty如何使用内存减少对像本身大小 用基本类型就不要用包装类 应该定义成类变量的不要定义为实例变量 对分配内存进行预估 对于已经可以预知固定 size 的 HashMap避免扩容 1234567891011121314=====com.google.common.collect.Maps#newHashMapWithExpectedSize===== public static &lt;K, V&gt; HashMap&lt;K, V&gt; newHashMapWithExpectedSize(int expectedSize) { return new HashMap(capacity(expectedSize)); } static int capacity(int expectedSize) { if (expectedSize &lt; 3) { CollectPreconditions.checkNonnegative(expectedSize, &quot;expectedSize&quot;); return expectedSize + 1; } else { return expectedSize &lt; 1073741824 ? (int)((float)expectedSize / 0.75F + 1.0F) : 2147483647; } } Netty 根据接受到的数据动态调整（guess）下个要分配的 Buffer 的大小 123456789101112131415161718192021222324252627======io.netty.channel.AdaptiveRecvByteBufAllocator====== /** * 接受数据buffer的容量会尽可能的足够大以接受数据 * 也尽可能的小以不会浪费它的空间 * @param actualReadBytes */ private void record(int actualReadBytes) { //尝试是否可以减小分配的空间仍然能满足需求： //尝试方法：当前实际读取的size是否小于或等于打算缩小的尺寸 if (actualReadBytes &lt;= SIZE_TABLE[max(0, index - INDEX_DECREMENT - 1)]) { //decreaseNow: 连续2次尝试减小都可以 if (decreaseNow) { //减小 index = max(index - INDEX_DECREMENT, minIndex); nextReceiveBufferSize = SIZE_TABLE[index]; decreaseNow = false; } else { decreaseNow = true; } //判断是否实际读取的数据大于等于预估的，如果是，尝试扩容 } else if (actualReadBytes &gt;= nextReceiveBufferSize) { index = min(index + INDEX_INCREMENT, maxIndex); nextReceiveBufferSize = SIZE_TABLE[index]; decreaseNow = false; } } Zero-Copy零拷贝 使用逻辑组合，代替实际复制。 12345678910111213141516171819202122232425262728293031323334=====io.netty.handler.codec.ByteToMessageDecoder======public static final Cumulator COMPOSITE_CUMULATOR = new Cumulator() { @Override public ByteBuf cumulate(ByteBufAllocator alloc, ByteBuf cumulation, ByteBuf in) { ByteBuf buffer; try { if (cumulation.refCnt() &gt; 1) { buffer = expandCumulation(alloc, cumulation, in.readableBytes()); buffer.writeBytes(in); } else { CompositeByteBuf composite; //创建composite bytebuf，如果已经创建过，就不用了 if (cumulation instanceof CompositeByteBuf) { composite = (CompositeByteBuf) cumulation; } else { composite = alloc.compositeBuffer(Integer.MAX_VALUE); composite.addComponent(true, cumulation); } //避免内存复制 composite.addComponent(true, in); in = null; buffer = composite; } return buffer; } finally { if (in != null) { // We must release if the ownership was not transferred as otherwise it may produce a leak if // writeBytes(...) throw for whatever release (for example because of OutOfMemoryError). in.release(); } } } }; 使用包装，代替实际复制。 12byte[] bytes = data.getBytes(); ByteBuf byteBuf = Unpooled.wrappedBuffer(bytes); 调用 JDK 的 Zero-Copy 接口。 Netty 中也通过在 DefaultFileRegion 中包装了 NIO 的 FileChannel.transferTo() 方法实现了零拷贝：io.netty.channel.DefaultFileRegion#transferTo 1234567891011121314151617181920212223242526272829@Override public long transferTo(WritableByteChannel target, long position) throws IOException { long count = this.count - position; if (count &lt; 0 || position &lt; 0) { throw new IllegalArgumentException( &quot;position out of range: &quot; + position + &quot; (expected: 0 - &quot; + (this.count - 1) + ')'); } if (count == 0) { return 0L; } if (refCnt() == 0) { throw new IllegalReferenceCountException(0); } // Call open to make sure fc is initialized. This is a no-oop if we called it before. open(); long written = file.transferTo(this.position + position, count, target);//这里调用了 if (written &gt; 0) { transferred += written; } else if (written == 0) { // If the amount of written data is 0 we need to check if the requested count is bigger then the // actual file itself as it may have been truncated on disk. // // See https://github.com/netty/netty/issues/8868 validate(this, position); } return written; } 堆外内存 优点： 更广阔的“空间 ”，缓解店铺内压力 -&gt; 破除堆空间限制，减轻 GC 压力 减少“冗余”细节（假设烧烤过程为了气氛在室外进行：烤好直接上桌：vs 烤好还要进店内）-&gt; 避免复制 缺点： 需要搬桌子 -&gt; 创建速度稍慢 受城管管、风险大 -&gt; 堆外内存受操作系统管理 内存池为什么引入对象池： 创建对象开销大 对象高频率创建且可复用 支持并发又能保护系统 维护、共享有限的资源 如何实现对象池？ 开源实现：Apache Commons Pool Netty 轻量级对象池实现 io.netty.util.Recycler 源码解读12345678910111213141516171819202122232425262728293031323334353637383940//主从模型 EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); //自定义业务逻辑 ChannelHandler final EchoServerHandler serverHandler = new EchoServerHandler(); try { //构造引导器实例ServerBootstrap ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) //设置NIO的channel .option(ChannelOption.SO_BACKLOG, 100) .handler(new LoggingHandler(LogLevel.INFO)) //两种设置keepalive风格 .childOption(ChannelOption.SO_KEEPALIVE, true) .childOption(NioChannelOption.SO_KEEPALIVE, true) //切换到unpooled的方式之一,切换内存池参数之一 .childOption(ChannelOption.ALLOCATOR, UnpooledByteBufAllocator.DEFAULT) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override public void initChannel(SocketChannel ch) throws Exception { ChannelPipeline p = ch.pipeline(); if (sslCtx != null) { p.addLast(sslCtx.newHandler(ch.alloc())); } p.addLast(new LoggingHandler(LogLevel.INFO)); p.addLast(serverHandler); } }); // Start the server. ChannelFuture f = b.bind(PORT).sync(); // Wait until the server socket is closed. f.channel().closeFuture().sync(); } finally { // Shut down all event loops to terminate all threads. bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } 12345678910111213141516171819202122232425262728293031323334353637383940414243=====DefaultChannelConfig====== //默认bytebuf分配器 private volatile ByteBufAllocator allocator = ByteBufAllocator.DEFAULT;======ByteBufUtil====== private static final byte WRITE_UTF_UNKNOWN = (byte) '?'; private static final int MAX_CHAR_BUFFER_SIZE; private static final int THREAD_LOCAL_BUFFER_SIZE; private static final int MAX_BYTES_PER_CHAR_UTF8 = (int) CharsetUtil.encoder(CharsetUtil.UTF_8).maxBytesPerChar(); static final int WRITE_CHUNK_SIZE = 8192; static final ByteBufAllocator DEFAULT_ALLOCATOR; static { //以io.netty.allocator.type为准，没有的话，安卓平台用非池化实现，其他用池化实现 //读取io.netty.allocator.typ，如果没有的话就看是不是安卓平台 String allocType = SystemPropertyUtil.get( &quot;io.netty.allocator.type&quot;, PlatformDependent.isAndroid() ? &quot;unpooled&quot; : &quot;pooled&quot;); allocType = allocType.toLowerCase(Locale.US).trim(); ByteBufAllocator alloc; if (&quot;unpooled&quot;.equals(allocType)) { alloc = UnpooledByteBufAllocator.DEFAULT; logger.debug(&quot;-Dio.netty.allocator.type: {}&quot;, allocType); } else if (&quot;pooled&quot;.equals(allocType)) { alloc = PooledByteBufAllocator.DEFAULT; logger.debug(&quot;-Dio.netty.allocator.type: {}&quot;, allocType); } else { //io.netty.allocator.type设置的不是&quot;unpooled&quot;或者&quot;pooled&quot;，就用池化实现。 alloc = PooledByteBufAllocator.DEFAULT; logger.debug(&quot;-Dio.netty.allocator.type: pooled (unknown: {})&quot;, allocType); } DEFAULT_ALLOCATOR = alloc; //默认使用了池化的技术 THREAD_LOCAL_BUFFER_SIZE = SystemPropertyUtil.getInt(&quot;io.netty.threadLocalDirectBufferSize&quot;, 0); logger.debug(&quot;-Dio.netty.threadLocalDirectBufferSize: {}&quot;, THREAD_LOCAL_BUFFER_SIZE); MAX_CHAR_BUFFER_SIZE = SystemPropertyUtil.getInt(&quot;io.netty.maxThreadLocalCharBufferSize&quot;, 16 * 1024); logger.debug(&quot;-Dio.netty.maxThreadLocalCharBufferSize: {}&quot;, MAX_CHAR_BUFFER_SIZE); } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061===== PooledDirectByteBuf ===== final class PooledDirectByteBuf extends PooledByteBuf&lt;ByteBuffer&gt; { private static final Recycler&lt;PooledDirectByteBuf&gt; RECYCLER = new Recycler&lt;PooledDirectByteBuf&gt;() { @Override protected PooledDirectByteBuf newObject(Handle&lt;PooledDirectByteBuf&gt; handle) { return new PooledDirectByteBuf(handle, 0); } }; //从“池”里借一个用 static PooledDirectByteBuf newInstance(int maxCapacity) { PooledDirectByteBuf buf = RECYCLER.get(); buf.reuse(maxCapacity); return buf; } ===== RECYCLER ===== public final T get() { if (maxCapacityPerThread == 0) { //表明没有开启池化 return newObject((Handle&lt;T&gt;) NOOP_HANDLE); } Stack&lt;T&gt; stack = threadLocal.get(); DefaultHandle&lt;T&gt; handle = stack.pop(); //试图从“池”中取出一个，没有就新建一个 if (handle == null) { handle = stack.newHandle(); handle.value = newObject(handle); } return (T) handle.value; } static final class DefaultHandle&lt;T&gt; implements Handle&lt;T&gt; { private int lastRecycledId; private int recycleId; boolean hasBeenRecycled; private Stack&lt;?&gt; stack; private Object value; DefaultHandle(Stack&lt;?&gt; stack) { this.stack = stack; } @Override public void recycle(Object object) { if (object != value) { throw new IllegalArgumentException(&quot;object does not belong to handle&quot;); } Stack&lt;?&gt; stack = this.stack; if (lastRecycledId != recycleId || stack == null) { throw new IllegalStateException(&quot;recycled already&quot;); } //释放用完的对象到池里面去 stack.push(this); } } 如何切换堆内内存和堆外内存方法 1：参数设置 io.netty.noPreferDirect = true; 方法 2：传入构造参数false ServerBootstrap serverBootStrap = new ServerBootstrap(); UnpooledByteBufAllocator unpooledByteBufAllocator = new UnpooledByteBufAllocator(false); serverBootStrap.childOption(ChannelOption.ALLOCATOR, unpooledByteBufAllocator) 123456789101112public static final UnpooledByteBufAllocator DEFAULT = new UnpooledByteBufAllocator(PlatformDependent.directBufferPreferred());====== PlatformDependent ======// We should always prefer direct buffers by default if we can use a Cleaner to release direct buffers. //使用堆外内存两个条件：1 有cleaner方法去释放堆外内存； 2 io.netty.noPreferDirect 不能设置为true DIRECT_BUFFER_PREFERRED = CLEANER != NOOP &amp;&amp; !SystemPropertyUtil.getBoolean(&quot;io.netty.noPreferDirect&quot;, false);//参数指定 if (logger.isDebugEnabled()) { logger.debug(&quot;-Dio.netty.noPreferDirect: {}&quot;, !DIRECT_BUFFER_PREFERRED); } 堆外内存的分配？ ByteBuffer.allocateDirect(initialCapacity)","link":"/2021/05/25/Netty%EF%BC%88%E4%BA%94%EF%BC%89%E9%94%81%E5%92%8C%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8/"},{"title":"Redis（七）统计模式和扩展数据类型","text":"围绕这四种场景： 手机 App 中的每天的用户登录信息：一天对应一系列用户 ID 或移动设备 ID； 电商网站上商品的用户评论列表：一个商品对应了一系列的评论； 用户在手机 App 上的签到打卡信息：一天对应一系列用户的签到记录； 应用网站上的网页访问信息：一个网页对应一系列的访问点击。 集合统计模式聚合统计Set所谓的聚合统计，就是指统计多个集合元素的聚合结果，包括： 统计多个集合的共有元素（交集统计）； 把两个集合相比，统计其中一个集合独有的元素（差集统计）； 统计多个集合的所有元素（并集统计）。 统计每天的新增用户数和第二天的留存用户数，正好对应了聚合统计，用两个集合：一个集合记录所有登录过 App 的用户 ID，同时，用另一个集合记录每一天登录过 App 的用户 ID。然后，再对这两个集合做聚合统计。 举例：userid 为key ，value 为累积所有用户的id userid+日期为可以，value为当日用户id 12345678910111213141516171819//redis127.0.0.1:6379&gt; sadd userid 10001 10002 // 向 userid 的集合里面加上10001，10002(integer) 2127.0.0.1:6379&gt; scard userid //目前累积用户id为2个(integer) 2127.0.0.1:6379&gt; sadd userid:20210515 10023 10024 10025 //20210515的时候有三个用户id访问了，放入一个集合中(integer) 3127.0.0.1:6379&gt; sdiff userid userid:20210515 //比较两个集合的差异1) &quot;10001&quot;2) &quot;10002&quot;127.0.0.1:6379&gt; sunionstore userid userid userid:20210515 //将userid和userid:20210515的集合取并集放入userid中(integer) 5 //目前累积用户数为5个了127.0.0.1:6379&gt; scard userid(integer) 5127.0.0.1:6379&gt; sadd userid:20210516 10001 10023 10025 10031 10032 //20210516又有这些id访问了(integer) 5127.0.0.1:6379&gt; sdiffstore user:new userid userid:20210516 //求0516的新增用户数：取差集并保存在user:new 的集合中(integer) 2127.0.0.1:6379&gt; Set 的差集、并集和交集的计算复杂度较高，在数据量较大的情况下，如果直接执行这些计算，会导致 Redis 实例阻塞。所以，我给你分享一个小建议：你可以从主从集群中选择一个从库，让它专门负责聚合计算，或者是把数据读取到客户端，在客户端来完成聚合统计，这样就可以规避阻塞主库实例和其他从库实例的风险了。 排序统计Sorted Set最新评论列表包含了所有评论中的最新留言，这就要求集合类型能对元素保序，也就是说，集合中的元素可以按序排列，这种对元素保序的集合类型叫作有序集合。 List 是按照元素进入 List 的顺序进行排序的，而 Sorted Set 可以根据元素的权重来排序，我们可以自己来决定每个元素的权重值。比如说，我们可以根据元素插入 Sorted Set的时间确定权重值，先插入的元素权重小，后插入的元素权重大。 List12345678910111213141516171819127.0.0.1:6379&gt; lpush product1 A B C D E F //product1 商品有6个评论分别为 A B C D E F(integer) 5127.0.0.1:6379&gt; lrange product1 0 2 //展示第一页的 3 个评论时1) &quot;F&quot;2) &quot;E&quot;3) &quot;D&quot;127.0.0.1:6379&gt; lrange product1 3 5 //获取第二页的 3 个评论，1) &quot;C&quot;2) &quot;B&quot;3) &quot;A&quot;//此时如果新添加了一个评论G127.0.0.1:6379&gt; lpush product1 G(integer) 7127.0.0.1:6379&gt; lrange product1 3 5 ////获取第二页的 3 个评论就发生变化了1) &quot;D&quot;2) &quot;C&quot;3) &quot;B&quot;127.0.0.1:6379&gt; Sorted Set按评论时间的先后给每条评论设置一个权重值，然后再把评论保存到 Sorted Set中。 1ZRANGEBYSCORE comments N-9 N //返回最新的10条评论 所以，在面对需要展示最新列表、排行榜等场景时，如果数据更新频繁或者需要分页显示，建议你优先考虑使用 Sorted Set。 二值状态统计BitMap在签到统计时，每个用户一天的签到用 1 个 bit 位就能表示，一个月（假设是 31 天）的签到情况用 31 个 bit 位就可以，而一年的签到也只需要用 365 个 bit 位，根本不用太复杂的集合类型。这个时候，我们就可以选择 Bitmap。这是 Redis 提供的扩展数据类型。我来给你解释一下它的实现原理。 基数统计","link":"/2021/05/15/Redis%EF%BC%88%E4%B8%83%EF%BC%89%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%92%8C%E6%89%A9%E5%B1%95%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"},{"title":"Redis（一）数据结构类型","text":"Redis为什么这么快呢？ 基于内存操作 优化后的数据结构 单线程 键和值的保存方式在Redis中，键值对是按一定的数据结构来组织的，操作键值对最终就是对数据结构进行增删改查操作，所以高效的数据结构是Redis快速处理数据的基础。 String (字符串)、List (列表)、Hash (哈希)、Set (集合)和Sorted Set (有序集合)这些只是Redis键值对中值的数据类型，也就是数据的保存形式。而这里，我们说的数据结构，是要去看看它们的底层实现 全局哈希表为了实现从键到值的快速访问，Redis 使用了一个哈希表来保存所有键值对。 一个哈希表，其实就是一个数组，数组的每个元素称为一个哈希桶。一个哈希表是由多个哈希桶组成的，每个哈希桶中保存了键值对数据。 其实，哈希桶中的元素保存的并不是值本身，而是指向具体值的指针。这也就是说，不管值是String，还是集合类型，哈希桶中的元素都是指向它们的指针。 为什么哈希操作会变慢当你往哈希表中写入更多数据时，哈希冲突是不可避免的问题。这里的哈希冲突，也就是指,两个key的哈希值和哈希桶计算对应关系时，正好落在了同一个哈希桶中。 Redis 解决哈希冲突的方式，就是链式哈希。链式哈希也很容易理解，就是指同一个哈希桶中的多个元素用一个链表来保存，它们之间依次用指针连接 当链表长度过长时，会导致这个链上元素查找事件越来越久，是一个O(n)的操作。 渐进式rehashRedis 会对哈希表做 rehash 操作。rehash 也就是增加现有的哈希桶数量，让逐渐增多的 entry 元素能在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个桶中的冲突。 其实，为了使 rehash 操作更高效，Redis 默认使用了两个全局哈希表：哈希表 1 和哈希表 2。一开始，当你刚插入数据时，默认使用哈希表 1，此时的哈希表 2 并没有被分配空间。随着数据逐步增多，Redis 开始执行 rehash，这个过程分为三步： 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍； 把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中； 释放哈希表 1 的空间。 第二步涉及大量的数据拷贝，如果一次性把哈希表 1 中的数据都迁移完，会造成 Redis 线程阻塞，无法服务其他请求。此时，Redis 就无法快速访问数据了。为了避免这个问题，Redis 采用了渐进式 rehash： 简单来说就是在第二步拷贝数据时，Redis 仍然正常处理客户端请求，每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝到哈希表 2 中；等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的entries。 把一次性大量拷贝的开销，分摊到了多次处理请求的过程中，避免了耗时操作，保证了数据的快速访问。 六种数据结构 简单动态字符串SDSsimple dynamic string 12345struct { int len; //记录字节数组中已经使用的长度 == 字符串的长度 4个字节 int free; //记录字节数组中未使用的长度 4个字节 char[] buf; //字节数组，保存字符串 4个字节} 和C中字符串的区别 常数复杂度获取字符串的长度,C中的字符串获取长度需要On，而SDS中保存了len属性 杜绝了缓冲区溢出。C语言中进行字符串拼接需要先分配足够的内存，否则会发生缓冲区溢出。而SDS中的API会自动的先检查空间是否足够，因为记录了字符串长度，所以比较快。（如果空间不够，不仅会分配足够的空间，还会分配足够的未使用空间） 减少了修改字符串带来的内存重分配。C中字符串每次赠长或缩短，都要进行内存重分配，否则会发生缓冲区溢出和内存泄露的问题。redis中进行会修改字符串，所以会有未使用的空间可以使用空间预分配和惰性空间释放。 二进制安全。C字符串末尾是\\0,所以不能包含空字符串，只能保存文本数据，不能保存二进制数据。而SDS是可以的。 兼容部分C字符串的函数。 SDS是如何减少内存重分配的 空间预分配：当用SDS的API对SDS进行修改的时候，不仅会分配修改所必须使用的空间，还会分配额外的未使用空间。在扩展SDS空间之前，会先检查未使用的空间是否足够，如果足够就直接使用而无须执行内存重分配。通过这种策略：连续赠长N次字符串的内存重分配次数由N次降低为最多N次。 惰性空间释放：SDS进行缩短操作的时候，不会立即内存重分配来回收缩短后多出来的字节，而是使用free属性记录下来。 案例分析保存一个10位数的图片id和图片存储id： 12photo_id: 1101000051photo_obj_id: 3301000051 我们保存了 1 亿张图片的信息，用了约 6.4GB 的内存，一个图片 ID 和图片存储对象 ID 的记录平均用了 64 字节。但实际上实际只需要 16 字节就可以了（两个 8 字节的Long 类型表示这两个 ID。）。 但其实，除了记录实际数据，String 类型还需要额外的内存空间记录数据长度、空间使用等信息，这些信息也叫作元数据。当实际保存的数据较小时，元数据的空间开销就显得比较大了，有点“喧宾夺主”的意思。 String 类型具体是怎么保存数据的呢？我来解释一下。 当你保存 64 位有符号整数时，String 类型会把它保存为一个 8 字节的 Long 类型整数，这种保存方式通常也叫作 int 编码方式。 当你保存的数据中包含字符时，会用SDS来保存 针对于RedisObject的优化： 一方面，当保存的是 Long 类型整数时，RedisObject 中的指针就直接赋值为整数数据了，这样就不用额外的指针再指向整数了，节省了指针的空间开销。 另一方面，当保存的是字符串数据，并且字符串小于等于 44 字节时，RedisObject 中的元数据、指针和 SDS 是一块连续的内存区域，这样就可以避免内存碎片。这种布局方式也被称为 embstr 编码方式。 当字符串大于 44 字节时，SDS 的数据量就开始变多了，Redis 就不再把 SDS 和RedisObject 布局在一起了，而是会给 SDS 分配独立的空间，并用指针指向 SDS 结构。这种布局方式被称为 raw 编码模式。 因为 10 位数的图片 ID 和图片存储对象 ID 是 Long 类型整数，所以可以直接用 int 编码的 RedisObject 保存。每个 int 编码的 RedisObject 元数据部分占 8 字节，指针部分被直接赋值为 8 字节的整数了。此时，每个 ID 会使用 16 字节，加起来一共是 32 字节。但是，另外的 32 字节去哪儿了呢？ Redis 会使用一个全局哈希表保存所有键值对，哈希表的每一项是一个 dictEntry 的结构体，用来指向一个键值对。dictEntry 结构中有三个 8 字节的指针，分别指向 key、value 以及下一个 dictEntry，三个指针共 24 字节， 但是，这三个指针只有 24 字节，为什么会占用了 32 字节呢？这就要提到 Redis 使用的内存分配库 jemalloc 了。jemalloc 在分配内存时，会根据我们申请的字节数 N，找一个比 N 大，但是最接近 N 的2 的幂次数作为分配的空间，这样可以减少频繁分配的次数。举个例子。如果你申请 6 字节空间，jemalloc 实际会分配 8 字节空间；如果你申请 24 字节空间，jemalloc 则会分配 32 字节。所以，在我们刚刚说的场景里，dictEntry 结构就占用了 32 字节。 双向列表压缩列表压缩列表实际上类似于一个数组，数组中的每一个元素都对应保存一个数据。和数组不同的是，压缩列表在表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的偏移量和列表中的 entry 个数；压缩列表在表尾还有一个 zlend，表示列表结束。 在压缩列表中，如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段的长度直接定位，复杂度是 O(1)。而查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O(N) 了。 压缩列表是一种非常紧凑的数据结构，占用的内存比链表要少。 哈希表跳表有序链表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表。具体来说，跳表在链表的基础上，增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位， 整形数组RedisObject因为 Redis 的数据类型有很多，而且，不同数据类型都有些相同的元数据要记录（比如最后一次访问的时间、被引用的次数等），所以，Redis 会用一个 RedisObject 结构体来统一记录这些元数据，同时指向实际数据。 一个 RedisObject 包含了 8 字节的元数据和一个 8 字节指针，这个指针再进一步指向具体数据类型的实际数据所在，例如指向 String 类型的 SDS 结构所在的内存地址，可以看一下下面的示意图。关于 RedisObject 的具体结构细节，我会在后面的课程中详细介绍，现在你只要了解它的基本结构和元数据开销就行了。 不同操作的时间复杂度第一，单元素操作，是指每一种集合类型对单个数据实现的增删改查操作。例如，Hash 类型的 HGET、HSET 和 HDEL，Set 类型的 SADD、SREM、SRANDMEMBER 等。这些操作的复杂度由集合采用的数据结构决定，例如，HGET、HSET 和 HDEL 是对哈希表做操作，所以它们的复杂度都是 O(1)；Set 类型用哈希表作为底层数据结构时，它的 SADD、SREM、SRANDMEMBER 复杂度也是 O(1)。这里，有个地方你需要注意一下，集合类型支持同时对多个元素进行增删改查，例如 Hash类型的 HMGET 和 HMSET，Set 类型的 SADD 也支持同时增加多个元素。此时，这些操作的复杂度，就是由单个元素操作复杂度和元素个数决定的。例如，HMSET 增加 M 个元素时，复杂度就从 O(1) 变成 O(M) 了。第二，范围操作，是指集合类型中的遍历操作，可以返回集合中的所有数据，比如 Hash类型的 HGETALL 和 Set 类型的 SMEMBERS，或者返回一个范围内的部分数据，比如 List类型的 LRANGE 和 ZSet 类型的 ZRANGE。这类操作的复杂度一般是 O(N)，比较耗时，我们应该尽量避免。不过，Redis 从 2.8 版本开始提供了 SCAN 系列操作（包括 HSCAN，SSCAN 和ZSCAN），这类操作实现了渐进式遍历，每次只返回有限数量的数据。这样一来，相比于HGETALL、SMEMBERS 这类操作来说，就避免了一次性返回所有元素而导致的 Redis 阻塞。第三，统计操作，是指集合类型对集合中所有元素个数的记录，例如 LLEN 和 SCARD。这类操作复杂度只有 O(1)，这是因为当集合类型采用压缩列表、双向链表、整数数组这些数据结构时，这些结构中专门记录了元素的个数统计，因此可以高效地完成相关操作。 第四，例外情况，是指某些数据结构的特殊记录，例如压缩列表和双向链表都会记录表头和表尾的偏移量。这样一来，对于 List 类型的 LPOP、RPOP、LPUSH、RPUSH 这四个操作来说，它们是在列表的头尾增删元素，这就可以通过偏移量直接定位，所以它们的复杂度也只有 O(1)，可以实现快速操作。","link":"/2021/05/13/Redis%EF%BC%88%E4%B8%80%EF%BC%89%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%B1%BB%E5%9E%8B/"},{"title":"Netty（二）Reactor模型","text":"本篇主要讲述了三种Reactor模型： Reactor单线程模型 Reactor多线程模型 Reactor主从模型 三种Reactor模型生活场景的类比生活场景：饭店规模的扩大 一个人包揽所有活：迎宾、点菜、做饭、上菜、送客等。 多找几个伙计：大家一起做上面的事。 进一步分工：搞一个或多个人专门做迎宾。 类比： 饭店伙计：线程 迎宾：接入连接 点菜：请求 做菜：业务处理 上菜：响应 送客：断连 Reactor单线程模型最简单的单Reactor单线程模型。Reactor线程是个多面手，负责多路分离套接字，Accept新连接，并分派请求到处理器链中。该模型适用于处理器链中业务处理组件能快速完成的场景。不过,这种单线程模型不能充分利用多核资源，所以实际使用的不多。、 Reactor多线程模型该模型在处理器链部分采用了多线程（线程池）。 Reactor主从模型是将Reactor分成两部分，mainReactor负责监听server socket, accept新连接，并将建立的socket分派给subReactor。subReactor负责 多路分离已连接的socket,读写网络数据，对业务处理功能，其扔给worker线程池完成。 通常，subReactor个数 上可与CPU个数等同。 主从Reactor源码分析简单说就是两种SocketChannel绑定到两种Reactor模型中去完成主从模型的支持。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class EchoServer { private int port; public EchoServer(int port) { this.port = port; } public static void main(String[] args) throws Exception { new EchoServer(8833).start(); } public void start() throws Exception { //1.Reactor模型的主、从多线程 EventLoopGroup mainGroup = new NioEventLoopGroup(); EventLoopGroup childGroup = new NioEventLoopGroup(); try { //2.构造引导器实例ServerBootstrap ServerBootstrap b = new ServerBootstrap(); b.group(mainGroup, childGroup) .channel(NioServerSocketChannel.class) //2.1 设置NIO的channel .localAddress(new InetSocketAddress(port)) //2.2 配置本地监听端口 .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { //2.3 初始化channel的时候，配置Handler @Override protected void initChannel(final SocketChannel socketChannel) { socketChannel.pipeline() .addLast(&quot;codec&quot;, new HttpServerCodec()) .addLast(&quot;compressor&quot;, new HttpContentCompressor()) .addLast(&quot;aggregator&quot;, new HttpObjectAggregator(65536)) .addLast(&quot;handler&quot;, new EchoServerHandler()); //2.4 加入自定义业务逻辑ChannelHandler } }); ChannelFuture f = b.bind().sync(); //3.启动监听 System.out.println(&quot;Http Server started， Listening on &quot; + port); f.channel().closeFuture().sync(); } finally { mainGroup.shutdownGracefully().sync(); childGroup.shutdownGracefully().sync(); } }}public class EchoServerHandler extends SimpleChannelInboundHandler&lt;FullHttpRequest&gt; { @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, FullHttpRequest fullHttpRequest) throws Exception { String content = String.format(&quot;Receive http request, url: %s , method: %s , content: %s%n&quot; ,fullHttpRequest.uri() ,fullHttpRequest.method() ,fullHttpRequest.content().toString().getBytes(StandardCharsets.UTF_8)); DefaultFullHttpResponse response = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.OK, Unpooled.wrappedBuffer(content.getBytes())); channelHandlerContext.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); }}shengbinbin@chengbinbindeMacBook-Pro netty_example % curl http://localhost:8833/abcReceive http request, url: /abc , method: GET , content: [B@3e40b233 只需要创建两个EventLoopGroup，然后绑定到引导器ServerBootstrap上就好了. mainGroup 是主 Reactor，childGroup 是从 Reactor。它们分别使用不同的 NioEventLoopGroup，主 Reactor 负责处理 Accept，然后把 Channel 注册到从 Reactor 上，从 Reactor 主要负责 Channel 生命周期内的所有 I/O 事件。 1）什么是Channel Channel 的字面意思是“通道”，它是网络通信的载体，提供了基本的 API 用于网络 I/O 操作，如 register、bind、connect、read、write、flush 等。 Netty 实现的 Channel 是以 JDK NIO Channel 为基础的，提供了更高层次的抽象，屏蔽了底层 Socket。 2）什么是ChannleHandler和ChannelPipeline ChannelHandler实现对客户端发送过来的数据进行处理，可能包括编解码、自定义业务逻辑处理等等。 ChannelPipeline 负责组装各种 ChannelHandler，当 I/O 读写事件触发时，ChannelPipeline 会依次调用 ChannelHandler 列表对 Channel 的数据进行拦截和处理。 3）什么是EventLoopGroup？ EventLoopGroup 本质是一个线程池， 是 Netty Reactor 线程模型的具体实现方式，主要负责接收 I/O 请求，并分配线程执行处理请求。我们在demo中使用了它的实现类 NioEventLoopGroup，也是 Netty 中最被推荐使用的线程模型。 我们还通过构建main EventLoopGroup 和 child EventLoopGroup 实现了 “主从Reactor模式”。 4）EventLoopGroup、EventLoop、Channel有什么关系？ 一个 EventLoopGroup 往往包含一个或者多个 EventLoop。 EventLoop 用于处理 Channel 生命周期内的所有 I/O 事件，如 accept、connect、read、write 等 I/O 事件。 EventLoop 同一时间会与一个线程绑定，每个 EventLoop 负责处理多个 Channel。 1234567891011// Configure the server. //主从模型 EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); //自定义业务逻辑 ChannelHandler final EchoServerHandler serverHandler = new EchoServerHandler(); try { //构造引导器实例ServerBootstrap ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) //加入主从group .channel(NioServerSocketChannel.class) //设置NIO的channel 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) { super.group(parentGroup); //加入主reactor模型 ObjectUtil.checkNotNull(childGroup, &quot;childGroup&quot;); if (this.childGroup != null) { throw new IllegalStateException(&quot;childGroup set already&quot;); } this.childGroup = childGroup; //加入从reactor模型 return this; }//====super.group(parentGroup)====public B group(EventLoopGroup group) { ObjectUtil.checkNotNull(group, &quot;group&quot;); if (this.group != null) { throw new IllegalStateException(&quot;group set already&quot;); } this.group = group; return self(); }//this.group = group;public abstract class AbstractBootstrap&lt;B extends AbstractBootstrap&lt;B, C&gt;, C extends Channel&gt; implements Cloneable { volatile EventLoopGroup group; //这个成员变量 final ChannelFuture initAndRegister() { Channel channel = null; try { channel = channelFactory.newChannel(); init(channel); } catch (Throwable t) { if (channel != null) { // channel can be null if newChannel crashed (eg SocketException(&quot;too many open files&quot;)) channel.unsafe().closeForcibly(); // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(channel, GlobalEventExecutor.INSTANCE).setFailure(t); } // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(new FailedChannel(), GlobalEventExecutor.INSTANCE).setFailure(t); } //开始register，将这个channel 注册到主Reactor模型中去，完成绑定关系，在这是ServerSocketChannel ChannelFuture regFuture = config().group().register(channel); if (regFuture.cause() != null) { if (channel.isRegistered()) { channel.close(); } else { channel.unsafe().closeForcibly(); } } // If we are here and the promise is not failed, it's one of the following cases: // 1) If we attempted registration from the event loop, the registration has been completed at this point. // i.e. It's safe to attempt bind() or connect() now because the channel has been registered. // 2) If we attempted registration from the other thread, the registration request has been successfully // added to the event loop's task queue for later execution. // i.e. It's safe to attempt bind() or connect() now: // because bind() or connect() will be executed *after* the scheduled registration task is executed // because register(), bind(), and connect() are all bound to the same thread. return regFuture; } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) { super.group(parentGroup); ObjectUtil.checkNotNull(childGroup, &quot;childGroup&quot;); if (this.childGroup != null) { throw new IllegalStateException(&quot;childGroup set already&quot;); } this.childGroup = childGroup; return this; }private volatile EventLoopGroup childGroup; //从reactor模型就是这个成员变量@Override void init(Channel channel) { setChannelOptions(channel, options0().entrySet().toArray(newOptionArray(0)), logger); setAttributes(channel, attrs0().entrySet().toArray(newAttrArray(0))); ChannelPipeline p = channel.pipeline(); final EventLoopGroup currentChildGroup = childGroup; //childGroup就是currentChildGroup final ChannelHandler currentChildHandler = childHandler; final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] currentChildOptions = childOptions.entrySet().toArray(newOptionArray(0)); final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] currentChildAttrs = childAttrs.entrySet().toArray(newAttrArray(0)); //ChannelInitializer一次性、初始化handler: //负责添加一个ServerBootstrapAcceptor handler，添加完后，自己就移除了: //ServerBootstrapAcceptor handler： 负责接收客户端连接创建连接后，对连接的初始化工作。 p.addLast(new ChannelInitializer&lt;Channel&gt;() { @Override public void initChannel(final Channel ch) { final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); if (handler != null) { pipeline.addLast(handler); } ch.eventLoop().execute(new Runnable() { @Override public void run() { pipeline.addLast(new ServerBootstrapAcceptor( //currentChildGroup传入进去 ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); } }); } }); }public void channelRead(ChannelHandlerContext ctx, Object msg) { final Channel child = (Channel) msg; //SocketChannel child.pipeline().addLast(childHandler); setChannelOptions(child, childOptions, logger); setAttributes(child, childAttrs); try { //这里的SocketChannel 绑定到从Reactor模型中去 childGroup.register(child).addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture future) throws Exception { if (!future.isSuccess()) { forceClose(child, future.cause()); } } }); } catch (Throwable t) { forceClose(child, t); } } 逻辑架构 服务端利用ServerBootstrap进行启动引导，绑定监听端口 启动初始化时有 main EventLoopGroup 和 child EventLoopGroup 两个组件，其中 main EventLoopGroup负责监听网络连接事件。当有新的网络连接时，就将 Channel 注册到 child EventLoopGroup。 child EventLoopGroup 会被分配一个 EventLoop 负责处理该 Channel 的读写事件。 当客户端发起 I/O 读写事件时，服务端 EventLoop 会进行数据的读取，然后通过 ChannelPipeline 依次有序触发各种ChannelHandler进行数据处理。 客户端数据会被依次传递到 ChannelPipeline 的 ChannelInboundHandler 中，在一个handler中处理完后就会传入下一个handler。 当数据写回客户端时，会将处理结果依次传递到 ChannelPipeline 的 ChannelOutboundHandler 中，在一个handler中处理完后就会传入下一个handler，最后返回客户端。","link":"/2021/05/24/Netty%EF%BC%88%E4%BA%8C%EF%BC%89Reactor%E6%A8%A1%E5%9E%8B/"},{"title":"Redis（三）过期键删除和持久化","text":"Redis过期键的删除和持久化策略 过期键删除键过期的本质是什么可能存在的删除策略Redis如何删除过期键的持久化 Redis 用做缓存时，直接从内存中读取数据，响应速度会非常快。但是一旦服务器宕机，内存中的数据将全部丢失。 从后端数据库恢复这些数据，但这种方式存在两个问题： 一是，需要频繁访问数据库，会给数据库带来巨大的压力； 二是，这些数据是从慢速数据库中读取出来的，性能肯定比不上从 Redis 中读取，导致使用这些数据的应用程序响应变慢。 所以，对 Redis 来说，实现数据的持久化，避免从后端数据库中进行恢复，是至关重要的。 AOF（Append Only File）为什么是写后日志之前学习MySQL的时候接触的都是数据库的写前日志（Write Ahead Log, WAL），也就是说，在实际写数据前，先把修改的数据记到日志文件中，以便故障时进行恢复。 但是在Redis中却恰恰相反， Redis 是先执行命令，把数据写入内存，然后才记录日志。 那么为什么要这样设置呢？ 传统数据库的日志，例如 redo log（重做日志），记录的是修改后的数据，而 AOF 里记录的是 Redis 收到的每一条命令，这些命令是以文本形式保存的。 “*3”表示当前命令有三个部分，每部分都是由“$+数字”开头，后面紧跟着具体的命令、键或值。这里，“数字”表示这部分中的命令、键或值一共有多少字节。例如，“$3set”表示这部分有 3 个字节，也就是“set”命令。 为了避免额外的检查开销，Redis 在向 AOF 里面记录日志的时候，并不会先去对这些命令进行语法检查。所以，如果先记日志再执行命令的话，日志中就有可能记录了错误的命令，Redis 在使用日志恢复数据时，就可能会出错。而写后日志这种方式，就是先让系统执行命令，只有命令能执行成功，才会被记录到日志中，否则，系统就会直接向客户端报错。所以，Redis 使用写后日志这一方式的一大好处是，可以避免出现记录错误命令的情况。除此之外，AOF 还有一个好处：它是在命令执行后才记录日志，所以不会阻塞当前的写操作。 有两个缺陷： 如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数据就有丢失的风险。 AOF 虽然避免了对当前命令的阻塞，但可能会给下一个操作带来阻塞风险。这是因为，AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就会导致写盘很慢，进而导致后续的操作也无法执行了。 三种写回策略在AOF的配置文件中，AOF 机制给我们提供了三个选择，也就是 AOF 配置项appendfsync 的三个可选值。 Always，同步写回：每个写命令执行完，立马同步地将日志写回磁盘； Everysec，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘； No，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。 “同步写回”可以做到基本不丢数据，但是它在每一个写命令后都有一个慢速的落盘操作，不可避免地会影响主线程性能；虽然“操作系统控制的写回”在写完缓冲区后，就可以继续执行后续的命令，但是落盘的时机已经不在 Redis 手中了，只要 AOF 记录没有写回磁盘，一旦宕机对应的数据就丢失了；“每秒写回”采用一秒写回一次的频率，避免了“同步写回”的性能开销，虽然减少了对系统性能的影响，但是如果发生宕机，上一秒内未落盘的命令操作仍然会丢失。所以，这只能算是，在避免影响主线程性能和避免数据丢失两者间取了个折中。 想要获得高性能，就选择 No 策略；如果想要得到高可靠性保证，就选择Always 策略；如果允许数据有一点丢失，又希望性能别受太大影响的话，那么就选择Everysec 策略。 AOF文件重写AOF 文件过大会带来一系列性能问题，主要在于以下三个方面： 文件系统本身对文件大小有限制，无法保存过大的文件； 如果文件太大，之后再往里面追加命令记录的话，效率也会变低； 如果发生宕机，AOF 中记录的命令要一个个被重新执行，用于故障恢复，如果日志文件太大，整个恢复过程就会非常缓慢，这就会影响到 Redis 的正常使用。 AOF 重写机制就是在重写时，Redis 根据数据库的现状创建一个新的 AOF 文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入。比如说，当读取了键值对“testkey”:“testvalue”之后，重写机制会记录 settestkey testvalue 这条命令。 我们知道，AOF 文件是以追加的方式，逐一记录接收到的写命令的。当一个键值对被多条写命令反复修改时，AOF 文件会记录相应的多条命令。但是，在重写的时候，是根据这个键值对当前的最新状态，为它生成对应的写入命令。这样一来，一个键值对在重写日志中只用一条命令就行了，而且，在日志恢复时，只用执行这条命令，就可以直接完成这个键值对的写入了。 AOF重写会阻塞主线程吗AOF 日志重写的时候，是由 bgrewriteaof 子进程来完成的，不用主线程参与，这也是为了避免阻塞主线程，导致数据库性能下降。 每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程。此时，子进程这里面就包含了数据库的最新数据。然后，bgrewriteaof 子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。 所谓fork子进程指的是，子进程是会拷贝父进程的页表，即虚实映射关系，而不会拷贝物理内存。子进程复制了父进程页表，也能共享访问父进程的内存数据了，此时，类似于有了父进程的所有内存数据。 因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指正在使用的 AOF 日志，Redis 会把这个操作写到它的缓冲区。这样一来，即使宕机了，这个 AOF 日志的操作仍然是齐全的，可以用于恢复。而第二处日志，就是指新的 AOF 重写日志。这个操作也会被写到重写日志的缓冲区。这样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以用新的 AOF 文件替代旧文件了。 RDB（Redis DataBase ）内存快照 给哪些内存数据做快照？Redis 的数据都在内存中，为了提供所有数据的可靠性保证，它执行的是全量快照，也就是说，把内存中的所有数据都记录到磁盘中 Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave save：在主线程中执行，会导致阻塞； bgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是Redis RDB 文件生成的默认配置。 避免阻塞和正常处理写操作并不是一回事！！！。此时，主线程的确没有阻塞，可以正常接收请求，但是，为了保证快照完整性，它只能处理读操作，因为不能修改正在执行快照的数据。为了快照而暂停写操作，肯定是不能接受的。所以这个时候，Redis 就会借助操作系统提供的写时复制技术（Copy-On-Write, COW），在执行快照的同时，正常处理写操作。 简单来说，bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。此时，如果主线程对这些数据也都是读操作（例如图中的键值对 A），那么，主线程和bgsave 子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对 C），那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave 子进程会把这个副本数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据。 快照频率如何确定如果频繁地执行全量快照，也会带来两方面的开销。 一方面，频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。 另一方面，bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了。 我们可以做增量快照，就是指，做了一次全量快照后，后续的快照只对修改的数据进行快照记录，这样可以避免每次全量快照的开销。 它需要我们使用额外的元数据信息去记录哪些数据被修改了，这会带来额外的空间开销问题。 虽然跟 AOF 相比，快照的恢复速度快，但是，快照的频率不好把握，如果频率太低，两次快照间一旦宕机，就可能有比较多的数据丢失。如果频率太高，又会产生额外开销，那么，还有什么方法既能利用 RDB 的快速恢复，又能以较小的开销做到尽量少丢数据呢？ AOF 日志和内存快照混合Redis 4.0 中提出了一个混合使用 AOF 日志和内存快照的方法。简单来说，内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。 这样一来，快照不用很频繁地执行，这就避免了频繁 fork 对主线程的影响。而且，AOF日志也只用记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现文件过大的情况了，也可以避免重写开销。 关于 AOF 和 RDB 的选择问题，我想再给你提三点建议： 数据不能丢失时，内存快照和 AOF 的混合使用是一个很好的选择； 如果允许分钟级别的数据丢失，可以只使用 RDB； 如果只用 AOF，优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个平衡。","link":"/2021/05/13/Redis%EF%BC%88%E4%B8%89%EF%BC%89%E8%BF%87%E6%9C%9F%E9%94%AE%E5%88%A0%E9%99%A4%E5%92%8C%E6%8C%81%E4%B9%85%E5%8C%96/"},{"title":"Redis（二）IO模型","text":"Redis 是单线程，主要是指Redis 的网络 IO和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程，但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。 为什么用单线程多线程的开销日常写程序时，我们经常会听到一种说法:“使用多线程，可以增加系统吞吐率，或是可以增加系统扩展性。”的确，对于一个多线程的系统来说，在有合理的资源分配的情况下，可以增加系统中处理请求操作的资源实体，进而提升系统能够同时处理的请求数，即吞吐率。下面的左图是我们采用多线程时所期待的结果。 系统中通常会存在被多线程同时访问的共享资源，比如一个共享的数据结构。当有多个线程要修改这个共享资源时，为了保证共享资源的正确性，就需要有额外的机制进行保证，而这个额外的机制，就会带来额外的开销。 单线程为什么快Redis 采用了多路复用机制，使其在网络 IO 操作中能并发处理大量的客户端请求，实现高吞吐率。 基本的IO模型和阻塞的地方 当 Redis监听到一个客户端有连接请求，但一直未能成功建立起连接时，会阻塞在 accept() 函数这里，导致其他客户端无法和 Redis 建立连接。 当 Redis 通过 recv() 从一个客户端读取数据时，如果数据一直没有到达，Redis 也会一直阻塞在 recv()。 非阻塞模式Socket 网络模型的非阻塞模式设置，主要体现在三个关键的函数调用上，如果想要使用socket 非阻塞模式，就必须要了解这三个函数的调用返回类型和设置模式： socket() 方法会返回主动套接字，然后调用 listen() 方法，将主动套接字转化为监听套接字，此时，可以监听来自客户端的连接请求。最后，调用 accept() 方法接收到达的客户端连接，并返回已连接套接字。 针对监听套接字，我们可以设置非阻塞模式：当 Redis 调用 accept() 但一直未有连接请求到达时，Redis 线程可以返回处理其他操作，而不用一直等待。但是，你要注意的是，调用 accept() 时，已经存在监听套接字了。 针对已连接套接字设置非阻塞模式：Redis 调用 recv() 后，如果已连接套接字上一直没有数据到达，Redis 线程同样可以返回处理其他操作。我们也需要有机制继续监听该已连接套接字，并在有数据达到时通知 Redis。 基于多路复用的高性能IO模型Linux 中的 IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的select/epoll 机制。简单来说，在 Redis 只运行单线程的情况下，该机制允许内核中，同时存在多个监听套接字和已连接套接字。内核会一直监听这些套接字上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个IO 流的效果。 Redis 网络框架调用 epoll 机制，让内核监听这些套接字。此时，Redis 线程不会阻塞在某一个特定的监听或已连接套接字上，也就是说，不会阻塞在某一个特定的客户端请求处理上。正因为此，Redis 可以同时和多个客户端连接并处理请求，从而提升并发性。 为了在请求到达时能通知到 Redis 线程，select/epoll 提供了基于事件的回调机制，即针对不同事件的发生，调用相应的处理函数。那么，回调机制是怎么工作的呢？ 其实，select/epoll 一旦监测到 FD 上有请求到达时，就会触发相应的事件。这些事件会被放进一个事件队列，Redis 单线程对该事件队列不断进行处理。这样一来，Redis 无需一直轮询是否有请求实际发生，这就可以避免造成 CPU 资源浪费。同时，Redis 在对事件队列中的事件进行处理时，会调用相应的处理函数，这就实现了基于事件的回调。因为 Redis 一直在对事件队列进行处理，所以能及时响应客户端请求，提升Redis 的响应性能。 以连接请求和读数据请求为例： 这两个请求分别对应 Accept 事件和 Read 事件，Redis 分别对这两个事件注册 accept 和get 回调函数。当 Linux 内核监听到有连接请求或读数据请求时，就会触发 Accept 事件和 Read 事件，此时，内核就会回调 Redis 相应的 accept 和 get 函数进行处理。 这就像病人去医院瞧病。在医生实际诊断前，每个病人（等同于请求）都需要先分诊、测体温、登记等。如果这些工作都由医生来完成，医生的工作效率就会很低。所以，医院都设置了分诊台，分诊台会一直处理这些诊断前的工作（类似于 Linux 内核监听请求），然后再转交给医生做实际诊断。这样即使一个医生（相当于 Redis 单线程），效率也能提升","link":"/2021/05/13/Redis%EF%BC%88%E4%BA%8C%EF%BC%89IO%E6%A8%A1%E5%9E%8B/"},{"title":"Redis（五）哨兵模式","text":"如果主库挂了，如何才能不间断的提供服务呢？ 哨兵机制是实现主从库自动切换的关键机制，它有效地解决了主从复制模式下故障转移的这三个问题。 哨兵的主要任务由一个或多个哨兵实例组成的哨兵系统可用监视多个主服务器及下属的所有从服务器，主服务器下线后会自动将从服务器升级成主服务器实现故障转移。哨兵本质上就是一个运行在特殊模式下的 Redis 进程，主从库实例运行的同时，它也在运行。哨兵主要负责的就是三个任务：监控、选主（选择主库）和通知。 监控监控是指哨兵进程在运行时，周期性地给所有的主从库发送 PING 命令，检测它们是否仍然在线运行。如果从库没有在规定时间内响应哨兵的 PING 命令，哨兵就会把它标记为“下线状态”；同样，如果主库也没有在规定时间内响应哨兵的 PING 命令，哨兵就会判定主库下线，然后开始自动切换主库的流程。 选主主库挂了以后，哨兵就需要从很多个从库里，按照一定的规则选择一个从库实例，把它作为新的主库。这一步完成后，现在的集群里就有了新主库。 通知在执行通知任务时，哨兵会把新主库的连接信息发给其他从库，让它们执行 replicaof 命令，和新主库建立连接，并进行数据复制。同时，哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上。 主服务下线的两种状态判定主观下线：哨兵默认每秒向创建命令连接的实例（主从服务器和其他哨兵）发送ping命令，根据回复判定是否在线。在配置文件中配置了down- after-minlliseconds属性，超过了这个时间没有回复就将实例的标识属性计作主观下线（不同哨兵对同一个实例的在线状态判定可能并不相同） 通常会采用多实例组成的集群模式进行部署，这也被称为哨兵集群。引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。 客观下线：哨兵认为主服务器主观下线时，还会向其他哨兵询问，看有多少哨兵也认为这个主服务器下线了，如果超过一个阈值就认为该主服务器客观下线，进行故障转移。 如何选定新主库筛选机制在选主时，除了要检查从库的当前在线状态，还要判断它之前的网络连接状态。如果从库总是和主库断连，而且断连次数超出了一定的阈值，我们就有理由相信，这个从库的网络状况并不是太好，就可以把这个从库筛掉了。 具体怎么判断呢？你使用配置项 down-after-milliseconds * 10。其中，down-after\u0002milliseconds 是我们认定主从库断连的最大连接超时时间。如果在 down-after\u0002milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了。如果发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主库。 打分机制 优先级最高的从库得分高。用户可以通过 slave-priority 配置项，给不同的从库设置不同优先级。比如，你有两个从库，它们的内存大小不一样，你可以手动给内存大的实例设置一个高优先级。在选主时，哨兵会给优先级高的从库打高分，如果有一个从库优先级最高，那么它就是新主库了。如果从库的优先级都一样，那么哨兵开始第二轮打分。 和旧主库同步程度最接近的从库得分高。主从库同步时有个命令传播的过程。在这个过程中，主库会用master_repl_offset 记录当前的最新写操作在 repl_backlog_buffer 中的位置，而从库会用 slave_repl_offset 这个值记录当前的复制进度。此时，我们想要找的从库，它的 slave_repl_offset 需要最接近 master_repl_offset。 ID 号小的从库得分高。在优先级和复制进度都相同的情况下，ID 号最小的从库得分最高，会被选为新主库。 哨兵集群在配置哨兵的信息时，我们只需要用到下面的这个配置项，设置主库的 IP和端口，并没有配置其他哨兵的连接信息。 sentinelmonitor 哨兵实例既然都不知道彼此的地址，又是怎么组成集群的呢？ pub/sub机制Redis的消息订阅发布机制为了区分不同应用的消息，Redis 会以频道的形式，对这些消息进行分门别类的管理。所谓的频道，实际上就是消息的类别。当消息类别相同时，它们就属于同一个频道。反之，就属于不同的频道。只有订阅了同一个频道的应用，才能通过发布的消息进行信息交换。 在主从集群中，主库上有一个名为“sentinel:hello”的频道，不同哨兵就是通过它来相互发现，实现互相通信的。 在下图中，哨兵 1 把自己的 IP（172.16.19.3）和端口（26579）发布到“sentinel:hello”频道上，哨兵 2 和 3 订阅了该频道。那么此时，哨兵 2 和 3 就可以从这个频道直接获取哨兵 1 的 IP 地址和端口号。然后，哨兵 2、3 可以和哨兵 1 建立网络连接。通过这个方式，哨兵 2 和 3 也可以建立网络连接，这样一来，哨兵集群就形成了。它们相互间可以通过网络连接进行通信，比如对主库有没有下线这件事儿进行判断和协商。 哨兵如何和从库建立连接在哨兵的监控任务中，它需要对主从库都进行心跳判断，而且在主从库切换完成后，它还需要通知从库，让它们和新主库进行同步。 哨兵是如何知道从库的 IP 地址和端口的呢？ 这是由哨兵向主库发送 INFO 命令来完成的。就像下图所示，哨兵 2 给主库发送 INFO 命令，主库接受到这个命令后，就会把从库列表返回给哨兵。接着，哨兵就可以根据从库列表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控。哨兵 1和 3 可以通过相同的方法和从库建立连接。 客户端如何感知发生了主库的切换呢每个哨兵实例也提供 pub/sub 机制，客户端可以从哨兵订阅消息。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件。 客户端读取哨兵的配置文件后，可以获得哨兵的地址和端口，和哨兵建立网络连接。 在客户端执行订阅命令，来获取不同的事件消息。 SUBSCRIBE+odown. //订阅“所有实例进入客观下线状态的事件”： PSUBSCRIBE * //订阅所有的事件 当哨兵把新主库选择出来后，客户端就会看到下面的 switch-master 事件。这个事件表示主库已经切换了，新主库的 IP 地址和端口信息已经有了。 switch-master 有了这些事件通知，客户端不仅可以在主从切换后得到新主库的连接信息，还可以监控到主从库切换过程中发生的各个重要事件。这样，客户端就可以知道主从切换进行到哪一步了，有助于了解切换进度。 哨兵的选举过程主库故障以后，哨兵集群有多个实例，那怎么确定由哪个哨兵来进行实际的主从切换呢？ 任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 is-master-down\u0002by-addr 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票。 一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”。这个所需的赞成票数是通过哨兵配置文件中的 quorum 配置项设定的。例如，现在有 5 个哨兵，quorum 配置的是 3，那么，一个哨兵需要 3 张赞成票，就可以标记主库为“客观下线”了。这 3 张赞成票包括哨兵自己的一张赞成票和另外两个哨兵的赞成票。此时，这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为“Leader 选举”。因为最终执行主从切换的哨兵称为 Leader，投票过程就是确定 Leader。 总而言之，任何一个想成为 Leader 的哨兵，要满足两个条件： 第一，拿到半数以上的赞成票； 第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。以 3 个哨兵为例，假设此时的 quorum 设置为 2，那么，任何一个想成为 Leader 的哨兵只要拿到2 张赞成票，就可以了。 在 T1 时刻，S1 判断主库为“客观下线”，它想成为 Leader，就先给自己投一张赞成票，然后分别向 S2 和 S3 发送命令，表示要成为 Leader。 在 T2 时刻，S3 判断主库为“客观下线”，它也想成为 Leader，所以也先给自己投一张赞成票，再分别向 S1 和 S2 发送命令，表示要成为 Leader。 在 T3 时刻，S1 收到了 S3 的 Leader 投票请求。因为 S1 已经给自己投了一票 Y，所以它不能再给其他哨兵投赞成票了，所以 S1 回复 N 表示不同意。同时，S2 收到了 T2 时 S3发送的 Leader 投票请求。因为 S2 之前没有投过票，它会给第一个向它发送投票请求的哨兵回复 Y，给后续再发送投票请求的哨兵回复 N，所以，在 T3 时，S2 回复 S3，同意 S3成为 Leader。 在 T4 时刻，S2 才收到 T1 时 S1 发送的投票命令。因为 S2 已经在 T3 时同意了 S3 的投票请求，此时，S2 给 S1 回复 N，表示不同意 S1 成为 Leader。发生这种情况，是因为S3 和 S2 之间的网络传输正常，而 S1 和 S2 之间的网络传输可能正好拥塞了，导致投票请求传输慢了。 在 T5 时刻，S1 得到的票数是来自它自己的一票 Y 和来自 S2 的一票 N。而 S3 除了自己的赞成票 Y 以外，还收到了来自 S2 的一票 Y。此时，S3 不仅获得了半数以上的Leader 赞成票，也达到预设的 quorum 值（quorum 为 2），所以它最终成为了Leader。接着，S3 会开始执行选主操作，而且在选定新主库后，会给其他从库和客户端通知新主库的信息。 如果 S3 没有拿到 2 票 Y，那么这轮投票就不会产生 Leader。哨兵集群会等待一段时间（也就是哨兵故障转移超时时间的 2 倍），再重新选举。这是因为，哨兵集群能够进行成功投票，很大程度上依赖于选举命令的正常网络传播。如果网络压力较大或有短时堵塞，就可能导致没有一个哨兵能拿到半数以上的赞成票。所以，等到网络拥塞好转之后，再进行投票选举，成功的概率就会增加。","link":"/2021/05/13/Redis%EF%BC%88%E4%BA%94%EF%BC%89%E5%93%A8%E5%85%B5%E6%A8%A1%E5%BC%8F/"},{"title":"Redis（六）集群模式","text":"Redis在使用 RDB 进行持久化时，会 fork 子进程来完成，而fork 操作的用时和 Redis 的数据量是正相关的， fork 在执行时会阻塞主线程。数据量越大，fork 操作造成的主线程阻塞的时间越长。 所以，在使用 RDB 对 几十G的数据进行持久化时，后台运行的子进程在 fork 创建时阻塞了主线程，于是就导致Redis 响应变慢了。因此需要进行切片集群 数据量越来越大该怎么办 纵向扩展：升级单个 Redis 实例的资源配置，包括增加内存容量、增加磁盘容量、使用更高配置的 CPU。就像下图中，原来的实例内存是 8GB，硬盘是 50GB，纵向扩展后，内存增加到 24GB，磁盘增加到 150GB。 横向扩展：横向增加当前 Redis 实例的个数，就像下图中，原来使用 1 个 8GB 内存、50GB 磁盘的实例，现在使用三个相同配置的实例。 Redis ClusterRedis Cluster 方案采用哈希槽（Hash Slot）来处理数据和实例之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。 数据映射到哈希槽过程分为下面两步： 根据键值对的 key，按照CRC16 算法计算一个 16 bit的值； 再用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。 哈希槽映射到redis实例分配过程： 我们在部署 Redis Cluster 方案时，可以使用 cluster create 命令创建集群，此时，Redis会自动把这些槽平均分布在集群实例上。例如，如果集群中有 N 个实例，那么，每个实例上的槽个数为 16384/N 个。 我们也可以使用 cluster meet 命令手动建立实例间的连接，形成集群，再使用cluster addslots 命令，指定每个实例上的哈希槽个数 假设集群中不同 Redis 实例的内存大小配置不一，如果把哈希槽均分在各个实例上，在保存相同数量的键值对时，和内存大的实例相比，内存小的实例就会有更大的容量压力。遇到这种情况时，你可以根据不同实例的资源配置情况，使用 cluster addslots命令手动分配哈希槽。 客户端如何定位到哪个redis实例中呢一般来说，客户端和集群实例建立连接后，实例就会把哈希槽的分配信息发给客户端。但是，在集群刚刚创建的时候，每个实例只知道自己被分配了哪些哈希槽，是不知道其他实例拥有的哈希槽信息的。 那么，客户端为什么可以在访问任何一个实例时，都能获得所有的哈希槽信息呢？这是因为，Redis 实例会把自己的哈希槽信息发给和它相连接的其它实例，来完成哈希槽分配信息的扩散。当实例之间相互连接后，每个实例就有所有哈希槽的映射关系了。客户端收到哈希槽信息后，会把哈希槽信息缓存在本地。当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了。 但是在集群中实例和哈希槽的对应关系会发生变化： 在集群中，实例有新增或删除，Redis 需要重新分配哈希槽； 为了负载均衡，Redis 需要把哈希槽在所有实例上重新分布一遍。 客户端是无法主动感知这些变化的。这就会导致，它缓存的分配信息和最新的分配信息就不一致了， 重定向机制MOVED所谓的“重定向”，就是指，客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令。 那客户端又是怎么知道重定向时的新实例的访问地址呢？当客户端把一个键值对的操作请求发给一个实例时，如果这个实例上并没有这个键值对映射的哈希槽，那么，这个实例就会给客户端返回下面的 MOVED 命令响应结果，这个结果中就包含了新实例的访问地址。 12GET hello:key(error) MOVED 13320 172.16.19.5:6379 MOVED 命令表示，客户端请求的键值对所在的哈希槽 13320，实际是在172.16.19.5 这个实例上。通过返回的 MOVED 命令，就相当于把哈希槽所在的新实例的信息告诉给客户端了。这样一来，客户端就可以直接和 172.16.19.5 连接，并发送操作请求了。 ASK在实际应用时，如果 Slot 2 中的数据比较多，就可能会出现一种情况：客户端向实例 2 发送请求，但此时，Slot 2 中的数据只有一部分迁移到了实例 3，还有部分数据没有迁移。在这种迁移部分完成的情况下，客户端就会收到一条 ASK 报错信息， 12GET hello:key(error) ASK 13320 172.16.19.5:6379 这个结果中的 ASK 命令就表示，客户端请求的键值对所在的哈希槽 13320，在172.16.19.5 这个实例上，但是这个哈希槽正在迁移。此时，客户端需要先给 172.16.19.5这个实例发送一个 ASKING 命令。这个命令的意思是，让这个实例允许执行客户端接下来发送的命令。然后，客户端再向这个实例发送 GET 命令，以读取数据。 在下图中，Slot 2 正在从实例 2 往实例 3 迁移，key1 和 key2 已经迁移过去，key3 和key4 还在实例 2。客户端向实例 2 请求 key2 后，就会收到实例 2 返回的 ASK 命令。 ASK 命令表示两层含义：第一，表明 Slot 数据还在迁移中；第二，ASK 命令把客户端所请求数据的最新实例地址返回给客户端，此时，客户端需要给实例 3 发送 ASKING 命令，然后再发送操作命令。 和 MOVED 命令不同，ASK 命令并不会更新客户端缓存的哈希槽分配信息。所以，在上图中，如果客户端再次请求 Slot 2 中的数据，它还是会给实例 2 发送请求。这也就是说，ASK 命令的作用只是让客户端能给新实例发送一次请求，而不像 MOVED 命令那样，会更改本地缓存，让后续所有命令都发往新实例。 问题：为什么 Redis 不直接用一个表，把键值对和实例的对应关系记录下来？ 如果使用表记录键值对和实例的对应关系，一旦键值对和实例的对应关系发生了变化（例如实例有增减或者数据重新分布），就要修改表。如果是单线程操作表，那么所有操作都要串行执行，性能慢；如果是多线程操作表，就涉及到加锁开销。此外，如果数据量非常大，使用表记录键值对和实例的对应关系，需要的额外存储空间也会增加。 基于哈希槽计算时，虽然也要记录哈希槽和实例的对应关系，但是哈希槽的个数要比键值对的个数少很多，无论是修改哈希槽和实例的对应关系，还是使用额外空间存储哈希槽和实例的对应关系，都比直接记录键值对和实例的关系的开销小得多。","link":"/2021/05/15/Redis%EF%BC%88%E5%85%AD%EF%BC%89%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/"},{"title":"Redis（四）主从复制","text":"Redis 具有高可靠性，又是什么意思呢？其实，这里有两层含义： 一是数据尽量少丢失，通过RDB和AOF来保证 二是服务尽量少中断，增加副本冗余量，将一份数据同时保存在多个实例上。 主从复制的作用 数据的热备份 故障恢复 读写分离 高可用的基石 主从之间的第一次同步的三个阶段当我们启动多个 Redis 实例的时候，它们相互之间就可以通过 replicaof（Redis 5.0 之前使用 slaveof）命令形成主库和从库的关系，之后会按照三个阶段完成数据的第一次同步。 第一阶段是主从库间建立连接、协商同步的过程，主要是为全量复制做准备。在这一步，从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了。 具体来说，从库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。 psync 命令包含了主库的 runID和复制进度 offset两个参数。 runID，是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例。当从库和主库第一次复制时，因为不知道主库的 runID，所以将 runID 设为“？”。 offset，此时设为 -1，表示第一次复制。 FULLRESYNC 响应表示第一次复制采用的全量复制，也就是说，主库会把当前所有的数据都复制给从库 在第二阶段，主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载。这个过程依赖于内存快照生成的 RDB 文件。 具体来说，主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。从库接收到RDB 文件后，会先清空当前数据库，然后加载 RDB 文件。这是因为从库在通过 replicaof命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把当前数据库清空。 在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。否则，Redis 的服务就被中断了。但是，这些请求中的写操作并没有记录到刚刚生成的 RDB 文件中。为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录RDB 文件生成后收到的所有写操作。 第三个阶段，主库会把第二阶段执行过程中新收到的写命令，再发送给从库。具体的操作是，当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了。 主从级联模式 在一次全量复制中，对于主库来说，需要完成两个耗时的操作：生成 RDB 文件和传输 RDB 文件。 如果从库数量很多，而且都要和主库进行全量复制的话，就会导致主库忙于 fork 子进程生成 RDB 文件，进行数据全量同步。fork 这个操作会阻塞主线程处理正常请求，从而导致主库响应应用程序的请求速度变慢。此外，传输 RDB 文件也会占用主库的网络带宽，同样会给主库的资源使用带来压力。那么，有没有好的解决方法可以分担主库压力呢？ 我们可以通过“主 - 从 - 从”模式将主库生成 RDB 和传输 RDB 的压力，以级联的方式分散到从库上 一旦主从库完成了全量复制，它们之间就会一直维护一个网络连接，主库会通过这个连接将后续陆续收到的命令操作再同步给从库，这个过程也称为基于长连接的命令传播，可以避免频繁建立连接的开销。 网络中断后如何实现增量复制复制偏移量master/slave_repl_offset主从服务器都会维护一个复制偏移量，记录存储数据的字节数。 刚开始的时候，主库和从库的写读位置在一起，这算是它们的起始位置。随着主库不断接收新的写操作，它在缓冲区中的写位置会逐步偏离起始位置，我们通常用偏移量来衡量这个偏移距离的大小，对主库来说，对应的偏移量就是 master_repl_offset。 主库接收的新写操作越多，这个值就会越大。同样，从库在复制完写操作命令后，它在缓冲区中的读位置也开始逐步偏移刚才的起始位置，此时，从库已复制的偏移量 slave_repl_offset 也在不断增加。正常情况下，这两个偏移量基本相等。 复制积压缓冲区repl_backlog_buffer当主从库断连后，主库会把断连期间收到的写操作命令，写入 replication buffer，同时也会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区。 repl_backlog_buffer 是一个环形缓冲区，主库会记录自己写到的位置，从库则会记录自己已经读到的位置。 当网络中断重新连接后，从库首先会给主库发送 psync 命令，并把自己当前的slave_repl_offset 发给主库，主库会判断自己的 master_repl_offset 和 slave_repl_offset之间的差距。在网络断连阶段，主库可能会收到新的写操作命令，所以，一般来说，master_repl_offset会大于 slave_repl_offset。此时，主库只用把 master_repl_offset 和 slave_repl_offset之间的命令操作同步给从库就行。 因为 repl_backlog_buffer 是一个环形缓冲区，所以在缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致。 我们可以调整repl_backlog_size这个参数。这个参数和所需的缓冲空间大小有关。 一个 Redis 实例的数据库不要太大，一个实例大小在几 GB 级别比较合适，这样可以减少 RDB 文件生成、传输和重新加载的开销。另外，为了避免多个从库同时和主库进行全量复制，给主库过大的同步压力，我们也可以采用“主 - 从 - 从”这一级联模式，来缓解主库的压力。 有三种模式：全量复制、基于长连接的命令传播，以及增量复制。 命令传播阶段命令传播阶段，从服务器默认每过一秒就会发送replconf ack + 复制偏移量 给主服务器 为什么全量复制用RDB文件而不是AOF RDB是经过压缩之后的二进制文件，无论是把RDB写入磁盘还是通过网络传输，IO效率都比AOF高 RDB恢复数据的效率要高于AOF","link":"/2021/05/13/Redis%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"},{"title":"RocketMQ概念和设计","text":"","link":"/2021/07/13/RocketMQ%E6%A6%82%E5%BF%B5%E5%92%8C%E8%AE%BE%E8%AE%A1/"},{"title":"Netty（六）流程源码解析","text":"本篇讲述了Netty启动过程中的源码分析： 启动服务 构建连接 接受数据 业务处理 发送数据 断开连接 关闭服务 实例代码1234567891011121314151617181920212223242526272829303132333435363738394041424344public class EchoServer { private int port; public EchoServer(int port) { this.port = port; } public static void main(String[] args) throws Exception { new EchoServer(8833).start(); } public void start() throws Exception { //1.Reactor模型的主、从多线程 EventLoopGroup mainGroup = new NioEventLoopGroup(); EventLoopGroup childGroup = new NioEventLoopGroup(); try { //2.构造引导器实例ServerBootstrap ServerBootstrap b = new ServerBootstrap(); b.group(mainGroup, childGroup) .channel(NioServerSocketChannel.class) //2.1 设置NIO的channel .option(ChannelOption.SO_BACKLOG, 1024) .localAddress(new InetSocketAddress(port)) //2.2 配置本地监听端口 .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { //2.3 初始化channel的时候，配置Handler @Override protected void initChannel(final SocketChannel socketChannel) { socketChannel.pipeline() .addLast(&quot;codec&quot;, new HttpServerCodec()) .addLast(&quot;compressor&quot;, new HttpContentCompressor()) .addLast(&quot;aggregator&quot;, new HttpObjectAggregator(65536)) .addLast(&quot;handler&quot;, new EchoServerHandler()); //2.4 加入自定义业务逻辑ChannelHandler } }); ChannelFuture f = b.bind().sync(); //3.启动监听 System.out.println(&quot;Http Server started， Listening on &quot; + port); f.channel().closeFuture().sync(); } finally { mainGroup.shutdownGracefully().sync(); childGroup.shutdownGracefully().sync(); } }} 启动服务 创建NioEventLoopGroup12EventLoopGroup bossGroup = new NioEventLoopGroup(); //bossGroup 负责的是接受请求EventLoopGroup workerGroup = new NioEventLoopGroup();//workerGroup 负责的是处理请求 EventLoopGroup 就是一个死循环，不停地检测IO事件，处理IO事件，执行任务。 创建selector1Selector selector = sun.nio.ch.SelectorProviderImpl.openSelector() ServerBootstrap 参数配置group()方法传入Reactor模型通过 ServerBootstrap 的方法 group() 传入之后，会设置成为 ServerBootstrap 的 parentGroup 和 childGroup 12 ServerBootstrap b = new ServerBootstrap();b.group(mainGroup, childGroup) 123456789101112131415161718192021=====ServerBootstrap=====public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) { super.group(parentGroup);//切换到 AbstractBootstrap ObjectUtil.checkNotNull(childGroup, &quot;childGroup&quot;); if (this.childGroup != null) { throw new IllegalStateException(&quot;childGroup set already&quot;); } this.childGroup = childGroup; return this; }//AbstractBootstrap 需要使用 parentGroup 来注册监听外部请求（OP_ACCEPT 事件）的 Channel=====AbstractBootstrap=====public B group(EventLoopGroup group) { ObjectUtil.checkNotNull(group, &quot;group&quot;); if (this.group != null) { throw new IllegalStateException(&quot;group set already&quot;); } this.group = group; return self(); } 设置服务端Channel1.channel(NioServerSocketChannel.class) 12345678910111213141516171819这个 channel 实际上是 AbstractBootstrap 抽象类的方法，因为这个是可以共用的。然后 AbstractBootstrap 会交给其成员函数 channelFactory()，通过实例化参数来控制怎么根据 class 来生成 channel。=====AbstractBootstrap=====public B channel(Class&lt;? extends C&gt; channelClass) { return channelFactory(new ReflectiveChannelFactory&lt;C&gt;( ObjectUtil.checkNotNull(channelClass, &quot;channelClass&quot;) )); }=====ReflectiveChannelFactory=====public ReflectiveChannelFactory(Class&lt;? extends T&gt; clazz) { ObjectUtil.checkNotNull(clazz, &quot;clazz&quot;); try { this.constructor = clazz.getConstructor(); //通过反射获取构造器，再创建channel实例 } catch (NoSuchMethodException e) { throw new IllegalArgumentException(&quot;Class &quot; + StringUtil.simpleClassName(clazz) + &quot; does not have a public non-arg constructor&quot;, e); } } 对于服务端来说，创建的一般是public io.netty.channel.socket.nio.NioServerSocketChannel() option方法设置TCP参数1.option(ChannelOption.SO_BACKLOG, 1024)//主要是设置 TCP 的 backlog 参数，这个设置之后主要是调用底层 C 对应的接口 为什么要设置这个参数呢？这个参数实际上指定内核为此套接口排队的最大连接个数。对于给定的套接字接口，内核要维护两个对列：未连接队列和已连接队列。那是因为在 TCP 的三次握手过程中三个分节来分隔这两个队列。下面是整个过程的一个讲解： 如果服务器处于 listen 时，收到客户端的 syn 分节（connect）时在未完成队列中创建一个新的条目，然后用三路握手的第二个分节即服务器的 syn 响应客户端。 新条目会直到第三个分节到达前（客户端对服务器 syn 的 ack）都会一直保留在未完成连接队列中，如果三路握手完成，该条目将从未完成队列搬到已完成队列的尾部。 当进程调用 accept 时，从已完成队列的头部取一条目给进程，当已完成队列为空的时候进程就睡眠，直到有条目在已完成连接队列中才唤醒。 现在说到了重点，backlog 其实是两个队列的总和的最大值，大多数实现默认值为 5。但是高并发的情况之下，并不够用。因为可能客户端 syn 的到达以及等待三路握手第三个分节的到达延时而增大。 所以我们需要根据实际场景和网络状况进行灵活配置 设置服务端的 Handler123456789101112//父类中的 Handler 是客户端新接入的连接 SocketChannel 对应的 ChannelPipeline 的 Handler.handler(new LoggingHandler(LogLevel.INFO)) //子类中的 Handler 是 NioServerSocketChannel 对应的 ChannelPipeline 的 Handler .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { //2.3 初始化channel的时候，配置Handler @Override protected void initChannel(final SocketChannel socketChannel) { socketChannel.pipeline() .addLast(&quot;codec&quot;, new HttpServerCodec()) .addLast(&quot;compressor&quot;, new HttpContentCompressor()) .addLast(&quot;aggregator&quot;, new HttpObjectAggregator(65536)) .addLast(&quot;handler&quot;, new EchoServerHandler()); //2.4 加入自定义业务逻辑ChannelHandler } 上面代码有两个 handler 方法，区别在于 handler() 方法是 NioServerSocketChannel 使用的，所有连接该监听端口的客户端都会执行它；父类 AbstractBootstrap 中的 Handler 是个工厂类，它为每个接入的客户端都创建一个新的 Handler。 绑定本地端口然后启动服务1ChannelFuture f = b.bind().sync(); //3.启动监听 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151=====AbstractBootstrap=====public ChannelFuture bind() { validate();//验证服务启动需要的必要参数 SocketAddress localAddress = this.localAddress; if (localAddress == null) { throw new IllegalStateException(&quot;localAddress not set&quot;); } return doBind(localAddress); } private ChannelFuture doBind(final SocketAddress localAddress) { final ChannelFuture regFuture = initAndRegister(); //初始化一个 channel， final Channel channel = regFuture.channel(); //获取 channel if (regFuture.cause() != null) { return regFuture; } if (regFuture.isDone()) {//如果这个 channel 的注册事件完成了 // At this point we know that the registration was complete and successful. //再产生一个异步任务，进行端口监听 ChannelPromise promise = channel.newPromise(); doBind0(regFuture, channel, localAddress, promise); return promise; } else { // Registration future is almost always fulfilled already, but just in case it's not. //设置一个进度条的任务，等待注册事件完成后，就开始端口的监听 final PendingRegistrationPromise promise = new PendingRegistrationPromise(channel); regFuture.addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture future) throws Exception { Throwable cause = future.cause(); if (cause != null) { // Registration on the EventLoop failed so fail the ChannelPromise directly to not cause an // IllegalStateException once we try to access the EventLoop of the Channel. promise.setFailure(cause); } else { // Registration was successful, so set the correct executor to use. // See https://github.com/netty/netty/issues/2586 promise.registered(); doBind0(regFuture, channel, localAddress, promise); } } }); return promise; } }final ChannelFuture initAndRegister() { Channel channel = null; try { channel = channelFactory.newChannel();//通过 channel 工厂生成一个 channel init(channel); } catch (Throwable t) { if (channel != null) { // channel can be null if newChannel crashed (eg SocketException(&quot;too many open files&quot;)) channel.unsafe().closeForcibly(); // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(channel, GlobalEventExecutor.INSTANCE).setFailure(t); } // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(new FailedChannel(), GlobalEventExecutor.INSTANCE).setFailure(t); } ChannelFuture regFuture = config().group().register(channel);//将这个 channel 注册进 parentEventLoop if (regFuture.cause() != null) { if (channel.isRegistered()) { channel.close(); } else { channel.unsafe().closeForcibly(); } } return regFuture; }===== ServerBootstrap ===== void init(Channel channel) throws Exception { final Map&lt;ChannelOption&lt;?&gt;, Object&gt; options = options0(); synchronized (options) { setChannelOptions(channel, options, logger);//设置 channel 的 option } final Map&lt;AttributeKey&lt;?&gt;, Object&gt; attrs = attrs0(); synchronized (attrs) { for (Entry&lt;AttributeKey&lt;?&gt;, Object&gt; e: attrs.entrySet()) { @SuppressWarnings(&quot;unchecked&quot;) AttributeKey&lt;Object&gt; key = (AttributeKey&lt;Object&gt;) e.getKey(); channel.attr(key).set(e.getValue()); ////设置属性 } } ChannelPipeline p = channel.pipeline(); final EventLoopGroup currentChildGroup = childGroup; final ChannelHandler currentChildHandler = childHandler; final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] currentChildOptions; final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] currentChildAttrs; synchronized (childOptions) { currentChildOptions = childOptions.entrySet().toArray(newOptionArray(0)); } synchronized (childAttrs) { currentChildAttrs = childAttrs.entrySet().toArray(newAttrArray(0)); } p.addLast(new ChannelInitializer&lt;Channel&gt;() { @Override public void initChannel(final Channel ch) throws Exception { final ChannelPipeline pipeline = ch.pipeline(); ////获取 pipeline //这里获取的 handler，对应的是 AbstractBootstrap 的 handler，这个是通过 ServerBootstrap.handler() 方法设置的 ChannelHandler handler = config.handler(); if (handler != null) { pipeline.addLast(handler);//添加进入 pipeline，这个是为了让每个处理的都能首先调用这个 handler } //执行任务，设置子 handler。这里对用的是 ServerBootstrap.childHandler() 方法设置的 handler ch.eventLoop().execute(new Runnable() { @Override public void run() { pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); } }); } }); }=====AbstractBootstrap=====private static void doBind0( final ChannelFuture regFuture, final Channel channel, final SocketAddress localAddress, final ChannelPromise promise) { // This method is invoked before channelRegistered() is triggered. Give user handlers a chance to set up // the pipeline in its channelRegistered() implementation. //执行到这里，说明任务已经被注册到 loopgroup //所以可以开始一个监听端口的任务 channel.eventLoop().execute(new Runnable() { @Override public void run() { if (regFuture.isSuccess()) { channel.bind(localAddress, promise).addListener(ChannelFutureListener.CLOSE_ON_FAILURE); } else { promise.setFailure(regFuture.cause()); } } }); } initAndRegister初始化注册到selectorchannelFactory.newChannel()1234567891011121314151617181920212223242526272829303132final ChannelFuture initAndRegister() { Channel channel = null; try { //1. 将channelClass作为ReflectiveChannelFactory的构造函数创建出一个ReflectiveChannelFactory channel = channelFactory.newChannel(); init(channel); } catch (Throwable t) { if (channel != null) { // channel can be null if newChannel crashed (eg SocketException(&quot;too many open files&quot;)) channel.unsafe().closeForcibly(); // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(channel, GlobalEventExecutor.INSTANCE).setFailure(t); } // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(new FailedChannel(), GlobalEventExecutor.INSTANCE).setFailure(t); } //config().group()指的是 ServerBootstrap 的 parentLoopGroup ChannelFuture regFuture = config().group().register(channel); if (regFuture.cause() != null) { if (channel.isRegistered()) { channel.close(); } else { channel.unsafe().closeForcibly(); } } return regFuture; } new 一个channel，这里的channel是在服务启动的时候创建，我们可以和普通Socket编程中的ServerSocket对应上，表示服务端绑定的时候经过的一条流水线。 将channelClass作为ReflectiveChannelFactory的构造函数创建出一个ReflectiveChannelFactory，最终创建channel相当于调用默认构造函数new出一个 NioServerSocketChannel对象 123456789101112public NioServerSocketChannel() { this(newSocket(DEFAULT_SELECTOR_PROVIDER)); }private static ServerSocketChannel newSocket(SelectorProvider provider) { try { return provider.openServerSocketChannel(); //创建一条server端channel } catch (IOException e) { throw new ChannelException( &quot;Failed to open a server socket.&quot;, e); } } 总结一下，用户调用方法 Bootstrap.bind(port) 第一步就是通过反射的方式new一个NioServerSocketChannel对象，并且在new的过程中创建了一系列的核心组件 init(channel)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253=====ServerBootstrap=====@Override void init(Channel channel) throws Exception { //1.设置option和attr final Map&lt;ChannelOption&lt;?&gt;, Object&gt; options = options0(); synchronized (options) { setChannelOptions(channel, options, logger); } final Map&lt;AttributeKey&lt;?&gt;, Object&gt; attrs = attrs0(); synchronized (attrs) { for (Entry&lt;AttributeKey&lt;?&gt;, Object&gt; e: attrs.entrySet()) { @SuppressWarnings(&quot;unchecked&quot;) AttributeKey&lt;Object&gt; key = (AttributeKey&lt;Object&gt;) e.getKey(); channel.attr(key).set(e.getValue()); } } ChannelPipeline p = channel.pipeline(); //2. 设置新接入channel的option和attr final EventLoopGroup currentChildGroup = childGroup; final ChannelHandler currentChildHandler = childHandler; final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] currentChildOptions; final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] currentChildAttrs; synchronized (childOptions) { currentChildOptions = childOptions.entrySet().toArray(newOptionArray(0)); } synchronized (childAttrs) { currentChildAttrs = childAttrs.entrySet().toArray(newAttrArray(0)); } //3.加入新连接处理器 //向serverChannel的流水线处理器中加入了一个 ServerBootstrapAcceptor(这是一个接入器，专门接受新请求，把新的请求扔给某个事件循环器) p.addLast(new ChannelInitializer&lt;Channel&gt;() { @Override public void initChannel(final Channel ch) throws Exception { final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); if (handler != null) { pipeline.addLast(handler); } ch.eventLoop().execute(new Runnable() { @Override public void run() { pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); } }); } }); } config().group().register(channel)1234=====MultithreadEventLoopGroup======public ChannelFuture register(Channel channel) { return next().register(channel); } 123456=====MultithreadEventExecutorGroup=====@Override public EventExecutor next() { return chooser.next(); }//这里的 chooser 是 MultithreadEventExecutorGroup 的成员属性，它可以对根据目前 ExectuorGroup 中的 EventExecutor 的情况策略选择 EventExecutor。这里默认使用的是 DefaultEventExecutorChooserFactory，这个是基于轮询策略操作的。 PowerOfTwoEventExecutorChooser 按位与(&amp;)操作符 GenericEventExecutorChooser 取模(%)运算符 12345678910111213141516171819202122232425262728===== DefaultEventExecutorChooserFactory =====private static final class PowerOfTwoEventExecutorChooser implements EventExecutorChooser { private final AtomicInteger idx = new AtomicInteger(); private final EventExecutor[] executors; PowerOfTwoEventExecutorChooser(EventExecutor[] executors) { this.executors = executors; } @Override public EventExecutor next() { return executors[idx.getAndIncrement() &amp; executors.length - 1]; } } private static final class GenericEventExecutorChooser implements EventExecutorChooser { private final AtomicInteger idx = new AtomicInteger(); private final EventExecutor[] executors; GenericEventExecutorChooser(EventExecutor[] executors) { this.executors = executors; } @Override public EventExecutor next() { return executors[Math.abs(idx.getAndIncrement() % executors.length)]; } } chooserFactory 最后会选择出 EventExecutor 后，就可以将 Channel 进行注册了。在 Netty 的 NioEventLoopGroup 中 EventExecutor 都是 SingleThreadEventLoop 来承担的（如果你继续跟进代码的话，你会发现其实 EventExecutor 实际上就是一个 Java 原生的线程池，最后实现的是一个 ExecutorService ）。 123456789101112=====SingleThreadEventLoop=====@Override public ChannelFuture register(Channel channel) { return register(new DefaultChannelPromise(channel, this)); } @Override public ChannelFuture register(final ChannelPromise promise) { ObjectUtil.checkNotNull(promise, &quot;promise&quot;); promise.channel().unsafe().register(this, promise); return promise; } 接下来，我们获取到了 EventExecutor 后，就可以让它帮忙注册了。 1234567891011121314151617181920212223242526===== AbstractChannel =====public final void register(EventLoop eventLoop, final ChannelPromise promise) { //...校验省略部分代码 //先将EventLoop事件循环器绑定到该NioServerSocketChannel上 AbstractChannel.this.eventLoop = eventLoop; if (eventLoop.inEventLoop()) { register0(promise); } else { try { eventLoop.execute(new Runnable() { @Override public void run() { register0(promise); } }); } catch (Throwable t) { logger.warn( &quot;Force-closing a channel whose registration task was not accepted by an event loop: {}&quot;, AbstractChannel.this, t); closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); } } } 1234567891011121314151617181920212223242526272829303132333435363738private void register0(ChannelPromise promise) { try { // check if the channel is still open as it could be closed in the mean time when the register // call was outside of the eventLoop if (!promise.setUncancellable() || !ensureOpen(promise)) { return; } boolean firstRegistration = neverRegistered; doRegister(); neverRegistered = false; registered = true; // Ensure we call handlerAdded(...) before we actually notify the promise. This is needed as the // user may already fire events through the pipeline in the ChannelFutureListener. pipeline.invokeHandlerAddedIfNeeded(); safeSetSuccess(promise); //开始通知成功了 pipeline.fireChannelRegistered(); // Only fire a channelActive if the channel has never been registered. This prevents firing // multiple channel actives if the channel is deregistered and re-registered. if (isActive()) { if (firstRegistration) { pipeline.fireChannelActive(); } else if (config().isAutoRead()) { // This channel was registered before and autoRead() is set. This means we need to begin read // again so that we process inbound data. // // See https://github.com/netty/netty/issues/4805 beginRead(); } } } catch (Throwable t) { // Close the channel directly to avoid FD leak. closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); } } 1234567891011121314151617181920212223===== AbstractNioChannel ===== @Override protected void doRegister() throws Exception { boolean selected = false; for (;;) { try { //将当前的channel注册到selector上 selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this); return; } catch (CancelledKeyException e) { if (!selected) { // Force the Selector to select now as the &quot;canceled&quot; SelectionKey may still be // cached and not removed because no Select.select(..) operation was called yet. eventLoop().selectNow(); selected = true; } else { // We forced a select operation on the selector before but the SelectionKey is still cached // for whatever reason. JDK bug ? throw e; } } } } 最终监听 OP_ACCEPT 是通过 bind 完成后的 fireChannelActive() 来触发的。 为什么注册 OP_ACCEPT(16) 到多路复用器上，怎么注册 0 呢？0 表示已注册，但不进行任何操作。这样做的原因是 注册方法是多台的。它既可以被 NioServerSocketChannel 用来监听客户端的连接接入，也可以注册 SocketChannel 用来监听网络读或写操作。 通过 SelectionKey 的 interceptOps(int pos) 可以方便修改监听的操作位。所以，此处注册需要获取 SelectionKey 并给 AbstractNioChannel 的成员变量 selectionKey 赋值。 doRegister完毕之后调用pipeline.invokeHandlerAddedIfNeeded(); 123456789final void invokeHandlerAddedIfNeeded() { assert channel.eventLoop().inEventLoop(); if (firstRegistration) { firstRegistration = false; // We are now registered to the EventLoop. It's time to call the callbacks for the ChannelHandlers, // that were added before the registration was done. callHandlerAddedForAllHandlers(); } } 当注册成功后，触发了 ChannelRegistered 事件，这个事件也是整个 pipeline 都会触发的。 ChannelRegistered 触发完后，就会判断是否 ServerSocketChannel 监听是否成功，如果成功，需要出发 NioServerSocketChannel 的 ChannelActive 事件。 123if(isAcitve()) { pipeline.fireChannelActive();} isAcitve() 方法也是多态。如果服务端判断是否监听启动；如果是客户端查看 TCP 是否连接完成。channelActive() 事件在 ChannelPipeline 中传递，找到了 pipeline.fireChannelActive(); 的发起调用的代码，不巧，刚好就是下面的doBind0()方法 123456789101112131415private static void doBind0( final ChannelFuture regFuture, final Channel channel, final SocketAddress localAddress, final ChannelPromise promise) { //通过包装一个Runnable进行异步化的，关于异步化task channel.eventLoop().execute(new Runnable() { @Override public void run() { if (regFuture.isSuccess()) { channel.bind(localAddress, promise).addListener(ChannelFutureListener.CLOSE_ON_FAILURE); } else { promise.setFailure(regFuture.cause()); } } }); } 123456789101112131415161718===== AbstractChannel =====@Override public ChannelFuture bind(SocketAddress localAddress, ChannelPromise promise) { return pipeline.bind(localAddress, promise); }===== DefaultChannelPipeline =====@Override public final ChannelFuture bind(SocketAddress localAddress, ChannelPromise promise) { return tail.bind(localAddress, promise); }===== HeadContext ======@Override public void bind( ChannelHandlerContext ctx, SocketAddress localAddress, ChannelPromise promise) { unsafe.bind(localAddress, promise); } 1234567891011121314151617181920212223242526272829303132333435363738394041@Override public final void bind(final SocketAddress localAddress, final ChannelPromise promise) { assertEventLoop(); if (!promise.setUncancellable() || !ensureOpen(promise)) { return; } // See: https://github.com/netty/netty/issues/576 if (Boolean.TRUE.equals(config().getOption(ChannelOption.SO_BROADCAST)) &amp;&amp; localAddress instanceof InetSocketAddress &amp;&amp; !((InetSocketAddress) localAddress).getAddress().isAnyLocalAddress() &amp;&amp; !PlatformDependent.isWindows() &amp;&amp; !PlatformDependent.maybeSuperUser()) { // Warn a user about the fact that a non-root user can't receive a // broadcast packet on *nix if the socket is bound on non-wildcard address. logger.warn( &quot;A non-root user can't receive a broadcast packet if the socket &quot; + &quot;is not bound to a wildcard address; binding to a non-wildcard &quot; + &quot;address (&quot; + localAddress + &quot;) anyway as requested.&quot;); } boolean wasActive = isActive(); try { doBind(localAddress); //最终调到了jdk里面的bind方法,如下 } catch (Throwable t) { safeSetFailure(promise, t); closeIfClosed(); return; } if (!wasActive &amp;&amp; isActive()) { invokeLater(new Runnable() { @Override public void run() { pipeline.fireChannelActive(); } }); } safeSetSuccess(promise); } 12345678910===== NioServerSocketChannel =====@Override protected void doBind(SocketAddress localAddress) throws Exception { if (PlatformDependent.javaVersion() &gt;= 7) { //判断JDK到版本 javaChannel().bind(localAddress, config.getBacklog()); } else { javaChannel().socket().bind(localAddress, config.getBacklog()); } } 完成之后根据配置决定是否自动出发 Channel 读操作，下面是代码实现 1234567891011121314151617181920=====DefaultChannelPipeline===== @Override public void channelActive(ChannelHandlerContext ctx) { ctx.fireChannelActive(); //注册读事件，读包括：创建连接/读取数据 readIfIsAutoRead(); }private void readIfIsAutoRead() { if (channel.config().isAutoRead()) { channel.read(); } }@Override public boolean isAutoRead() { return autoRead == 1; } AbstractChannel 的读操作出发了 ChannelPipeline 的读操作，最终调用到 HeadHandler 的读方法，代码如下 12345@Override public Channel read() { pipeline.read(); return this; } 123public void read(ChannelHandlerContext ctx){ unsafe.beginRead();} 继续看 AbstractUnsafe 的 beginRead 方法，代码如下: 由于不同类型的 Channel 对于读操作的处理是不同的，所以合格 beginRead 也算是多态方法。对于 NIO 的 channel，无论是客户端还是服务端，都是修改网络监听操作位为自身感兴趣的shi 123456789101112131415protected void doBeginRead() throws Exception { // Channel.read() or ChannelHandlerContext.read() was called //this.selectionKey就是我们在前面register步骤返回的对象，前面我们在register的时候，注册测ops是0 final SelectionKey selectionKey = this.selectionKey; if (!selectionKey.isValid()) { return; } readPending = true; final int interestOps = selectionKey.interestOps(); if ((interestOps &amp; readInterestOp) == 0) { //readInterestOp 是不是监听了ops=16 selectionKey.interestOps(interestOps | readInterestOp); } } JDK SelectionKey 有四种操作类型，分别为： OP_READ = 1&lt;&lt;0 OP_WRITE = 1&lt;&lt;2 OP_CONNECT = 1&lt;&lt;3 OP_ACCEPT = 1&lt;&lt;4 每个操作位代表一种网络操作类型，分别为 0001，0010，0100，1000，这样做的好处是方便地通过位操作来进行网络操作位的状态判断和状态修改，从而提升操作性能。 总结netty启动一个服务所经过的流程 设置启动类参数，最重要的就是设置channel 创建server对应的channel，创建各大组件，包括ChannelConfig,ChannelId,ChannelPipeline,ChannelHandler,Unsafe等 初始化server对应的channel，设置一些attr，option，以及设置子channel的attr，option，给server的channel添加新channel接入器，并出发addHandler,register等事件 调用到jdk底层做端口绑定，并触发active事件，active触发的时候，真正做服务端口绑定 构建连接 接受连接本质就是对OP_ACCEPT事件对处理，对应于源码就是NioEventLoop的run方法： 断点打在processSelectedKeysOptimized方法中，然后启动服务端，再启动客户端进行debug 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102@Override protected void run() { for (;;) { //死循环监听事件 try { try { switch (selectStrategy.calculateStrategy(selectNowSupplier, hasTasks())) { case SelectStrategy.CONTINUE: continue; case SelectStrategy.BUSY_WAIT: case SelectStrategy.SELECT: select(wakenUp.getAndSet(false)); //select轮询 if (wakenUp.get()) { selector.wakeup(); } // fall through default: } } catch (IOException e) { // If we receive an IOException here its because the Selector is messed up. Let's rebuild // the selector and retry. https://github.com/netty/netty/issues/8566 rebuildSelector0(); handleLoopException(e); continue; } cancelledKeys = 0; needsToSelectAgain = false; final int ioRatio = this.ioRatio; if (ioRatio == 100) { try { processSelectedKeys(); } finally { // Ensure we always run tasks. runAllTasks(); } } else { final long ioStartTime = System.nanoTime(); try { processSelectedKeys(); //处理事件 } finally { // Ensure we always run tasks. final long ioTime = System.nanoTime() - ioStartTime; runAllTasks(ioTime * (100 - ioRatio) / ioRatio); } } } catch (Throwable t) { handleLoopException(t); } // Always handle shutdown even if the loop processing threw an exception. try { if (isShuttingDown()) { closeAll(); if (confirmShutdown()) { return; } } } catch (Throwable t) { handleLoopException(t); } } }private void processSelectedKeys() { if (selectedKeys != null) { processSelectedKeysOptimized(); //优化后的 } else { processSelectedKeysPlain(selector.selectedKeys()); } }private void processSelectedKeysOptimized() { for (int i = 0; i &lt; selectedKeys.size; ++i) { final SelectionKey k = selectedKeys.keys[i]; // null out entry in the array to allow to have it GC'ed once the Channel close // See https://github.com/netty/netty/issues/2363 selectedKeys.keys[i] = null; final Object a = k.attachment(); //这里的attachment就是NioServerSocketChannel if (a instanceof AbstractNioChannel) { processSelectedKey(k, (AbstractNioChannel) a); //处理SelectedKey } else { @SuppressWarnings(&quot;unchecked&quot;) NioTask&lt;SelectableChannel&gt; task = (NioTask&lt;SelectableChannel&gt;) a; processSelectedKey(k, task); } if (needsToSelectAgain) { // null out entries in the array to allow to have it GC'ed once the Channel close // See https://github.com/netty/netty/issues/2363 selectedKeys.reset(i + 1); selectAgain(); i = -1; } } } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) { final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe(); if (!k.isValid()) { final EventLoop eventLoop; try { eventLoop = ch.eventLoop(); } catch (Throwable ignored) { // If the channel implementation throws an exception because there is no event loop, we ignore this // because we are only trying to determine if ch is registered to this event loop and thus has authority // to close ch. return; } // Only close ch if ch is still registered to this EventLoop. ch could have deregistered from the event loop // and thus the SelectionKey could be cancelled as part of the deregistration process, but the channel is // still healthy and should not be closed. // See https://github.com/netty/netty/issues/5125 if (eventLoop != this || eventLoop == null) { return; } // close the channel if the key is not valid anymore unsafe.close(unsafe.voidPromise()); return; } try { int readyOps = k.readyOps(); //什么事件发生了，如果是16代表的就是OP_ACCEPT // We first need to call finishConnect() before try to trigger a read(...) or write(...) as otherwise // the NIO JDK channel implementation may throw a NotYetConnectedException. if ((readyOps &amp; SelectionKey.OP_CONNECT) != 0) { // remove OP_CONNECT as otherwise Selector.select(..) will always return without blocking // See https://github.com/netty/netty/issues/924 int ops = k.interestOps(); ops &amp;= ~SelectionKey.OP_CONNECT; k.interestOps(ops); unsafe.finishConnect(); } // Process OP_WRITE first as we may be able to write some queued buffers and so free memory. if ((readyOps &amp; SelectionKey.OP_WRITE) != 0) { // Call forceFlush which will also take care of clear the OP_WRITE once there is nothing left to write ch.unsafe().forceFlush(); } // Also check for readOps of 0 to workaround possible JDK bug which may otherwise lead // to a spin loop if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) { unsafe.read(); //如果是16，进入这个方法 } } catch (CancelledKeyException ignored) { unsafe.close(unsafe.voidPromise()); } } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869private final class NioMessageUnsafe extends AbstractNioUnsafe { private final List&lt;Object&gt; readBuf = new ArrayList&lt;Object&gt;(); @Override public void read() { assert eventLoop().inEventLoop(); //初始化 final ChannelConfig config = config(); final ChannelPipeline pipeline = pipeline(); final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle(); allocHandle.reset(config); boolean closed = false; Throwable exception = null; try { try { do { int localRead = doReadMessages(readBuf);// if (localRead == 0) { break; } if (localRead &lt; 0) { closed = true; break; } allocHandle.incMessagesRead(localRead); //记录一下创建的次数 } while (allocHandle.continueReading());//判断是不是需要继续去读 } catch (Throwable t) { exception = t; } int size = readBuf.size(); for (int i = 0; i &lt; size; i ++) { readPending = false; //创建连接的结果通过fireChannelRead传播出去，就是pipeline中各种handle的执行 //其中有个关键的就是ServerBootStrapAcceptor 负责初始化创建的SocketChannel的 pipeline.fireChannelRead(readBuf.get(i)); } readBuf.clear(); allocHandle.readComplete(); pipeline.fireChannelReadComplete(); if (exception != null) { closed = closeOnReadError(exception); pipeline.fireExceptionCaught(exception); } if (closed) { inputShutdown = true; if (isOpen()) { close(voidPromise()); } } } finally { // Check if there is a readPending which was not processed yet. // This could be for two reasons: // * The user called Channel.read() or ChannelHandlerContext.read() in channelRead(...) method // * The user called Channel.read() or ChannelHandlerContext.read() in channelReadComplete(...) method // // See https://github.com/netty/netty/issues/2254 if (!readPending &amp;&amp; !config.isAutoRead()) { removeReadOp(); } } } } 123456789101112131415161718192021====== NioServerSocketChannel ======@Override protected int doReadMessages(List&lt;Object&gt; buf) throws Exception { SocketChannel ch = SocketUtils.accept(javaChannel());//接受新连接创建SocketChannel try { if (ch != null) { buf.add(new NioSocketChannel(this, ch)); //创建完SocketChannel之后放入到buf数组中去 return 1; //返回1代表仅仅创建了一个连接 } } catch (Throwable t) { logger.warn(&quot;Failed to create a new channel from an accepted socket.&quot;, t); try { ch.close(); } catch (Throwable t2) { logger.warn(&quot;Failed to close a socket.&quot;, t2); } } return 0; } 1234567891011121314===== SocketUtils =====public static SocketChannel accept(final ServerSocketChannel serverSocketChannel) throws IOException { try { return AccessController.doPrivileged(new PrivilegedExceptionAction&lt;SocketChannel&gt;() { @Override public SocketChannel run() throws IOException { //非阻塞模式下，没有连接请求时返回null return serverSocketChannel.accept();//接受创建连接请求来创建一个SocketChannel } }); } catch (PrivilegedActionException e) { throw (IOException) e.getCause(); } } 1234567@Override public boolean continueReading(UncheckedBooleanSupplier maybeMoreDataSupplier) { return config.isAutoRead() &amp;&amp; (!respectMaybeMoreData || maybeMoreDataSupplier.get()) &amp;&amp; totalMessages &lt; maxMessagePerRead &amp;&amp; //maxMessagePerRead 最大次数16次 totalBytesRead &gt; 0; //读取的字节数为0 } 找到ServerBootStrapAcceptor 12345678910111213141516171819202122232425===== ServerBootStrap ======public void channelRead(ChannelHandlerContext ctx, Object msg) { final Channel child = (Channel) msg; child.pipeline().addLast(childHandler); setChannelOptions(child, childOptions, logger); for (Entry&lt;AttributeKey&lt;?&gt;, Object&gt; e: childAttrs) { child.attr((AttributeKey&lt;Object&gt;) e.getKey()).set(e.getValue()); } try { childGroup.register(child).addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture future) throws Exception { if (!future.isSuccess()) { forceClose(child, future.cause()); } } }); } catch (Throwable t) { forceClose(child, t); } } 123456789101112131415161718 @Override public ChannelFuture register(Channel channel) { return next().register(channel); //选择一个子的元素 }===== SingleThreadEventLoop ===== 此时已经是workereventloop了 @Override public ChannelFuture register(Channel channel) { return register(new DefaultChannelPromise(channel, this)); } @Override public ChannelFuture register(final ChannelPromise promise) { ObjectUtil.checkNotNull(promise, &quot;promise&quot;); promise.channel().unsafe().register(this, promise); return promise; } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102===== AbstractChannel ======@Override public final void register(EventLoop eventLoop, final ChannelPromise promise) { if (eventLoop == null) { throw new NullPointerException(&quot;eventLoop&quot;); } if (isRegistered()) { promise.setFailure(new IllegalStateException(&quot;registered to an event loop already&quot;)); return; } if (!isCompatible(eventLoop)) { promise.setFailure( new IllegalStateException(&quot;incompatible event loop type: &quot; + eventLoop.getClass().getName())); return; } AbstractChannel.this.eventLoop = eventLoop; if (eventLoop.inEventLoop()) { //判断当前是不是eventloop所选定的线程 register0(promise); } else { try { eventLoop.execute(new Runnable() { @Override public void run() { register0(promise); } }); } catch (Throwable t) { logger.warn( &quot;Force-closing a channel whose registration task was not accepted by an event loop: {}&quot;, AbstractChannel.this, t); closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); } } }private void register0(ChannelPromise promise) { try { // check if the channel is still open as it could be closed in the mean time when the register // call was outside of the eventLoop if (!promise.setUncancellable() || !ensureOpen(promise)) { return; } boolean firstRegistration = neverRegistered; doRegister(); neverRegistered = false; registered = true; // Ensure we call handlerAdded(...) before we actually notify the promise. This is needed as the // user may already fire events through the pipeline in the ChannelFutureListener. pipeline.invokeHandlerAddedIfNeeded(); safeSetSuccess(promise); //这里是创建连接，就不会到bind的代码中 pipeline.fireChannelRegistered(); // Only fire a channelActive if the channel has never been registered. This prevents firing // multiple channel actives if the channel is deregistered and re-registered. if (isActive()) { //此时连接已经建立好了，已经是active了 if (firstRegistration) { pipeline.fireChannelActive();// 此时也是第一次注册，走到该方法中，触发HeadCOntent中的read方法 } else if (config().isAutoRead()) { // This channel was registered before and autoRead() is set. This means we need to begin read // again so that we process inbound data. // // See https://github.com/netty/netty/issues/4805 beginRead(); } } } catch (Throwable t) { // Close the channel directly to avoid FD leak. closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); } }===== AbstractNioChannel =====@Override protected void doRegister() throws Exception { boolean selected = false; for (;;) { try { selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this); return; } catch (CancelledKeyException e) { if (!selected) { // Force the Selector to select now as the &quot;canceled&quot; SelectionKey may still be // cached and not removed because no Select.select(..) operation was called yet. eventLoop().selectNow(); selected = true; } else { // We forced a select operation on the selector before but the SelectionKey is still cached // for whatever reason. JDK bug ? throw e; } } } } 12345==== HeadContext =====@Override public void read(ChannelHandlerContext ctx) { unsafe.beginRead(); //和启动服务后面的类似了，接受的是read事件 } selector.select()/selectNow()/select(timeoutMillis) 发现 OP_ACCEPT 事件，处理： SocketChannel socketChannel = serverSocketChannel.accept() selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this); selectionKey.interestOps(OP_READ); 注册OP_READ事件 创建连接的初始化和注册是通过 pipeline.fireChannelRead 在 ServerBootstrapAcceptor 中完成的。 第一次 Register 并不是监听 OP_READ ，而是 0 ： selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this) 。 最终监听 OP_READ 是通过“Register”完成后的fireChannelActive（io.netty.channel.AbstractChannel.AbstractUnsafe#register0中）来触发的 Worker’s NioEventLoop 是通过 Register 操作执行来启动。 接受连接的读操作，不会尝试读取更多次（16次）。 接受数据 读数据技巧 自适应数据大小的分配器（AdaptiveRecvByteBufAllocator）： 发放东西时，拿多大的桶去装？小了不够，大了浪费，所以会自己根据实际装的情况猜一猜下次情况，从而决定下次带多大的桶。 连续读（defaultMaxMessagesPerRead）： 发放东西时，假设拿的桶装满了，这个时候，你会觉得可能还有东西发放，所以直接拿个新桶等着装，而不是回家，直到后面出现没有装上的情况或者装了很多次需要给别人一点机会等原因才停止，回家。 断点位置在NioEventLoop中的697行的read方法中： 12345if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) { //如果是SocketChannel的话，跳转到NioByteChannel //如果是ServerSocketChannel的话，跳转到NioMessageChannel unsafe.read(); } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263===== AbstractNioByteChannel ======@Override public final void read() { final ChannelConfig config = config(); if (shouldBreakReadReady(config)) { clearReadPending(); return; } final ChannelPipeline pipeline = pipeline(); final ByteBufAllocator allocator = config.getAllocator(); //ByteBuf分配器 //AdaptiveRecvByteBufAllocator，帮忙决定下次分配多少 final RecvByteBufAllocator.Handle allocHandle = recvBufAllocHandle(); allocHandle.reset(config); ByteBuf byteBuf = null; boolean close = false; try { do { //经可能分配合适的大小 byteBuf = allocHandle.allocate(allocator); //读并且记录读了多少，如果读满了，下次continue的话就直接扩容 allocHandle.lastBytesRead(doReadBytes(byteBuf)); if (allocHandle.lastBytesRead() &lt;= 0) { // nothing was read. release the buffer. byteBuf.release(); byteBuf = null; close = allocHandle.lastBytesRead() &lt; 0; if (close) { // There is nothing left to read as we received an EOF. readPending = false; } break; } allocHandle.incMessagesRead(1); readPending = false; //在pipeline上执行，业务逻辑就是在这个地方 pipeline.fireChannelRead(byteBuf); byteBuf = null; } while (allocHandle.continueReading()); //判断是不是要继续读，此时不是创建连接的时候了，最后一个判断满足 //记录这次读事件总共读了多少数据，计算下次分配大小 allocHandle.readComplete(); pipeline.fireChannelReadComplete(); //相当于完成本次读事件的处理 if (close) { closeOnRead(pipeline); } } catch (Throwable t) { handleReadException(pipeline, byteBuf, t, close, allocHandle); } finally { // Check if there is a readPending which was not processed yet. // This could be for two reasons: // * The user called Channel.read() or ChannelHandlerContext.read() in channelRead(...) method // * The user called Channel.read() or ChannelHandlerContext.read() in channelReadComplete(...) method // // See https://github.com/netty/netty/issues/2254 if (!readPending &amp;&amp; !config.isAutoRead()) { removeReadOp(); } } } } 1234@Override public ByteBuf allocate(ByteBufAllocator alloc) { return alloc.ioBuffer(guess()); //猜，一开始是1024 } 1234567===== NioSocketChannel ======@Override protected int doReadBytes(ByteBuf byteBuf) throws Exception { final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle(); allocHandle.attemptedBytesRead(byteBuf.writableBytes()); return byteBuf.writeBytes(javaChannel(), allocHandle.attemptedBytesRead()); } 12345678910===== AbstractByteBuf ======@Override public int writeBytes(ScatteringByteChannel in, int length) throws IOException { ensureWritable(length); int writtenBytes = setBytes(writerIndex, in, length); if (writtenBytes &gt; 0) { writerIndex += writtenBytes; } return writtenBytes; } 123456789=====PooledByteBuf======@Override public final int setBytes(int index, ScatteringByteChannel in, int length) throws IOException { try { return in.read(internalNioBuffer(index, length)); } catch (ClosedChannelException ignored) { return -1; } } 123456789101112131415@Override public boolean continueReading(UncheckedBooleanSupplier maybeMoreDataSupplier) { return config.isAutoRead() &amp;&amp; (!respectMaybeMoreData || maybeMoreDataSupplier.get()) &amp;&amp; //maybeMoreDataSupplier.get()判断是不是满载而归 totalMessages &lt; maxMessagePerRead &amp;&amp; totalBytesRead &gt; 0; }private final UncheckedBooleanSupplier defaultMaybeMoreSupplier = new UncheckedBooleanSupplier() { @Override public boolean get() { return attemptedBytesRead == lastBytesRead; } }; 总结读取数据本质：sun.nio.ch.SocketChannelImpl#read(java.nio.ByteBuffer) NioSocketChannel read() 是读数据， NioServerSocketChannel read() 是创建连接 pipeline.fireChannelReadComplete(); 一次读事件处理完成 pipeline.fireChannelRead(byteBuf); 一次读数据完成，一次读事件处理可能会包含多次读数据操作。 为什么最多只尝试读取 16 次？“雨露均沾” AdaptiveRecvByteBufAllocator 对 bytebuf 的猜测：放大果断，缩小谨慎（需要连续 2 次判断） 业务处理 还是从NioEventLoop中的unsafe.read()开始 123if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) { unsafe.read(); } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354===== AbstractNioByteChannel =====@Override public final void read() { final ChannelConfig config = config(); if (shouldBreakReadReady(config)) { clearReadPending(); return; } final ChannelPipeline pipeline = pipeline(); final ByteBufAllocator allocator = config.getAllocator(); final RecvByteBufAllocator.Handle allocHandle = recvBufAllocHandle(); allocHandle.reset(config); ByteBuf byteBuf = null; boolean close = false; try { do { byteBuf = allocHandle.allocate(allocator); allocHandle.lastBytesRead(doReadBytes(byteBuf)); if (allocHandle.lastBytesRead() &lt;= 0) { // nothing was read. release the buffer. byteBuf.release(); byteBuf = null; close = allocHandle.lastBytesRead() &lt; 0; if (close) { // There is nothing left to read as we received an EOF. readPending = false; } break; } allocHandle.incMessagesRead(1); readPending = false; pipeline.fireChannelRead(byteBuf); //在这里处理业务逻辑 byteBuf = null; } while (allocHandle.continueReading()); allocHandle.readComplete(); pipeline.fireChannelReadComplete(); if (close) { closeOnRead(pipeline); } } catch (Throwable t) { handleReadException(pipeline, byteBuf, t, close, allocHandle); } finally { if (!readPending &amp;&amp; !config.isAutoRead()) { removeReadOp(); } } } } 123456789101112131415161718192021222324252627282930313233343536373839404142====== DefaultChannelPipeline ======@Override public final ChannelPipeline fireChannelRead(Object msg) { AbstractChannelHandlerContext.invokeChannelRead(head, msg); return this; }===== AbstractChannelHandlerContext =====static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) { final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, &quot;msg&quot;), next); EventExecutor executor = next.executor(); //默认是NioEventLoop，可以指定 if (executor.inEventLoop()) { next.invokeChannelRead(m); } else { executor.execute(new Runnable() { @Override public void run() { next.invokeChannelRead(m); } }); } }private void invokeChannelRead(Object msg) { if (invokeHandler()) { try { ((ChannelInboundHandler) handler()).channelRead(this, msg); //channelRead方法可以跳转到业务逻辑那里 } catch (Throwable t) { notifyHandlerException(t); } } else { fireChannelRead(msg); } }====== DefaultChannelPipeline ======@Override public void channelRead(ChannelHandlerContext ctx, Object msg) { ctx.fireChannelRead(msg); } 1234567891011121314151617181920212223242526272829303132===== AbstractChannelHandlerContext =====@Override public ChannelHandlerContext fireChannelRead(final Object msg) { //找head的下面一个可以执行CHANNEL_READ的方法 invokeChannelRead(findContextInbound(MASK_CHANNEL_READ), msg); return this; }private AbstractChannelHandlerContext findContextInbound(int mask) { AbstractChannelHandlerContext ctx = this; do { ctx = ctx.next; //先next一下 } while ((ctx.executionMask &amp; mask) == 0); //判断是不是有执行的资格 return ctx; }static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) { final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, &quot;msg&quot;), next); EventExecutor executor = next.executor(); if (executor.inEventLoop()) { next.invokeChannelRead(m); } else { executor.execute(new Runnable() { @Override public void run() { next.invokeChannelRead(m); } }); } } 1234567public class EchoServerHandler extends ChannelInboundHandlerAdapter { @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { //这里写业务逻辑 ctx.write(msg); } 总结 处理业务本质：数据在 pipeline 中所有的 handler 的 channelRead() 执行过程Handler 要实现 io.netty.channel.ChannelInboundHandler#channelRead (ChannelHandlerContext ctx, Object msg)，且不能加注解 @Skip 才能被执行到。中途可退出，不保证执行到 Tail Handler。 默认处理线程就是 Channel 绑定的 NioEventLoop 线程，也可以设置其他： pipeline.addLast(new UnorderedThreadPoolEventExecutor(10), serverHandler) 发送数据写数据的三种方式 快递场景（包裹） Netty 写数据（数据） 揽收到仓库 write：写到一个 buffer 从仓库发货 flush: 把 buffer 里的数据发送出去 揽收到仓库并立马发货 （加急件） writeAndFlush：写到 buffer，立马发送 揽收与发货之间有个缓冲的仓库 Write 和 Flush 之间有个 ChannelOutboundBuffer 对方仓库爆仓时，送不了的时候，会停止送，协商等电话通知什么时候好了，再送。 Netty 写数据，写不进去时，会停止写，然后注册一个 OP_WRITE 事件，来通知什么时候可以写进去了再写。 发送快递时，对方仓库都直接收下，这个时候再发送快递时，可以尝试发送更多的快递试试，这样效果更好。 Netty 批量写数据时，如果想写的都写进去了，接下来的尝试写更多（调整 maxBytesPerGatheringWrite） 发送快递时，发到某个地方的快递特别多，我们会连续发，但是快递车毕竟有限，也会考虑下其他地方。 Netty 只要有数据要写，且能写的出去，则一直尝试，直到写不出去或者满 16 次（writeSpinCount）。 揽收太多，发送来不及时，爆仓，这个时候会出个告示牌：收不下了，最好过 2 天再来邮寄吧。 Netty 待写数据太多，超过一定的水位线（**writeBufferWaterMark.high()**），会将可写的标志位改成 false ，让应用端自己做决定要不要发送数据了。 ​ 源码分析接着上面的断点： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354@Override public ChannelFuture write(Object msg) { return write(msg, newPromise()); } @Override public ChannelFuture write(final Object msg, final ChannelPromise promise) { write(msg, false, promise); return promise; }===== AbstractChannelHandlerContext ===== private void write(Object msg, boolean flush, ChannelPromise promise) { ObjectUtil.checkNotNull(msg, &quot;msg&quot;); try { if (isNotValidPromise(promise, true)) { ReferenceCountUtil.release(msg); // cancelled return; } } catch (RuntimeException e) { ReferenceCountUtil.release(msg); throw e; } //当前的handle是我们自己写的业务逻辑处理handle，这里找下一个handle final AbstractChannelHandlerContext next = findContextOutbound(flush ? (MASK_WRITE | MASK_FLUSH) : MASK_WRITE); final Object m = pipeline.touch(msg, next); EventExecutor executor = next.executor(); if (executor.inEventLoop()) { if (flush) { next.invokeWriteAndFlush(m, promise); } else { next.invokeWrite(m, promise); } } else { final AbstractWriteTask task; if (flush) { task = WriteAndFlushTask.newInstance(next, m, promise); } else { task = WriteTask.newInstance(next, m, promise); //这里wirte } if (!safeExecute(executor, task, promise, m)) { // We failed to submit the AbstractWriteTask. We need to cancel it so we decrement the pending bytes // and put it back in the Recycler for re-use later. // // See https://github.com/netty/netty/issues/8343. task.cancel(); } } } 1234567891011121314151617181920private void invokeWrite(Object msg, ChannelPromise promise) { if (invokeHandler()) { invokeWrite0(msg, promise); } else { write(msg, promise); } } private void invokeWrite0(Object msg, ChannelPromise promise) { try { ((ChannelOutboundHandler) handler()).write(this, msg, promise); } catch (Throwable t) { notifyOutboundHandlerException(t, promise); } }@Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) { unsafe.write(msg, promise); } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public final void write(Object msg, ChannelPromise promise) { assertEventLoop(); ChannelOutboundBuffer outboundBuffer = this.outboundBuffer; //类比前面发快递的仓库了 if (outboundBuffer == null) { // If the outboundBuffer is null we know the channel was closed and so // need to fail the future right away. If it is not null the handling of the rest // will be done in flush0() // See https://github.com/netty/netty/issues/2362 safeSetFailure(promise, newClosedChannelException(initialCloseCause)); // release message now to prevent resource-leak ReferenceCountUtil.release(msg); return; } int size; try { msg = filterOutboundMessage(msg); size = pipeline.estimatorHandle().size(msg); if (size &lt; 0) { size = 0; } } catch (Throwable t) { safeSetFailure(promise, t); ReferenceCountUtil.release(msg); return; } outboundBuffer.addMessage(msg, size, promise);//把消息放入buffer中 }===== ChannelOutboundBuffer =====public void addMessage(Object msg, int size, ChannelPromise promise) { Entry entry = Entry.newInstance(msg, size, total(msg), promise); if (tailEntry == null) { flushedEntry = null; } else { Entry tail = tailEntry; tail.next = entry; } tailEntry = entry; if (unflushedEntry == null) { unflushedEntry = entry; //追加到队尾 } // increment pending bytes after adding message to the unflushed arrays. // See https://github.com/netty/netty/issues/1619 incrementPendingOutboundBytes(entry.pendingSize, false); //根据前面算的size来更改buffer中还有多少数据未处理 }private void incrementPendingOutboundBytes(long size, boolean invokeLater) { if (size == 0) { return; } long newWriteBufferSize = TOTAL_PENDING_SIZE_UPDATER.addAndGet(this, size); if (newWriteBufferSize &gt; channel.config().getWriteBufferHighWaterMark()) { setUnwritable(invokeLater); } } write完之后就到了发送flush过程 1234567891011public class EchoServerHandler extends ChannelInboundHandlerAdapter { @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { ctx.write(msg); } @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { ctx.flush(); //到了这里了 } 1234567891011121314151617@Override public ChannelHandlerContext flush() { //找到下一级的handle final AbstractChannelHandlerContext next = findContextOutbound(MASK_FLUSH); EventExecutor executor = next.executor(); if (executor.inEventLoop()) { next.invokeFlush(); } else { Tasks tasks = next.invokeTasks; if (tasks == null) { next.invokeTasks = tasks = new Tasks(next); } safeExecute(executor, tasks.invokeFlushTask, channel().voidPromise(), null); } return this; } 123456789101112131415private void invokeFlush() { if (invokeHandler()) { invokeFlush0(); } else { flush(); } } private void invokeFlush0() { try { ((ChannelOutboundHandler) handler()).flush(this); } catch (Throwable t) { notifyHandlerException(t); } } 123456789101112131415161718@Override public void flush(ChannelHandlerContext ctx) { unsafe.flush(); }@Override public final void flush() { assertEventLoop(); ChannelOutboundBuffer outboundBuffer = this.outboundBuffer; if (outboundBuffer == null) { return; } outboundBuffer.addFlush(); flush0(); } 12345678910111213141516171819202122232425public void addFlush() { // There is no need to process all entries if there was already a flush before and no new messages // where added in the meantime. // // See https://github.com/netty/netty/issues/2577 Entry entry = unflushedEntry; //将没有flush的数据加入到数组中 if (entry != null) { if (flushedEntry == null) { // there is no flushedEntry yet, so start with the entry flushedEntry = entry; } do { flushed ++; if (!entry.promise.setUncancellable()) { // Was cancelled so make sure we free up memory and notify about the freed bytes int pending = entry.cancel(); decrementPendingOutboundBytes(pending, false, true); } entry = entry.next; } while (entry != null); // All flushed so reset unflushedEntry unflushedEntry = null; } } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546protected void flush0() { if (inFlush0) { // Avoid re-entrance return; } final ChannelOutboundBuffer outboundBuffer = this.outboundBuffer; if (outboundBuffer == null || outboundBuffer.isEmpty()) { return; } inFlush0 = true; // Mark all pending write requests as failure if the channel is inactive. if (!isActive()) { try { if (isOpen()) { outboundBuffer.failFlushed(new NotYetConnectedException(), true); } else { // Do not trigger channelWritabilityChanged because the channel is closed already. outboundBuffer.failFlushed(newClosedChannelException(initialCloseCause), false); } } finally { inFlush0 = false; } return; } try { doWrite(outboundBuffer); //在这里进入 } catch (Throwable t) { if (t instanceof IOException &amp;&amp; config().isAutoClose()) { initialCloseCause = t; close(voidPromise(), t, newClosedChannelException(t), false); } else { try { shutdownOutput(voidPromise(), t); } catch (Throwable t2) { initialCloseCause = t; close(voidPromise(), t2, newClosedChannelException(t), false); } } } finally { inFlush0 = false; } } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263===== NioSocketChannel =====protected void doWrite(ChannelOutboundBuffer in) throws Exception { SocketChannel ch = javaChannel(); int writeSpinCount = config().getWriteSpinCount(); do { if (in.isEmpty()) { // All written so clear OP_WRITE clearOpWrite(); // Directly return here so incompleteWrite(...) is not called. return; } // Ensure the pending writes are made of ByteBufs only. int maxBytesPerGatheringWrite = ((NioSocketChannelConfig) config).getMaxBytesPerGatheringWrite(); //最多返回1024个数据，总的size尽量不超过 maxBytesPerGatheringWrite ByteBuffer[] nioBuffers = in.nioBuffers(1024, maxBytesPerGatheringWrite); int nioBufferCnt = in.nioBufferCount(); // Always us nioBuffers() to workaround data-corruption. // See https://github.com/netty/netty/issues/2761 switch (nioBufferCnt) { case 0: // We have something else beside ByteBuffers to write so fallback to normal writes. writeSpinCount -= doWrite0(in); break; case 1: { // Only one ByteBuf so use non-gathering write // Zero length buffers are not added to nioBuffers by ChannelOutboundBuffer, so there is no need // to check if the total size of all the buffers is non-zero. ByteBuffer buffer = nioBuffers[0]; int attemptedBytes = buffer.remaining(); final int localWrittenBytes = ch.write(buffer); if (localWrittenBytes &lt;= 0) { incompleteWrite(true); return; } adjustMaxBytesPerGatheringWrite(attemptedBytes, localWrittenBytes, maxBytesPerGatheringWrite); in.removeBytes(localWrittenBytes); --writeSpinCount; break; } default: { // Zero length buffers are not added to nioBuffers by ChannelOutboundBuffer, so there is no need // to check if the total size of all the buffers is non-zero. // We limit the max amount to int above so cast is safe long attemptedBytes = in.nioBufferSize(); final long localWrittenBytes = ch.write(nioBuffers, 0, nioBufferCnt); if (localWrittenBytes &lt;= 0) { incompleteWrite(true); return; } // Casting to int is safe because we limit the total amount of data in the nioBuffers to int above. adjustMaxBytesPerGatheringWrite((int) attemptedBytes, (int) localWrittenBytes, maxBytesPerGatheringWrite); in.removeBytes(localWrittenBytes); --writeSpinCount; break; } } } while (writeSpinCount &gt; 0); incompleteWrite(writeSpinCount &lt; 0); } 总结 写的本质： Single write: sun.nio.ch.SocketChannelImpl#write(java.nio.ByteBuffer) gathering write：sun.nio.ch.SocketChannelImpl#write(java.nio.ByteBuffer[], int, int) 写数据写不进去时，会停止写，注册一个 OP_WRITE 事件，来通知什么时候可以写进去了。 OP_WRITE 不是说有数据可写，而是说可以写进去，所以正常情况，不能注册，否则一直触发。 批量写数据时，如果尝试写的都写进去了，接下来会尝试写更多（maxBytesPerGatheringWrite）。 只要有数据要写，且能写，则一直尝试，直到 16 次（writeSpinCount），写 16 次还没有写完，就直接 schedule 一个 task 来继续写，而不是用注册写事件来触发，更简洁有力。 待写数据太多，超过一定的水位线（writeBufferWaterMark.high()），会将可写的标志位改成 false ，让应用端自己做决定要不要继续写。 channelHandlerContext.channel().write() ：从 TailContext 开始执行； channelHandlerContext.write() : 从当前的 Context 开始。 ​ ​ 断开连接后续更新～ 关闭服务","link":"/2021/05/25/Netty%EF%BC%88%E5%85%AD%EF%BC%89%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"},{"title":"SpringBoot（一）入门","text":"SpringBoot入门主要讲了简介和如何快速使用！ SpringBoot简介HelloWorld 新建一个SpringBoot项目：IDEA -&gt; File -&gt; New -&gt;New Project -&gt; Spring Initializr(https://start.spring.io) -&gt;next即可。 依赖分析： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!--spring-boot-starter-parent指定了当前项目为一个Spring Boot项目，它提供了诸多的默认Maven依赖--&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.5.3&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.binshow&lt;/groupId&gt; &lt;artifactId&gt;Chapter2_DataManager&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;Chapter2_DataManager&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;!--properties标签可以手动的改依赖的版本--&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--maven会自动下载spring-boot-starter-web模块所包含的jar文件--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 快速的搭建一个web服务： 12345678910111213@RestController@SpringBootApplicationpublic class DemoApplication { @RequestMapping(&quot;/&quot;) String index() { return &quot;hello spring boot&quot;; } public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); }} 访问http://localhost:8080/看到hello spring boot说明启动成功了。 基础配置全局配置文件application.properties在src/main/resources目录下，Spring Boot提供了一个名为application.properties的全局配置文件，可对一些默认配置的配置值进行修改 不仅仅可以配置所有官方属性，还可以自定义属性： 在application.properties定义一些属性： 12binshow.name = shengbinbinbinshow.age = 18 自己编写一个Bean：通过Value注解取到配置文件中的值 12345678910@Component@Datapublic class BinshowPropeties { //通过 @Value(&quot;${属性名}&quot;) 来加载配置文件中的属性值 @Value(&quot;${binshow.name}&quot;) private String name; @Value(&quot;${binshow.age}&quot;) private int age;} 编写Controller，查看是否注入到属性了 12345678910@RestControllerpublic class IndexController { @Autowired private BinshowPropeties binshowPropeties; @RequestMapping(&quot;/&quot;) String index() { return binshowPropeties.getName()+&quot;，&quot;+binshowPropeties.getAge(); }} http://localhost:8080/ 看到shengbinbin，18。说明值已经被取出来了。 在属性非常多的情况下，也可以定义一个和配置文件对应的Bean： 12345678910@ConfigurationProperties(prefix = &quot;binshow&quot;) // 注入配置文件中前缀为 binshow的属性@Configuration@Datapublic class ConfigBean { private String name; private int age; private String address;} 配置文件中的属性之间还可以相互引用： 1234binshow.name = shengbinbinbinshow.age = 18binshow.address = anhuiWuhubinshow.info = ${binshow.name}--${binshow.age} 自定义配置文件在src/main/resources目录下新建一个test.properties: 12test.name=zkdtest.age=25 测试： 123456789@Data@Configuration@ConfigurationProperties(prefix = &quot;test&quot;)@PropertySource(&quot;classpath:test.properties&quot;) // 指定要用到的配置文件public class TestConfigBean { private String name; private int age;} 1234567891011121314151617181920212223242526@RestControllerpublic class IndexController { @Autowired private BinshowPropeties binshowPropeties; @Autowired private ConfigBean configBean; @Autowired private TestConfigBean testConfigBean; @RequestMapping(&quot;/&quot;) String index() { return binshowPropeties.getName()+&quot;，&quot;+binshowPropeties.getAge(); } @RequestMapping(&quot;/config&quot;) String indexConfig() { return configBean.getName()+&quot;, &quot;+configBean.getAge() + &quot;, &quot; + configBean.getAddress(); } @RequestMapping(&quot;/test&quot;) String indexTest() { return testConfigBean.getName()+&quot;, &quot;+testConfigBean.getAge(); }}","link":"/2021/08/10/SpringBoot%EF%BC%88%E4%B8%80%EF%BC%89%E5%85%A5%E9%97%A8/"},{"title":"SpringBoot（三）整合视图层","text":"","link":"/2021/08/10/SpringBoot%EF%BC%88%E4%B8%89%EF%BC%89%E6%95%B4%E5%90%88%E8%A7%86%E5%9B%BE%E5%B1%82/"},{"title":"git之基本操作","text":"git是工作用到的分布式的版本控制工具 –学自极客时间git专栏 使用Git之前的最小配置配置用户名和邮箱12345678910shengbinbin@192 ~ % git versiongit version 2.24.3 (Apple Git-128)//配置用户名和邮箱 shengbinbin@192 ~ % git config --global user.name 'binshow'shengbinbin@192 ~ % git config --global user.email '1157024800@qq.com'--local ： 只对某个仓库有效--global 对当前用户的所有仓库有效--system 对系统所有登陆的用户有效，基本不用 123456shengbinbin@192 ~ % git config --list //显示git的配置credential.helper=osxkeychainuser.name=binshowuser.mail=1157024800@qq.comuser.email=1157024800@qq.comshengbinbin@192 ~ % 创建第一个仓库并配置local用户信息123456789101112131415161718192021222324252627282930313233343536shengbinbin@chengbinbindeMacBook-Pro Code % pwd/Users/shengbinbin/Documents/Codeshengbinbin@chengbinbindeMacBook-Pro Code % git init git_learning //1.创建好一个git仓库文件夹，命名为 git_learninghint: Using 'master' as the name for the initial branch. This default branch namehint: is subject to change. To configure the initial branch name to use in allhint: of your new repositories, which will suppress this warning, call:hint:hint: git config --global init.defaultBranch &lt;name&gt;hint:hint: Names commonly chosen instead of 'master' are 'main', 'trunk' andhint: 'development'. The just-created branch can be renamed via this command:hint:hint: git branch -m &lt;name&gt;Initialized empty Git repository in /Users/shengbinbin/Documents/Code/git_learning/.git/shengbinbin@chengbinbindeMacBook-Pro Code % cd git_learningshengbinbin@chengbinbindeMacBook-Pro git_learning % ls -altotal 0drwxr-xr-x 3 shengbinbin staff 96 6 22 21:47 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 9 shengbinbin staff 288 6 22 21:47 .git //2. .git是核心文件夹 //3.设置local的相关信息shengbinbin@chengbinbindeMacBook-Pro git_learning % git config --local user.name 'shengbinbin'shengbinbin@chengbinbindeMacBook-Pro git_learning % git config --local user.email '1157024800@qq.com'shengbinbin@chengbinbindeMacBook-Pro git_learning % git config --local --list //4.查看local的相关信息core.repositoryformatversion=0core.filemode=truecore.bare=falsecore.logallrefupdates=truecore.ignorecase=truecore.precomposeunicode=trueuser.name=shengbinbinuser.email=1157024800@qq.com 添加第一个文件readMe1234567891011121314151617181920212223242526272829303132333435363738394041 //1. 将材料中的readMe拷贝到当前目录下shengbinbin@chengbinbindeMacBook-Pro git_learning % cp ../0-material/readme .shengbinbin@chengbinbindeMacBook-Pro git_learning % ls -altotal 8drwxr-xr-x 4 shengbinbin staff 128 6 22 21:51 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 9 shengbinbin staff 288 6 22 21:50 .git-rw-r--r--@ 1 shengbinbin staff 51 6 22 21:51 readme //2. 直接进行commit提交会报错，（-m表示这次提交的意义是什么）shengbinbin@chengbinbindeMacBook-Pro git_learning % git commit -m'Add readMe'On branch masterInitial commitUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) readmenothing added to commit but untracked files present (use &quot;git add&quot; to track) //3. 先用git add 将文件加入暂存区shengbinbin@chengbinbindeMacBook-Pro git_learning % git add readmeshengbinbin@chengbinbindeMacBook-Pro git_learning % git status //4. 查看状态On branch masterNo commits yetChanges to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: readmeshengbinbin@chengbinbindeMacBook-Pro git_learning % git commit -m'Add readMe' //5. 最后成功提交[master (root-commit) d59544f] Add readMe 1 file changed, 2 insertions(+) create mode 100644 readmeshengbinbin@chengbinbindeMacBook-Pro git_learning % git logcommit d59544f7fba30a55f9511993709de2403c9cfbe5 (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt; //local的配置参数优先级较高Date: Tue Jun 22 21:52:44 2021 +0800 Add readMeshengbinbin@chengbinbindeMacBook-Pro git_learning % 工作区和暂存区 添加index + logo 123456789101112131415161718192021222324shengbinbin@chengbinbindeMacBook-Pro git_learning % cp ../0-material/index.html.01 index.html //复制一个网页过来shengbinbin@chengbinbindeMacBook-Pro git_learning % cp -r ../0-material/images . //复制整个文件夹过来shengbinbin@chengbinbindeMacBook-Pro git_learning % git statusOn branch masterUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) images/ index.htmlnothing added to commit but untracked files present (use &quot;git add&quot; to track)shengbinbin@chengbinbindeMacBook-Pro git_learning % git add index.html images //3.将两个文件加入到暂存区shengbinbin@chengbinbindeMacBook-Pro git_learning % git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) new file: images/git-logo.png new file: index.htmlshengbinbin@chengbinbindeMacBook-Pro git_learning % git commit -m 'ADD index + logo'[master 2a76eaa] ADD index + logo 2 files changed, 49 insertions(+) create mode 100644 images/git-logo.png create mode 100644 index.htmlshengbinbin@chengbinbindeMacBook-Pro git_learning % 添加css： 1234567891011121314151617181920212223shengbinbin@chengbinbindeMacBook-Pro git_learning % mkdir stylesshengbinbin@chengbinbindeMacBook-Pro git_learning % cp ../0-material/styles/style.css.01 styles/style.cssshengbinbin@chengbinbindeMacBook-Pro git_learning % ls -altotal 16drwxr-xr-x 7 shengbinbin staff 224 6 22 22:00 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 12 shengbinbin staff 384 6 22 21:57 .gitdrwxr-xr-x@ 3 shengbinbin staff 96 6 22 21:57 images-rw-r--r--@ 1 shengbinbin staff 1303 6 22 21:57 index.html-rw-r--r--@ 1 shengbinbin staff 51 6 22 21:51 readmedrwxr-xr-x 3 shengbinbin staff 96 6 22 22:00 stylesshengbinbin@chengbinbindeMacBook-Pro git_learning % git statusOn branch masterUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) styles/nothing added to commit but untracked files present (use &quot;git add&quot; to track)shengbinbin@chengbinbindeMacBook-Pro git_learning % git add stylesshengbinbin@chengbinbindeMacBook-Pro git_learning % git commit -m 'ADD style.css'[master 9bd6521] ADD style.css 1 file changed, 50 insertions(+) create mode 100644 styles/style.css 添加js： 123456789101112131415161718192021222324252627282930313233343536373839404142shengbinbin@chengbinbindeMacBook-Pro git_learning % cp -r ../0-material/js .shengbinbin@chengbinbindeMacBook-Pro git_learning % ls -altotal 16drwxr-xr-x 8 shengbinbin staff 256 6 22 22:04 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 12 shengbinbin staff 384 6 22 22:01 .gitdrwxr-xr-x@ 3 shengbinbin staff 96 6 22 21:57 images-rw-r--r--@ 1 shengbinbin staff 1303 6 22 21:57 index.htmldrwxr-xr-x@ 3 shengbinbin staff 96 6 22 22:04 js-rw-r--r--@ 1 shengbinbin staff 51 6 22 21:51 readmedrwxr-xr-x 3 shengbinbin staff 96 6 22 22:00 stylesshengbinbin@chengbinbindeMacBook-Pro git_learning % git add jsshengbinbin@chengbinbindeMacBook-Pro git_learning % git commit -m 'ADD js'[master 7a430c9] ADD js 1 file changed, 15 insertions(+) create mode 100644 js/script.jsshengbinbin@chengbinbindeMacBook-Pro git_learning % git log //查看git 提交日志commit 7a430c9c43daee3f83b4b3b607a8ef82e2061a46 (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:06:20 2021 +0800 ADD jscommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caaAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMe: 文件重命名git mv123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263shengbinbin@chengbinbindeMacBook-Pro git_learning % ls -altotal 16drwxr-xr-x 8 shengbinbin staff 256 6 22 22:04 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 12 shengbinbin staff 384 6 22 22:06 .gitdrwxr-xr-x@ 3 shengbinbin staff 96 6 22 21:57 images-rw-r--r--@ 1 shengbinbin staff 1303 6 22 21:57 index.htmldrwxr-xr-x@ 3 shengbinbin staff 96 6 22 22:04 js-rw-r--r--@ 1 shengbinbin staff 51 6 22 21:51 readmedrwxr-xr-x 3 shengbinbin staff 96 6 22 22:00 stylesshengbinbin@chengbinbindeMacBook-Pro git_learning % mv readMe readMe.md //1.本地目录重命名shengbinbin@chengbinbindeMacBook-Pro git_learning % ls -altotal 16drwxr-xr-x 8 shengbinbin staff 256 6 22 22:13 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 12 shengbinbin staff 384 6 22 22:06 .gitdrwxr-xr-x@ 3 shengbinbin staff 96 6 22 21:57 images-rw-r--r--@ 1 shengbinbin staff 1303 6 22 21:57 index.htmldrwxr-xr-x@ 3 shengbinbin staff 96 6 22 22:04 js-rw-r--r--@ 1 shengbinbin staff 51 6 22 21:51 readMe.mddrwxr-xr-x 3 shengbinbin staff 96 6 22 22:00 stylesshengbinbin@chengbinbindeMacBook-Pro git_learning % git status //2. git 认为是删除了readMe 新建了readMe.mdOn branch masterChanges not staged for commit: (use &quot;git add/rm &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) deleted: readmeUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) readMe.mdno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)shengbinbin@chengbinbindeMacBook-Pro git_learning % git add readMe.md //3.将readMe.md 加到暂存区shengbinbin@chengbinbindeMacBook-Pro git_learning % git rm readme //4. 删除git上的readmerm 'readme'shengbinbin@chengbinbindeMacBook-Pro git_learning % git status //5. git就识别出来了On branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) renamed: readme -&gt; readMe.mdshengbinbin@chengbinbindeMacBook-Pro git_learning % git reset --hard //6.清除本次提交，包括本地的修改HEAD is now at 7a430c9 ADD jsshengbinbin@chengbinbindeMacBook-Pro git_learning % git mv readme readme.md //7.在git上直接进行重命名shengbinbin@chengbinbindeMacBook-Pro git_learning % git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) renamed: readme -&gt; readme.md //8. 提交shengbinbin@chengbinbindeMacBook-Pro git_learning % git commit readme.md -m &quot;rename readme to readme.md&quot;[master aeb4213] rename readme to readme.md 1 file changed, 2 insertions(+) create mode 100644 readme.mdshengbinbin@chengbinbindeMacBook-Pro git_learning % 查看历史 git log123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104shengbinbin@chengbinbindeMacBook-Pro git_learning % git log --oneline //1. aeb4213 (HEAD -&gt; master) rename readme to readme.md7a430c9 ADD js9bd6521 ADD style.css2a76eaa ADD index + logod59544f Add readMeshengbinbin@chengbinbindeMacBook-Pro git_learning % git log -n2 --oneline // 看最近2次的提交历史aeb4213 (HEAD -&gt; master) rename readme to readme.md7a430c9 ADD jsshengbinbin@chengbinbindeMacBook-Pro git_learning % git branch -v //查看本地分支* master aeb4213 rename readme to readme.mdshengbinbin@chengbinbindeMacBook-Pro git_learning % git checkout -b temp 9bd6521 //创建一个新的分支D readmeSwitched to a new branch 'temp'shengbinbin@chengbinbindeMacBook-Pro git_learning % git branch -v master aeb4213 rename readme to readme.md* temp 9bd6521 ADD style.cssshengbinbin@chengbinbindeMacBook-Pro git_learning % ls -altotal 16drwxr-xr-x 7 shengbinbin staff 224 6 22 22:37 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 13 shengbinbin staff 416 6 22 22:37 .gitdrwxr-xr-x@ 3 shengbinbin staff 96 6 22 21:57 images-rw-r--r-- 1 shengbinbin staff 1303 6 22 22:36 index.html-rw-r--r-- 1 shengbinbin staff 51 6 22 22:37 readmedrwxr-xr-x 3 shengbinbin staff 96 6 22 22:00 stylesshengbinbin@chengbinbindeMacBook-Pro git_learning % vim readme // temp 分支修改了一下readmeshengbinbin@chengbinbindeMacBook-Pro git_learning % git commit -am'Add test' //直接提交（-am）[temp 2a5925a] Add test 1 file changed, 1 insertion(+)shengbinbin@chengbinbindeMacBook-Pro git_learning % git branch -av master aeb4213 rename readme to readme.md* temp 2a5925a Add testshengbinbin@chengbinbindeMacBook-Pro git_learning % git log //查看当前分支的提交记录commit 2a5925a9f096a881a838238a5ced6a5ff3b7f52a (HEAD -&gt; temp)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:38:52 2021 +0800 Add testcommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caaAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMe shengbinbin@chengbinbindeMacBook-Pro git_learning % git log --all --graph //查看全部分支的提交历史，图形化显示* commit 2a5925a9f096a881a838238a5ced6a5ff3b7f52a (HEAD -&gt; temp)| Author: shengbinbin &lt;1157024800@qq.com&gt;| Date: Tue Jun 22 22:38:52 2021 +0800|| Add test|| * commit aeb4213f9fe55e5ffa1a866df58a519c9dcc18e4 (master)| | Author: shengbinbin &lt;1157024800@qq.com&gt;| | Date: Tue Jun 22 22:23:50 2021 +0800| || | rename readme to readme.md| || * commit 7a430c9c43daee3f83b4b3b607a8ef82e2061a46|/ Author: shengbinbin &lt;1157024800@qq.com&gt;| Date: Tue Jun 22 22:06:20 2021 +0800|| ADD js|* commit 9bd65211b66a0a27503644f8fe66a66a428d4ecb| Author: shengbinbin &lt;1157024800@qq.com&gt;| Date: Tue Jun 22 22:01:17 2021 +0800|| ADD style.css|* commit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caa| Author: shengbinbin &lt;1157024800@qq.com&gt;| Date: Tue Jun 22 21:57:59 2021 +0800|| ADD index + logo|* commit d59544f7fba30a55f9511993709de2403c9cfbe5 Author: shengbinbin &lt;1157024800@qq.com&gt; Date: Tue Jun 22 21:52:44 2021 +0800 Add readMe shengbinbin@chengbinbindeMacBook-Pro git_learning % git log --all --graph --oneline* 2a5925a (HEAD -&gt; temp) Add test| * aeb4213 (master) rename readme to readme.md| * 7a430c9 ADD js|/* 9bd6521 ADD style.css* 2a76eaa ADD index + logo* d59544f Add readMeshengbinbin@chengbinbindeMacBook-Pro git_learning % gitk可视化工具 探密.git裸仓库HEAD 和 config12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667shengbinbin@192 git_learning % ls -al //1.查看初始化的仓库中有什么total 16drwxr-xr-x 7 shengbinbin staff 224 6 22 22:38 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 14 shengbinbin staff 448 6 22 22:45 .git //核心文件夹drwxr-xr-x@ 3 shengbinbin staff 96 6 22 21:57 images-rw-r--r-- 1 shengbinbin staff 1303 6 22 22:36 index.html-rw-r--r-- 1 shengbinbin staff 57 6 22 22:38 readmedrwxr-xr-x 3 shengbinbin staff 96 6 22 22:00 stylesshengbinbin@192 git_learning % cd .gitshengbinbin@192 .git % ls -al //2. 进入核心文件夹 .git 查看内容total 56drwxr-xr-x 14 shengbinbin staff 448 6 22 22:45 .drwxr-xr-x 7 shengbinbin staff 224 6 22 22:38 ..-rw-r--r-- 1 shengbinbin staff 9 6 22 22:38 COMMIT_EDITMSG-rw-r--r-- 1 shengbinbin staff 21 6 22 22:34 HEAD-rw-r--r-- 1 shengbinbin staff 41 6 22 22:22 ORIG_HEAD-rw-r--r-- 1 shengbinbin staff 191 6 22 21:50 config-rw-r--r-- 1 shengbinbin staff 73 6 22 21:47 description-rw-r--r-- 1 shengbinbin staff 461 6 22 22:45 gitk.cachedrwxr-xr-x 15 shengbinbin staff 480 6 22 21:47 hooks-rw-r--r-- 1 shengbinbin staff 447 6 22 22:38 indexdrwxr-xr-x 3 shengbinbin staff 96 6 22 21:47 infodrwxr-xr-x 4 shengbinbin staff 128 6 22 21:52 logsdrwxr-xr-x 24 shengbinbin staff 768 6 22 22:38 objectsdrwxr-xr-x 4 shengbinbin staff 128 6 22 21:47 refsshengbinbin@192 .git % cat HEAD //3. 观察发现HEAD是一个引用，目前正指在 temp分支上ref: refs/heads/tempshengbinbin@192 .git % cd ..shengbinbin@192 git_learning % git checkout master //4. 切换分支Switched to branch 'master'shengbinbin@192 git_learning % cat .git/HEAD //5. 观察HEAD的引用也随之发生了变化ref: refs/heads/mastershengbinbin@192 git_learning % cat .git/config //6. 观察发现 config中存放的都是本地相关的配置[core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true[user] name = shengbinbin email = 1157024800@qq.comshengbinbin@192 git_learning % vim .git/config //7. 直接修改这个配置文件中的usernameshengbinbin@192 git_learning % git config --local user.name //8.发现username确实发生了变化zhangkedanshengbinbin@192 git_learning % git config --local user.name 'shengbinbin' //9.还原user.nameshengbinbin@192 git_learning % git config --local user.nameshengbinbinshengbinbin@192 git_learning % cat .git/config[core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true[user] name = shengbinbin email = 1157024800@qq.comshengbinbin@192 git_learning %shengbinbin@192 git_learning % ref123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172shengbinbin@192 git_learning % cd .gitshengbinbin@192 .git % ls -altotal 56drwxr-xr-x 14 shengbinbin staff 448 6 26 23:29 .drwxr-xr-x 9 shengbinbin staff 288 6 26 23:25 ..-rw-r--r-- 1 shengbinbin staff 9 6 22 22:38 COMMIT_EDITMSG-rw-r--r-- 1 shengbinbin staff 23 6 26 23:25 HEAD-rw-r--r-- 1 shengbinbin staff 41 6 22 22:22 ORIG_HEAD-rw-r--r-- 1 shengbinbin staff 191 6 26 23:29 config-rw-r--r-- 1 shengbinbin staff 73 6 22 21:47 description-rw-r--r-- 1 shengbinbin staff 461 6 22 22:45 gitk.cachedrwxr-xr-x 15 shengbinbin staff 480 6 22 21:47 hooks-rw-r--r-- 1 shengbinbin staff 626 6 26 23:25 indexdrwxr-xr-x 3 shengbinbin staff 96 6 22 21:47 infodrwxr-xr-x 4 shengbinbin staff 128 6 22 21:52 logsdrwxr-xr-x 24 shengbinbin staff 768 6 22 22:38 objectsdrwxr-xr-x 4 shengbinbin staff 128 6 22 21:47 refsshengbinbin@192 .git % cd refsshengbinbin@192 refs % ls -altotal 0drwxr-xr-x 4 shengbinbin staff 128 6 22 21:47 .drwxr-xr-x 14 shengbinbin staff 448 6 26 23:29 ..drwxr-xr-x 4 shengbinbin staff 128 6 22 22:38 heads //1. 存放的分支信息drwxr-xr-x 3 shengbinbin staff 96 6 22 22:53 tagsshengbinbin@192 refs % cd headsshengbinbin@192 heads % ls -altotal 16drwxr-xr-x 4 shengbinbin staff 128 6 22 22:38 .drwxr-xr-x 4 shengbinbin staff 128 6 22 21:47 ..-rw-r--r-- 1 shengbinbin staff 41 6 22 22:23 master-rw-r--r-- 1 shengbinbin staff 41 6 22 22:38 tempshengbinbin@192 heads % pwd/Users/shengbinbin/Documents/Code/git_learning/.git/refs/headsshengbinbin@192 heads % cat master //2. 查看master的内容（就是一个哈希码）aeb4213f9fe55e5ffa1a866df58a519c9dcc18e4shengbinbin@192 heads % git cat-file -t aeb4213f9 //3. 使用git的命令看一下这个哈希码代表了什么东西commitshengbinbin@192 heads % git branch -av* master aeb4213 rename readme to readme.md temp 2a5925a Add test shengbinbin@192 heads % cat temp2a5925a9f096a881a838238a5ced6a5ff3b7f52ashengbinbin@192 heads % cd ..shengbinbin@192 refs % ls tags //4. tags文件中存放的就是之前在gitk中加上的tag。（里程碑的概念）js01shengbinbin@192 refs % cd tagsshengbinbin@192 tags % ls -altotal 8drwxr-xr-x 3 shengbinbin staff 96 6 22 22:53 .drwxr-xr-x 4 shengbinbin staff 128 6 22 21:47 ..-rw-r--r-- 1 shengbinbin staff 41 6 22 22:53 js01shengbinbin@192 tags % cat js01 //5. tag本身有一个哈希码966bd62acdabd98c6aaf60f2cdd4f9517c810915shengbinbin@192 tags % git cat-file -t 966bd6tagshengbinbin@192 tags % git cat-file -p 966bd6 //6. 查看tag的内容，里面存放有一个object对象object 7a430c9c43daee3f83b4b3b607a8ef82e2061a46type committag js01tagger shengbinbin &lt;1157024800@qq.com&gt; 1624373582 +0800first jsshengbinbin@192 tags % git cat-file -t 7a430c9 //7.这个object对象其实就是一个提交commitshengbinbin@192 tags % object1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495shengbinbin@192 .git % cd objectsshengbinbin@192 objects % ls -altotal 0drwxr-xr-x 24 shengbinbin staff 768 6 22 22:38 .drwxr-xr-x 14 shengbinbin staff 448 6 26 23:29 ..drwxr-xr-x 3 shengbinbin staff 96 6 22 21:57 01drwxr-xr-x 4 shengbinbin staff 128 6 22 22:38 2adrwxr-xr-x 3 shengbinbin staff 96 6 22 21:52 31drwxr-xr-x 3 shengbinbin staff 96 6 22 21:52 4bdrwxr-xr-x 3 shengbinbin staff 96 6 22 21:57 6adrwxr-xr-x 3 shengbinbin staff 96 6 22 22:06 7adrwxr-xr-x 3 shengbinbin staff 96 6 22 21:52 7cdrwxr-xr-x 3 shengbinbin staff 96 6 22 22:06 87drwxr-xr-x 3 shengbinbin staff 96 6 22 22:01 91drwxr-xr-x 4 shengbinbin staff 128 6 22 22:53 96drwxr-xr-x 3 shengbinbin staff 96 6 22 22:23 99drwxr-xr-x 4 shengbinbin staff 128 6 22 22:38 9bdrwxr-xr-x 3 shengbinbin staff 96 6 22 22:06 abdrwxr-xr-x 4 shengbinbin staff 128 6 22 22:23 aedrwxr-xr-x 3 shengbinbin staff 96 6 22 22:06 b7drwxr-xr-x 3 shengbinbin staff 96 6 22 22:23 cddrwxr-xr-x 3 shengbinbin staff 96 6 22 21:52 d5drwxr-xr-x 3 shengbinbin staff 96 6 22 21:57 dadrwxr-xr-x 3 shengbinbin staff 96 6 22 22:01 efdrwxr-xr-x 3 shengbinbin staff 96 6 22 22:38 f8drwxr-xr-x 2 shengbinbin staff 64 6 22 21:47 infodrwxr-xr-x 2 shengbinbin staff 64 6 22 21:47 packshengbinbin@192 objects % cd ae //1. 先随便进入一个文件夹查看里面有什么shengbinbin@192 ae % ls -altotal 16drwxr-xr-x 4 shengbinbin staff 128 6 22 22:23 .drwxr-xr-x 24 shengbinbin staff 768 6 22 22:38 ..-r--r--r-- 1 shengbinbin staff 164 6 22 22:23 b4213f9fe55e5ffa1a866df58a519c9dcc18e4-r--r--r-- 1 shengbinbin staff 53 6 22 22:01 e37060401d19e7bd9f80b7b33920a000e96b5bshengbinbin@192 ae % git cat-file -t aeb4213f9fe55e5ffa1a866df58a519c9dcc18e4 //2.将ae和后面的哈希码拼接之后发现类型commitshengbinbin@192 ae % git cat-file -t aee37060401treeshengbinbin@192 ae % git cat-file -p aee37060401 //3. 查看这个tree里面有什么内容。发现有个blob100644 blob ef3f137d8af338a8604544a3e482090684321d93 style.cssshengbinbin@192 ae % git cat-file -p ef3f137d8 //4. 查看这个blob，发现就是之前加入的cssbody{ background-color: orange; font-family: 'Monaco', sans-serif; color: white;}body a{ color: white;}header{ text-align: center; margin-top: 50px;}h3{ color: red;}header-img{ width: 400px;}header-words{ line-height: 10px; font-size: 50px; font-family: 'Monaco', sans-serif; margin-bottom: 75px;}section{ padding: 0 50px 0px 50px; text-align: left;}div.accordion { cursor: pointer; border: none; outline: none;}div.accordion.active, div.accordion:hover { background-color: white; color: #1D2031;}div.panel { padding: 0 18px 0 0; display: none;} 在git中，只要文件内容相同，就被视为是同一个blog！！！ commit/tree/blob三个对象的关系一个commit就代表了一棵树 blob指的就是某个具体的文件 分离头指针状态指的是commit提交的时候没有基于某个分支来做，HEAD并未指向任何分支，可能会被git清理掉。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111shengbinbin@192 git_learning % git logcommit aeb4213f9fe55e5ffa1a866df58a519c9dcc18e4 (HEAD -&gt; master) //1. 一开始的HEAD指向master分支Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800 rename readme to readme.mdcommit 7a430c9c43daee3f83b4b3b607a8ef82e2061a46 (tag: js01)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:06:20 2021 +0800 ADD jscommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caabody{Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMeshengbinbin@192 git_learning % git checkout 9bd6521 //2. 切换分支，切换到某次commit上的时候，Note: switching to '9bd6521'. //3. 提示当前处于分离头指针的情况You are in 'detached HEAD' state. You can look around, make experimentalchanges and commit them, and you can discard any commits you make in thisstate without impacting any branches by switching back to a branch.If you want to create a new branch to retain commits you create, you maydo so (now or later) by using -c with the switch command. Example: git switch -c &lt;new-branch-name&gt;Or undo this operation with: git switch -Turn off this advice by setting config variable advice.detachedHead to falseHEAD is now at 9bd6521 ADD style.cssshengbinbin@192 git_learning % ls -altotal 16drwxr-xr-x 7 shengbinbin staff 224 6 27 16:02 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 14 shengbinbin staff 448 6 27 16:02 .gitdrwxr-xr-x@ 3 shengbinbin staff 96 6 22 21:57 images-rw-r--r-- 1 shengbinbin staff 1303 6 22 22:36 index.html-rw-r--r-- 1 shengbinbin staff 51 6 26 23:25 readmedrwxr-xr-x 3 shengbinbin staff 96 6 22 22:00 stylesshengbinbin@192 git_learning % vim styles/style.css //4. 在分离头指针的情况下进行修改shengbinbin@192 git_learning % git status HEAD detached at 9bd6521Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: styles/style.cssno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)shengbinbin@192 git_learning % git commit -am'Background to green'[detached HEAD fcad9c2] Background to green 1 file changed, 1 insertion(+), 1 deletion(-)shengbinbin@192 git_learning % git logcommit fcad9c2c62b980debc083ff34ef0496512ab2756 (HEAD)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Sun Jun 27 16:03:31 2021 +0800 Background to greencommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caaAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMe shengbinbin@192 git_learning % git checkout master // 5. 切换分支的时候提示你是否对当前提交进行保存Warning: you are leaving 1 commit behind, not connected toany of your branches: fcad9c2 Background to greenIf you want to keep it by creating a new branch, this may be a good timeto do so with: git branch &lt;new-branch-name&gt; fcad9c2Switched to branch 'master'shengbinbin@192 git_learning % git branch fix_css fcad9c2 //6. 将当前分支修改的内容用分支 fix_css来保存下来shengbinbin@192 git_learning % git的基本操作比较两次提交的差异123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//比较两次提交的差异 git diff + 两个 commitIdshengbinbin@192 git_learning % git checkout -b fix_readme fix_cssSwitched to a new branch 'fix_readme'shengbinbin@192 git_learning % git log -n1commit fcad9c2c62b980debc083ff34ef0496512ab2756 (HEAD -&gt; fix_readme, fix_css)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Sun Jun 27 16:03:31 2021 +0800 Background to greenshengbinbin@192 git_learning % gitk --allshengbinbin@192 git_learning % cat .git/HEADref: refs/heads/fix_readmeshengbinbin@192 git_learning % cat .git/refs/heads/fix_readmefcad9c2c62b980debc083ff34ef0496512ab2756shengbinbin@192 git_learning % git cat-file -t fcad9c2commitshengbinbin@192 git_learning % git logcommit fcad9c2c62b980debc083ff34ef0496512ab2756 (HEAD -&gt; fix_readme, fix_css)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Sun Jun 27 16:03:31 2021 +0800 Background to greencommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caaAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMe shengbinbin@192 git_learning % git diff fcad9c2 9bd6521 //比较两次提交的差异diff --git a/styles/style.css b/styles/style.cssindex 4c6bc45..ef3f137 100644--- a/styles/style.css+++ b/styles/style.css@@ -1,5 +1,5 @@ body{- background-color: green;+ background-color: orange; font-family: 'Monaco', sans-serif; color: white; }shengbinbin@192 git_learning % 删除不需要的分支 123456789101112shengbinbin@192 git_learning % git branch -av fix_css fcad9c2 Background to green* fix_readme fcad9c2 Background to green master aeb4213 rename readme to readme.md temp 2a5925a Add testshengbinbin@192 git_learning % gitk --allshengbinbin@192 git_learning % git branch -d fix_css // 删除 fix_css这个分支Deleted branch fix_css (was fcad9c2).shengbinbin@192 git_learning % gitk --allshengbinbin@192 git_learning % 修改commit的message修改上一次提交的commit，使用 git commit –amend 1234567891011121314151617181920shengbinbin@192 git_learning % git log -n1commit aeb4213f9fe55e5ffa1a866df58a519c9dcc18e4 (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800 rename readme to readme.mdshengbinbin@192 git_learning % git commit --amend 'Move filename readme to readme.md'error: pathspec 'Move filename readme to readme.md' did not match any file(s) known to gitshengbinbin@192 git_learning % git commit --amend[master 76459c3] Mobe filename readme to readme.md Date: Tue Jun 22 22:23:50 2021 +0800 1 file changed, 2 insertions(+) create mode 100644 readme.mdshengbinbin@192 git_learning % git log -n1commit 76459c3e75e7a91af40e3fcd340e4c488f4ed589 (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800 Mobe filename readme to readme.mdshengbinbin@192 git_learning % 修改之前的commit信息，不是上一个的：git rebase -i 变基式操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344shengbinbin@192 git_learning % git log -3commit 76459c3e75e7a91af40e3fcd340e4c488f4ed589 (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800 Mobe filename readme to readme.mdcommit 7a430c9c43daee3f83b4b3b607a8ef82e2061a46 (tag: js01)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:06:20 2021 +0800 ADD jscommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.cssshengbinbin@192 git_learning % git rebase -i 9bd65211[detached HEAD 95a2ce3] ADD a js Date: Tue Jun 22 22:06:20 2021 +0800 1 file changed, 15 insertions(+) create mode 100644 js/script.jsSuccessfully rebased and updated refs/heads/master.shengbinbin@192 git_learning % git log -n3commit f8367296bd1af13c79e6e23f1ade3cef425bb26a (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800 Mobe filename readme to readme.mdcommit 95a2ce35bfe6ea75912d0ab6ee6ad793e090657bAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:06:20 2021 +0800 ADD a jscommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.css 1234567891011121314151617181920212223242526pick 7a430c9 ADD js //将pick改成reword，保存退出pick 76459c3 Mobe filename readme to readme.md# Rebase 9bd6521..76459c3 onto 9bd6521 (2 commands)## Commands:# p, pick &lt;commit&gt; = use commit# r, reword &lt;commit&gt; = use commit, but edit the commit message# e, edit &lt;commit&gt; = use commit, but stop for amending# s, squash &lt;commit&gt; = use commit, but meld into previous commit# f, fixup &lt;commit&gt; = like &quot;squash&quot;, but discard this commit's log message# x, exec &lt;command&gt; = run command (the rest of the line) using shell# b, break = stop here (continue rebase later with 'git rebase --continue')# d, drop &lt;commit&gt; = remove commit# l, label &lt;label&gt; = label current HEAD with a name# t, reset &lt;label&gt; = reset HEAD to a label# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]# . create a merge commit using the original merge commit's# . message (or the oneline, if no original merge commit was# . specified). Use -c &lt;commit&gt; to reword the commit message.## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted. 1234567891011121314151617ADD a js //此时修改message# Please enter the commit message for your changes. Lines starting# with '#' will be ignored, and an empty message aborts the commit.## Date: Tue Jun 22 22:06:20 2021 +0800## interactive rebase in progress; onto 9bd6521# Last command done (1 command done):# reword 7a430c9 ADD js# Next command to do (1 remaining command):# pick 76459c3 Mobe filename readme to readme.md# You are currently editing a commit while rebasing branch 'master' on '9bd6521'.## Changes to be committed:# new file: js/script.js# 将连续的commit整理成一个12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364shengbinbin@192 git_learning % git log // 查看历史commit f8367296bd1af13c79e6e23f1ade3cef425bb26a (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800 Mobe filename readme to readme.mdcommit 95a2ce35bfe6ea75912d0ab6ee6ad793e090657bAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:06:20 2021 +0800 ADD a jscommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caaAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMeshengbinbin@192 git_learning % git rebase -i d59544f7 //将这个commit 后面的三个commit合并成一个[detached HEAD 88f9280] create a complete web page ADD index + logo ADD style.css ADD a js Date: Tue Jun 22 21:57:59 2021 +0800 4 files changed, 114 insertions(+) create mode 100644 images/git-logo.png create mode 100644 index.html create mode 100644 js/script.js create mode 100644 styles/style.cssSuccessfully rebased and updated refs/heads/master.shengbinbin@192 git_learning % git log //合并完之后再看一下logcommit ea5ccd12fd5b46a7536a92fec3cf6619dbe993f3 (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800 Mobe filename readme to readme.mdcommit 88f92801204a631a4eb1239a997b5457bc4f8c34Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 create a complete web page ADD index + logo ADD style.css ADD a jscommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMeshengbinbin@192 git_learning % 1234567891011121314151617181920212223242526272829pick 2a76eaa ADD index + logopick 9bd6521 ADD style.css //将pick改成 spick 95a2ce3 ADD a js //将pick改成spick f836729 Mobe filename readme to readme.md# Rebase d59544f..f836729 onto d59544f (4 commands)## Commands:# p, pick &lt;commit&gt; = use commit# r, reword &lt;commit&gt; = use commit, but edit the commit message# e, edit &lt;commit&gt; = use commit, but stop for amending# s, squash &lt;commit&gt; = use commit, but meld into previous commit# f, fixup &lt;commit&gt; = like &quot;squash&quot;, but discard this commit's log message# x, exec &lt;command&gt; = run command (the rest of the line) using shell# b, break = stop here (continue rebase later with 'git rebase --continue')# d, drop &lt;commit&gt; = remove commit# l, label &lt;label&gt; = label current HEAD with a name# t, reset &lt;label&gt; = reset HEAD to a label# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]# . create a merge commit using the original merge commit's# . message (or the oneline, if no original merge commit was# . specified). Use -c &lt;commit&gt; to reword the commit message.## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted.# 1234567891011121314151617181920212223242526272829303132# This is a combination of 3 commits.# This is the 1st commit message:create a complete web page //新加一个commit信息ADD index + logo# This is the commit message #2:ADD style.css# This is the commit message #3:ADD a js# Please enter the commit message for your changes. Lines starting# with '#' will be ignored, and an empty message aborts the commit.## Date: Tue Jun 22 21:57:59 2021 +0800## interactive rebase in progress; onto d59544f# Last commands done (3 commands done):# squash 9bd6521 ADD style.css# squash 95a2ce3 ADD a js# Next command to do (1 remaining command):# pick f836729 Mobe filename readme to readme.md# You are currently rebasing branch 'master' on 'd59544f'.## Changes to be committed:# new file: images/git-logo.png# new file: index.html# new file: js/script.js# new file: styles/style.css# 将间隔的commit整理成一个123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172shengbinbin@192 git_learning % git logcommit ea5ccd12fd5b46a7536a92fec3cf6619dbe993f3 (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800# This is a combination of 2 commits. Mobe filename readme to readme.md // 1 commit 88f92801204a631a4eb1239a997b5457bc4f8c34Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 create a complete web page ADD index + logo ADD style.css ADD a jscommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMe //2 ,将这个和上面的1合并在一起 shengbinbin@192 git_learning % git rebase -i d59544fSuccessfully rebased and updated refs/heads/master.shengbinbin@192 git_learning % git rebase -i d59544fThe previous cherry-pick is now empty, possibly due to conflict resolution.If you wish to commit it anyway, use: git commit --allow-emptyOtherwise, please use 'git rebase --skip'interactive rebase in progress; onto d59544fLast command done (1 command done): pick d59544fNext commands to do (2 remaining commands): squash ea5ccd1 Mobe filename readme to readme.md pick 88f9280 create a complete web page ADD index + logo ADD style.css ADD a js (use &quot;git rebase --edit-todo&quot; to view and edit)You are currently rebasing branch 'master' on 'd59544f'. (all conflicts fixed: run &quot;git rebase --continue&quot;)nothing to commit, working tree cleanCould not apply d59544f...shengbinbin@192 git_learning % git rebase --continue[detached HEAD 0a8f323] Add a new readme Date: Tue Jun 22 21:52:44 2021 +0800 2 files changed, 4 insertions(+) create mode 100644 readme create mode 100644 readme.mdSuccessfully rebased and updated refs/heads/master.shengbinbin@192 git_learning % git logcommit 01b4b51917535a1b2cdafef8fc275ab17e8b4a1d (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 create a complete web page ADD index + logo ADD style.css ADD a jscommit 0a8f323893329d23eb7bc2e68049391ee3957e53Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add a new readme Add readMe Mobe filename readme to readme.md 1234567891011121314151617181920212223242526272829pick d59544fs ea5ccd1 Mobe filename readme to readme.md //改成spick 88f9280 create a complete web page ADD index + logo ADD style.css ADD a js# Rebase d59544f..ea5ccd1 onto d59544f (2 commands)## Commands:# p, pick &lt;commit&gt; = use commit# r, reword &lt;commit&gt; = use commit, but edit the commit message# e, edit &lt;commit&gt; = use commit, but stop for amending# s, squash &lt;commit&gt; = use commit, but meld into previous commit# f, fixup &lt;commit&gt; = like &quot;squash&quot;, but discard this commit's log message# x, exec &lt;command&gt; = run command (the rest of the line) using shell# b, break = stop here (continue rebase later with 'git rebase --continue')# d, drop &lt;commit&gt; = remove commit# l, label &lt;label&gt; = label current HEAD with a name# t, reset &lt;label&gt; = reset HEAD to a label# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]# . create a merge commit using the original merge commit's# . message (or the oneline, if no original merge commit was# . specified). Use -c &lt;commit&gt; to reword the commit message.## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted.# 123456789101112131415161718192021222324252627282930# This is a combination of 2 commits.Add a new readme# This is the 1st commit message:Add readMe# This is the commit message #2:Mobe filename readme to readme.md# Please enter the commit message for your changes. Lines starting# with '#' will be ignored, and an empty message aborts the commit.## Date: Tue Jun 22 21:52:44 2021 +0800## interactive rebase in progress; onto d59544f# Last commands done (2 commands done):# pick d59544f# squash ea5ccd1 Mobe filename readme to readme.md# Next command to do (1 remaining command):# pick 88f9280 create a complete web page ADD index + logo ADD style.css ADD a js# You are currently rebasing branch 'master' on 'd59544f'.### Initial commit## Changes to be committed:# new file: readme# new file: readme.md# 暂存区和HEAD做比较head 为已经提交过后的状态 12345678910111213141516171819202122232425shengbinbin@192 git_learning % git statusOn branch masternothing to commit, working tree cleanshengbinbin@192 git_learning % vi index.html shengbinbin@192 git_learning % git add index.html shengbinbin@192 git_learning % git diff --cached //比较暂存区和Head最新的提交的差异diff --git a/index.html b/index.htmlindex 6ad4c68..0f0ee94 100644--- a/index.html+++ b/index.html@@ -14,7 +14,7 @@ &lt;div class=&quot;accordion&quot;&gt;&lt;h1&gt;Terminologys&lt;/h1&gt;&lt;/div&gt; &lt;div class=&quot;panel&quot;&gt; &lt;ol&gt;- &lt;li&gt;&lt;/li&gt;+ &lt;li&gt;add&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; shengbinbin@192 git_learning % git commit -m'Add the first git command with config'[master 6cff2f6] Add the first git command with config 1 file changed, 1 insertion(+), 1 deletion(-) 暂存区和工作区文件的比较123456789101112131415161718192021222324body{shengbinbin@192 git_learning % git branch fix_readme* master tempshengbinbin@192 git_learning % vim index.htmlshengbinbin@192 git_learning % git add index.html //加入到暂存区shengbinbin@192 git_learning % vim styles/style.cssshengbinbin@192 git_learning % git diff //默认比较的就是工作区和暂存区diff --git a/styles/style.css b/styles/style.cssindex ef3f137..f2a08e0 100644--- a/styles/style.css+++ b/styles/style.css@@ -1,7 +1,7 @@ body{ background-color: orange; font-family: 'Monaco', sans-serif;- color: white;+ color: black; } body a{shengbinbin@192 git_learning % 将暂存区恢复成HEAD一样123456789101112131415161718192021222324252627282930313233343536shengbinbin@192 git_learning % git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: index.htmlChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: styles/style.cssshengbinbin@192 git_learning % git add styles/style.cssshengbinbin@192 git_learning % git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: index.html modified: styles/style.cssshengbinbin@192 git_learning % git reset HEAD //恢复暂存区Unstaged changes after reset:M index.htmlM styles/style.cssshengbinbin@192 git_learning % git statusOn branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: index.html modified: styles/style.cssno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)shengbinbin@192 git_learning % git diff --cachedshengbinbin@192 git_learning % 将工作区恢复成暂存区12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849shengbinbin@192 git_learning % git statusOn branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: index.html modified: styles/style.cssno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)shengbinbin@192 git_learning % git add index.htmlshengbinbin@192 git_learning % git diff --cacheddiff --git a/index.html b/index.htmlindex 0f0ee94..5ec8fa6 100644--- a/index.html+++ b/index.html@@ -15,7 +15,7 @@ &lt;div class=&quot;panel&quot;&gt; &lt;ol&gt; &lt;li&gt;add&lt;/li&gt;- &lt;li&gt;&lt;/li&gt;+ &lt;li&gt;bare repo&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt;shengbinbin@192 git_learning % vim index.htmlshengbinbin@192 git_learning % git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: index.htmlChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: index.html modified: styles/style.cssshengbinbin@192 git_learning % git restore index.htmlshengbinbin@192 git_learning % git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: index.htmlChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: styles/style.css 回退到之前的commit状态12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758shengbinbin@192 git_learning % git branch -av fix_readme fcad9c2 Background to green* master 6cff2f6 Add the first git command with config temp 2a5925a Add testshengbinbin@192 git_learning % git checkout tempSwitched to branch 'temp'shengbinbin@192 git_learning % git logcommit 2a5925a9f096a881a838238a5ced6a5ff3b7f52a (HEAD -&gt; temp)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:38:52 2021 +0800 Add testcommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caaAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMeshengbinbin@192 git_learning % gitk --allshengbinbin@192 git_learning % git reset --hard 9bd65211 //回退到指定的commitHEAD is now at 9bd6521 ADD style.cssshengbinbin@192 git_learning % gitk --allshengbinbin@192 git_learning % git logcommit 9bd65211b66a0a27503644f8fe66a66a428d4ecb (HEAD -&gt; temp)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caaAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMeshengbinbin@192 git_learning % 查看不同commit的指定文件的差异1234567891011121314151617181920shengbinbin@192 git_learning % git branch -av fix_readme fcad9c2 Background to green master 6cff2f6 Add the first git command with config* temp 9bd6521 ADD style.cssshengbinbin@192 git_learning % git diff 9bd6521 6cff2f6 -- index.html //查看不同提交指定文件的差异diff --git a/index.html b/index.htmlindex 6ad4c68..0f0ee94 100644--- a/index.html+++ b/index.html@@ -14,7 +14,7 @@ &lt;div class=&quot;accordion&quot;&gt;&lt;h1&gt;Terminologys&lt;/h1&gt;&lt;/div&gt; &lt;div class=&quot;panel&quot;&gt; &lt;ol&gt;- &lt;li&gt;&lt;/li&gt;+ &lt;li&gt;add&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt;shengbinbin@192 git_learning % 删除文件123456789101112131415161718192021222324252627282930shengbinbin@192 git_learning % ls -altotal 16drwxr-xr-x 7 shengbinbin staff 224 6 28 09:49 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 14 shengbinbin staff 448 6 28 09:49 .gitdrwxr-xr-x 3 shengbinbin staff 96 6 27 21:59 images-rw-r--r-- 1 shengbinbin staff 1303 6 28 09:47 index.html-rw-r--r-- 1 shengbinbin staff 51 6 28 09:49 readmedrwxr-xr-x 3 shengbinbin staff 96 6 28 09:46 stylesshengbinbin@192 git_learning % git branch fix_readme master* tempshengbinbin@192 git_learning % git rm readme //git 删除指定文件rm 'readme'shengbinbin@192 git_learning % git statusOn branch tempChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) deleted: readmeshengbinbin@192 git_learning % ls -altotal 8drwxr-xr-x 6 shengbinbin staff 192 6 28 09:54 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 14 shengbinbin staff 448 6 28 09:54 .gitdrwxr-xr-x 3 shengbinbin staff 96 6 27 21:59 images-rw-r--r-- 1 shengbinbin staff 1303 6 28 09:47 index.htmldrwxr-xr-x 3 shengbinbin staff 96 6 28 09:46 stylesshengbinbin@192 git_learning % 指定不需要git管理的文件12345678910111213141516171819202122232425262728293031shengbinbin@192 git_learning % echo 'I am a file' &gt;docshengbinbin@192 git_learning % ls -altotal 32drwxr-xr-x 9 shengbinbin staff 288 6 28 10:03 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 14 shengbinbin staff 448 6 28 10:02 .git-rw-r--r-- 1 shengbinbin staff 5 6 28 10:02 .gitignore-rw-r--r-- 1 shengbinbin staff 12 6 28 10:03 docdrwxr-xr-x 3 shengbinbin staff 96 6 27 21:59 images-rw-r--r-- 1 shengbinbin staff 1303 6 28 09:47 index.html-rw-r--r-- 1 shengbinbin staff 51 6 28 09:56 readmedrwxr-xr-x 3 shengbinbin staff 96 6 28 09:46 stylesshengbinbin@192 git_learning % cat .gitignoredoc/shengbinbin@192 git_learning % git statusOn branch tempUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) .gitignore docnothing added to commit but untracked files present (use &quot;git add&quot; to track)shengbinbin@192 git_learning % vi .gitignoreshengbinbin@192 git_learning % git statusOn branch tempUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) .gitignorenothing added to commit but untracked files present (use &quot;git add&quot; to track)shengbinbin@192 git_learning % Git的备份常用的传输协议： 哑协议和智能协议的区别： 直观区别：哑协议传输进度不可见，而智能协议传输是可见的。 传输速度：智能协议比哑协议传输速度更快 123456789101112131415161718192021222324shengbinbin@192 git_learning % pwd/Users/shengbinbin/Documents/Code/git_learningshengbinbin@192 git_learning % cd ..shengbinbin@192 Code % mkdir backupshengbinbin@192 Code % cd backupshengbinbin@192 backup % pwd/Users/shengbinbin/Documents/Code/backupshengbinbin@192 backup % cd ..shengbinbin@192 Code % pwd/Users/shengbinbin/Documents/Codeshengbinbin@192 Code % git clone --bare /Users/shengbinbin/Documents/Code/git_learning/.git ya.gitCloning into bare repository 'ya.git'...done.shengbinbin@192 Code % git clone --bare file:///Users/shengbinbin/Documents/Code/git_learning/.git zhineng.gitCloning into bare repository 'zhineng.git'...remote: Enumerating objects: 28, done.remote: Counting objects: 100% (28/28), done.remote: Compressing objects: 100% (23/23), done.remote: Total 28 (delta 8), reused 0 (delta 0), pack-reused 0Receiving objects: 100% (28/28), 22.06 KiB | 22.06 MiB/s, done.Resolving deltas: 100% (8/8), done.shengbinbin@192 Code % 123456789shengbinbin@192 git_learning % git remote -vshengbinbin@192 git_learning % git remote add zhineng file:///Users/shengbinbin/Documents/Code/backup/zhineng.gitshengbinbin@192 git_learning % git branch -av fix_readme fcad9c2 Background to green master 6cff2f6 Add the first git command with config* temp 9bd6521 ADD style.css//可以将当前文件夹的内容备份到本地的另外一个文件夹 GitHub的交互过程配置SSH的公私钥https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account 检查一下是否已经存在了SSH密钥：https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/checking-for-existing-ssh-keys 1234567891011shengbinbin@chengbinbindeMacBook-Pro ~ % cd ~/.sshshengbinbin@chengbinbindeMacBook-Pro .ssh % ls -altotal 24drwx------ 5 shengbinbin staff 160 5 6 15:26 .drwxr-xr-x+ 35 shengbinbin staff 1120 6 22 10:25 ..-rw------- 1 shengbinbin staff 2602 5 6 15:22 id_rsa-rw-r--r--@ 1 shengbinbin staff 571 5 6 15:22 id_rsa.pub //已经有了-rw-r--r-- 1 shengbinbin staff 1197 5 6 16:51 known_hostsshengbinbin@chengbinbindeMacBook-Pro .ssh % cat id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDjEseQlMpM7ANSuie198dnHoopBWLaNEuIkBSwSA1NeDHY1a0gKXp8TZkujdLPUIJDcd7AOXPTZ4AmhMmHN7eC5XNR2MuXMZxbzWgdd/nTuwbeve8DmvQbgBQ4Jn6mqDdmnJO62r4XlO1CvScyQ5i+HW5LcLBXtdzUjMpaUb5mnzqIqg7QAR6m3j2oYyMzf4ccfoMEu1XbdTAV2opeRdOadUwpN3STq5X5JM5ECbcXfitMrVz1FnNNvH+UEmKqA/I4Ea4vB7npeaaECb4krMfo4InozLIdD0reZSyvHG1AP2/OdHD6Aw4/3XA8XTKv9JWwV8Xhce3B3VFjL2ajDwl0CJVFuQ6GZd778T4/hlPSSOmmQJOilsvOEqRq4EoojJyBx3TvU/GwlkwpRgbeQvKHwiV095VDAV8DKB2qMFmgQ1cvOTLRyTPgvuO6CkkZjR0UD9OQ3+uWSz3B8TA9ULALsbYsWP9LaACSkI2PyGwfLWuyCQ14ckir84sub/Bzw4s= 1157024800@qq.comshengbinbin@chengbinbindeMacBook-Pro .ssh % 将公钥粘到github上去。 创建个人仓库 将本地仓库同步到GitHub123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960shengbinbin@192 git_learning % git remote -v //查看远程分支信息 ,现在是有2个本地的备份zhineng file:///Users/shengbinbin/Documents/Code/backup/zhineng.git (fetch)zhineng file:///Users/shengbinbin/Documents/Code/backup/zhineng.git (push) ////新增远程的github仓库地址，命名为githubshengbinbin@192 git_learning % git remote add github git@github.com:binshow/git_learning.gitshengbinbin@192 git_learning % git remote -vgithub git@github.com:binshow/git_learning.git (fetch)github git@github.com:binshow/git_learning.git (push)zhineng file:///Users/shengbinbin/Documents/Code/backup/zhineng.git (fetch)zhineng file:///Users/shengbinbin/Documents/Code/backup/zhineng.git (push)shengbinbin@192 git_learning % git push github --all //将本地的所以分支push到github上Enumerating objects: 25, done.Counting objects: 100% (25/25), done.Delta compression using up to 8 threadsCompressing objects: 100% (20/20), done.Writing objects: 100% (25/25), 21.83 KiB | 7.28 MiB/s, done.Total 25 (delta 6), reused 0 (delta 0), pack-reused 0remote: Resolving deltas: 100% (6/6), done.remote:remote: Create a pull request for 'fix_readme' on GitHub by visiting:remote: https://github.com/binshow/git_learning/pull/new/fix_readmeremote:To github.com:binshow/git_learning.git * [new branch] fix_readme -&gt; fix_readme ! [rejected] master -&gt; master (fetch first) ! [rejected] temp -&gt; temp (fetch first)error: failed to push some refs to 'github.com:binshow/git_learning.git'hint: Updates were rejected because the remote contains work that you dohint: not have locally. This is usually caused by another repository pushinghint: to the same ref. You may want to first integrate the remote changeshint: (e.g., 'git pull ...') before pushing again.hint: See the 'Note about fast-forwards' in 'git push --help' for details.shengbinbin@192 git_learning % git pull github masterhint: Pulling without specifying how to reconcile divergent branches ishint: discouraged. You can squelch this message by running one of the followinghint: commands sometime before your next pull:hint:hint: git config pull.rebase false # merge (the default strategy)hint: git config pull.rebase true # rebasehint: git config pull.ff only # fast-forward onlyhint:hint: You can replace &quot;git config&quot; with &quot;git config --global&quot; to set a defaulthint: preference for all repositories. You can also pass --rebase, --no-rebase,hint: or --ff-only on the command line to override the configured default perhint: invocation.remote: Enumerating objects: 13, done.remote: Counting objects: 100% (13/13), done.remote: Compressing objects: 100% (8/8), done.remote: Total 13 (delta 2), reused 13 (delta 2), pack-reused 0Unpacking objects: 100% (13/13), 20.76 KiB | 2.59 MiB/s, done.From github.com:binshow/git_learning * branch master -&gt; FETCH_HEAD * [new branch] master -&gt; github/masterfatal: refusing to merge unrelated historiesshengbinbin@192 git_learning % 协同开发的常见操作不同人修改了不同文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106shengbinbin@chengbinbindeMacBook-Pro git_learn % git clone git@github.com:binshow/git_learning.gitCloning into 'git_learning'...remote: Enumerating objects: 20, done.remote: Counting objects: 100% (20/20), done.remote: Compressing objects: 100% (12/12), done.remote: Total 20 (delta 3), reused 14 (delta 3), pack-reused 0Receiving objects: 100% (20/20), 22.62 KiB | 279.00 KiB/s, done.Resolving deltas: 100% (3/3), done.shengbinbin@chengbinbindeMacBook-Pro git_learn % ls -altotal 16drwxr-xr-x 8 shengbinbin staff 256 6 22 14:45 .drwx------@ 29 shengbinbin staff 928 6 21 09:09 ..-rw-r--r--@ 1 shengbinbin staff 6148 6 22 09:23 .DS_Storedrwxr-xr-x@ 8 shengbinbin staff 256 11 25 2018 0-materialdrwxr-xr-x 4 shengbinbin staff 128 6 22 13:02 backupdrwxr-xr-x@ 8 shengbinbin staff 256 6 22 14:14 git_leanrningdrwxr-xr-x 5 shengbinbin staff 160 6 22 14:45 git_learningdrwxr-xr-x 11 shengbinbin staff 352 6 22 09:17 git_study//两个分支shengbinbin@chengbinbindeMacBook-Pro git_learn % git clone git@github.com:binshow/git_learning.git git_learning_02Cloning into 'git_learning_02'...remote: Enumerating objects: 20, done.remote: Counting objects: 100% (20/20), done.remote: Compressing objects: 100% (12/12), done.remote: Total 20 (delta 3), reused 14 (delta 3), pack-reused 0Receiving objects: 100% (20/20), 22.62 KiB | 11.31 MiB/s, done.Resolving deltas: 100% (3/3), done.shengbinbin@chengbinbindeMacBook-Pro git_learn % clearshengbinbin@chengbinbindeMacBook-Pro git_learn % ls -altotal 16drwxr-xr-x 9 shengbinbin staff 288 6 22 14:45 .drwx------@ 29 shengbinbin staff 928 6 21 09:09 ..-rw-r--r--@ 1 shengbinbin staff 6148 6 22 09:23 .DS_Storedrwxr-xr-x@ 8 shengbinbin staff 256 11 25 2018 0-materialdrwxr-xr-x 4 shengbinbin staff 128 6 22 13:02 backupdrwxr-xr-x@ 8 shengbinbin staff 256 6 22 14:14 git_leanrningdrwxr-xr-x 5 shengbinbin staff 160 6 22 14:45 git_learningdrwxr-xr-x 5 shengbinbin staff 160 6 22 14:45 git_learning_02drwxr-xr-x 11 shengbinbin staff 352 6 22 09:17 git_studyshengbinbin@chengbinbindeMacBook-Pro git_learn % cd git_learning_02shengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git config --add --local user.name 'zkd'shengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git config --add --local user.eamil 'zkd@qq.com'shengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git config --local --listcore.repositoryformatversion=0core.filemode=truecore.bare=falsecore.logallrefupdates=truecore.ignorecase=truecore.precomposeunicode=trueremote.origin.url=git@github.com:binshow/git_learning.gitremote.origin.fetch=+refs/heads/*:refs/remotes/origin/*branch.main.remote=originbranch.main.merge=refs/heads/mainuser.name=zkduser.eamil=zkd@qq.comshengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git branch -av* main 7e5f6d1 Create README.md remotes/origin/HEAD -&gt; origin/main remotes/origin/feature/add_git_commands 7e5f6d1 Create README.md remotes/origin/main 7e5f6d1 Create README.md remotes/origin/master 8773e48 ADD js remotes/origin/temp a01c150 ADD js //基于远地的分支创建一个本地的分支shengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git checkout -b feature/add_git_commands origin/feature/add_git_commandsBranch 'feature/add_git_commands' set up to track remote branch 'feature/add_git_commands' from 'origin'.Switched to a new branch 'feature/add_git_commands'//第一个人修改了readMeshengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git branch* feature/add_git_commands mainshengbinbin@chengbinbindeMacBook-Pro git_learning_02 % ls -altotal 16drwxr-xr-x 5 shengbinbin staff 160 6 22 14:45 .drwxr-xr-x 9 shengbinbin staff 288 6 22 14:45 ..drwxr-xr-x 12 shengbinbin staff 384 6 22 14:50 .git-rw-r--r-- 1 shengbinbin staff 1064 6 22 14:45 LICENSE-rw-r--r-- 1 shengbinbin staff 35 6 22 14:45 README.mdshengbinbin@chengbinbindeMacBook-Pro git_learning_02 % vim README.mdshengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git add README.mdshengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git statusOn branch feature/add_git_commandsYour branch is up to date with 'origin/feature/add_git_commands'.Changes to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: README.mdshengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git commit -m'ADD git commands description in readMe'[feature/add_git_commands 1a26577] ADD git commands description in readMe 1 file changed, 2 insertions(+)shengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git pushEnumerating objects: 5, done.Counting objects: 100% (5/5), done.Delta compression using up to 8 threadsCompressing objects: 100% (3/3), done.Writing objects: 100% (3/3), 350 bytes | 350.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0), pack-reused 0To github.com:binshow/git_learning.git 7e5f6d1..1a26577 feature/add_git_commands -&gt; feature/add_git_commands shengbinbin@chengbinbindeMacBook-Pro git_learning_02 %","link":"/2021/06/22/git%E4%B9%8B%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"},{"title":"SpringBoot（二）整合持久层","text":"本节主要讲述的是SpringBoot对数据层的操作，包括： 对数据源的自动装配 对jdbc的整合 对mybatis的整合 对mybatis-plus的整合 以最新的Spring-boot 2.5.3 为例： 一、SQL数据源的自动配置-HikariDataSource导入JDBC场景123456789101112131415 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jdbc&lt;/artifactId&gt; &lt;/dependency&gt;查看这个依赖整合了哪些：spring-boot-starter-data-jdbc： - spring-boot-start-jdbc -HikariCP：数据源 -Spring-jdbc - spring-data-jdbc 数据库驱动？ 为什么导入JDBC场景，官方不导入驱动？官方不知道我们接下要操作什么数据库。 数据库版本和驱动版本对应： 12345678910 &lt;!--导入mysql驱动--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt;默认版本： &lt;version&gt;8.0.26&lt;/version&gt;可以自定义修改版本： 分析自动配置自动配置的类： DataSourceAutoConfiguration：数据源的自动配置 1234567891011121314151617181920212223242526272829package org.springframework.boot.autoconfigure.jdbc;@Configuration(proxyBeanMethods = false)@ConditionalOnClass({ DataSource.class, EmbeddedDatabaseType.class })@ConditionalOnMissingBean(type = &quot;io.r2dbc.spi.ConnectionFactory&quot;)@EnableConfigurationProperties(DataSourceProperties.class)@Import({ DataSourcePoolMetadataProvidersConfiguration.class, DataSourceInitializationConfiguration.InitializationSpecificCredentialsDataSourceInitializationConfiguration.class, DataSourceInitializationConfiguration.SharedCredentialsDataSourceInitializationConfiguration.class })public class DataSourceAutoConfiguration { @Configuration(proxyBeanMethods = false) @Conditional(EmbeddedDatabaseCondition.class) @ConditionalOnMissingBean({ DataSource.class, XADataSource.class }) @Import(EmbeddedDataSourceConfiguration.class) protected static class EmbeddedDatabaseConfiguration { } //数据库连接池的配置 @Configuration(proxyBeanMethods = false) @Conditional(PooledDataSourceCondition.class) @ConditionalOnMissingBean({ DataSource.class, XADataSource.class }) @Import({ DataSourceConfiguration.Hikari.class, DataSourceConfiguration.Tomcat.class, DataSourceConfiguration.Dbcp2.class, DataSourceConfiguration.OracleUcp.class, DataSourceConfiguration.Generic.class, DataSourceJmxConfiguration.class }) protected static class PooledDataSourceConfiguration { } 12@ConfigurationProperties(prefix = &quot;spring.datasource&quot;)public class DataSourceProperties implements BeanClassLoaderAware, InitializingBean { 修改数据源的相关配置：spring.datasource 数据库连接池的配置，是自己容器中没有DataSource才自动配置的 底层配置好的连接池是：HikariDataSource DataSourceTransactionManagerAutoConfiguration：事务管理器的自动配置 JdbcTemplateAutoConfiguration：JdbcTemplate的自动配置，可以来对数据库进行crud 123456789101112package org.springframework.boot.autoconfigure.jdbc;@Configuration(proxyBeanMethods = false)@ConditionalOnClass({ DataSource.class, JdbcTemplate.class })@ConditionalOnSingleCandidate(DataSource.class)@AutoConfigureAfter(DataSourceAutoConfiguration.class)@EnableConfigurationProperties(JdbcProperties.class)@Import({ DatabaseInitializationDependencyConfigurer.class, JdbcTemplateConfiguration.class, NamedParameterJdbcTemplateConfiguration.class })public class JdbcTemplateAutoConfiguration {} 123@ConfigurationProperties(prefix = &quot;spring.jdbc&quot;)public class JdbcProperties { 修改jdbcTemplate的相关配置：spring.jdbc JndiDataSourceAutoConfiguration： jndi的自动配置 XADataSourceAutoConfiguration： 分布式事务相关的 修改配置项123456spring: datasource: url: jdbc:mysql://localhost:3306/db_account username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver 测试：123456789101112131415161718@Slf4j@SpringBootTestclass Boot05WebAdminApplicationTests { @Autowired JdbcTemplate jdbcTemplate; @Test void contextLoads() {// jdbcTemplate.queryForObject(&quot;select * from account_tbl&quot;)// jdbcTemplate.queryForList(&quot;select * from account_tbl&quot;,) Long aLong = jdbcTemplate.queryForObject(&quot;select count(*) from account_tbl&quot;, Long.class); log.info(&quot;记录总数：{}&quot;,aLong); }} 使用Druid数据源官方文档：https://github.com/alibaba/druid 自定义使用步骤 引入Druid的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; 编写自己的配置类： 1234567891011121314151617/** * @author shengbinbin * @description: Druid 数据源的配置类 * @date 2021/8/118:19 下午 */@Configurationpublic class MyDataSourceConfig { // SpringBoot 默认的自动配置类 是先判断容器中有没有DataSource.class,没有才会自动配置 @ConfigurationProperties(&quot;spring.datasource&quot;) // 将配置文件中 spring.datasource 前缀的值相注入到dataSource中 @Bean public DataSource dataSource(){ DruidDataSource druidDataSource = new DruidDataSource(); return druidDataSource; } } 编写yaml配置文件，以spring.datasource输入一些属性注入数据源： 123456spring: datasource: url: jdbc:mysql://localhost:3306/binshow username: root password: bin123456 driver-class-name: com.mysql.cj.jdbc.Driver 编写测试类，观察现在容器中的数据源是什么：(发现已经是Druid了) 12345678910111213@SpringBootTest@Slf4jclass ApplicationTests { @Autowired DataSource dataSource; @Test void contextLoads() { log.info(&quot;容器中的数据源类型为：{}&quot;,dataSource.getClass()); //容器中的数据源类型为：class com.alibaba.druid.pool.DruidDataSource }} 查看监控页添加一个Servlet 组件，StatViewServlet的用途包括： 提供监控信息展示的html页面 提供监控信息的JSON API 添加一个Servlet：ServletRegistrationBean，并设置druidDataSource的Filter的属性 12345678910111213141516171819202122232425@Configurationpublic class MyDataSourceConfig { // SpringBoot 默认的自动配置类 是先判断容器中有没有DataSource.class,没有才会自动配置 @ConfigurationProperties(&quot;spring.datasource&quot;) // 将配置文件中 spring.datasource 前缀的值相注入到dataSource中 @Bean public DataSource dataSource() throws SQLException { DruidDataSource druidDataSource = new DruidDataSource(); // 开启监控功能 druidDataSource.setFilters(&quot;stat&quot;); return druidDataSource; } /** * 配置Druid的监控页功能 * @return 返回注入的Bean */ @Bean public ServletRegistrationBean statViewServlet(){ StatViewServlet statViewServlet = new StatViewServlet(); ServletRegistrationBean&lt;StatViewServlet&gt; registrationBean = new ServletRegistrationBean&lt;&gt;(statViewServlet, &quot;/druid/*&quot;); return registrationBean; }} 开启Web服务，访问http://127.0.0.1:8080/druid/ 发现是可以正常访问的。 随便开启一个sql查询，访问：http://127.0.0.1:8080/sql 123456789101112@Controllerpublic class IndexController { @Autowired private JdbcTemplate jdbcTemplate; @GetMapping(&quot;/sql&quot;) @ResponseBody public String getFromDb(){ Long res = jdbcTemplate.queryForObject(&quot;select count(*) from user&quot;, Long.class); return res.toString(); }} StatFilter用于统计监控信息；如SQL监控、URI监控 123需要给数据源中配置如下属性；可以允许多个filter，多个用，分割；如：&lt;property name=&quot;filters&quot; value=&quot;stat,slf4j&quot; /&gt; 系统中所有filter： 别名 Filter类名 default com.alibaba.druid.filter.stat.StatFilter stat com.alibaba.druid.filter.stat.StatFilter mergeStat com.alibaba.druid.filter.stat.MergeStatFilter encoding com.alibaba.druid.filter.encoding.EncodingConvertFilter log4j com.alibaba.druid.filter.logging.Log4jFilter log4j2 com.alibaba.druid.filter.logging.Log4j2Filter slf4j com.alibaba.druid.filter.logging.Slf4jLogFilter commonlogging com.alibaba.druid.filter.logging.CommonsLogFilter 慢SQL记录配置 123456&lt;bean id=&quot;stat-filter&quot; class=&quot;com.alibaba.druid.filter.stat.StatFilter&quot;&gt; &lt;property name=&quot;slowSqlMillis&quot; value=&quot;10000&quot; /&gt; &lt;property name=&quot;logSlowSql&quot; value=&quot;true&quot; /&gt;&lt;/bean&gt;使用 slowSqlMillis 定义慢SQL的时长 使用官方starter方式引入druid-starter12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.17&lt;/version&gt;&lt;/dependency&gt; 分析自动配置 扩展配置项 spring.datasource.druid DruidSpringAopConfiguration.class, 监控SpringBean的；配置项：spring.datasource.druid.aop-patterns DruidStatViewServletConfiguration.class, 监控页的配置：spring.datasource.druid.stat-view-servlet；默认开启 DruidWebStatFilterConfiguration.class, web监控配置；spring.datasource.druid.web-stat-filter；默认开启 DruidFilterConfiguration.class}) 所有Druid自己filter的配置 12345678private static final String FILTER_STAT_PREFIX = &quot;spring.datasource.druid.filter.stat&quot;;private static final String FILTER_CONFIG_PREFIX = &quot;spring.datasource.druid.filter.config&quot;;private static final String FILTER_ENCODING_PREFIX = &quot;spring.datasource.druid.filter.encoding&quot;;private static final String FILTER_SLF4J_PREFIX = &quot;spring.datasource.druid.filter.slf4j&quot;;private static final String FILTER_LOG4J_PREFIX = &quot;spring.datasource.druid.filter.log4j&quot;;private static final String FILTER_LOG4J2_PREFIX = &quot;spring.datasource.druid.filter.log4j2&quot;;private static final String FILTER_COMMONS_LOG_PREFIX = &quot;spring.datasource.druid.filter.commons-log&quot;;private static final String FILTER_WALL_PREFIX = &quot;spring.datasource.druid.filter.wall&quot;; 配置示例1234567891011121314151617181920212223242526272829303132spring: datasource: url: jdbc:mysql://localhost:3306/db_account username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver druid: aop-patterns: com.atguigu.admin.* #监控SpringBean filters: stat,wall # 底层开启功能，stat（sql监控），wall（防火墙） stat-view-servlet: # 配置监控页功能 enabled: true login-username: admin login-password: admin resetEnable: false web-stat-filter: # 监控web enabled: true urlPattern: /* exclusions: '*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*' filter: stat: # 对上面filters里面的stat的详细配置 slow-sql-millis: 1000 logSlowSql: true enabled: true wall: enabled: true config: drop-table-allow: false SpringBoot配置示例 https://github.com/alibaba/druid/tree/master/druid-spring-boot-starter 配置项列表https://github.com/alibaba/druid/wiki/DruidDataSource%E9%85%8D%E7%BD%AE%E5%B1%9E%E6%80%A7%E5%88%97%E8%A1%A8 整合MyBatis操作https://github.com/mybatis 1234&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt; 配置模式 全局配置文件 SqlSessionFactory: 自动配置好了 SqlSession：自动配置了 SqlSessionTemplate 组合了SqlSession @Import(AutoConfiguredMapperScannerRegistrar.class）； Mapper： 只要我们写的操作MyBatis的接口标准了 @Mapper 就会被自动扫描进来 123456@EnableConfigurationProperties(MybatisProperties.class) ： MyBatis配置项绑定类。@AutoConfigureAfter({ DataSourceAutoConfiguration.class, MybatisLanguageDriverAutoConfiguration.class })public class MybatisAutoConfiguration{}@ConfigurationProperties(prefix = &quot;mybatis&quot;)public class MybatisProperties 可以修改配置文件中 mybatis 开始的所有： 12345678910111213141516# 配置mybatis规则mybatis: config-location: classpath:mybatis/mybatis-config.xml #全局配置文件位置 mapper-locations: classpath:mybatis/mapper/*.xml #sql映射文件位置 Mapper接口---&gt;绑定Xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.atguigu.admin.mapper.AccountMapper&quot;&gt;&lt;!-- public Account getAcct(Long id); --&gt; &lt;select id=&quot;getAcct&quot; resultType=&quot;com.atguigu.admin.bean.Account&quot;&gt; select * from account_tbl where id=#{id} &lt;/select&gt;&lt;/mapper&gt; 配置 private Configuration configuration; mybatis.configuration下面的所有，就是相当于改mybatis全局配置文件中的值 12345678# 配置mybatis规则mybatis:# config-location: classpath:mybatis/mybatis-config.xml mapper-locations: classpath:mybatis/mapper/*.xml configuration: map-underscore-to-camel-case: true 可以不写全局；配置文件，所有全局配置文件的配置都放在configuration配置项中即可 导入mybatis官方starter 编写mapper接口。标准@Mapper注解 编写sql映射文件并绑定mapper接口 在application.yaml中指定Mapper配置文件的位置，以及指定全局配置文件的信息 （建议；配置在mybatis.configuration） 2、注解模式123456789@Mapperpublic interface CityMapper { @Select(&quot;select * from city where id=#{id}&quot;) public City getById(Long id); public void insert(City city);} 3、混合模式123456789@Mapperpublic interface CityMapper { @Select(&quot;select * from city where id=#{id}&quot;) public City getById(Long id); public void insert(City city);} 最佳实战： 引入mybatis-starter 配置application.yaml中，指定mapper-location位置即可 编写Mapper接口并标注@Mapper注解 简单方法直接注解方式 复杂方法编写mapper.xml进行绑定映射 @MapperScan(“com.atguigu.admin.mapper”) 简化，其他的接口就可以不用标注@Mapper注解 整合MyBatisPlus操作二、NoSQL整合Redis","link":"/2021/08/10/SpringBoot%EF%BC%88%E4%BA%8C%EF%BC%89%E6%95%B4%E5%90%88%E6%8C%81%E4%B9%85%E5%B1%82/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/05/06/hello-world/"},{"title":"测试图上传","text":"","link":"/2021/05/06/%E6%B5%8B%E8%AF%95%E5%9B%BE%E4%B8%8A%E4%BC%A0/"},{"title":"操作系统（一）概述","text":"讲一下内核和中断 内核由来计算机是由各种外部硬件设备组成的，比如CPU、内存和硬盘等，如果让每个应用对和这些硬件设备对接通信协议的话很麻烦，所以就出现了一个连接硬件设备的桥梁，称为内核。应用程序只需要关心和内核的交互，而不需要关心硬件的细节了。 作用 管理进程、线程，决定哪个进程、线程使⽤ CPU，也就是进程调度的能⼒； 管理内存，决定内存的分配和回收，也就是内存管理的能⼒； 管理硬件设备，为进程与硬件设备之间提供通信能⼒，也就是硬件通信能⼒； 提供系统调⽤，如果应⽤程序要运⾏更⾼权限运⾏的服务，那么就需要有系统调⽤，它是⽤户程序与操作系统之间的接⼝。 如何工作的内核具有很⾼的权限，可以控制 cpu、内存、硬盘等硬件，⽽应⽤程序具有的权限很⼩，因此⼤多数操作系统，把内存分成了两个区域： 内核空间，这个内存空间只有内核程序可以访问； ⽤户空间，这个内存空间专⻔给应⽤程序使⽤； 用户空间的代码被限制了只能使用一个局部的内存空间，称为用户态；而内核空间的代码可以访问所有内存，称为内核态。 系统调用过程应⽤程序如果需要进⼊内核空间，就需要通过系统调⽤，下⾯来看看系统调⽤的过程： 内核程序执⾏在内核态，⽤户程序执⾏在⽤户态。当应⽤程序使⽤系统调⽤时，会产⽣⼀个中断。发⽣中断后， CPU 会中断当前在执⾏的⽤户程序，转⽽跳转到中断处理程序，也就是开始执⾏内核程序。内核处理完后，主动触发中断，把 CPU 执⾏权限交回给⽤户程序，回到⽤户态继续⼯作。 线程的实现方式一个应用程序启动后会在内存中创建一个执行副本，称为进程。 Linux的内核是一个宏内核，可以看做一个进程。 用户态进程如果要执行程序，是否也需要向内核申请呢？ 主要有三种线程的实现⽅式： ⽤户线程（User Thread）：在⽤户空间实现的线程，不是由内核管理的线程，是由⽤户态的线程库来完成线程的管理； 内核线程（Kernel Thread）：在内核中实现的线程，是由内核管理的线程； 轻量级进程（LightWeight Process）：在内核中来⽀持⽤户线程； 用户态线程当线程的概念出现时，操作系统厂商可不能直接就去修改操作系统的内核，为了保证系统的稳定性！所以当时研究人员就写了一个线程函数库来完成线程的创建，包括线程的创建、终⽌、同步和调度等。 这个线程函数库是位于用户态的，也就是说操作系统对这个库是一无所知的。 也就是说：我用线程库写的一个多线程进程，只能一次在一个 CPU 核心上运行 同时：如果AB是同一个进程的两个线程的话，A 正在运行的时候，线程 B 想要运行的话，只能等待 A 主动放弃 CPU，也就是主动调用 pthread_yield 函数。（一个CPU同时只能执行这个进程的一个线程） 用户态线程无法做到进程那样的轮转调度。 如果进程中的某一个线程阻塞了会发生什么？ 在操作系统眼里，是进程阻塞了，那么整个进程就会进入阻塞态，在阻塞操作结束前，这个进程都无法得到 CPU 资源。那就相当于，所有的线程都被阻塞了 为了解决这个问题，就出现了一种解决方案：jacket：就是把一个产生阻塞的系统调用转化成一个非阻塞的系统调用。 小白惊讶地问：“这怎么做得到？该阻塞的调用，还能变得不阻塞？” 小明答道：“我来举个例子吧，不是直接调用一个系统 I/O 例程，而是调用一个应用级别的 I/O jacket 例程，这个 jacket 例程中的代码会检查并且确定 I/O 设备是不是正忙，如果忙的话，就在用户态下将该线程阻塞，然后把控制权交给另一个线程。隔一段时间后再次检查 I/O 设备。就像你说的，最后还是会执行阻塞调用，但使用 jacket 可以缩短被阻塞的时间。不过有些情况下是可以不被阻塞的，取决于具体的实现。 用户线程的优势： 创建开销小 切换成本低 缺点： 与内核协作成本高，比如发生IO时需要频繁的在用户态和内核态之间切换 线程间协作成本高，线程通信需要IO，IO需要系统调用 无法利用多核优势，这些线程只能占用一个CPU核，无法并行加速。 操作系统是无法感知到用户态线程的，也就不会进行线程的切换。 内核态线程本质上就是操作系统能够看到的线程。 许多操作系统都已经支持内核级线程了。为了实现线程，内核里就需要有用来记录系统里所有线程的线程表。当需要创建一个新线程的时候，就需要进行一个系统调用，然后由操作系统进行线程表的更新。 内核态线程的优点： 操作系统内核如果知道线程的存在，就可以像调度多个进程一样，把这些线程放在好几个 CPU 核心上，就能做到实际上的并行 假如线程 A 阻塞了，与他同属一个进程的线程也不会被阻塞。这是内核级线程的绝对优势 内核态线程的缺点： 让操作系统进行线程调度，那意味着每次切换线程，就需要「陷入」内核态，而操作系统从用户态到内核态的转变是有开销的，所以说内核级线程切换的代价要比用户级线程大。 线程表是存放在操作系统固定的表格空间或者堆栈空间里，所以内核级线程的数量是有限的，扩展性比不上用户级线程 java中的线程是内核态线程还是用户态线程呢？ 中断是什么中断（interrupt）是计算机系统中的基本机制之一。即：在计算机运行过程中，当发生某个事件后，CPU 会停止当前程序流，转而去处理该事件，并在处理完毕后继续执行原程序流。 相关概念 描述 中断分类 硬中断（Hardware Interrupt）&amp; 软中断（Software Interrupt） 中断向量表（Interrupt Vector Table） 记录了中断号与中断服务程序内存地址的映射关系 中断服务程序 / 中断处理程序（Interrupt Service） 通过中断向量表定位到的特定处理程序 这样做的好处是化被动为主动，如果没有中断机制的话，就需要CPU轮询来判断某个条件是否成立，增加系统的开销。 分类硬中断：硬中断由外部设备（例如：磁盘，网卡，键盘，时钟）产生，用来通知操作系统外设状态变化 时钟中断： 一种硬中断，用于定期打断 CPU 执行的线程，以便切换给其他线程以得到执行机会。 处理流程： 1、外设 将中断请求发送给中断控制器； 2、中断控制器 根据中断优先级，有序地将中断传递给 CPU； 3、CPU 终止执行当前程序流，将 CPU 所有寄存器的数值保存到栈中； 4、CPU 根据中断向量，从中断向量表中查找中断处理程序的入口地址，执行中断处理程序； 5、CPU 恢复寄存器中的数值，返回原程序流停止位置继续执行。 软中断：是一条 CPU 指令，由当前正在运行的进程产生。流程就和上面的345一样。 系统调用： 是一种软中断处理程序，用于让程序从用户态陷入内核态，以执行相应的操作。","link":"/2021/10/06/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%80%EF%BC%89%E6%A6%82%E8%BF%B0/"},{"title":"操作系统（二）进程和线程","text":"主要分为四个部分： 进程基础知识 线程基础知识 进程间通信方式 线程间通信方式 进程调度算法 多线程同步 死锁问题 进程基础知识我们编写的代码只是⼀个存储在硬盘的静态⽂件，通过编译后就会⽣成⼆进制可执⾏⽂件，当我们运⾏这个可执⾏⽂件后，它会被装载到内存中，接着 CPU 会执⾏程序中的每⼀条指令，那么这个运⾏中的程序，就被称为「进程」（Process） CPU是支持多个程序交替执行的思想，这种执行方式称为并发执行。 比如一个程序需要去硬盘中读取数据，那么运行到读取文件的指令时，就会去硬盘中读取数据，但是硬盘的读写速度是⾮常慢的，那么在这个时候，如果 CPU傻傻的等硬盘返回数据的话，那 CPU 的利⽤率是⾮常低的。 你烧开水的时候会一直等到水烧开吗？肯定不会，中间可以去干别的事情。 虽然单核的 CPU 在某⼀个瞬间，只能运⾏⼀个进程。但在 1 秒钟期间，它可能会运⾏多个进程，这样就产⽣并⾏的错觉，实际上这是并发： 进程发展历程先从进程线程的发展过程开始说起： 很久以前，CPU只能挨个挨个的处理程序，程序A处理完了才能继续处理程序B，此时叫批处理系统 随着CPU的迅速发展，运行速度越来越快，出现了程序的执行速度和磁盘的IO速度不匹配的矛盾。当程序A需要从硬盘读取数据的时候，CPU要进行等待，非常浪费。所以当程序A读取数据的时候，先对A的执行现场进行保存，再对程序B进行处理，等程序A读取完毕的时候再通过中断机制使得CPU继续执行他。 当程序A被再次调度执行的时候，IO操作还没有完成，只能卡住不动。所以程序A内部出现了多个线程，一个线程去读取IO操作，另外的线程去处理其他的事，比如响应用户等等 每个线程又可以执行多个协程，操作系统内核完全不用参与，相当于用户态线程了，开销非常小 总而言之： 进程是系统进行资源分配和调度的基本单位。 进程存在的作用就是合理的隔离资源、运行环境、提升资源利用率 注意进程和程序的区别： 程序是指指令和数据的有序集合，是一个静态的概念。 进程是程序的一次运行，是一个具有独立功能的程序关于某个数据集合的一次运行活动，是系统进行资源分配和调度的基本单位。 单个CPU可以被若干进程共享，它使用某种调度算法决定何时停止一个进程的工作，并转而为另外一个进程提供服务。另外需要注意的是，如果一个进程运行了两遍，则被认为是两个进程。 举例区分： 进程和程序之间的区别是非常微妙的，但是通过一个例子可以让你加以区分：想想一位会做饭的计算机科学家正在为他的女儿制作生日蛋糕。他有做生日蛋糕的食谱，厨房里有所需的原谅：面粉、鸡蛋、糖、香草汁等。在这个比喻中，做蛋糕的食谱就是程序、计算机科学家就是 CPU、而做蛋糕的各种原谅都是输入数据。进程就是科学家阅读食谱、取来各种原料以及烘焙蛋糕等一系例了动作的总和。 进程的状态尽管每个进程是一个独立的实体，有其自己的程序计数器和内部状态，但是，进程之间仍然需要相互帮助。例如，一个进程的结果可以作为另一个进程的输入，在 shell 命令中 1cat chapter1 chapter2 chapter3 | grep tree 第一个进程是 cat，将三个文件级联并输出。第二个进程是 grep，它从输入中选择具有包含关键字 tree 的内容，根据这两个进程的相对速度（这取决于两个程序的相对复杂度和各自所分配到的 CPU 时间片），可能会发生下面这种情况，grep 准备就绪开始运行，但是输入进程还没有完成，于是必须阻塞 grep 进程，直到输入完毕。 当一个进程开始运行时，它可能会经历下面三状态模型： 图中会涉及三种状态 运行态，运行态指的就是进程实际占用 CPU 时间片运行时 就绪态，就绪态指的是可运行，但因为其他进程正在运行而处于就绪状态 阻塞态，除非某种外部事件发生，否则进程不能运行 逻辑上来说，运行态和就绪态是很相似的。这两种情况下都表示进程可运行，但是第二种情况没有获得 CPU 时间分片。第三种状态与前两种状态不同的原因是这个进程不能运行，CPU 空闲时也不能运行。 状态变迁： NULL -&gt; 创建状态：⼀个新进程被创建时的第⼀个状态； 创建状态 -&gt; 就绪状态：当进程被创建完成并初始化后，⼀切就绪准备运⾏时，变为就绪状态，这个过程是很快的； 就绪态 -&gt; 运⾏状态：处于就绪状态的进程被操作系统的进程调度器选中后，就分配给CPU 正式运⾏该进程； 运⾏状态 -&gt; 结束状态：当进程已经运⾏完成或出错时，会被操作系统作结束状态处理； 运⾏状态 -&gt; 就绪状态：处于运⾏状态的进程在运⾏过程中，由于分配给它的运⾏时间⽚⽤完，操作系统会把该进程变为就绪态，接着从就绪态选中另外⼀个进程运⾏； 运⾏状态 -&gt; 阻塞状态：当进程请求某个事件且必须等待时，例如请求 I/O 事件； 阻塞状态 -&gt; 就绪状态：当进程要等待的事件完成时，它从阻塞状态变到就绪状态； 如果有⼤量处于阻塞状态的进程，进程可能会占⽤着物理内存空间，显然不是我们所希望的，毕竟物理内存空间是有限的，被阻塞状态的进程占⽤着物理内存就⼀种浪费物理内存的⾏为。 所以，在虚拟内存管理的操作系统中，通常会把阻塞状态的进程的物理内存空间换出到硬盘，等需要再次运⾏的时候，再从硬盘换⼊到物理内存。 因此就需要一个新的状态来描述这种进程没有占⽤实际的物理内存空间的情况，称为挂起。 阻塞挂起状态：进程在外存（硬盘）并等待某个事件的出现； 就绪挂起状态：进程在外存（硬盘），但只要进⼊内存，即刻⽴刻运⾏； 通过 sleep 让进程间歇性挂起，其工作原理是设置一个定时器，到期后唤醒进程。 用户希望挂起一个程序的执行，比如在 Linux 中用 Ctrl+Z 挂起进程； 进程的控制结构PCBPCB 是进程存在的唯⼀标识，这意味着⼀个进程的存在，必然会有⼀个 PCB，如果进程消失了，那么 PCB 也会随之消失 PCB包含的内容： 进程描述信息：进程标识符和⽤户标识符等 进程控制和管理信息：进程当前状态和进程优先级等 资源分配清单：有关内存地址空间或虚拟地址空间的信息，所打开⽂件的列表和所使⽤的 I/O 设备信息 CPU 相关信息：CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB中，以便进程重新执⾏时，能从断点处继续执⾏。 PCB的组织方式： 通常是通过链表的⽅式进⾏组织，把具有相同状态的进程链在⼀起，组成各种队列。⽐如： 将所有处于就绪状态的进程链在⼀起，称为就绪队列； 把所有因等待某事件⽽处于等待状态的进程链在⼀起就组成各种阻塞队列； 另外，对于运⾏队列在单核 CPU 系统中则只有⼀个运⾏指针了，因为单核 CPU 在某个时间，只能运⾏⼀个程序。 进程的控制进程的创建操作系统允许⼀个进程创建另⼀个进程，⽽且允许⼦进程继承⽗进程所拥有的资源，当⼦进程被终⽌时，其在⽗进程处继承的资源应当还给⽗进程。同时，终⽌⽗进程时同时也会终⽌其所有的⼦进程。 Linux 操作系统对于终⽌有⼦进程的⽗进程，会把⼦进程交给 1 号进程接管。 创建过程： 为新进程分配⼀个唯⼀的进程标识号，并申请⼀个空⽩的 PCB，PCB 是有限的，若申请失败则创建失败； 为进程分配资源，此处如果资源不⾜，进程就会进⼊等待状态，以等待资源；初始化 PCB； 如果进程的调度队列能够接纳新进程，那就将进程插⼊到就绪队列，等待被调度运⾏； 对于虚拟空间地址来说，子进程会拷贝父进程的虚拟地址空间。所以，fork后子进程的用户区与父进程的用户区相同，也会拷贝内核区内容，仅仅是进程的 pid不同。 实际上，准确的来说，Linux的fork 是通过 写时拷贝 (copy-on-write)实现。 写时拷贝是一种可以推迟甚至不用避免拷贝的技术。 更具体来讲，在执行fork语句后，内核并不复制父进程的整个地址空间，而是父子进程共享父进程的地址空间（此时父子进程对于地址空间是只读指令），在父进程或者子进程进行写指令时，子进程才会复制一份地址空间，从而使得父子进程拥有自己的虚拟地址空间，在自己的地址空间进行写操作。也就是说，资源的复制是在需要写入时才会进行，在此之前，只会以只读方式进行共享。 进程的终止 正常退出：当编译器完成了所给定程序的编译之后，编译器会执行一个系统调用告诉操作系统它完成了工作。这个调用在 UNIX 中是 exit ，在 Windows 中是 ExitProcess 错误退出：比如编译时发现文件不存在，这时候应用程序通常会弹出一个对话框告知用户发生了系统错误，是需要重试还是退出 严重错误：通常是由于程序中的错误所导致的。例如，执行了一条非法指令，引用不存在的内存，或者除数是 0 等 被其他进程杀死：某个进程执行系统调用告诉操作系统杀死某个进程。在 UNIX 中，这个系统调用是 kill 查找需要终⽌的进程的 PCB； 如果处于执⾏状态，则⽴即终⽌该进程的执⾏，然后将 CPU 资源分配给其他进程； 如果其还有⼦进程，则应将其所有⼦进程终⽌； 将该进程所拥有的全部资源都归还给⽗进程或操作系统； 将其从 PCB 所在队列中删除； 进程的上下文切换各个进程之间是共享 CPU 资源的，在不同的时候进程之间需要切换，让不同的进程可以在CPU 执⾏，那么这个⼀个进程切换到另⼀个进程运⾏，称为进程的上下⽂切换。 任务是交给 CPU 运⾏的，那么在每个任务运⾏前，CPU 需要知道任务从哪⾥加载，⼜从哪⾥开始运⾏。所以，操作系统需要事先帮 CPU 设置好 CPU 寄存器和程序计数器 CPU 寄存器是 CPU 内部⼀个容量⼩，但是速度极快的内存（缓存）。我举个例⼦，寄存器像是你的⼝袋，内存像你的书包，硬盘则是你家⾥的柜⼦，如果你的东⻄存放到⼝袋，那肯定是⽐你从书包或家⾥柜⼦取出来要快的多。 程序计数器则是⽤来存储 CPU 正在执⾏的指令位置、或者即将执⾏的下⼀条指令位置。 进程是由内核管理和调度的，所以进程的切换只能发⽣在内核态。 所以，进程的上下⽂切换不仅包含了虚拟内存、栈、全局变量等⽤户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。 通常，会把交换的信息保存在进程的 PCB，当要运⾏另外⼀个进程的时候，我们需要从这个进程的 PCB 取出上下⽂，然后恢复到 CPU 中，这使得这个进程可以继续执⾏，如下图所示： 发生线程切换的常见场景： 为了保证所有进程可以得到公平调度，CPU 时间被划分为⼀段段的时间⽚，这些时间⽚再被轮流分配给各个进程。这样，当某个进程的时间⽚耗尽了，进程就从运⾏状态变为就绪状态，系统从就绪队列选择另外⼀个进程运⾏； 进程在系统资源不⾜（⽐如内存不⾜）时，要等到资源满⾜后才可以运⾏，这个时候进程也会被挂起，并由系统调度其他进程运⾏； 当进程通过睡眠函数 sleep 这样的⽅法将⾃⼰主动挂起时，⾃然也会重新调度； 当有优先级更⾼的进程运⾏时，为了保证⾼优先级进程的运⾏，当前进程会被挂起，由⾼优先级进程来运⾏； 发⽣硬件中断时，CPU 上的进程会被中断挂起，转⽽执⾏内核中的中断服务程序； 线程基础知识线程的由来为什么要在进程的基础上再创建一个线程的概念？ 多线程之间会共享同一块地址空间和所有可用数据的能力，这是进程所不具备的 线程要比进程更轻量级，由于线程更轻，所以它比进程更容易创建，也更容易撤销。 第三个原因可能是性能方面的探讨，如果多个线程都是 CPU 密集型的，那么并不能获得性能上的增强，但是如果存在着大量的计算和大量的 I/O 处理，拥有多个线程能在这些活动中彼此重叠进行，从而会加快应用程序的执行速度 线程可以看做进程当中的⼀条执⾏流程。 同⼀个进程内多个线程之间可以共享代码段、数据段、打开的⽂件等资源，但每个线程各⾃都有⼀套独⽴的寄存器和栈，这样可以确保线程的控制流是相对独⽴的。 进程和线程的区别我们编写的代码只是一个存储在硬盘的静态文件，通过编译之后生成二进制的可执行文件，当运行这个可执行文件的时候，会被装载到内存中，接着CPU会执行程序的每一条指令，那么这个运行中的程序就称为进程。 Ⅰ 拥有资源：进程是操作系统资源分配的基本单位，但是线程不拥有资源，线程可以访问隶属进程的资源 Ⅱ 调度：线程是任务调度和执行的基本单位，一个进程中可以有多个线程，它们共享进程资源 在操作系统中能同时运行多个进程（程序）；而在同一个进程（程序）中有多个线程同时执行（通过CPU调度，在每个时间片中只有一个线程执行） 在同一进程中，线程的切换不会引起进程切换，从一个进程中的线程切换到另一个进程中的线程时，会引起进程切换。 QQ 和浏览器是两个进程，浏览器进程里面有很多线程，例如 HTTP 请求线程、事件响应线程、渲染线程等等，线程的并发执行使得在浏览器中点击一个新链接从而发起 HTTP 请求时，浏览器还可以响应用户的其它事件。 Ⅲ 系统开销：由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O 设备等，所付出的开销远大于创建或撤销线程时的开销。类似地，在进行进程切换时，因为线程具有相同的地址空间（虚拟内存共享），这意味着同⼀个进程的线程都具有同⼀个⻚表，那么在切换的时候不需要切换⻚表。⽽对于进程之间的切换，切换的时候要把⻚表给切换掉，⽽⻚表的切换过程开销是⽐较⼤的； Ⅳ 通信方面：线程间可以通过直接读写同一进程中的数据进行通信，但是进程通信需要借助 IPC，需要通过内核。 个人理解：操作系统给进程分配的资源有文件资源、内存资源、CPU资源。而进程把CPU资源更细粒度化的给了该进程下的所有线程，所以线程变成了CPU调度和执行的基本单位。 线程的上下文切换 当两个线程不是属于同⼀个进程，则切换的过程就跟进程上下⽂切换⼀样； 当两个线程是属于同⼀个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据； 线程的三种实现主要有三种实现方式 在用户空间中实现线程； 在内核空间中实现线程； 在用户和内核空间中混合实现线程。 下面我们分开讨论一下 用户线程第一种方法是把整个线程包放在用户空间中，内核对线程一无所知，它不知道线程的存在。所有的这类实现都有同样的通用结构 用户线程的优点： 每个进程都需要有它私有的线程控制块（TCB）列表，⽤来跟踪记录它各个线程状态信息（PC、栈指针、寄存器），TCB 由⽤户级线程库函数来维护，线程的调度都是在用户空间完成，不需要切换到内核，效率很高。 可以允许进程自己实现调度算法：比如JVM中的垃圾收集的线程可以自己实现。 用户线程扩展性比较好，不需要占用内核线程的空间。 用户线程的劣势： 一个进程下面的所有用户进程其实都是运行在一个CPU上面的，无法利用多核优势，也不能由操作系统调度。如果⼀个线程发起了系统阻塞调⽤或者缺页中断⽽阻塞，那进程所包含的⽤户线程都不能执⾏了。 内核线程内核线程是由操作系统管理的，线程对应的 TCB ⾃然是放在操作系统⾥的，这样线程的创建、终⽌和管理都是由操作系统负责。 当某个线程希望创建一个新线程或撤销一个已有线程时，它会进行一个系统调用，这个系统调用通过对线程表的更新来完成线程创建或销毁工作。 内核线程的模型就是所谓的1对1模型，也就是一个用户线程对应一个内核线程。 内核线程的优点： 在⼀个进程当中，如果某个内核线程发起系统调⽤⽽被阻塞，并不会影响其他内核线程的运⾏；所有能够阻塞的调用都会通过系统调用的方式来实现，当一个线程阻塞时，内核可以进行选择，是运行在同一个进程中的另一个线程（如果有就绪线程的话）还是运行一个另一个进程中的线程 内核线程的缺点： 线程的创建、终⽌和切换都是通过系统调⽤的⽅式来进⾏，因此对于系统来说，系统开销⽐较⼤； 混合实现结合用户空间和内核空间的优点，设计人员采用了一种内核级线程的方式，然后将用户级线程与某些或者全部内核线程多路复用起来 在这种模型中，编程人员可以自由控制用户线程和内核线程的数量，具有很大的灵活度。采用这种方法，内核只识别内核级线程，并对其进行调度。其中一些内核级线程会被多个用户级线程多路复用。 进程间通信方式每个进程各自有不同的用户地址空间，任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核，在内核中开辟一块缓冲区，进程1把数据从用户空间拷到内核缓冲区，进程2再从内核缓冲区把数据读走，内核提供的这种机制称为进程间通信（IPC，InterProcess Communication） 管道/命名管道12ps -ef | grep mysql这个Linux命令中的 | 其实就是管道：它的功能是将前⼀个命令（ ps auxf ）的输出，作为后⼀个命令（ grep mysql ）的输⼊ 管道是单向传输的，只能够支持父子进程或兄弟进程之间的通信。 由于普通管道文件没有文件名，所以进程无法使用open函数打开文件，从而得到文件描述符，所以只有一种办法。那就是父进程先调用pipe创建出管道，并得到管道的文件描述符号。然后fork出子进程，让子进程继承父进程打开的文件描述符，父子进程就能通过同一管道，从而实现通信。 管道的实质是一个内核缓冲区，进程以先进先出的方式从缓冲区存取数据，管道一端的进程顺序的将数据写入缓冲区，另一端的进程则顺序的读出数据。 通信的数据是⽆格式的流并且⼤⼩受限 **2.*命名管道 对于命名管道，它可以在不相关的进程间也能相互通信。因为命名管道，提前创建了⼀个类型为管道的设备⽂件，在进程⾥只要使⽤这个设备⽂件，就可以相互通信。 12345678910111213# shengbinbin @ binshow in ~/Documents/test [13:26:34]$ mkfifo myPipe # 创建一个管道# shengbinbin @ binshow in ~/Documents/test [13:26:42]$ ls -l # p表示 pipe管道的意思total 0prw-r--r-- 1 shengbinbin staff 0 10 4 13:26 myPipe# shengbinbin @ binshow in ~/Documents/test [13:26:50]$ echo &quot;hello&quot; &gt; myPipe # 将数据写进管道 ，到这会一直卡住，因为管道中的内容一直没有被读取 # 知道打开另外一个终端， 执行 cat &lt; myPipe // 读取管道⾥的数据才结束 匿名管道的创建： 1234int pipe(int fd[2]) //通过这个系统调用 //这⾥表示创建⼀个匿名管道，并返回了两个描述符， //⼀个是管道的读取端描述符 fd[0] ， //另⼀个是管道的写⼊端描述符 fd[1] 。注意，这个匿名管道是特殊的⽂件，只存在于内存，不存于⽂件系统中。 管道的实质是一个内核缓冲区 这两个描述符都是在⼀个进程⾥⾯，并没有起到进程间通信的作⽤，怎么样才能使得管道是跨过两个进程的呢? 我们可以使⽤ fork 创建⼦进程，创建的⼦进程会复制⽗进程的⽂件描述符，这样就做到了两个进程各有两个「 fd[0] 与 fd[1] 」，两个进程就可以通过各⾃的 fd 写⼊和读取同⼀个管道⽂件实现跨进程通信了。 管道只能⼀端写⼊，另⼀端读出，所以上⾯这种模式容易造成混乱，因为⽗进程和⼦进程都可以同时写⼊，也都可以读出。那么，为了避免这种情况，通常的做法是： ⽗进程关闭读取的 fd[0]，只保留写⼊的 fd[1]； ⼦进程关闭写⼊的 fd[1]，只保留读取的 fd[0]； 但实际在 shell ⾥⾯执⾏ A | B 命令的时候，A 进程和 B 进程都是 shell 创建出来的⼦进程，A 和 B之间不存在⽗⼦关系，它俩的⽗进程都是 shell： 所以说，在 shell ⾥通过「 | 」匿名管道将多个命令连接在⼀起，实际上也就是创建了多个⼦进程，那么在我们编写 shell 脚本时，能使⽤⼀个管道搞定的事情，就不要多⽤⼀个管道，这样可以减少创建⼦进程的系统开销。 对于匿名管道，它的通信范围是存在⽗⼦关系的进程。因为管道没有实体，也就是没有管道⽂件，只能通过 fork 来复制⽗进程 fd ⽂件描述符，来达到通信的⽬的。 对于命名管道，它可以在不相关的进程间也能相互通信。因为命令管道，提前创建了⼀个类型为管道的设备⽂件，在进程⾥只要使⽤这个设备⽂件，就可以相互通信。 不管是匿名管道还是命名管道，进程写⼊的数据都是缓存在内核中，另⼀个进程读取数据时候⾃然也是从内核中获取，同时通信数据都遵循先进先出原则，不⽀持 lseek 之类的⽂件定位操作。 消息队列**消息队列(Message Queuing)**：是存放在内核中的消息链表，每个消息队列由消息队列标识符表示。 消息队列可以解决管道的问题，⽐如，A 进程要给 B 进程发送消息，A 进程把数据放在对应的消息队列后就可以正常返回了，B 进程需要的时候再去读取数据就可以了。同理，B 进程要给 A 进程发送消息也是如此。 消息队列是保存在内核中的消息链表，在发送数据时，会分成⼀个⼀个独⽴的数据单元，也就是消息体（数据块），消息体是⽤户⾃定义的数据类型，消息的发送⽅和接收⽅要约定好消息体的数据类型，所以每个消息体都是固定⼤⼩的存储块，不像管道是⽆格式的字节流数据。如果进程从消息队列中读取了消息体，内核就会把这个消息体删除。 管道是随进程的创建⽽建⽴，随进程的结束⽽销毁，而消息队列⽣命周期随内核，如果没有释放消息队列或者没有关闭操作系统，消息队列会⼀直存在 消息队列的限制： 消息队列不适合⽐较⼤数据的传输,因为消息体有长度的限制。 通信过程中存在⽤户态与内核态之间的数据拷⻉开销 共享内存共享内存是为了解决消息队列中存在的⽤户态与内核态之间的数据拷⻉开销： 共享内存的机制，就是从通信的两个进程中拿出⼀块虚拟地址空间来，映射到相同的物理内存中。这样这个进程写⼊的东⻄，另外⼀个进程⻢上就能看到了，都不需要拷⻉来拷⻉去，传来传去，⼤⼤提⾼了进程间通信的速度。 由于多个进程共享一段内存，可能会有并发安全问题，因此需要依靠某种同步机制（如信号量）来达到进程间的同步及互斥。 信号量共享内存的缺陷就是：如果多个进程同时修改同⼀个共享内存，很有可能就冲突了。 为了防⽌多进程竞争共享资源，⽽造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被⼀个进程访问。正好，信号量就实现了这⼀保护机制。 信号量其实是⼀个整型的计数器，主要⽤于实现进程间的互斥与同步。 信号量表示资源的数量，控制信号量的⽅式有两种原⼦操作： ⼀个是 P 操作，这个操作会把信号量减去 1，相减后如果信号量 &lt; 0，则表明资源已被占⽤，进程需阻塞等待；相减后如果信号量 &gt;= 0，则表明还有资源可使⽤，进程可正常继续执⾏。 另⼀个是 V 操作，这个操作会把信号量加上 1，相加后如果信号量 &lt;= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运⾏；相加后如果信号量 &gt; 0，则表明当前没有阻塞中的进程； P 操作是⽤在进⼊共享资源之前，V 操作是⽤在离开共享资源之后，这两个操作是必须成对出现的。 举例： 两个进程互斥访问共享内存，信号量为1： 具体的过程如下： 进程 A 在访问共享内存前，先执⾏了 P 操作，由于信号量的初始值为 1，故在进程 A 执 ⾏ P 操作后信号量变为 0，表示共享资源可⽤，于是进程 A 就可以访问共享内存。 若此时，进程 B 也想访问共享内存，执⾏了 P 操作，结果信号量变为了 -1，这就意味着临界资源已被占⽤，因此进程 B 被阻塞。 直到进程 A 访问完共享内存，才会执⾏ V 操作，使得信号量恢复为 0，接着就会唤醒阻塞中的线程 B，使得进程 B 可以访问共享内存，最后完成共享内存的访问后，执⾏ V 操作，使信号量恢复到初始值 1。 可以发现，信号初始化为 1 ，就代表着是互斥信号量，它可以保证共享内存在任何时刻只有⼀个进程在访问，这就很好的保护了共享内存。 两个进程同步的方式：例如，进程 A 是负责⽣产数据，⽽进程 B 是负责读取数据，可以初始化信号量为0： 具体过程： 如果进程 B ⽐进程 A 先执⾏了，那么执⾏到 P 操作时，由于信号量初始值为 0，故信号量会变为 -1，表示进程 A 还没⽣产数据，于是进程 B 就阻塞等待； 接着，当进程 A ⽣产完数据后，执⾏了 V 操作，就会使得信号量变为 0，于是就会唤醒阻塞在 P 操作的进程 B； 最后，进程 B 被唤醒后，意味着进程 A 已经⽣产了数据，于是进程 B 就可以正常读取数据了。 可以发现，信号初始化为 0 ，就代表着是同步信号量，它可以保证进程 A 应在进程 B 之前执⾏。 信号信号和信号量完全不一样!!! 在 Linux 操作系统中， 为了响应各种各样的事件，提供了⼏⼗种信号，分别代表不同的意义。我们可以通过 kill -l 命令，查看所有的信号： 123# shengbinbin @ binshow in ~ [13:56:41]$ kill -lHUP INT QUIT ILL TRAP ABRT EMT FPE KILL BUS SEGV SYS PIPE ALRM TERM URG STOP TSTP CONT CHLD TTIN TTOU IO XCPU XFSZ VTALRM PROF WINCH INFO USR1 USR2 我们可以通过键盘输⼊某些组合键的时候，给进程发送信号。例如 Ctrl+C 产⽣ SIGINT 信号，表示终⽌该进程； Ctrl+Z 产⽣ SIGTSTP 信号，表示停⽌该进程，但还未结束； 如果进程在后台运⾏，可以通过 kill 命令的⽅式给进程发送信号，但前提需要知道运⾏中的进程 PID 号，例如： kill -9 1050 ，表示给 PID 为 1050 的进程发送 SIGKILL 信号，⽤来⽴即结束该进程 信号来源 信号是软件层次上对中断机制的一种模拟，是一种异步通信方式，，信号可以在用户空间进程和内核之间直接交互，内核可以利用信号来通知用户空间的进程发生了哪些系统事件，信号事件主要有两个来源： 硬件来源：用户按键输入Ctrl+C退出、硬件异常如无效的存储访问等。 软件终止：终止进程信号、其他进程调用kill函数、软件异常产生信号。 信号是进程间通信机制中唯⼀的异步通信机制，因为可以在任何时候发送信号给某⼀进程，⼀旦有信号产⽣，我们就有下⾯这⼏种，⽤户进程对信号的处理⽅式。 **1.**执⾏默认操作。Linux 对每种信号都规定了默认操作，例如，上⾯列表中的 SIGTERM 信号，就是终⽌进程的意思。 **2.**捕捉信号。我们可以为信号定义⼀个信号处理函数。当信号发⽣时，我们就执⾏相应的信号处理函数。 **3.**忽略信号。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应⽤进程⽆法捕捉和忽略的，即 SIGKILL 和 SEGSTOP ，它们⽤于在任何时候中断或结束某⼀进程。 信号生命周期和处理流程 （1）信号被某个进程产生，并设置此信号传递的对象（一般为对应进程的pid），然后传递给操作系统； （2）操作系统根据接收进程的设置（是否阻塞）而选择性的发送给接收者，如果接收者阻塞该信号（且该信号是可以阻塞的），操作系统将暂时保留该信号，而不传递，直到该进程解除了对此信号的阻塞（如果对应进程已经退出，则丢弃此信号），如果对应进程没有阻塞，操作系统将传递此信号。 （3）目的进程接收到此信号后，将根据当前进程对此信号设置的预处理方式，暂时终止当前代码的执行，保护上下文（主要包括临时寄存器数据，当前程序位置以及当前CPU的状态）、转而执行中断服务程序，执行完成后在回复到中断的位置。当然，对于抢占式内核，在中断返回时还将引发新的调度。 套接字Socket跨⽹络与不同主机上的进程之间通信，就需要 Socket 通信了。 创建一个Socket的系统调用： 12345int socket(int domain, int type, int protocal)//domain 参数⽤来指定协议族，⽐如 AF_INET ⽤于 IPV4、AF_INET6 ⽤于 IPV6、AF_LOCAL/AF_UNIX ⽤于本机； //type 参数⽤来指定通信特性，⽐如 SOCK_STREAM 表示的是字节流，对应 TCP、SOCK_DGRAM 表示的是数据报，对应 UDP、SOCK_RAW 表示的是原始套接字 //protocal:数原本是⽤来指定通信协议的，但现在基本废弃。因为协议已经通过前⾯两个参数指定完成，protocol ⽬前⼀般写成 0 即可 根据创建 socket 类型的不同，通信的⽅式也就不同： 实现 TCP 字节流通信： socket 类型是 AF_INET 和 SOCK_STREAM； 实现 UDP 数据报通信：socket 类型是 AF_INET 和 SOCK_DGRAM； 实现本地进程间通信： 「本地字节流 socket 」类型是 AF_LOCAL 和 SOCK_STREAM，「本地数据报 socket 」类型是 AF_LOCAL 和 SOCK_DGRAM。另外，AF_UNIX 和AF_LOCAL 是等价的，所以 AF_UNIX 也属于本地 socket； TCP的Socket模型 服务端和客户端初始化 socket ，得到⽂件描述符； 服务端调⽤ bind ，将绑定在 IP 地址和端⼝; 服务端调⽤ listen ，进⾏监听； 服务端调⽤ accept ，等待客户端连接； 客户端调⽤ connect ，向服务器端的地址和端⼝发起连接请求； 服务端 accept 返回⽤于传输的 socket 的⽂件描述符； 客户端调⽤ write 写⼊数据；服务端调⽤ read 读取数据； 客户端断开连接时，会调⽤ close ，那么服务端 read 读取数据的时候，就会读取到了EOF ，待处理完数据后，服务端调⽤ close ，表示连接关闭。 所以，监听的 socket 和真正⽤来传送数据的 socket，是「两个」 socket，⼀个叫作监听socket**，⼀个叫作已完成连接 **socket。 成功连接建⽴之后，双⽅开始通过 read 和 write 函数来读写数据，就像往⼀个⽂件流⾥⾯写东⻄⼀样。 UDP的Socket模型 UDP 是没有连接的，所以不需要三次握⼿，也就不需要像 TCP 调⽤ listen 和 connect，但是UDP 的交互仍然需要 IP 地址和端⼝号，因此也需要 bind。 对于 UDP 来说，不需要要维护连接，那么也就没有所谓的发送⽅和接收⽅，甚⾄都不存在客户端和服务端的概念，只要有⼀个 socket 多台机器就可以任意通信，因此每⼀个 UDP 的socket 都需要 bind。 另外，每次通信时，调⽤ sendto 和 recvfrom，都要传⼊⽬标主机的 IP 地址和端⼝。 本地字节流 socket 和 本地数据报 socket 在 bind 的时候，不像 TCP 和 UDP 要绑定 IP 地址和端⼝，⽽是绑定⼀个本地⽂件，这也就是它们之间的最⼤区别。 线程间的通信方式同个进程下的线程之间都是共享进程的资源，只要是共享变量都可以做到线程间通信，⽐如全局变量，所以对于线程间关注的不是通信⽅式，⽽是关注多线程竞争共享资源的问题，信号量也同样可以在线程间实现互斥与同步： 互斥的⽅式，可保证任意时刻只有⼀个线程访问共享资源； 同步的⽅式，可保证线程 A 应在线程 B 之前执⾏； 进程调度算法当 CPU 空闲时，操作系统就选择内存中的某个「就绪状态」的进程，并给其分配 CPU。 当一个计算机是多道程序设计系统时，会频繁的有很多进程或者线程来同时竞争 CPU 时间片。当两个或两个以上的进程/线程处于就绪状态时，就会发生这种情况。如果只有一个 CPU 可用，那么必须选择接下来哪个进程/线程可以运行。操作系统中有一个叫做 调度程序(scheduler) 的角色存在，它就是做这件事儿的，该程序使用的算法叫做 调度算法(scheduling algorithm) 。 尽管有一些不同，但许多适用于进程调度的处理方法同样也适用于线程调度。当内核管理线程的时候，调度通常会以线程级别发生，很少或者根本不会考虑线程属于哪个进程。下面我们会首先专注于进程和线程的调度问题，然后会明确的介绍线程调度以及它产生的问题。 调度原则在所有的情况中，公平是很重要的。对一个进程给予相较于其他等价的进程更多的 CPU 时间片对其他进程来说是不公平的。当然，不同类型的进程可以采用不同的处理方式。 与公平有关的是系统的强制执行，什么意思呢？如果某公司的薪资发放系统计划在本月的15号，那么碰上了疫情大家生活都很拮据，此时老板说要在14号晚上发放薪资，那么调度程序必须强制使进程执行 14 号晚上发放薪资的策略。 另一个共同的目标是保持系统的所有部分尽可能的忙碌。如果 CPU 和所有的 I/O 设备能够一直运行，那么相对于让某些部件空转而言，每秒钟就可以完成更多的工作。例如，在批处理系统中，调度程序控制哪个作业调入内存运行。在内存中既有一些 CPU 密集型进程又有一些 I/O 密集型进程是一个比较好的想法，好于先调入和运行所有的 CPU 密集型作业，然后在它们完成之后再调入和运行所有 I/O 密集型作业的做法。使用后者这种方式会在 CPU 密集型进程启动后，争夺 CPU ，而磁盘却在空转，而当 I/O 密集型进程启动后，它们又要为磁盘而竞争，CPU 却又在空转。。。。。。显然，通过结合 I/O 密集型和 CPU 密集型，能够使整个系统运行更流畅，效率更高。 先来先服务调度算法每次从就绪队列选择最先进⼊队列的进程，然后⼀直运⾏，直到进程退出或被阻塞，才会继续从队列中选择第⼀个进程接着运⾏。 对长作业有利，适用于CPU密集型的系统。 最短作业优先调度算法它会优先选择运⾏时间最短的进程来运⾏，这有助于提⾼系统的吞吐量。 ⾼响应⽐优先调度算法⾼响应⽐优先（Highest Response Ratio Next, HRRN）调度算法主要是权衡了短作业和⻓作业。 时间⽚轮转调度算法每个进程被分配⼀个时间段，称为时间⽚（Quantum），即允许该进程在该时间段中运⾏。如果时间⽚⽤完，进程还在运⾏，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外⼀个进程；如果该进程在时间⽚结束前阻塞或结束，则 CPU ⽴即进⾏切换； 最⾼优先级调度算法但是，对于多⽤户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度程序能从就绪队列中选择最⾼优先级的进程进⾏运⾏，这称为最⾼优先级（Highest Priority First，HPF）调度算法。 多级反馈队列调度算法多级反馈队列（Multilevel Feedback Queue）调度算法是「时间⽚轮转算法」和「最⾼优先级算法」的综合和发展。 「多级」表示有多个队列，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短。 「反馈」表示如果有新的进程加⼊优先级⾼的队列时，⽴刻停⽌当前正在运⾏的进程，转⽽去运⾏优先级⾼的队列； 工作原理： 设置了多个队列，赋予每个队列不同的优先级，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短； 新的进程会被放⼊到第⼀级队列的末尾，按先来先服务的原则排队等待被调度，如果在第⼀级队列规定的时间⽚没运⾏完成，则将其转⼊到第⼆级队列的末尾，以此类推，直⾄完成； 当较⾼优先级的队列为空，才调度较低优先级的队列中的进程运⾏。如果进程运⾏时，有新进程进⼊较⾼优先级的队列，则停⽌当前运⾏的进程并将其移⼊到原队列末尾，接着让较⾼优先级的进程运⾏； 可以发现，对于短作业可能可以在第⼀级队列很快被处理完。对于⻓作业，如果在第⼀级队列处理不完，可以移⼊下次队列等待被执⾏，虽然等待的时间变⻓了，但是运⾏时间也会更⻓了，所以该算法很好的兼顾了⻓短作业，同时有较好的响应时间。 多线程同步 时分复用： 前面说过：在单核CPU的时候，为了实现多个程序同时运⾏的假象，操作系统通常以时间⽚调度的⽅式，让每个进程执⾏每次执⾏⼀个时间⽚，时间⽚⽤完了，就切换下⼀个进程运⾏，由于这个时间⽚的时间很短，于是就造成了「并发」的现象。 空分复用： 如果⼀个程序只有⼀个执⾏流程，也代表它是单线程的。当然⼀个程序可以有多个执⾏流程，也就是所谓的多线程程序，线程是调度的基本单位，进程则是资源分配的基本单位。线程之间是可以共享进程之间的资源的，比如代码段、堆空间、数据段等，每个线程也有自己独立的栈空间。 这样的话就会造成问题：多个线程竞争共享的资源，就会造成并发安全的问题。 互斥当多个线程操作共享变量的代码可以会导致竞争状态，这段代码称为临界区。 互斥指的就是 保证一个线程在临界区执行的时候其他的线程会被阻止进入。 同步互斥解决的是并发线程/进程对临界区的访问问题。 而在多线程中，每个线程并不一定是顺序执行的，它们基本都是各自独立的以不可预知的速度往前推进，这个时候我们又出现了新的需求：就是希望多个线程能密切合作来完成一个共同的任务。 而同步的概念就是为了解决这个问题，在一些并发进程/线程的关键点上进行相互等待和消息互通。 总结一下： 同步就好⽐：「操作 A 应在操作 B 之前执⾏」，「操作 C 必须在操作 A 和操作 B 都完成之后才能执⾏」等； 互斥就好⽐：「操作 A 和操作 B 不能在同⼀时刻执⾏」； 实现和使用主要有两种： 加锁：可以实现互斥。 信号量：PV操作，操作系统执行的时候已经是具有原子性的。 信号量通常表示资源的数量，一个整型数值： 通过操作系统的两个系统调用函数来控制信号量： P 操作：将 sem 减 1 ，相减后，如果 sem &lt; 0 ，则进程/线程进⼊阻塞等待，否则继续，表明 P 操作可能会阻塞； V 操作：将 sem 加 1 ，相加后，如果 sem &lt;= 0 ，唤醒⼀个等待中的进程/线程，表明V 操作不会阻塞； 操作系统实现PV操作 信号量如何实现互斥为每类共享资源设置⼀个信号量 s ，其初值为 1 ，表示该临界资源未被占⽤。 只要把进⼊临界区的操作置于 P(s) 和 V(s) 之间，即可实现进程/线程互斥。 此时，任何想进⼊临界区的线程，必先在互斥信号量上执⾏ P 操作，在完成对临界资源的访 问后再执⾏ V 操作。由于互斥信号量的初始值为 1，故在第⼀个线程执⾏ P 操作后 s 值变为 0，表示临界资源为空闲，可分配给该线程，使之进⼊临界区。 若此时⼜有第⼆个线程想进⼊临界区，也应先执⾏ P 操作，结果使 s 变为负值，这就意味着 临界资源已被占⽤，因此，第⼆个线程被阻塞。 并且，直到第⼀个线程执⾏ V 操作，释放临界资源⽽恢复 s 值为 0 后，才唤醒第⼆个线程， 使之进⼊临界区，待它完成临界资源的访问后，⼜执⾏ V 操作，使 s 恢复到初始值 1。 信号量如何实现同步同步的⽅式是设置⼀个信号量，其初值为 0 。 经典同步问题哲学家就餐问题 5 个⽼⼤哥哲学家，闲着没事做，围绕着⼀张圆桌吃⾯； 巧就巧在，这个桌⼦只有 5 ⽀叉⼦，每两个哲学家之间放⼀⽀叉⼦； 哲学家围在⼀起先思考，思考中途饿了就会想进餐； 奇葩的是，这些哲学家要两⽀叉⼦才愿意吃⾯，也就是需要拿到左右两边的叉⼦才进餐； 吃完后，会把两⽀叉⼦放回原处，继续思考； 如何保证他们的动作时有序的进程，不会出现有人永远拿不到叉子呢？ 信号量的方式： 不过，这种解法存在⼀个极端的问题：假设五位哲学家同时拿起左边的叉⼦，桌⾯上就没有叉⼦了， 这样就没有⼈能够拿到他们右边的叉⼦，也就说每⼀位哲学家都会在 P(fork[(i + 1) % P(fork[(i + 1) %N ]) N ]) 这条语句阻塞了，很明显这发⽣了死锁的现象。 方案二：在拿叉子前加上一个互斥信号量： 这样问题虽然解决了，但是同一时间只有一个哲学家可以吃饭，效率比较低。 方案三：解决方案一的问题：根据哲学家的编号的不同，⽽采取不同的动作 即让偶数编号的哲学家「先拿左边的叉⼦后拿右边的叉⼦」，奇数编号的哲学家「先拿右边的叉⼦后拿左边的叉⼦」 上⾯的程序，在 P 操作时，根据哲学家的编号不同，拿起左右两边叉⼦的顺序不同。另外，V操作是不需要分⽀的，因为 V 操作是不会阻塞的。 读者-写者问题死锁问题死锁概念当两个线程为了保护两个不同的共享资源⽽使⽤了两个互斥锁，那么这两个互斥锁应⽤不当的时候，可能会造成两个线程都在等待对⽅释放锁，在没有外⼒的作⽤下，这些线程会⼀直相互等待，就没办法继续运⾏，这种情况就是发⽣了死锁。 比如线程A已经获得了资源1，此时线程B已经获得了资源2。 而线程A又想要去获得资源2，线程B想要获得资源1，这样就会形成死锁~如下代码所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class DeadLockTest { public static String obj1 = &quot;obj1&quot;; public static String obj2 = &quot;obj2&quot;; public static void main(String[] args) { LockA la = new LockA(); new Thread(la).start(); LockB lb = new LockB(); new Thread(lb).start(); }}class LockA implements Runnable{ public void run() { try { System.out.println(new Date().toString() + &quot; LockA 开始执行&quot;); while(true){ synchronized (DeadLockTest.obj1) { System.out.println(new Date().toString() + &quot; LockA 锁住 obj1&quot;); Thread.sleep(3000); // 此处等待是给B能锁住机会 synchronized (DeadLockTest.obj2) { System.out.println(new Date().toString() + &quot; LockA 锁住 obj2&quot;); Thread.sleep(60 * 1000); // 为测试，占用了就不放 } } } } catch (Exception e) { e.printStackTrace(); } }}class LockB implements Runnable{ public void run() { try { System.out.println(new Date().toString() + &quot; LockB 开始执行&quot;); while(true){ synchronized (DeadLockTest.obj2) { System.out.println(new Date().toString() + &quot; LockB 锁住 obj2&quot;); Thread.sleep(3000); // 此处等待是给A能锁住机会 synchronized (DeadLockTest.obj1) { System.out.println(new Date().toString() + &quot; LockB 锁住 obj1&quot;); Thread.sleep(60 * 1000); // 为测试，占用了就不放 } } } } catch (Exception e) { e.printStackTrace(); } }} 死锁发生的4个条件 互斥：多个线程不能同时使用同一个资源。 占用并等待：线程A持有资源1之后又想去申请资源2，发现资源2被其他线程拿走了，需要等待。在等待的过程中不会释放自己已经持有的资源。 不可剥夺：线程持有资源后在使用完之前不能被其他线程获取。 环路等待：在死锁发生的时候，两个线程获取资源的顺序构成了环形链。 死锁如何预防破坏上面的4个条件其中一个即可~ 如何排查死锁todo","link":"/2021/10/06/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BA%8C%EF%BC%89%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/"},{"title":"消息队列基础问题","text":"消息队列基础篇，主要描述了四个问题： 为什么要用消息队列 如何保证消息不丢失 如何保证消息不重复消费 如何处理消息的积压 消息队列的使用场景异步处理我们以常见的电商秒杀场景为例，秒杀系统需要解决的核心问题是，如何利用有限的服务器资源，尽可能多地处理短时间内的海量请求。 一个秒杀通常包含了一下几个步骤： 风险控制 库存锁定 生成订单 短信通知 更新统计数据等等。。 如果没有任何优化处理的话，正常的处理流程是：App 将请求发送给网关，依次调用上述 5 个流程，然后将结果返回给 APP。但实际上，用户能否秒杀成功主要是看风险控制和库存锁定两步，也就是说当服务端完成前面 2 个步骤，确定本次请求的秒杀结果后，就可以马上给用户返回响应，剩余的步骤都可以放入消息队列中，异步的进行后续的处理。 这样处理之后，在秒杀期间我们可以用更多的服务器资源来处理秒杀请求，秒杀结束后再异步的处理后续的步骤。 在这个场景中，消息队列被用于实现服务的异步处理。这样做的好处是： 可以更快地返回结果； 减少等待，自然实现了步骤之间的并发，提升系统总体的性能。 流量削峰还是以秒杀场景为例，我们已经通过部分异步处理提高了用户的响应效率，下一个问题是如何避免瞬间过多的请求压垮我们的秒杀系统？ 一个设计健壮的程序有自我保护的能力，也就是说，它应该可以在海量的请求下，还能在自身能力范围内尽可能多地处理请求，拒绝处理不了的请求并且保证自身运行正常。不幸的是，现实中很多程序并没有那么“健壮”，而直接拒绝请求返回错误对于用户来说也是不怎么好的体验。 使用消息队列隔离网关和后端服务，以达到流量控制和保护后端服务的目的。 加入消息队列后，整个秒杀流程变为： 网关在收到请求后，将请求放入请求消息队列； 后端服务从请求消息队列中获取 APP 请求，完成后续秒杀处理过程，然后返回结果。 秒杀开始后，当短时间内大量的秒杀请求到达网关时，不会直接冲击到后端的秒杀服务，而是先堆积在消息队列中，后端服务按照自己的最大处理能力，从消息队列中消费请求进行处理。 对于超时的请求可以直接丢弃，APP 将超时无响应的请求处理为秒杀失败即可。运维人员还可以随时增加秒杀服务的实例数量进行水平扩容，而不用对系统的其他部分做任何更改。 这种设计的优点是：能根据下游的处理能力自动调节流量，达到“削峰填谷”的作用。但这样做同样是有代价的： 增加了系统调用链环节，导致总体的响应时延变长。 上下游系统都要将同步调用改为异步消息，增加了系统的复杂度。 消息队列实现令牌桶进行限流令牌桶控制流量的原理是：单位时间内只发放固定数量的令牌到令牌桶中，规定服务在处理请求之前必须先从令牌桶中拿出一个令牌，如果令牌桶中没有令牌，则拒绝请求。这样就保证单位时间内，能处理的请求不超过发放令牌的数量，起到了流量控制的作用。 网关在处理 APP 请求时增加一个获取令牌的逻辑。 令牌桶可以简单地用一个有固定容量的消息队列加一个“令牌发生器”来实现：令牌发生器按照预估的处理能力，匀速生产令牌并放入令牌队列（如果队列满了则丢弃令牌），网关在收到请求时去令牌队列消费一个令牌，获取到令牌则继续调用后端秒杀服务，如果获取不到令牌则直接返回秒杀失败。 服务解耦订单信息是电商系统中比较核心的数据，当一个新订单创建时： 支付系统需要发起支付流程； 风控系统需要审核订单的合法性； 客服系统需要给用户发短信告知用户； 经营分析系统需要更新统计数据； 这些订单下游的系统都需要实时获得订单数据。随着业务不断发展，这些订单下游系统不断的增加，不断变化，并且每个系统可能只需要订单数据的一个子集，负责订单服务的开发团队不得不花费很大的精力，应对不断增加变化的下游系统，不停地修改调试订单系统与这些下游系统的接口。任何一个下游系统接口变更，都需要订单模块重新进行一次上线，对于一个电商的核心服务来说，这几乎是不可接受的。 所有的电商都选择用消息队列来解决类似的系统耦合过于紧密的问题。引入消息队列后，订单服务在订单变化时发送一条消息到消息队列的一个主题 Order 中，所有下游系统都订阅主题 Order，这样每个下游系统都可以获得一份实时完整的订单数据。 如何保证消息不丢失如何检测消息是否丢失 如果是 IT 基础设施比较完善的公司，一般都有分布式链路追踪系统，使用类似的追踪系统可以很方便地追踪每一条消息。 如果没有这样的追踪系统，这里我提供一个比较简单的方法，来检查是否有消息丢失的情况。我们可以利用消息队列的有序性来验证是否有消息丢失。原理非常简单，在 Producer 端，我们给每个发出的消息附加一个连续递增的序号，然后在 Consumer 端来检查这个序号的连续性。如果没有消息丢失，Consumer 收到消息的序号必然是连续递增的，或者说收到的消息，其中的序号必然是上一条消息的序号 +1。如果检测到序号不连续，那就是丢消息了。还可以通过缺失的序号来确定丢失的是哪条消息，方便进一步排查原因。 大多数消息队列的客户端都支持拦截器机制，你可以利用这个拦截器机制，在 Producer 发送消息之前的拦截器中将序号注入到消息中，在 Consumer 收到消息的拦截器中检测序号的连续性，这样实现的好处是消息检测的代码不会侵入到你的业务代码中，待你的系统稳定后，也方便将这部分检测的逻辑关闭或者删除。 分布式系统中需要注意的点 首先，像 Kafka 和 RocketMQ 这样的消息队列，它是不保证在 Topic 上的严格顺序的，只能保证分区上的消息是有序的，所以我们在发消息的时候必须要指定分区，并且，在每个分区单独检测消息序号的连续性。 如果你的系统中 Producer 是多实例的，由于并不好协调多个 Producer 之间的发送顺序，所以也需要每个 Producer 分别生成各自的消息序号，并且需要附加上 Producer 的标识，在 Consumer 端按照每个 Producer 分别来检测序号的连续性。 Consumer 实例的数量最好和分区数量一致，做到 Consumer 和分区一一对应，这样会比较方便地在 Consumer 内检测消息序号的连续性。 如何确保消息可靠传递 生产阶段: 在这个阶段，从消息在 Producer 创建出来，经过网络传输发送到 Broker 端。 存储阶段: 在这个阶段，消息在 Broker 端存储，如果是集群，消息会在这个阶段被复制到其他的副本上。 消费阶段: 在这个阶段，Consumer 从 Broker 上拉取消息，经过网络传输发送到 Consumer 上。 1. 生产阶段： 在生产阶段，消息队列通过最常用的请求确认机制，来保证消息的可靠传递：当你的代码调用发消息方法时，消息队列的客户端会把消息发送到 Broker，Broker 收到消息后（最好写入到磁盘中才返回确认消息），会给客户端返回一个确认响应，表明消息已经收到了。客户端收到响应后，完成了一次正常消息的发送。 12345678你在编写发送消息代码时，需要注意，正确处理返回值或者捕获异常，就可以保证这个阶段的消息不会丢失。 try { RecordMetadata metadata = producer.send(record).get(); System.out.println(&quot; 消息发送成功。&quot;);} catch (Throwable e) { System.out.println(&quot; 消息发送失败！&quot;); System.out.println(e);} 异步发送时，则需要在回调方法里进行检查。这个地方是需要特别注意的，很多丢消息的原因就是，我们使用了异步发送，却没有在回调中检查发送结果。 12345678producer.send(record, (metadata, exception) -&gt; { if (metadata != null) { System.out.println(&quot; 消息发送成功。&quot;); } else { System.out.println(&quot; 消息发送失败！&quot;); System.out.println(exception); }}); 2.存储阶段： 在存储阶段正常情况下，只要 Broker 在正常运行，就不会出现丢失消息的问题，但是如果 Broker 出现了故障，比如进程死掉了或者服务器宕机了，还是可能会丢失消息的。 对于单个节点的 Broker，需要配置 Broker 参数，在收到消息后，将消息写入磁盘后再给 Producer 返回确认响应，这样即使发生宕机，由于消息已经被写入磁盘，就不会丢失消息，恢复后还可以继续消费。例如，在 RocketMQ 中，需要将刷盘方式 flushDiskType 配置为 SYNC_FLUSH 同步刷盘。 如果是 Broker 是由多个节点组成的集群，需要将 Broker 集群配置成：至少将消息发送到 2 个以上的节点，再给客户端回复发送确认响应。这样当某个 Broker 宕机时，其他的 Broker 可以替代宕机的 Broker，也不会发生消息丢失。 3.消费阶段 消费阶段采用和生产阶段类似的确认机制来保证消息的可靠传递，客户端从 Broker 拉取消息后，执行用户的消费业务逻辑，成功后，才会给 Broker 发送消费确认响应。如果 Broker 没有收到消费确认响应，下次拉消息的时候还会返回同一条消息，确保消息不会在网络传输过程中丢失，也不会因为客户端在执行消费逻辑中出错导致丢失。 你在写消费代码时需要注意的是，不要在收到消息后就立即发送消费确认，而是应该在执行完所有消费业务逻辑之后，再发送消费确认。 如何保证消费过程中的重复消息（幂等性）在消息传递过程中，如果出现传递失败的情况，发送方会执行重试，重试的过程中就有可能会产生重复的消息。对使用消息队列的业务系统来说，如果没有对重复消息进行处理，就有可能会导致系统的数据出现错误 消息重复的情况必然存在在 MQTT 协议中，给出了三种传递消息时能够提供的服务质量标准，这三种服务质量从低到高依次是： At most once: 至多一次。消息在传递时，最多会被送达一次。换一个说法就是，没什么消息可靠性保证，允许丢消息。一般都是一些对消息可靠性要求不太高的监控场景使用，比如每分钟上报一次机房温度数据，可以接受数据少量丢失。 At least once: 至少一次。消息在传递时，至少会被送达一次。也就是说，不允许丢消息，但是允许有少量重复消息出现。 Exactly once：恰好一次。消息在传递时，只会被送达一次，不允许丢失也不允许重复，这个是最高的等级。 这个服务质量标准不仅适用于 MQTT，对所有的消息队列都是适用的。我们现在常用的绝大部分消息队列提供的服务质量都是 At least once，包括 RocketMQ、RabbitMQ 和 Kafka 都是这样。也就是说，消息队列很难保证消息不重复。 用幂等性解决重复消息问题一个幂等操作的特点是，其任意多次执行所产生的影响均与一次执行的影响相同。 利用数据库的唯一约束实现幂等例如我们刚刚提到的那个不具备幂等特性的转账的例子：将账户 X 的余额加 100 元。在这个例子中，我们可以通过改造业务逻辑，让它具备幂等性。 首先，我们可以限定，对于每个转账单每个账户只可以执行一次变更操作，在分布式系统中，这个限制实现的方法非常多，最简单的是我们在数据库中建一张转账流水表，这个表有三个字段：转账单 ID、账户 ID 和变更金额，然后给转账单 ID 和账户 ID 这两个字段联合起来创建一个唯一约束，这样对于相同的转账单 ID 和账户 ID，表里至多只能存在一条记录。 基于这个思路，不光是可以使用关系型数据库，只要是支持类似“INSERT IF NOT EXIST”语义的存储类系统都可以用于实现幂等，比如，你可以用 Redis 的 SETNX 命令来替代数据库中的唯一约束，来实现幂等消费。 为更新的数据设置前置条件给数据变更设置一个前置条件，如果满足条件就更新数据，否则拒绝更新数据，在更新数据的时候，同时变更前置条件中需要判断的数据。这样，重复执行这个操作时，由于第一次更新数据的时候已经变更了前置条件中需要判断的数据，不满足前置条件，则不会重复执行更新数据操作。 比如，刚刚我们说过，“将账户 X 的余额增加 100 元”这个操作并不满足幂等性，我们可以把这个操作加上一个前置条件，变为：“如果账户 X 当前的余额为 500 元，将余额加 100 元”，这个操作就具备了幂等性。对应到消息队列中的使用时，可以在发消息时在消息体中带上当前的余额，在消费的时候进行判断数据库中，当前余额是否与消息中的余额相等，只有相等才执行变更操作。 更加通用的方法是，给你的数据增加一个版本号属性，每次更数据前，比较当前数据的版本号是否和消息中的版本号一致，如果不一致就拒绝更新数据，更新数据的同时将版本号 +1，一样可以实现幂等更新 记录并检查操作我们还有一种通用性最强，适用范围最广的实现幂等性方法：记录并检查操作，也称为“Token 机制或者 GUID（全局唯一 ID）机制”，实现的思路特别简单：在执行数据更新操作之前，先检查一下是否执行过这个更新操作。 具体的实现方法是，在发送消息时，给每条消息指定一个全局唯一的 ID，消费时，先根据这个 ID 检查这条消息是否有被消费过，如果没有消费过，才更新数据，然后将消费状态置为已消费。 更加麻烦的是，在“检查消费状态，然后更新数据并且设置消费状态”中，三个操作必须作为一组操作保证原子性，才能真正实现幂等，否则就会出现 Bug 比如说，对于同一条消息：“全局 ID 为 8，操作为：给 ID 为 666 账户增加 100 元”，有可能出现这样的情况： t0 时刻：Consumer A 收到条消息，检查消息执行状态，发现消息未处理过，开始执行“账户增加 100 元”； t1 时刻：Consumer B 收到条消息，检查消息执行状态，发现消息未处理过，因为这个时刻，Consumer A 还未来得及更新消息执行状态。 这样就会导致账户被错误地增加了两次 100 元，这是一个在分布式系统中非常容易犯的错误，一定要引以为戒。 对于这个问题，当然我们可以用事务来实现，也可以用锁来实现，但是在分布式系统中，无论是分布式事务还是分布式锁都是比较难解决问题。 消息积压了该怎么办优化性能来避免消息积压发送端性能优化优化消息收发性能，预防消息积压的方法有两种，增加批量或者是增加并发，在发送端这两种方法都可以使用。 Producer 发送消息的过程，Producer 发消息给 Broker，Broker 收到消息后返回确认响应，这是一次完整的交互。假设这一次交互的平均时延是 1ms，我们把这 1ms 的时间分解开，它包括了下面这些步骤的耗时： 发送端准备数据、序列化消息、构造请求等逻辑的时间，也就是发送端在发送网络请求之前的耗时； 发送消息和返回响应在网络传输中的耗时； Broker 处理消息的时延 无论是增加每次发送消息的批量大小，还是增加并发，都能成倍地提升发送性能 比如说，你的消息发送端是一个微服务，主要接受 RPC 请求处理在线业务。很自然的，微服务在处理每次请求的时候，就在当前线程直接发送消息就可以了，因为所有 RPC 框架都是多线程支持多并发的，自然也就实现了并行发送消息。并且在线业务比较在意的是请求响应时延，选择批量发送必然会影响 RPC 服务的时延。这种情况，比较明智的方式就是通过并发来提升发送性能。 如果你的系统是一个离线分析系统，离线系统在性能上的需求是什么呢？它不关心时延，更注重整个系统的吞吐量。发送端的数据都是来自于数据库，这种情况就更适合批量发送，你可以批量从数据库读取数据，然后批量来发送消息，同样用少量的并发就可以获得非常高的吞吐量。 消费端性能优化使用消息队列的时候，大部分的性能问题都出现在消费端，如果消费的速度跟不上发送端生产消息的速度，就会造成消息积压。如果这种性能倒挂的问题只是暂时的，那问题不大，只要消费端的性能恢复之后，超过发送端的性能，那积压的消息是可以逐渐被消化掉的。 我们在设计系统的时候，一定要保证消费端的消费性能要高于生产端的发送性能，这样的系统才能健康的持续运行。 消费端的性能优化除了优化消费业务逻辑以外，也可以通过水平扩容，增加消费端的并发数来提升总体的消费性能。特别需要注意的一点是，在扩容 Consumer 的实例数量的同时，必须同步扩容主题中的分区（也叫队列）数量，确保 Consumer 的实例数和分区数量是相等的。如果 Consumer 的实例数量超过分区数量，这样的扩容实际上是没有效果的。原因我们之前讲过，因为对于消费者来说，在每个分区上实际上只能支持单线程消费 消息积压了该如何处理？能导致积压突然增加，最粗粒度的原因，只有两种：要么是发送变快了，要么是消费变慢了。 大部分消息队列都内置了监控的功能，只要通过监控数据，很容易确定是哪种原因。如果是单位时间发送的消息增多，比如说是赶上大促或者抢购，短时间内不太可能优化消费端的代码来提升消费性能，唯一的方法是通过扩容消费端的实例数来提升总体的消费能力。 如果短时间内没有足够的服务器资源进行扩容，没办法的办法是，将系统降级，通过关闭一些不重要的业务，减少发送方发送的数据量，最低限度让系统还能正常运转，服务一些重要业务。 还有一种不太常见的情况，你通过监控发现，无论是发送消息的速度还是消费消息的速度和原来都没什么变化，这时候你需要检查一下你的消费端，是不是消费失败导致的一条消息反复消费这种情况比较多，这种情况也会拖慢整个系统的消费速度。 总结","link":"/2021/05/11/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%9F%BA%E7%A1%80%E9%97%AE%E9%A2%98/"},{"title":"effectiveJava&lt;类与接口&gt;","text":"本章主要讲述的是关于类和接口的几个设计原则： 设计接口的时候预留后面的变动 只能用接口来定义类型，不能定义常量 类的层次结构优先于标签类 静态成员类优于非静态成员类 限制源文件为单个顶级类 Item21 设计接口的时候预留后面的变动为后面的变动来设计接口 在Java8之前，是不可能在已经存在实现类的接口中添加方法的，因为一旦在接口中添加的话，所有的实现类都要进行修改。 在Java8之后，接口中引入了默认方法构造，以此来给存在实现类的接口中添加方法，但是这还是比较有风险的。 123456789101112131415161718192021222324252627282930313233public interface UserService { //1. addUser 和 deleteUser 方法所有UserService的实现类都要重写 int addUser(); void deleteUser(); //2. updateUser 属于默认方法，实现类可以选择重写，也可以选择不重写 // 有一个问题就是无法保证 default void updateUser(){ //提供一些默认的方法实现 }}public class UserServiceImpl implements UserService{ @Override public int addUser() { return 0; } @Override public void deleteUser() { }}public static void main(String[] args) { UserServiceImpl userService = new UserServiceImpl(); // UserServiceImpl 这个实现类并没有实现 userService 接口的 updateUser 方法，但是也可以调用 // 这里就存在一个问题： 默认的方法实现不一定保证在所有的实现类中都是有效的！ userService.updateUser();// } 默认的方法实现便利了lambda表达式，但不一定保证在所有的实现类中都是有效的！ 1234567891011121314//java.util.Collection#removeIf //Collection 接口中的默认方法default boolean removeIf(Predicate&lt;? super E&gt; filter) { Objects.requireNonNull(filter); boolean removed = false; final Iterator&lt;E&gt; each = iterator(); //1. 用迭代器遍历集合 while (each.hasNext()) { if (filter.test(each.next())) { //2. 用断言来确定是否要移除该元素 each.remove(); removed = true; } } return removed; } 这个默认实现已经是 最好的通用方式了，但是在下面的包中的某些实现类来说并不能正常工作 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-collections4&lt;/artifactId&gt; &lt;version&gt;4.4&lt;/version&gt; &lt;/dependency&gt; 123456789101112131415161718//org.apache.commons.collections4.collection.SynchronizedCollection //所有的方法在委托给包装的集合之前，都先在锁定对象上进行同步。public class SynchronizedCollection&lt;E&gt; implements Collection&lt;E&gt;, Serializable { private static final long serialVersionUID = 2412805092710877986L; private final Collection&lt;E&gt; collection; protected final Object lock; //Apache的版本添加了一个可以使用客户端提供的对象来锁定的功能 //... //已经覆盖了这个方法，如果没有覆盖的话就会调用默认的方法实现（接口里面的），这样的话就对锁object的情况不知道，。 //如果调用了一个SynchronizedCollection实例的removeIf方法，而另一个线程又并发地修改了这个集合，就会导致ConcurrentModificationException或者其他异常行为 public boolean removeIf(Predicate&lt;? super E&gt; filter) { synchronized(this.lock) { return this.decorated().removeIf(filter); } } } 为了防止类似的事情出现在Java平台类库的实现里，比如Collections.synchronizedCollection返回的包级私有类里，JDK的维护者必须要覆盖默认的removeIf方法，以及其他的类似需要在调用默认方法前执行必要同步的方法。而不属于Java平台的已经存在的类就没有机会和接口做同步的修改，有些现在已经修改了。 结论： 除非必要，否则应该避免使用默认方法来给已经存在的接口添加新的方法。在添加的时候，也必须认真努力思考，是否有已存在的接口实现可能会被新增的默认方法实现破坏。 Item22 只能用接口来定义类型，不能定义常量只用接口来定义类型，而不能定义常量 12345// 常量接口，是明令禁止的。public interface PhysicalConstants { static final double AVOGADROS_NUMBER = 6.022; static final double ELECTRON_MASS = 6.022;} 如果确实要定义某些常量，一般有以下三种方式： 如果常量只和某些类或接口有关系，就定义到类或接口中。比如Integer中就封装了MIN_VALUE和MAX_VALUE 如果这些常量可以很好地看做是枚举类型的成员，那就应该用枚举类型（Item34)来导出它们。 用一个不可实例化的工具类（Item4）来导出这些常量 12345678910public class PhysicalConstants { private PhysicalConstants() { } // Prevents instantiation public static final double AVOGADROS_NUMBER = 6.022_140_857e23; public static final double BOLTZMANN_CONST =1.380_648_52e-23; public static final double ELECTRON_MASS = 9.109_383_56e-31;}// 通过 类名.常量名 来访问这个常量 double avogadrosNumber = PhysicalConstants.AVOGADROS_NUMBER; Item23 类的层次结构优先于标签类类的层次性要优于标签类 Item24 静态成员类优于非静态成员类嵌套类就是定义在其他类里面的类。嵌套类存在的目的只是为其外围类提供服务。 内部类分为以下四种：静态成员类、非静态成员类、匿名类、和局部类 静态成员类: 静态成员类是最简单的嵌套类。最好是把它看做普通的类，只是碰巧声明在了另一个类内部，同时可以访问外围类的所有成员，包括哪些声明为私有的成员。静态成员类也是其外围类的一个静态成员，和其他的静态成员遵守相同的访问规则。如果它被声明为私有的，那么他就只能被外围类访问，等等。 一个静态成员类的常见用法是作为公有的辅助类，只有和其外围类一起合作才有用。 1234567891011121314151617181920212223242526272829303132333435363738//String的compareToIgnoreCase方法public int compareToIgnoreCase(String str) { return CASE_INSENSITIVE_ORDER.compare(this, str);} // 外围类这里做了一个封装。public static final Comparator&lt;String&gt; CASE_INSENSITIVE_ORDER = new CaseInsensitiveComparator(); // 这个私有静态内部类的功能只有一个，就是提供一个忽略大小写的字符串比较的功能 // 并不需要使用到外围类String的属性或者方法。所以设计为静态成员类 private static class CaseInsensitiveComparator implements Comparator&lt;String&gt;, java.io.Serializable { // use serialVersionUID from JDK 1.2.2 for interoperability private static final long serialVersionUID = 8575799808933029326L; public int compare(String s1, String s2) { int n1 = s1.length(); int n2 = s2.length(); int min = Math.min(n1, n2); for (int i = 0; i &lt; min; i++) { char c1 = s1.charAt(i); char c2 = s2.charAt(i); if (c1 != c2) { c1 = Character.toUpperCase(c1); c2 = Character.toUpperCase(c2); if (c1 != c2) { c1 = Character.toLowerCase(c1); c2 = Character.toLowerCase(c2); if (c1 != c2) { // No overflow because of numeric promotion return c1 - c2; } } } } return n1 - n2; } 非静态成员类: 从语法上，静态和非静态成员类之间的区别只在于其声明中是否有static修饰符。 但是这两种内部类是非常不同的。每一个非静态内部类都隐式地和一个指向外围类实例相关联。在非静态成员类的实例方法中，你可以使用修饰过的this来获得外围实例的引用，从而调用外围实例的方法。如果一个嵌套类的实例，可以与其外部类实例分开而独立存在，那么这个嵌套类就必须是静态成员类。在不存在外围类实例的情况下，无法创建非静态成员类的实例。 当这个非静态成员类的实例被创建的时候，该实例和其外部类实例之间的关联就建立起来了，并且，这种关联关系不能被修改了。通常情况下，这种关联是通过外围类的实例方法调用非静态成员类的构造器，自动创建的。也可以通过表达式“enclosingInstance.new MemberClass(args)”手动地创建这中关系，但实际中很少这么做。正如你所想的那样，这种关联会占用非静态成员类实例的空间，并且会增加其构造的时间。 非静态成员类的一个常见用法是定义一个Adapter。允许一个外围类的实例被看做是一些不相关的实例类的实例。比如Set和List，通常也使用非静态成员类来实现他们的迭代器。如下： 如果内嵌类的实例需要用到外围类实例的属性或方法的时候，使用非静态成员类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445//ArrayList的迭代器方法——iterator() // 使用到了外围类的属性和方法，符合使用非静态成员类的条件 ,所以此处不应该加static来修饰 // 没有ArrayList对象，独立于ArrayList对象的new Itr()没有意义 private class Itr implements Iterator&lt;E&gt; { int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such int expectedModCount = modCount; // modCount 为外围类的属性 Itr() {} public boolean hasNext() { return cursor != size; } @SuppressWarnings(&quot;unchecked&quot;) public E next() { checkForComodification(); int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i]; } public void remove() { if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try { ArrayList.this.remove(lastRet); // 这里的this也是外围类的实例 cursor = lastRet; lastRet = -1; expectedModCount = modCount; } catch (IndexOutOfBoundsException ex) { throw new ConcurrentModificationException(); } } //。。。 } 如果你声明的成员类不需要访问其外围类实例，那么就应该在声明中添加static修饰符，让这个成员类是静态的而不是非静态的。如果你省略了这个修饰符，每一个内部类实例都会有一个额外的隐藏的指向外部类实例的引用。正如前面提到的那样，保存这个引用会花费时间和空间。更严重地是，它可能导致一个本该被垃圾回收的外部类实例被保留下来，这个问题导致的内存泄露问题是灾难性的。但是却难以发现，因为这个引用时不可见的。 私有静态成员类的常见的用法是代表外围类代表的对象组件。以Map实例为例，它把键值关联起来，在很多的Map的实现中，都有一个内部Entry对象来表示map中的每一个键值对。虽然每一个entry都和map相关联，但是其方法（getKey, getValue, 和setValue）都不需要访问map。因此，使用非静态成员类来表示entry是比较浪费的，私有静态成员类就是最好的选择。如果你不小心漏掉了entry声明前的stati修饰符，这个map也能工作，但是它的每一个entry都包含一个指向map的多余的引用，既浪费时间又浪费空间。 匿名类: 匿名类是没有名字的。它也不是其外围类的一个成员。在使用的时候同时声明和实例化，而不是和其他成员一起被声明。匿名类可以在代码中任何地方出现，只要其表达式是正确的。在lambda被加入到Java之前（Chapter6），匿名类是用来创建小的函数对象和过程对象的最佳方法，但是现在lambda表达式更受欢迎了（Item42）。另外一个匿名类的常见用法是在静态工厂方法的实现中（见Item20里的intArrayAsList）。 局部类: 局部类在四种嵌套类中使用频率最低。一个局部类几乎可以在任何本地变量可以声明的地方进行声明，并且遵守和本地变量相同的作用域规则。局部类和其他嵌套类有都一点共同的属性。和成员类一样，局部类有名字，也可以被重复使用；和匿名类样，局部类也只有定义在非静态环境中才拥有外围实例对象，也不能包含静态成员；和匿名类一样，局部类为了不破坏代码可读性，也应该很短。 Item25 限制源文件为单个顶级类永远都不要在一个源文件里写多个顶级类或者接口","link":"/2021/07/27/effectiveJava-%E7%B1%BB%E4%B8%8E%E6%8E%A5%E5%8F%A3/"},{"title":"算法题(一)链表","text":"链表专题 反转链表问题1. 反转单链表https://leetcode-cn.com/problems/reverse-linked-list/ 123456789101112131415class Solution { public ListNode reverseList(ListNode head) { if(head == null) return head; ListNode pre = null; ListNode cur = head; while(cur != null){ ListNode help = cur.next; // pre cur help cur.next = pre; pre = cur; cur = help; } return pre; }} 2. k个一组反转单链表https://leetcode-cn.com/problems/reverse-nodes-in-k-group/ 1234567891011121314151617181920212223242526272829class Solution { public ListNode reverseKGroup(ListNode head, int k) { if(head == null || head.next == null) return head; ListNode cur = head; for(int i = 0 ; i &lt; k ; i++){ if(cur == null) return head; cur = cur.next; } ListNode newHead = reverse(head , cur); head.next = reverseKGroup(cur , k); return newHead; } ListNode reverse(ListNode head , ListNode end){ ListNode cur = head; ListNode pre = null; while(cur != end){ ListNode help = cur.next; //pre cur help cur.next = pre; pre = cur; cur = help; } return pre; }} 3. 反转left到right的单链表https://leetcode-cn.com/problems/reverse-linked-list-ii/ 123456789101112131415161718192021222324252627282930313233class Solution { public ListNode reverseBetween(ListNode head, int l, int r) { if(head == null || head.next == null) return head; ListNode dummy = new ListNode(-1); dummy.next = head; //1. 此时 pre 指向的是第l个节点的前一个节点 ListNode pre = dummy; for(int i = 1 ; i &lt; l ; i++) pre = pre.next; ListNode cur = pre.next; //2. 开始一个个的翻转 //思路：head表示需要反转的头节点，pre表示需要反转头节点的前驱节点 //我们需要反转n-m次，我们将head的next节点移动到需要反转链表部分的首部，需要反转链表部分剩余节点依旧保持相对顺序即可 //比如1-&gt;2-&gt;3-&gt;4-&gt;5,m=1,n=5 //第一次反转：1(head) 2(next) 3 4 5 反转为 2 1 3 4 5 //第二次反转：2 1(head) 3(next) 4 5 反转为 3 2 1 4 5 //第三次发转：3 2 1(head) 4(next) 5 反转为 4 3 2 1 5 //第四次反转：4 3 2 1(head) 5(next) 反转为 5 4 3 2 1 for(int i = l ; i &lt; r ; i++){ ListNode a = cur.next; // pre cur a cur.next = a.next; a.next = pre.next; pre.next = a; } return dummy.next; }} 4. 两两交换链表的节点https://leetcode-cn.com/problems/swap-nodes-in-pairs/ 123456789101112131415161718192021222324252627282930class Solution { public ListNode swapPairs(ListNode head) { //1. 数据校验 if(head == null || head.next == null) return head; //2. 定义一个虚拟头结点，使其next指针指向head ListNode dummy = new ListNode(0); dummy.next = head; //3. 开始翻转 ListNode cur = dummy; //因为要保证后面还有2个结点可以交换，所以后面两个结点不为空 while(cur.next != null &amp;&amp; cur.next.next != null){ //定义两个指针保存位置 ListNode a = cur.next; ListNode b = cur.next.next; //开始交换 // cur a b ListNode c = b.next; cur.next = b; b.next = a; a.next = c; cur = a; } return dummy.next; }} 5. 判断是不是回文链表https://leetcode-cn.com/problems/palindrome-linked-list/ 123456789101112131415161718192021222324252627282930313233343536373839class Solution { public boolean isPalindrome(ListNode head) { if(head == null || head.next == null) return true; //快慢指针找中点 ListNode fast = head; ListNode slow = head; while(fast.next != null &amp;&amp; fast.next.next != null){ fast = fast.next.next; slow = slow.next; } //slow 为中点 ListNode a = slow.next; slow.next = null; ListNode newHead = reverse(a); ListNode cur = head; while(cur != null &amp;&amp; newHead != null){ if(cur.val != newHead.val) return false; cur =cur.next; newHead = newHead.next; } return true; } ListNode reverse(ListNode head){ ListNode pre = null; ListNode cur = head; while(cur != null){ ListNode help = cur.next; cur.next = pre; pre = cur; cur = help; } return pre; }} 6. 重排链表https://leetcode-cn.com/problems/reorder-list/ 12345678910111213141516171819202122232425262728293031323334353637class Solution { public void reorderList(ListNode head) { if(head == null) return; ListNode fast = head , slow = head; while(fast.next != null &amp;&amp; fast.next.next != null){ fast = fast.next.next; slow = slow.next; } ListNode a = reverse(slow.next); slow.next = null; ListNode b = head; while(a != null &amp;&amp; b != null){ ListNode c = a.next; ListNode d = b.next; //a c //b d b.next = a; a.next = d; a = c; b = d; } } ListNode reverse(ListNode head){ ListNode pre = null; ListNode cur = head; while(cur != null){ ListNode help = cur.next; cur.next = pre; pre = cur; cur = help; } return pre; }} 7. 奇偶链表https://leetcode-cn.com/problems/odd-even-linked-list/ 123456789101112131415161718192021222324class Solution { public ListNode oddEvenList(ListNode head) { if(head == null || head.next == null) return head; ListNode o1 = head; ListNode e1 = head.next; ListNode a = head; ListNode b = head.next; while(a.next != null &amp;&amp; b.next != null){ ListNode c = b.next; ListNode d = c.next; // a b c d a.next = c; b.next = d; a = c; b = d; } a.next = e1; return head; }} 8. 两个链表的第一个公共节点https://leetcode-cn.com/problems/liang-ge-lian-biao-de-di-yi-ge-gong-gong-jie-dian-lcof/ 1234567891011public class Solution { public ListNode getIntersectionNode(ListNode headA, ListNode headB) { if(headA == null || headB == null) return null; ListNode l1 = headA , l2 = headB; while(l1 != l2){ l1 = l1 == null ? headB : l1.next; l2 = l2 == null ? headA : l2.next; } return l1; }} 9. 旋转链表https://leetcode-cn.com/problems/rotate-list/ 12345678910111213141516171819202122232425class Solution { public ListNode rotateRight(ListNode head, int k) { if(head == null || k == 0) return head; //1. 求出链表长度 int len = 1; ListNode cur = head; while(cur.next != null){ cur = cur.next; len++; } //2. 首尾相连，并找出x cur.next = head; k = k % len; int x = len - k; //3. 找到第x个节点，后面那个就是要返回的头结点 while(x &gt; 0){ cur = cur.next; x--; } ListNode res = cur.next; cur.next = null; return res; }} 合并排序链表10. 合并两个排序单链表https://leetcode-cn.com/problems/merge-two-sorted-lists/ 1234567891011121314151617181920212223class Solution { public ListNode mergeTwoLists(ListNode l1, ListNode l2) { if(l1 == null || l2 == null) return l1 == null ? l2 : l1; ListNode dummy = new ListNode(-1); ListNode cur = dummy; while(l1 != null &amp;&amp; l2 != null){ if(l1.val &lt; l2.val){ cur.next = l1; cur = cur.next; l1 = l1.next; }else{ cur.next = l2; cur = cur.next; l2 = l2.next; } } if(l1 == null) cur.next = l2; if(l2 == null) cur.next = l1; return dummy.next; }} 11. 合并k个排序单链表https://leetcode-cn.com/problems/merge-k-sorted-lists/ 123456789101112131415161718192021class Solution { public ListNode mergeKLists(ListNode[] lists) { //1. 建立小根堆 Queue&lt;ListNode&gt; heap = new PriorityQueue&lt;&gt;((a , b) -&gt; (a.val - b.val)); //2. 将所有头结点放入堆中，这样堆顶就是最小的结点 for(ListNode node : lists){ if(node != null) heap.add(node); } //3. 建立虚拟结点 ListNode dummy = new ListNode(-1); ListNode cur = dummy; //4. 一直弹出堆顶的结点，放入cur的后面，并将弹出的结点的下一个结点再压入堆中 while(!heap.isEmpty()){ ListNode minNode = heap.poll(); cur.next = minNode; cur = cur.next; if(minNode.next != null) heap.add(minNode.next); } return dummy.next; }} 12. 分割链表https://leetcode-cn.com/problems/partition-list/ 12345678910111213141516171819202122232425class Solution { public ListNode partition(ListNode head, int x) { if(head == null) return head; ListNode dummy1 = new ListNode(-1); ListNode a = dummy1; ListNode dummy2 = new ListNode(-1); ListNode b = dummy2; ListNode cur = head; while(cur != null){ if(cur.val &lt; x){ a.next = cur; cur = cur.next; a = a.next; }else{ b.next = cur; cur = cur.next; b = b.next; } } a.next = dummy2.next; b.next = null; return dummy1.next; }} 快慢指针找倒数第n个节点13.删除链表的倒数第 n 个结点https://leetcode-cn.com/problems/remove-nth-node-from-end-of-list/】 123456789101112131415161718192021class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(-1); dummy.next = head; ListNode fast = dummy; ListNode slow = dummy; while(n &gt; 0){ fast = fast.next; n--; } while(fast.next != null){ fast = fast.next; slow = slow.next; } slow.next = slow.next.next; return dummy.next; }} 14.找链表的中间节点-如果有两个中间结点，则返回第二个中间结点https://leetcode-cn.com/problems/middle-of-the-linked-list/ 1234567891011121314class Solution { //如果有两个中间结点，则返回第二个中间结点。 public ListNode middleNode(ListNode head) { if(head == null) return head; ListNode fast = head; ListNode slow = head; while(fast != null &amp;&amp; fast.next != null){ fast = fast.next.next; slow = slow.next; } return slow; }} 链表排序15. 链表归并排序https://leetcode-cn.com/problems/sort-list/ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution { public ListNode sortList(ListNode head) { if(head == null) return head; return mergeSort(head); } ListNode mergeSort(ListNode head){ //1. 递归终止条件 if(head == null || head.next == null) return head; //2. 快慢指针找中点 ListNode fast = head; ListNode slow = head; while(fast.next != null &amp;&amp; fast.next.next != null){ fast = fast.next.next; slow = slow.next; } //3. 递归排序右边 ListNode r = mergeSort(slow.next); slow.next = null; //4. 递归排序左边 ListNode l = mergeSort(head); //5. 合并两个链表 return merge(l,r); } ListNode merge(ListNode l , ListNode r){ ListNode dummy = new ListNode(-1); ListNode cur = dummy; while(l != null &amp;&amp; r != null){ if(l.val &lt;= r.val){ cur.next = l; l = l.next; cur = cur.next; }else{ cur.next = r; r = r.next; cur = cur.next; } } cur.next = l == null ? r : l; return dummy.next; }} 16. 链表插入排序https://leetcode-cn.com/problems/insertion-sort-list/ 12345678910111213141516171819202122232425class Solution { public ListNode insertionSortList(ListNode head) { ListNode dummy = new ListNode(0), pre; dummy.next = head; while(head != null &amp;&amp; head.next != null) { //需要一个指针指向当前已排序的最后一个位置，这里用的是head指针 if(head.val &lt;= head.next.val) { head = head.next; continue; } pre = dummy; //需要另外一个指针pre,每次从表头循环，这里用的是dummy表头指针。 //每次拿出未排序的节点，先和前驱比较，如果大于或者等于前驱，就不用排序了，直接进入下一次循环 //如果前驱小，则进入内层循环，依次和pre指针比较，插入对应位置即可。 while (pre.next.val &lt; head.next.val) pre = pre.next; ListNode curr = head.next; head.next = curr.next; curr.next = pre.next; pre.next = curr; } return dummy.next; }} 删除链表中的重复元素17. 只删除一个重复元素https://leetcode-cn.com/problems/remove-duplicates-from-sorted-list/ 123456789101112class Solution { public ListNode deleteDuplicates(ListNode head) { if(head == null || head.next == null) return head; ListNode cur = head; while(cur.next != null){ if(cur.val == cur.next.val) cur.next = cur.next.next; else cur = cur.next; } return head; }} 18. 删除全部的重复元素https://leetcode-cn.com/problems/remove-duplicates-from-sorted-list-ii/ 12345678910111213141516171819class Solution { public ListNode deleteDuplicates(ListNode head) { ListNode dummy = new ListNode(-1); dummy.next = head; ListNode fast = head; ListNode slow = dummy; while(fast != null){ if(fast.next != null &amp;&amp; fast.val == fast.next.val){ while(fast.next != null &amp;&amp; fast.val == fast.next.val ) fast = fast.next; slow.next = fast.next; fast = fast.next; }else{ fast = fast.next; slow = slow.next; } } return dummy.next; }} 复制链表问题19. 复制带随机指针的链表https://leetcode-cn.com/problems/copy-list-with-random-pointer/ 1234567891011121314151617181920212223242526272829303132class Solution { public Node copyRandomList(Node head) { if(head == null) return head; //1. 复制每个节点 Node cur = head; while(cur != null){ Node tem = new Node(cur.val); tem.next = cur.next; cur.next = tem; cur = cur.next.next; } //2. 复制随机节点 cur = head; while(cur != null){ if(cur.random != null) cur.next.random = cur.random.next; cur = cur.next.next; } //3. 分离奇偶链表 Node res = head.next; cur = head; while(cur.next != null){ // cur help Node help = cur.next; cur.next = help.next; cur = help; } return res; }} 20.分割等长链表https://leetcode-cn.com/problems/split-linked-list-in-parts/ 1234567891011121314151617181920212223242526class Solution { public ListNode[] splitListToParts(ListNode root, int k) { //1. 获取链表长度 int n = 0; ListNode cur = root; while(cur != null){ cur = cur.next; n++; } int mod = n % k ; //有几个链表长度为size+1的 int size = n / k; //每个链表长度为size //2.开始分割 ListNode[] res = new ListNode[k]; cur = root; for(int i = 0 ; i &lt; k &amp;&amp; cur != null ; i++){ res[i] = cur; int curSize = size + (mod-- &gt; 0 ? 1 : 0); //当前链表的长度 for(int j = 0 ; j &lt; curSize-1 ; j++) cur = cur.next; ListNode tem = cur.next; cur.next = null; //置空 cur = tem; //下一个循环 } return res; }} 链表计算问题21.两数相加https://leetcode-cn.com/problems/add-two-numbers/ 1234567891011121314151617181920class Solution { public ListNode addTwoNumbers(ListNode l1, ListNode l2) { if(l1 == null || l2 == null) return l1 == null ? l2 : l1; ListNode dummy = new ListNode(-1); ListNode cur = dummy; int carry = 0; while(l1 != null || l2 != null){ int n1 = l1 == null ? 0 : l1.val; int n2 = l2 == null ? 0 : l2.val; int sum = n1 + n2 + carry; cur.next = new ListNode(sum % 10); cur = cur.next; carry = sum / 10; if(l1 != null) l1 = l1.next; if(l2 != null) l2 = l2.next; } if(carry == 1) cur.next = new ListNode(1); return dummy.next; }}","link":"/2021/09/29/%E7%AE%97%E6%B3%95%E9%A2%98-%E4%B8%80-%E9%93%BE%E8%A1%A8/"},{"title":"设计模式（一）面向对象","text":"本篇描述了： 面向对象 接口和抽象类 组合和继承 贫血模式的传统架构和充血模式的DDD 面向对象的四大特性封装 封装也叫作信息隐藏或者数据访问保护。类通过暴露有限的访问接口，授权外部仅能通过类提供的方式（或者叫函数）来访问内部信息或者数据。 举例验证：设计一个虚拟钱包 12345678910111213141516171819202122232425262728293031323334353637383940public class Wallet { private String id; //钱包的唯一编号 private long createTime; //创建时间 private BigDecimal balance; //余额 private long balanceLastModifiedTime; //上次余额修改的时间 // ... 省略其他属性... public Wallet() { this.id = IdGenerator.getInstance().generate(); this.createTime = System.currentTimeMillis(); this.balance = BigDecimal.ZERO; this.balanceLastModifiedTime = System.currentTimeMillis(); } // 注意：下面对 get 方法做了代码折叠，是为了减少代码所占文章的篇幅 public String getId() { return this.id; } public long getCreateTime() { return this.createTime; } public BigDecimal getBalance() { return this.balance; } public long getBalanceLastModifiedTime() { return this.balanceLastModifiedTime} public void increaseBalance(BigDecimal increasedAmount) { if (increasedAmount.compareTo(BigDecimal.ZERO) &lt; 0) { throw new InvalidAmountException(&quot;...&quot;); } this.balance.add(increasedAmount); this.balanceLastModifiedTime = System.currentTimeMillis(); } public void decreaseBalance(BigDecimal decreasedAmount) { if (decreasedAmount.compareTo(BigDecimal.ZERO) &lt; 0) { throw new InvalidAmountException(&quot;...&quot;); }if (decreasedAmount.compareTo(this.balance) &gt; 0) { throw new InsufficientAmountException(&quot;...&quot;); } this.balance.subtract(decreasedAmount); this.balanceLastModifiedTime = System.currentTimeMillis(); } } 细节总结 private 关键字修饰的属性只能类本身访问，这样外部就不能通过wallet.id来进行修改了 对于钱包来说，id和创建时间是不能修改的，就不会对外暴露set方法。 对于余额来说，只能增加或减少，不能重新设置，也不能对外暴露set方法 上次余额的修改时间这个属性 和余额增加/减少是捆绑的，也不对外暴露修改这个属性。 抽象 抽象讲的是如何隐藏方法的具体实现，让调用者只需要关心方法提供了哪些功能，并不需要知道这些功能是如何实现的。 12345678910111213public interface IPictureStorage { void savePicture(Picture picture); Image getPicture(String pictureId); void deletePicture(String pictureId); void modifyMetaInfo(String pictureId, PictureMetaInfo metaInfo); }public class PictureStorage implements IPictureStorage { // ... 省略其他属性... @Override public void savePicture(Picture picture) { ... } @Override public Image getPicture(String pictureId) { ... } @Override public void deletePicture(String pictureId) { ... } @Override public void modifyMetaInfo(String pictureId, PictureMetaInfo metaInfo) { ... }} 细节总结： 调用者在使用图片存储功能时，只需要了解相应的接口暴露了哪些功能即可，不需要去查看具体类中方法的具体实现。 抽象作为一种只关注功能点不关注实现的设计思路，正好帮我们的大脑过滤掉许多非必要的信息。 继承 继承是用来表示类之间的is-a 关系，比如猫是一种哺乳动物。从继承关系上来讲，继承可以分为两种模式，单继承和多继承。单继承表示一个子类只继承一个父类，多继承表示一个子类可以继承多个父类，比如猫既是哺乳动物，又是爬行动物。 继承最大的一个好处就是代码复用。假如两个类有一些相同的属性和方法，我们就可以将这些相同的部分，抽取到父类中，让两个子类继承父类。这样，两个子类就可以重用父类中的代码，避免代码重复写多遍。 继承来关联两个类，反应真实世界中的这种关系，非常符合人类的认知。 多态 多态是指，子类可以替换父类，在实际的代码运行过程中，调用子类的方法实现。 通过继承加方法重写： 父类对象可以引用子类对象 支持继承 子类要重写父类中的方法 通过接口和实现类来实现。 面向对象和面向过程的优势定义面向对象的定义： 面向对象编程是一种编程范式或编程风格。它以类或对象作为组织代码的基本单元，并将封装、抽象、继承、多态四个特性，作为代码设计和实现的基石 。 面向对象编程语言是支持类或对象的语法机制，并有现成的语法机制，能方便地实现面向对象编程四大特性（封装、抽象、继承、多态）的编程语言。 面向过程的定义： 面向过程编程也是一种编程范式或编程风格。它以过程（可以为理解方法、函数、操作）作为组织代码的基本单元，以数据（可以理解为成员变量、属性）与方法相分离为最主要的特点。面向过程风格是一种流程化的编程风格，通过拼接一组顺序执行的方法来操作数据完成一项功能。 举例：需求：假设我们有一个记录了用户信息的文本文件 users.txt，每行文本的格式是 name&amp;age&amp;gender（比如，小王 &amp;28&amp; 男）。我们希望写一个程序，从 users.txt 文件中逐行读取用户信息，然后格式化成 name\\tage\\tgender（其中，\\t 是分隔符）这种文本格式，并且按照 age 从小到达排序之后，重新写入到另一个文本文件 formatted_users.txt 中。 12345678910111213141516171819202122232425262728293031323334353637//面向过程：面向过程风格的代码被组织成了一组方法集合及其数据结构（struct User），方法和数据结构的定义是分开的struct User { char name[64]; int age; char gender[16]; };struct User parse_to_user(char* text) { // 将 text(“小王 &amp;28&amp; 男”) 解析成结构体 struct User }char* format_to_text(struct User user) { // 将结构体 struct User 格式化成文本（&quot; 小王\\t28\\t 男 &quot;） }void sort_users_by_age(struct User users[]) { // 按照年龄从小到大排序 users }void format_user_file(char* origin_file_path, char* new_file_path) { // open files... struct User users[1024]; // 假设最大 1024 个用户 int count = 0; while(1) { // read until the file is empty struct User user = parse_to_user(line); users[count++] = user; } sort_users_by_age(users); for (int i = 0; i &lt; count; ++i) { char* formatted_user_text = format_to_text(users[i]); // write to new file... } // close files... }int main(char** args, int argv) { format_user_file(&quot;/home/zheng/user.txt&quot;, &quot;/home/zheng/formatted_users.txt&quot;); } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//面向对象的思想： 方法和数据结构被绑定一起，定义在类中。public class User { private String name; private int age; private String gender; public User(String name, int age, String gender) { this.name = name; this.age = age; this.gender = gender; } public static User praseFrom(String userInfoText) { // 将 text(“小王 &amp;28&amp; 男”) 解析成类 User } public String formatToText() { // 将类 User 格式化成文本（&quot; 小王\\t28\\t 男 &quot;） } }public class UserFileFormatter { public void format(String userFile, String formattedUserFile) { // Open files... List users = new ArrayList&lt;&gt;(); while (1) { // read until file is empty // read from file into userText... User user = User.parseFrom(userText); users.add(user); } // sort users by age... for (int i = 0; i &lt; users.size(); ++i) { String formattedUserText = user.formatToText(); // write to new file... } // close files... } } public class MainApplication { public static void main(Sring[] args) { UserFileFormatter userFileFormatter = new UserFileFormatter(); userFileFormatter.format(&quot;/home/zheng/users.txt&quot;, &quot;/home/zheng/formatted_us } } 面向对象的优势 OOP更加能够应对大规模复杂程序的开发 对于大规模复杂程序的开发来说，整个程序的处理流程错综复杂，并非只有一条主线。如果把整个程序的处理流程画出来的话，会是一个网状结构。如果我们再用面向过程编程这种流程化、线性的思维方式，去翻译这个网状结构，去思考如何把程序拆解为一组顺序执行的方法，就会比较吃力。这个时候，面向对象的编程风格的优势就比较明显了。 面向对象编程是以类为思考对象。在进行面向对象编程的时候，我们并不是一上来就去思考，如何将复杂的流程拆解为一个一个方法，而是采用曲线救国的策略，先去思考如何给业务建模，如何将需求翻译为类，如何给类之间建立交互关系，而完成这些工作完全不需要考虑错综复杂的处理流程。当我们有了类的设计之后，然后再像搭积木一样，按照处理流程，将类组装起来形成整个程序。这种开发模式、思考问题的方式，能让我们在应对复杂程序开发的时候，思路更加清晰。 OOP 风格的代码更易复用、易扩展、易维护 多态特性。基于这个特性，我们在需要修改一个功能实现的时候，可以通过实现一个新的子类的方式，在子类中重写原来的功能逻辑，用子类替换父类。在实际的代码运行过程中，调用子类新的功能逻辑，而不是在原有代码上做修改。这就遵从了“对修改关闭、对扩展开放”的设计原则，提高代码的扩展性。除此之外，利用多态特性，不同的类对象可以传递给相同的方法，执行不同的代码逻辑，提高了代码的复用性。 OOP 语言更加人性化、更加高级、更加智能 接口和抽象类定义抽象类Logger 是一个记录日志的抽象类，FileLogger 和 MessageQueueLogger 继承 Logger，分别实现两种不同的日志记录方式：记录日志到文件中和记录日志到消息队列中。FileLogger 和MessageQueueLogger 两个子类复用了父类 Logger 中的 name、enabled、minPermittedLevel 属性和 log() 方法，但因为这两个子类写日志的方式不同，它们又各自重写了父类中的 doLog() 方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 // 抽象类public abstract class Logger { private String name; private boolean enabled; private Level minPermittedLevel; public Logger(String name, boolean enabled, Level minPermittedLevel) { this.name = name; this.enabled = enabled; this.minPermittedLevel = minPermittedLevel; } public void log(Level level, String message) { boolean loggable = enabled &amp;&amp; (minPermittedLevel.intValue() &lt;= level.intValue()); if (!loggable) return; doLog(level, message); } protected abstract void doLog(Level level, String message); } // 抽象类的子类：输出日志到文件public class FileLogger extends Logger { private Writer fileWriter; public FileLogger(String name, boolean enabled, Level minPermittedLevel, String filepath) { super(name, enabled, minPermittedLevel); this.fileWriter = new FileWriter(filepath); } @Override public void doLog(Level level, String mesage) { // 格式化 level 和 message, 输出到日志文件 fileWriter.write(...); } } // 抽象类的子类: 输出日志到消息中间件 (比如 kafka) public class MessageQueueLogger extends Logger { private MessageQueueClient msgQueueClient; public MessageQueueLogger(String name, boolean enabled, Level minPermittedLevel, MessageQueueClient msgQueueClient) { super(name, enabled, minPermittedLevel); this.msgQueueClient = msgQueueClient; } @Override protected void doLog(Level level, String mesage) { // 格式化 level 和 message, 输出到消息中间件 msgQueueClient.send(...); } } 抽象类的特性： 不允许被实例化，只能被继承。也就是说不能直接new一个抽象类出来 抽象类可以包含属性和方法。方法既可以包含代码实现也可以不包含代码实现。不包含代码实现的方法为抽象方法 子类继承抽象类，必须实现抽象类中的所有抽象方法 定义接口1234567891011121314151617181920212223242526272829303132333435//接口public interface Filter { void doFilter(RpcRequest req) throws RpcException;}// 接口实现类：鉴权过滤器public class AuthencationFilter implements Filter { @Override public void doFilter(RpcRequest req) throws RpcException { //... 鉴权逻辑.. }}// 接口实现类：限流过滤器public class RateLimitFilter implements Filter { @Override public void doFilter(RpcRequest req) throws RpcException { //... 限流逻辑... }}// 过滤器使用demopublic class Application { filters.add(new AuthencationFilter()); filters.add(new RateLimitFilter()); private List&lt;Filter&gt; filters = new ArrayList&lt;&gt;(); public void handleRpcRequest(RpcRequest req) { try { for (Filter filter : fitlers) { filter.doFilter(req); } } catch(RpcException e) { // ... 处理过滤结果... }// ... 省略其他处理逻辑... }} 接口特性： 接口不能包含属性（也就是成员变量）。 接口只能声明方法，方法不能包含代码实现。 类实现接口的时候，必须实现接口中声明的所有方法。 相对于抽象类的 is-a 关系来说，接口表示一种 has-a 关系 抽象类和接口存在的意义抽象类也是为代码复用而生的。多个子类可以继承抽象类中定义的属性和方法，避免在子类中，重复编写相同的代码。 应用场景 如果要表示一种is-a 的关系，并且是为了解决代码复用问题，我们就用抽象类； 如果要表示 一种 has-a 关系，并且是为了解决抽象而非代码复用问题，那我们就用接口。 基于接口而非实现编程 越抽象、越顶层、越脱离具体某一实现的设计，越能提高代码的灵活性，越能应对未来的需求变化。好的代码设计，不仅能应对当下的需求，而且在将来需求发生变化的时候，仍然能够在不破坏原有代码设计的情况下灵活应对。而抽象就是提高代码扩展性、灵活性、可维护性最有效的手段之一。 图片存储案例1234567891011121314151617181920212223242526272829303132333435363738public class AliyunImageStore { //... 省略属性、构造函数等... //创建存储目录 public void createBucketIfNotExisting(String bucketName) { // ... 创建 bucket 代码逻辑... // ... 失败会抛出异常.. } //生成访问凭证 public String generateAccessToken() { // ... 根据 accesskey/secrectkey 等生成 access token } //携带 token 和图片上传到指定目录中 public String uploadToAliyun(Image image, String bucketName, String accessToken){ //... 上传图片到阿里云... //... 返回图片存储在阿里云上的地址 (url）... } public Image downloadFromAliyun(String url, String accessToken) { //... 从阿里云下载图片... }}// AliyunImageStore 类的使用举例public class ImageProcessingJob { private static final String BUCKET_NAME = &quot;ai_images_bucket&quot;; //... 省略其他无关代码... public void process() { Image image = ...; // 处理图片，并封装为 Image 对象 AliyunImageStore imageStore = new AliyunImageStore(/* 省略参数 */); imageStore.createBucketIfNotExisting(BUCKET_NAME); String accessToken = imageStore.generateAccessToken(); imagestore.uploadToAliyun(image, BUCKET_NAME, accessToken); }} 业务需求发生了变化，图片不再存储在阿里云上，而是存储在私有云上，该怎么处理呢？ 这就要求我们必须将 AliyunImageStore 类中所定义的所有 public 方法，在 PrivateImageStore 类中都逐一定义并重新实现一遍。而这样做就会存在一些问题，我总结了下面两点： AliyunImageStore 类中有些函数命名暴露了实现细节，比如，uploadToAliyun()和 downloadFromAliyun()。 将图片存储到阿里云的流程，跟存储到私有云的流程，可能并不是完全一致的。比如，阿里云的图片上传和下载的过程中，需要生产 access token，而私有不需要 accesstoken。一方面，AliyunImageStore 中定义的 generateAccessToken() 方法不能照抄到PrivateImageStore 中；另一方面，我们在使用AliyunImageStore 上传、下载图片的时候，代码中用到了 generateAccessToken() 方法，如果要改为私有云的上传下载流程，这些代码都需要做调整。 解决思路： 函数的命名不能暴露任何实现细节。比如：upload()。 封装具体的实现细节。比如，跟阿里云相关的特殊上传（或下载）流程不应该暴露给调用者。我们对上传（或下载）流程进行封装，对外提供一个包裹所有上传（或下载）细节的方法，给调用者使用。 为实现类定义抽象的接口。具体的实现类都依赖统一的接口定义，遵从一致的上传功能协议。使用者依赖接口，而不是具体的实现类来编程 代码重构： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657//图片存储接口public interface ImageStore { String upload(Image image, String bucketName); Image download(String url);}public class AliyunImageStore implements ImageStore { //... 省略属性、构造函数等... public String upload(Image image, String bucketName) { createBucketIfNotExisting(bucketName); String accessToken = generateAccessToken(); //... 上传图片到阿里云... //... 返回图片在阿里云上的地址 (url)... } public Image download(String url){ String accessToken = generateAccessToken(); } private void createBucketIfNotExisting(String bucketName) { // ... 创建 bucket... // ... 失败会抛出异常.. } private String generateAccessToken() { // ... 根据 accesskey/secrectkey 等生成 access token }}// 上传下载流程改变：私有云不需要支持 access tokenpublic class PrivateImageStore implements ImageStore { public String upload(Image image, String bucketName) { createBucketIfNotExisting(bucketName); //... 上传图片到私有云... //... 返回图片的 url... } public Image download(String url) { //... 从私有云下载图片... } private void createBucketIfNotExisting(String bucketName) { // ... 创建 bucket... // ... 失败会抛出异常.. }}// ImageStore 的使用举例public class ImageProcessingJob { private static final String BUCKET_NAME = &quot;ai_images_bucket&quot;; //... 省略其他无关代码... public void process() { Image image = ...;// 处理图片，并封装为 Image 对象 ImageStore imageStore = new PrivateImageStore(...); imagestore.upload(image, BUCKET_NAME); }} 这条原则的设计初衷是，将接口和实现相分离，封装不稳定的实现，暴露稳定的接口。上游系统面向接口而非实现编程，不依赖不稳定的实现细节，这样当实现发生变化的时候，上游系统的代码基本上不需要做改动，以此来降低代码间的耦合性，提高代码的扩展性。 组合优于继承为什么不推荐使用继承虽然继承有诸多作用，但继承层次过深、过复杂，也会影响到代码的可维护性。 案例：设计一个鸟类： 123456789101112//抽象类鸟public class AbstractBird { //... 省略其他属性和方法... public void fly() { //... } }如果很多鸟不会飞怎么办呢？ 每种不会飞的鸟都要重写fly方法来抛出异常吗？如果再加一个是否会叫的抽象方法呢？如果再加一个是否会下蛋的抽象方法呢？ 继承关系变的越来越复杂了！！！ 组合的使用还是上面的例子： 1234567891011121314151617181920212223242526272829303132333435public interface Flyable { void fly(); }public interface Tweetable { void tweet(); }public interface EggLayable { void layEgg(); }public class Ostrich implements Tweetable, EggLayable {// 鸵鸟//... 省略其他属性和方法... @Override public void tweet() { //... } @Override public void layEgg() { //... } }public class Sparrow impelents Flayable, Tweetable, EggLayable {// 麻雀//... 省略其他属性和方法... @Override public void fly() { //... } @Override public void tweet() { //... } @Override public void layEgg() { //... }} 每个会下蛋的鸟都要实现一遍layEgg() 方法，并且实现逻辑是一样的，这就会导致代码重复的问题。那这个问题又该如何解决呢？ 我们可以针对三个接口再定义三个实现类，它们分别是：实现了 fly() 方法的 FlyAbility类、实现了 tweet() 方法的 TweetAbility 类、实现了 layEgg() 方法的 EggLayAbility 类。然后，通过组合和委托技术来消除代码重复： 12345678910111213141516171819202122232425public interface Flyable { void fly(); }public class FlyAbility implements Flyable { @Override public void fly() { //... } }// 省略 Tweetable/TweetAbility/EggLayable/EggLayAbilitypublic class Ostrich implements Tweetable, EggLayable {// 鸵鸟 private TweetAbility tweetAbility = new TweetAbility(); // 组合 private EggLayAbility eggLayAbility = new EggLayAbility(); // 组合 //... 省略其他属性和方法... @Override public void tweet() { tweetAbility.tweet(); // 委托 } @Override public void layEgg() { eggLayAbility.layEgg(); // 委托 } } 如何判断该用组合还是继承？如果类之间的继承结构稳定，层次比较浅，关系不复杂，我们就可以大胆地使用继承。反之，我们就尽量使用组合来替代继承。除此之外，还有一些设计模式、特殊的应用场景，会固定使用继承或者组合。 对于业务开发的MVC架构探讨贫血模型的传统架构MVC 三层架构中的 M 表示 Model，V 表示 View，C 表示 Controller。它将整个项目分为三层：展示层、逻辑层、数据层。 现在很多 Web 或者 App 项目都是前后端分离的，后端负责暴露接口给前端调用。这种情况下，我们一般就将后端项目分为 Repository 层、Service 层、Controller 层。其中，Repository 层负责数据访问，Service 层负责业务逻辑，Controller 层负责暴露接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748////////// Controller+VO(View Object) //////////public class UserController { private UserService userService; // 通过构造函数或者 IOC 框架注入 public UserVo getUserById(Long userId) { UserBo userBo = userService.getUserById(userId); UserVo userVo = [...convert userBo to userVo...]; return userVo; }}public class UserVo {// 省略其他属性、get/set/construct 方法 private Long id; private String name; private String cellphone;}////////// Service+BO(Business Object) //////////public class UserService { private UserRepository userRepository; // 通过构造函数或者 IOC 框架注入 public UserBo getUserById(Long userId) { UserEntity userEntity = userRepository.getUserById(userId); UserBo userBo = [...convert userEntity to userBo...]; return userBo; }}public class UserBo {// 省略其他属性、get/set/construct 方法 private Long id; private String name; private String cellphone;}////////// Repository+Entity //////////public class UserRepository { public UserEntity getUserById(Long userId) { //... }}public class UserEntity {// 省略其他属性、get/set/construct 方法 private Long id; private String name; private String cellphone;} 像 UserBo 这样，只包含数据，不包含业务逻辑的类，就叫作贫血模型（Anemic Domain Model）。同理，UserEntity、UserVo 都是基于贫血模型设计的。这种贫血模型将数据与操作分离，破坏了面向对象的封装特性，是一种典型的面向过程的编程风格。 充血模型的 DDD 开发模式？在贫血模型中，数据和业务逻辑被分割到不同的类中。 充血模型（Rich Domain Model）正好相反，数据和对应的业务逻辑被封装到同一个类中。因此，这种充血模型满足面向对象的封装特性，是典型的面向对象编程风格。 什么是DDD领域驱动设计，即 DDD，主要是用来指导如何解耦业务系统，划分业务模块，定义业务领域模型及其交互。 我们知道，除了监控、调用链追踪、API 网关等服务治理系统的开发之外，微服务还有另外一个更加重要的工作，那就是针对公司的业务，合理地做微服务拆分。而领域驱动设计恰好就是用来指导划分服务的。所以，微服务加速了领域驱动设计的盛行。 实际上，基于充血模型的 DDD 开发模式实现的代码，也是按照 MVC 三层架构分层的。Controller 层还是负责暴露接口，Repository 层还是负责数据存取，Service 层负责核心业务逻辑。它跟基于贫血模型的传统开发模式的区别主要在 Service 层。 在基于贫血模型的传统开发模式中，Service 层包含 Service 类和 BO 类两部分，BO 是贫血模型，只包含数据，不包含具体的业务逻辑。业务逻辑集中在 Service 类中。在基于充血模型的 DDD 开发模式中，Service 层包含 Service 类和 Domain 类两部分。Domain 就相当于贫血模型中的 BO。不过，Domain 与 BO 的区别在于它是基于充血模型开发的，既包含数据，也包含业务逻辑。而 Service 类变得非常单薄。总结一下的话就是，基于贫血模型的传统的开发模式，重 Service 轻 BO；基于充血模型的 DDD 开发模式，轻 Service 重Domain。","link":"/2021/06/28/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%B8%80%EF%BC%89%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"title":"算法题","text":"哈哈 第一部分：四种基本情况 1. 无重复数字的二分查找https://leetcode-cn.com/problems/binary-search/ 123456789101112class Solution { public int search(int[] nums, int target) { int n = nums.length; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r + 1 &gt;&gt; 1; if(nums[mid] &gt; target) r = mid-1; else l = mid; } return nums[l] == target ? l : -1; }} 2. 有重复数字的二分查找第一个位置和最后一个位置https://leetcode-cn.com/problems/find-first-and-last-position-of-element-in-sorted-array/ 12345678910111213141516171819202122232425262728class Solution { public int[] searchRange(int[] nums, int target) { int[] res = new int[2]; int n = nums.length; if(n == 0) return new int[]{-1,-1}; //找到第一个出现的位置 int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; if(nums[mid] &lt; target) l = mid+1; else r = mid; } if(nums[l] != target) return new int[]{-1,-1}; else res[0] = l; //找到最后一个出现的位置 l = 0; r = n-1; while(l &lt; r){ int mid = l + r +1 &gt;&gt; 1; if(nums[mid] &gt; target) r = mid-1; else l = mid; } res[1] = l; return res; }} 3. 搜索插入位置https://leetcode-cn.com/problems/search-insert-position/ 12345678910111213141516class Solution { public int searchInsert(int[] nums, int target) { int n = nums.length; //1. 注意如果最后一个数小于target的话，就返回数组长度 if (n == 0 || nums[n-1] &lt; target) return n; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; if(nums[mid] &lt; target) l = mid + 1; else r = mid; } return l; }} 4. x的平方根（只保留整数部分）12345678910111213 class Solution { public int mySqrt(int x) { int l = 0 , r = x; while(l &lt; r){ int mid = l + r + 1 &gt;&gt; 1; if(mid &gt; x / mid) r = mid-1; else l = mid; } return l; }} 5. 寻找重复的数https://leetcode-cn.com/problems/find-the-duplicate-number/ 123456789101112131415161718192021class Solution { public int findDuplicate(int[] nums) { int n = nums.length; //对值的范围进而二分 int l = 0 , r = n-1; while (l &lt; r){ int mid = l+r &gt;&gt;1; //看一下数组中比mid小的数有多少 int count = 0; for (int num : nums){ if (num &lt;= mid) count++; } //比mid小的数大于mid，说明在左边,可能是mid if (count &gt; mid) r = mid; else l = mid+1; } return l; }} 6. 实现Pow(x,n)https://leetcode-cn.com/problems/powx-n/comments/ 12345678910111213class Solution { public double myPow(double x, int n) { double res = 1.0; for(int i = n ; i != 0 ; i /=2){ if(i % 2 != 0) res = res * x; x *= x; } return n &lt; 0 ? 1/res : res; }} 7. 寻找两个排序数组的中位数https://leetcode-cn.com/problems/median-of-two-sorted-arrays/ 1234567891011121314151617181920212223242526class Solution { public double findMedianSortedArrays(int[] nums1, int[] nums2) { int n = nums1.length; int m = nums2.length; int l = (n + m + 1)/2; int r = (n + m + 2)/2; return (getK(nums1, 0 , n-1 , nums2 , 0 , m-1 , l) + getK(nums1 , 0 ,n-1 , nums2 ,0,m-1,r))/2.0; } //从两个正序数组中获取第k大的数 int getK(int[] nums1 , int s1 , int e1 , int[] nums2 , int s2 , int e2 , int k){ int len1 = e1 - s1 + 1; int len2 = e2 - s2 + 1; if(len1 &gt; len2) return getK(nums2 , s2 , e2 , nums1 , s1 , e1 , k); if(len1 == 0) return nums2[s2 + k -1]; if(k == 1) return Math.min(nums1[s1] , nums2[s2]); int i = s1 + Math.min(len1 , k/2)-1; //每次取一半的值 int j = s2 + Math.min(len2 , k/2)-1; //每一轮都将较小的那半组数据舍去 if(nums1[i] &gt; nums2[j]) return getK(nums1 , s1 ,e1 , nums2 ,j+1, e2 ,k-(j-s2+1)); else return getK(nums1, i+1 ,e1 , nums2 , s2 , e2 , k-(i-s1+1)); }} 第二部分：旋转排序数组1. 寻找旋转排序数组中的最小值(无重复值)https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array/ 1234567891011121314class Solution { public int findMin(int[] nums) { int n = nums.length; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; //34512 if(nums[mid] &gt; nums[r]) l = mid+1; else r = mid; } return nums[l]; }} 2. 寻找旋转排序数组中的最小值(有重复值)https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array-ii/ 12345678910111213class Solution { public int findMin(int[] nums) { int n = nums.length; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; if(nums[mid] &gt; nums[r]) l = mid+1; else if(nums[mid] &lt; nums[r]) r = mid; else r--; } return nums[l]; }} 3. 寻找旋转排序数组中的指定值(无重复值)https://leetcode-cn.com/problems/search-in-rotated-sorted-array/ 123456789101112131415161718192021class Solution { public int search(int[] nums, int target) { int n = nums.length; if(n == 0) return -1; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; //34512 if(nums[mid] &gt; nums[r]){ if(target &gt;= nums[l] &amp;&amp; target &lt;= nums[mid]) r = mid; else l = mid+1; }else{ //561234 if(target &gt; nums[mid] &amp;&amp; target &lt;= nums[r]) l = mid+1; else r = mid; } } return nums[l] == target ? l : -1; }} 第三部分：二维矩阵1. 搜索二维矩阵https://leetcode-cn.com/problems/search-a-2d-matrix/ 123456789101112131415class Solution { public boolean searchMatrix(int[][] matrix, int target) { int m = matrix.length; //行数 int n = matrix[0].length; //列数 int l = 0 , r = m *n -1; while(l &lt; r){ int mid = l + r + 1 &gt;&gt; 1; //将一维的数组转换成二维的坐标 if(matrix[mid / n][ mid % n] &gt; target) r = mid-1; else l = mid; } return matrix[l/n][l%n] == target; }} 2. 搜索二维矩阵2https://leetcode-cn.com/problems/search-a-2d-matrix-ii/submissions/ 1234567891011121314class Solution { public boolean searchMatrix(int[][] matrix, int target) { int m = matrix.length; //行数 int n = matrix[0].length; //列数 int i = 0 , j = n-1; while(i &lt;= m-1 &amp;&amp; j &gt;= 0){ if(matrix[i][j] &gt; target) j--; else if(matrix[i][j] &lt; target) i++; else return true; } return false; }} 3. 有序矩阵中第k小的数https://leetcode-cn.com/problems/kth-smallest-element-in-a-sorted-matrix/","link":"/2021/05/06/%E7%AE%97%E6%B3%95%E9%A2%98/"},{"title":"设计模式（二）设计原则","text":"六大软件设计原则： 单一职责 开闭原则 里式替换 迪米特法则 接口隔离 依赖反转 单一职责原则Single Responsibility Principe 简单的来说就是：一个类或者模块只负责完成一个职责（或者功能）, 它约定了一个类应该有且只有一个改变类的原因 换个角度来讲就是，一个类包含了两个或者两个以上业务不相干的功能，那我们就说它职责不够单一，应该将它拆分成多个功能更加单一、粒度更细的类。比如，一个类里既包含订单的一些操作，又包含用户的一些操作。而订单和用户是两个独立的业务领域模型，我们将两个不相干的功能放到同一个类中，那就违反了单一职责原则。 如何判断类的职责是否单一不同的应用场景、不同阶段的需求背景、不同的业务层面，对同一个类的职责是否单一，可能会有不同的判定结果。 比如下面的userinfo类表示用户信息： 12345678910111213141516public class UserInfo { private long userId; private String username; private String email; private String telephone; private long createTime; private long lastLoginTime; private String avatarUrl; private String provinceOfAddress; // 省 private String cityOfAddress; // 市 private String regionOfAddress; // 区 private String detailedAddress; // 详细地址 // ... 省略其他属性和方法... } 问题： 用户的信息占比很重，是否要单独抽出来建一个新的类userAddress呢？ 如果在这个社交产品中，用户的地址信息跟其他信息一样，只是单纯地用来展示，那 UserInfo 现在的设计就是合理的。 如果这个社交产品发展得比较好，之后又在产品中添加了电商的模块，用户的地址信息还会用在电商物流中，那我们最好将地址信息从 UserInfo 中拆分出来，独立成用户物流信息（或者叫地址信息、收货信息等）。 公司希望支持统一账号系统，也就是用户一个账号可以在公司内部的所有产品中登录。这个时候，我们就需要继续对 UserInfo 进行拆分，将跟身份认证相关的信息（比如，email、telephone 等）抽取成独立的类。 在真实的业务开发中，我们可以先写一个粗粒度的类，满足业务需求。随着业务的发展，如果粗粒度的类越来越庞大，代码越来越多，这个时候，我们就可以将这个粗粒度的类，拆分成几个更细粒度的类。这就是所谓的持续重构 下面几条判断原则： 类中的代码行数、函数或者属性过多； 类依赖的其他类过多，或者依赖类的其他类过多； 私有方法过多； 比较难给类起一个合适的名字； 类中大量的方法都是集中操作类中的某几个属性。 模拟场景一个视频网站用户分类的例子： 访客用户，视频清晰度只有480P，并时刻提醒用户需要注册才能看高清视频。 普通会员，可以看高清视频，但会有广告 VIP会员，可以选择是否跳过广告 违背SRP的设计1234567891011121314151617181920212223242526/** * @author shengbinbin * @description: 违背 SRP 原则 * @date 2021/8/1510:51 下午 */public class VideoUserService { public void serveGrade(String userType){ if (&quot;VIP会员&quot;.equals(userType)){ System.out.println(&quot;VIP会员，视频1080p蓝光&quot;); }else if (&quot;普通会员&quot;.equals(userType)){ System.out.println(&quot;普通会员，视频720p高清&quot;); }else if (&quot;访客用户&quot;.equals(userType)){ System.out.println(&quot;访客用户，视频360p标清&quot;); } } // 测试这个类的使用 public static void main(String[] args) { VideoUserService service = new VideoUserService(); /** 调用方法时，所有的用户都走的是一个方法，很难维护 */ service.serveGrade(&quot;VIP会员&quot;); service.serveGrade(&quot;普通会员&quot;); service.serveGrade(&quot;访客用户&quot;); }} SRP改善视频播放是核心功能，不能用一个类把所有的职责行为混为一谈。而是提供一个上层的接口类，对不同的差异化用户提供自己的实现类，拆分各自的职责边界。 上层接口，核心功能类： 123456public interface IVideoUserService { // 定义视频的清晰级别 void definition(); // 定义是否有广告 void advertisement();} 不同的实现类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * @author shengbinbin * @description: 访客用户在视频网站上的广告和视频清晰度 * @date 2021/8/1511:01 下午 */public class GuestVideoUserServiceImpl implements IVideoUserService{ @Override public void definition() { System.out.println(&quot;访客用户，视频360p标清&quot;); } @Override public void advertisement() { System.out.println(&quot;访客用户，存在广告&quot;); }}/** * @author shengbinbin * @description: 普通会员在视频网站上的广告和视频清晰度 * @date 2021/8/1511:03 下午 */public class OrdinaryVideoUserServiceImpl implements IVideoUserService{ @Override public void definition() { System.out.println(&quot;普通会员，视频720p标清&quot;); } @Override public void advertisement() { System.out.println(&quot;普通会员，存在广告&quot;); }}/** * @author shengbinbin * @description: vip会员在视频网站上的广告和视频清晰度 * @date 2021/8/1511:04 下午 */public class VipVideoUserServiceImpl implements IVideoUserService { @Override public void definition() { System.out.println(&quot;VIP会员，视频1080p蓝光&quot;); } @Override public void advertisement() { System.out.println(&quot;VIP会员，可以选择是否有广告&quot;); }} 通过SRP优化后，现在每个类都只负责自己的用户行为。后续的无论是扩展还是修改某个用户行为类都比较方便。 在项目开发的过程中，要尽可能的保证接口的定义、类的实现以及方法的开发保持单一职责。 开闭原则 Open Closed Principle：软件实体（模块、类、方法等）应该“对扩展开放、对修改关闭”，这意味着应该抽象的定义结构，面向抽象编程。 举例：API接口监控的代码： 12345678910111213141516171819202122public class Alert { private AlertRule rule; // AlertRule 存储报警规则 private Notification notification; // Notification 告警通知类 public Alert(AlertRule rule, Notification notification) { this.rule = rule; this.notification = notification; } public void check(String api, long requestCount, long errorCount, long duration){ long tps = requestCount / durationOfSeconds; //当接口的 TPS 超过某个预先设置的最大值时 if (tps &gt; rule.getMatchedRule(api).getMaxTps()) { notification.notify(NotificationEmergencyLevel.URGENCY, &quot;...&quot;); } //接口请求出错数大于某个最大允许值时 if (errorCount &gt; rule.getMatchedRule(api).getMaxErrorCount()) notification.notify(NotificationEmergencyLevel.SEVERE, &quot;...&quot;); } 需求：增加功能：当每秒钟接口超时请求个数超过某个预先设置的最大值时，也会触发告警通知。 方案一： 1234567891011121314151617181920212223242526272829public class Alert { private AlertRule rule; // AlertRule 存储报警规则 private Notification notification; // Notification 告警通知类 public Alert(AlertRule rule, Notification notification) { this.rule = rule; this.notification = notification; } // 1. 增加参数 超时请求个数timeoutCount public void check(String api, long requestCount, long errorCount, long duration, long timeoutCount){ long tps = requestCount / durationOfSeconds; //当接口的 TPS 超过某个预先设置的最大值时 if (tps &gt; rule.getMatchedRule(api).getMaxTps()) { notification.notify(NotificationEmergencyLevel.URGENCY, &quot;...&quot;); } //接口请求出错数大于某个最大允许值时 if (errorCount &gt; rule.getMatchedRule(api).getMaxErrorCount()) notification.notify(NotificationEmergencyLevel.SEVERE, &quot;...&quot;); //2. 增加相应的处理逻辑 long timeoutTps = timeoutCount / durationOfSeconds; if (timeoutTps &gt; rule.getMatchedRule(api).getMaxTimeoutTps()) { notification.notify(NotificationEmergencyLevel.URGENCY, &quot;...&quot;); } } 问题： 对接口进行了修改，调用这个接口的代码都要修改 修改了check函数，单元测试也需要修改 方案二: 进行重构: 第一部分是将 check() 函数的多个入参封装成 ApiStatInfo 类； 第二部分是引入 handler 的概念，将 if 判断逻辑分散在各个 handler 中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class Alert { //讲处理的规则封装成handler private List&lt;AlertHandler&gt; alertHandlers = new ArrayList&lt;&gt;(); public void addAlertHandler(AlertHandler alertHandler) { this.alertHandlers.add(alertHandler); } public void check(ApiStatInfo apiStatInfo) { for (AlertHandler handler : alertHandlers) { handler.check(apiStatInfo); } }}public class ApiStatInfo {// 省略 constructor/getter/setter 方法 private String api; private long requestCount; private long errorCount; private long durationOfSeconds; }public abstract class AlertHandler { protected AlertRule rule; protected Notification notification; public AlertHandler(AlertRule rule, Notification notification) { this.rule = rule; this.notification = notification; } public abstract void check(ApiStatInfo apiStatInfo); }public class TpsAlertHandler extends AlertHandler { public TpsAlertHandler(AlertRule rule, Notification notification) { super(rule, notification); } @Override public void check(ApiStatInfo apiStatInfo) { long tps = apiStatInfo.getRequestCount()/ apiStatInfo.getDurationOfSeconds if (tps &gt; rule.getMatchedRule(apiStatInfo.getApi()).getMaxTps()) { notification.notify(NotificationEmergencyLevel.URGENCY, &quot;...&quot;); } } }public class ErrorAlertHandler extends AlertHandler { public ErrorAlertHandler(AlertRule rule, Notification notification){ super(rule, notification); } @Override public void check(ApiStatInfo apiStatInfo) { if (apiStatInfo.getErrorCount() &gt; rule.getMatchedRule(apiStatInfo.getApi()) { notification.notify(NotificationEmergencyLevel.SEVERE, &quot;...&quot;); } } 使用代码： 12345678910111213141516171819202122232425262728293031public class ApplicationContext { private AlertRule alertRule; private Notification notification; private Alert alert; public void initializeBeans() { alertRule = new AlertRule(/*. 省略参数.*/); // 省略一些初始化代码 notification = new Notification(/*. 省略参数.*/); // 省略一些初始化代码 alert = new Alert(); alert.addAlertHandler(new TpsAlertHandler(alertRule, notification)); alert.addAlertHandler(new ErrorAlertHandler(alertRule, notification)); } public Alert getAlert() { return alert; } // 饿汉式单例 private static final ApplicationContext instance = new ApplicationContext(); private ApplicationContext() { instance.initializeBeans(); } public static ApplicationContext getInstance() { return instance; }}public class Demo { public static void main(String[] args) { ApiStatInfo apiStatInfo = new ApiStatInfo(); // ... 省略设置 apiStatInfo 数据值的代码 ApplicationContext.getInstance().getAlert().check(apiStatInfo); } } 增加功能改动如下： 第一处改动是：在 ApiStatInfo 类中添加新的属性 timeoutCount。 第二处改动是：添加新的 TimeoutAlertHander 类。 第三处改动是：在 ApplicationContext 类的 initializeBeans() 方法中，往 alert 对象中注册新的 timeoutAlertHandler。 第四处改动是：在使用 Alert 类的时候，需要给 check() 函数的入参 apiStatInfo 对象设置 timeoutCount 的值。 这样的话，check函数的逻辑不需要改变，只需要创建新的handler类即可。 里式替换原则Liskov Substitution Principle： 继承必须保证超类所拥有的性质必须在子类中仍然成立。 子类对象（object ofsubtype/derived class）能够替换程序（program）中父类对象（object of base/parentclass）出现的任何地方，并且保证原来程序的逻辑行为（behavior）不变及正确性不被破坏。 在编码上的体现也就是：当子类继承父类时，除了新添加的方法且完成新增功能外，尽量不要重写父类的方法，包含了以下四点： 子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法。 子类可以增加自己特有的方法 当子类的方法重载父类的方法时，方法的输入参数要比父类的方法更加宽松。 当子类的方法实现父类的方法时，方法的输出或返回值要比父类更严格或相等。 模拟场景不同种类的银行卡比如信用卡，储蓄卡都具备一定的消费能力，但是又有所不同，比如信用卡不宜提现，如果提现会产生高额的利息。 信用卡和储蓄卡都有支付、还款、提现、充值的功能。但对于支付来说，储蓄卡是账户扣款，信用卡是生成贷款单的动作。信用卡继承储蓄卡 违背LSP的代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * @author shengbinbin * @description: 储蓄卡 * @date 2021/8/1511:26 下午 */public class CashCard { private Logger logger = LoggerFactory.getLogger(CashCard.class); /** * 提现功能 * @param orderId ：单号 * @param amount ：金额 * @return 0000 成功 0001 失败 0002 重复 */ public String withdrawal(String orderId , BigDecimal amount){ logger.debug(&quot;提现成功，单号: {} , 金额: {}&quot;, orderId, amount); return &quot;0000&quot;; } /** * 储蓄 * @param orderId 单号 * @param amount 金额 * @return */ public String recharge(String orderId , BigDecimal amount){ //模拟充值成功 logger.debug(&quot;储蓄成功，单号:{},金额:{}&quot;, orderId,amount); return &quot;0000&quot;; } /** * 交易流水查询 * @return 交易流水 */ public List&lt;String&gt; tradeFlow(){ logger.debug(&quot;交易流水查询&quot;); List&lt;String&gt; tradeList = new ArrayList&lt;&gt;(); tradeList.add(&quot;100001,100.00&quot;); tradeList.add(&quot;100001,80.00&quot;); tradeList.add(&quot;100001,76.50&quot;); tradeList.add(&quot;100001,126.00&quot;); return tradeList; }}/** * @author shengbinbin * @description: 信用卡 * @date 2021/8/1610:45 下午 */public class CreditCard extends CashCard { private Logger logger = LoggerFactory.getLogger(CreditCard.class); @Override public String withdrawal(String orderId, BigDecimal amount) { //1. 校验 if (amount.compareTo(new BigDecimal(1000)) &gt;= 0){ logger.debug(&quot;贷款金额校验（限额1000元）, 单号:{} , 金额:{}&quot; , orderId, amount); return &quot;0001&quot;; } // 2. 模拟生成贷款单 logger.debug(&quot;生成贷款单,单号:{} , 金额:{}&quot; , orderId, amount); // 3. 模拟支付成功 logger.debug(&quot;贷款成功,单号:{} , 金额:{}&quot; , orderId, amount); return &quot;0000&quot;; } @Override public String recharge(String orderId, BigDecimal amount) { logger.debug(&quot;生成还款单,单号:{} , 金额:{}&quot; , orderId, amount); logger.debug(&quot;还款成功,单号:{} , 金额:{}&quot; , orderId, amount); return &quot;0000&quot;; } @Override public List&lt;String&gt; tradeFlow() { return super.tradeFlow(); }} 此时继承父类实现的信用卡类并不满足里式替换原则。 LSP改善代码储蓄卡和信用卡在功能使用上有些许类似，有很多可以复用的属性和逻辑。因此最好的方式是抽象一个抽象类，由抽象类定义所有卡的公用核心属性和逻辑： 12345678910111213141516171819202122232425```# 迪米特法则又称最小知道原则，指一个对象类对于其他对象类而言，知道的越少越好。这样相互之间的耦合才会最小。## 模拟场景校长想知道一个班级的总分和平均分，是应该去找老师要还是应该找每个学生要来统计呢？## 违背代码```java 接口隔离原则 Interface Segregation Principle依赖反转原则控制反转举例: 下面的代码都是由程序员来控制的 123456789101112public class UserServiceTest { public static boolean doTest(){ if (System.currentTimeMillis() % 2 == 0) return true; else return false; } public static void main(String[] args) { //这部分逻辑可以放入框架中 if (doTest()) System.out.println(&quot;Test succeed&quot;); else System.out.println(&quot;Test failed&quot;); }} 使用框架来实现同样的功能： 1234567891011121314151617181920212223242526272829303132333435//1. 写一个抽象类public abstract class TestCase { public void run(){ if (doTest()) System.out.println(&quot;Test succeed&quot;); else System.out.println(&quot;Test failed&quot;); } public abstract boolean doTest();}//2. 写一个抽象类的子类，重写doTest方法public class UserServiceTest extends TestCase{ @Override public boolean doTest() { //这里就是写业务逻辑，啥时候返回true，啥时候返回false if (System.currentTimeMillis() % 2 == 0) return true; else return false; }}//3. 编写一个启动类,只需要在框架预留的扩展点，也就是TestCase 类中的 doTest() 抽象函数中，填充具体的测试代码就可以实现之前的功能了，完全不需要写负责执行流程的 main() 函数了public class JunitApplication { private static final List&lt;TestCase&gt; testCases = new ArrayList&lt;&gt;(); public static void register(TestCase testCase){ testCases.add(testCase); } public static void main(String[] args) { register(new UserServiceTest()); // 注册操作还可以通过配置的方式来实现，不需要程序员显示调用 register() for (TestCase testCase : testCases) testCase.run(); }} 程序员利用框架进行开发的时候，只需要往预留的扩展点上，添加跟自己业务相关的代码，就可以利用框架来驱动整个程序流程的执行。 依赖注入不通过 new() 的方式在类内部创建依赖类对象，而是将依赖的类对象在外部创建好之后，通过构造函数、函数参数等方式传递（或注入）给类使用。 举例： 1234567891011121314151617181920212223242526//不使用依赖注入的方式：public class MessageSender { public void send(String cellphone, String message){ //发送逻辑 }}/** * Notification 依赖 MessageSender 类来发送通知 */public class Notification { private MessageSender messageSender; public Notification(){ this.messageSender = new MessageSender(); //硬编码，在类内部通过new的方式创建依赖类对象 } public void sendMessage(String cellphone , String message){ //校验逻辑省略 this.messageSender.send(cellphone,message); }}// 使用 Notification 的方式Notification notification = new Notification(); 1234567891011121314151617//使用依赖注入的方式：public class Notification { private MessageSender messageSender; public Notification(MessageSender messageSender){ //通过构造方法传入，而不是直接在内部创建 this.messageSender = messageSender; } public void sendMessage(String cellphone , String message){ //校验逻辑省略 this.messageSender.send(cellphone,message); }}// 使用 Notification MessageSender messageSender = new MessageSender(); Notification notification = new Notification(messageSender); 123456789101112131415161718192021//继续优化，讲MessageSender定义为接口public interface MessageSender { public void send(String cellphone, String message);}//短信发送类public class SmsSender implements MessageSender { @Override public void send(String cellphone, String message) { //.... } }// 站内信发送类public class InboxSender implements MessageSender { @Override public void send(String cellphone, String message) { //.... } }// 使用 Notification MessageSender messageSender = new SmsSender(); Notification notification = new Notification(messageSender); 依赖倒置原则（DIP） High-level modules shouldn’t depend on low-level modules. Both modules should depend on abstractions. In addition, abstractions shouldn’t depend on details. Details depend on abstractions. 高层模块（high-level modules）不要依赖低层模块（low-level）。高层模块和低层模块应该通过抽象（abstractions）来互相依赖。除此之外，抽象（abstractions）不要依赖具体实现细节（details），具体实现细节（details）依赖抽象（abstractions）","link":"/2021/06/29/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%BA%8C%EF%BC%89%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"title":"设计模式（五）结构型","text":"结构型的设计模式结构型模式旨在通过改变代码结构来达到解耦的目的，使得我们的代码容易维护和扩展。主要包含： 代理模式 适配器模式 桥梁模式 装饰模式 门面模式 组合模式 享元模式 代理模式代理模式简单来说就是用一个代理类来隐藏具体实现类的实现细节。通常还会在真实的实现前后加上一部分逻辑。对于客户端来说，代理表现的就像真实的实现类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public interface FoodService { Food makeChicken(); Food makeNoodle();}//实际的实现类public class FoodServiceImpl implements FoodService { public Food makeChicken() { Food f = new Chicken() f.setChicken(&quot;1kg&quot;); f.setSpicy(&quot;1g&quot;); f.setSalt(&quot;3g&quot;); return f; } public Food makeNoodle() { Food f = new Noodle(); f.setNoodle(&quot;500g&quot;); f.setSalt(&quot;5g&quot;); return f; }}// 代理要表现得“就像是”真实实现类，所以需要实现 FoodServicepublic class FoodServiceProxy implements FoodService { // 内部一定要有一个真实的实现类，当然也可以通过构造方法注入 private FoodService foodService = new FoodServiceImpl(); public Food makeChicken() { System.out.println(&quot;我们马上要开始制作鸡肉了&quot;); // 如果我们定义这句为核心代码的话，那么，核心代码是真实实现类做的， // 代理只是在核心代码前后做些“无足轻重”的事情 Food food = foodService.makeChicken(); System.out.println(&quot;鸡肉制作完成啦，加点胡椒粉&quot;); // 增强 food.addCondiment(&quot;pepper&quot;); return food; } public Food makeNoodle() { System.out.println(&quot;准备制作拉面~&quot;); Food food = foodService.makeNoodle(); System.out.println(&quot;制作完成啦&quot;) return food; }} 客户端调用： 123// 这里用代理类来实例化FoodService foodService = new FoodServiceProxy();foodService.makeChicken(); 代理模式说白了就是“方法包装” 或做 “方法增强” 在 AOP 中，其实就是动态代理的过程。比如 Spring 中，我们自己不定义代理类，但是 Spring 会帮我们动态来定义代理，然后把我们定义在 @Before、@After、@Around 中的代码逻辑动态添加到代理中。 说到动态代理，又可以展开说 …… Spring 中实现动态代理有两种，一种是如果我们的类定义了接口，如 UserService 接口和 UserServiceImpl 实现，那么采用 JDK 的动态代理，感兴趣的读者可以去看看 java.lang.reflect.Proxy 类的源码；另一种是我们自己没有定义接口的，Spring 会采用 CGLIB 进行动态代理，它是一个 jar 包，性能还不错。 适配器模式适配器模式做的就是，有一个接口需要实现，但是我们现成的对象都不满足，需要加一层适配器来进行适配。 一般分为三种： 默认适配器模式1234567891011// Appache commons-io 包中的 FileAlterationListener :对文件或文件夹进行监控，一旦发生了对应的操作，就会触发相应的方法public interface FileAlterationListener { void onStart(final FileAlterationObserver observer); void onDirectoryCreate(final File directory); void onDirectoryChange(final File directory); void onDirectoryDelete(final File directory); void onFileCreate(final File file); void onFileChange(final File file); void onFileDelete(final File file); void onStop(final FileAlterationObserver observer);} 如果我们只是想要监控文件夹中的文件创建和文件删除事件，可是我们还是不得不实现所有的方法,是不是太冗余了？ 需要一个适配器：实现所有方法，但实现为空。这样的话我们可以自定义类继承这个适配器实现类，只需要重写需要的方法： 123456789101112131415161718192021222324252627public class FileAlterationListenerAdaptor implements FileAlterationListener { public void onStart(final FileAlterationObserver observer) { } public void onDirectoryCreate(final File directory) { } public void onDirectoryChange(final File directory) { } public void onDirectoryDelete(final File directory) { } public void onFileCreate(final File file) { } public void onFileChange(final File file) { } public void onFileDelete(final File file) { } public void onStop(final FileAlterationObserver observer) { }} 123456789101112public class FileMonitor extends FileAlterationListenerAdaptor { public void onFileCreate(final File file) { // 文件创建 doSomething(); } public void onFileDelete(final File file) { // 文件删除 doSomething(); }} 对象适配器模式看看怎么将鸡适配成鸭，这样鸡也能当鸭来用。因为，现在鸭这个接口，我们没有合适的实现类可以用，所以需要适配器： 123456789101112131415161718192021public interface Duck { public void quack(); // 鸭的呱呱叫 public void fly(); // 飞}public interface Cock { public void gobble(); // 鸡的咕咕叫 public void fly(); // 飞}public class WildCock implements Cock { public void gobble() { System.out.println(&quot;咕咕叫&quot;); } public void fly() { System.out.println(&quot;鸡也会飞哦&quot;); }} 鸭接口有 fly() 和 quare() 两个方法，鸡 Cock 如果要冒充鸭，fly() 方法是现成的，但是鸡不会鸭的呱呱叫，没有 quack() 方法。这个时候就需要适配了： 12345678910111213141516171819202122// 毫无疑问，首先，这个适配器肯定需要 implements Duck，这样才能当做鸭来用// 鸡的适配器继承鸭这个接口public class CockAdapter implements Duck { Cock cock; // 构造方法中需要一个鸡的实例，此类就是将这只鸡适配成鸭来用 public CockAdapter(Cock cock) { this.cock = cock; } // 实现鸭的呱呱叫方法 @Override public void quack() { // 内部其实是一只鸡的咕咕叫 cock.gobble(); } @Override public void fly() { cock.fly(); }} 客户端调用： 12345678public static void main(String[] args) { // 有一只野鸡 Cock wildCock = new WildCock(); // 成功将野鸡适配成鸭 Duck duck = new CockAdapter(wildCock); ...} 无非是我们需要一只鸭，但是我们只有一只鸡，这个时候就需要定义一个适配器，由这个适配器来充当鸭，但是适配器里面的方法还是由鸡来实现的。 类适配器模式通过继承的方法，适配器自动获得了所需要的大部分方法。这个时候，客户端使用更加简单，直接 Target t = new SomeAdapter(); 就可以了。 适配器模式总结 类适配和对象适配的异同 一个采用继承，一个采用组合； 类适配属于静态实现，对象适配属于组合的动态实现，对象适配需要多实例化一个对象。 总体来说，对象适配用得比较多。 适配器模式和代理模式的异同 比较这两种模式，其实是比较对象适配器模式和代理模式，在代码结构上，它们很相似，都需要一个具体的实现类的实例。但是它们的目的不一样，代理模式做的是增强原方法的活；适配器做的是适配的活，为的是提供“把鸡包装成鸭，然后当做鸭来使用”，而鸡和鸭它们之间原本没有继承关系。 桥梁模式就是理解代码抽象和解耦。 现在有一个接口定义了一个抽象方法，有不同的实现类： 123456789101112131415161718192021222324public interface DrawAPI { public void draw(int radius, int x, int y);}public class RedPen implements DrawAPI { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用红色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }}public class GreenPen implements DrawAPI { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用绿色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }}public class BluePen implements DrawAPI { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用蓝色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }} 有一个抽象类，使用了上面的接口。 抽象类有一系列的子类，都可以直接使用这个接口的方法 123456789101112131415161718192021222324252627282930313233343536373839public abstract class Shape { protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI){ this.drawAPI = drawAPI; } public abstract void draw(); }// 圆形public class Circle extends Shape { private int radius; public Circle(int radius, DrawAPI drawAPI) { super(drawAPI); this.radius = radius; } public void draw() { drawAPI.draw(radius, 0, 0); }}// 长方形public class Rectangle extends Shape { private int x; private int y; public Rectangle(int x, int y, DrawAPI drawAPI) { super(drawAPI); this.x = x; this.y = y; } public void draw() { drawAPI.draw(0, x, y); }} 客户端调用： 12345678public static void main(String[] args) { Shape greenCircle = new Circle(10, new GreenPen()); Shape redRectangle = new Rectangle(4, 8, new RedPen()); greenCircle.draw(); redRectangle.draw();} 装饰模式既然说是装饰，那么往往就是添加小功能这种，而且，我们要满足可以添加多个小功能。最简单的，代理模式就可以实现功能的增强，但是代理不容易实现多个功能的增强，当然你可以说用代理包装代理的方式，但是那样的话代码就复杂了。 接口 Component 其实已经有了 ConcreteComponentA 和 ConcreteComponentB 两个实现类了，但是，如果我们要增强这两个实现类的话，我们就可以采用装饰模式，用具体的装饰器来装饰实现类，以达到增强的目的。 所有的具体装饰者们 ConcreteDecorator都可以作为 Component 来使用，因为它们都实现了 Component 中的所有接口。它们和 Component 实现类 ConcreteComponent的区别是，它们只是装饰者，起装饰作用。 举例： 最近大街上流行起来了“快乐柠檬”，我们把快乐柠檬的饮料分为三类：红茶、绿茶、咖啡，在这三大类的基础上，又增加了许多的口味，什么金桔柠檬红茶、金桔柠檬珍珠绿茶、芒果红茶、芒果绿茶、芒果珍珠红茶、烤珍珠红茶、烤珍珠芒果绿茶、椰香胚芽咖啡、焦糖可可咖啡等等，每家店都有很长的菜单，但是仔细看下，其实原料也没几样，但是可以搭配出很多组合，如果顾客需要，很多没出现在菜单中的饮料他们也是可以做的。 在这个例子中，红茶、绿茶、咖啡是最基础的饮料，其他的像金桔柠檬、芒果、珍珠、椰果、焦糖等都属于装饰用的。当然，在开发中，我们确实可以像门店一样，开发这些类：LemonBlackTea、LemonGreenTea、MangoBlackTea、MangoLemonGreenTea……但是，很快我们就发现，这样子干肯定是不行的，这会导致我们需要组合出所有的可能，而且如果客人需要在红茶中加双份柠檬怎么办？三份柠檬怎么办？万一有个变态要四份柠檬，所以这种做法是给自己找加班的。 123456789101112131415161718192021222324252627282930// 定义饮料抽象基类public abstract class Beverage { // 返回描述 public abstract String getDescription(); // 返回价格 public abstract double cost();}//三个基础饮料实现类，红茶、绿茶和咖啡：public class BlackTea extends Beverage { public String getDescription() { return &quot;红茶&quot;; } public double cost() { return 10; }}public class GreenTea extends Beverage { public String getDescription() { return &quot;绿茶&quot;; } public double cost() { return 11; }}...// 咖啡省略 定义调料，也就是装饰者的基类，此类必须继承自 Beverage： 12345678910111213141516171819202122232425262728293031323334353637// 调料public abstract class Condiment extends Beverage {}// 具体调料public class Lemon extends Condiment { private Beverage bevarage; // 这里很关键，需要传入具体的饮料，如需要传入没有被装饰的红茶或绿茶， // 当然也可以传入已经装饰好的芒果绿茶，这样可以做芒果柠檬绿茶 public Lemon(Beverage bevarage) { this.bevarage = bevarage; } public String getDescription() { // 装饰 return bevarage.getDescription() + &quot;, 加柠檬&quot;; } public double cost() { // 装饰 return beverage.cost() + 2; // 加柠檬需要 2 元 }}public class Mango extends Condiment { private Beverage bevarage; public Mango(Beverage bevarage) { this.bevarage = bevarage; } public String getDescription() { return bevarage.getDescription() + &quot;, 加芒果&quot;; } public double cost() { return beverage.cost() + 3; // 加芒果需要 3 元 }}...// 给每一种调料都加一个类 客户端调用： 1234567891011public static void main(String[] args) { // 首先，我们需要一个基础饮料，红茶、绿茶或咖啡 Beverage beverage = new GreenTea(); // 开始装饰 beverage = new Lemon(beverage); // 先加一份柠檬 beverage = new Mongo(beverage); // 再加一份芒果 System.out.println(beverage.getDescription() + &quot; 价格：￥&quot; + beverage.cost()); //&quot;绿茶, 加柠檬, 加芒果 价格：￥16&quot;} 如下图： Java中IO的装饰模式 InputStream 代表了输入流，具体的输入来源可以是文件（FileInputStream）、管道（PipedInputStream）、数组（ByteArrayInputStream）等，这些就像前面奶茶的例子中的红茶、绿茶，属于基础输入流。 FilterInputStream 承接了装饰模式的关键节点，其实现类是一系列装饰器，比如 BufferedInputStream 代表用缓冲来装饰，也就使得输入流具有了缓冲的功能，LineNumberInputStream 代表用行号来装饰，在操作的时候就可以取得行号了，DataInputStream 的装饰，使得我们可以从输入流转换为 java 中的基本类型值。 当然，在 java IO 中，如果我们使用装饰器的话，就不太适合面向接口编程了，如： 1InputStream inputStream = new LineNumberInputStream(new BufferedInputStream(new FileInputStream(&quot;&quot;))); 这样的结果是，InputStream 还是不具有读取行号的功能，因为读取行号的方法定义在 LineNumberInputStream 类中。 我们应该像下面这样使用： 123DataInputStream is = new DataInputStream( new BufferedInputStream( new FileInputStream(&quot;&quot;))); 门面模式也叫外观模式，Facade Pattern 常见的编码方式： 1234567891011121314151617181920212223242526272829303132//1. 定义接口public interface Shape { void draw();}//2. 定义实现类public class Circle implements Shape { @Override public void draw() { System.out.println(&quot;Circle::draw()&quot;); }}public class Rectangle implements Shape { @Override public void draw() { System.out.println(&quot;Rectangle::draw()&quot;); }}//3. 客户端调用public static void main(String[] args) { // 画一个圆形 Shape circle = new Circle(); circle.draw(); // 画一个长方形 Shape rectangle = new Rectangle(); rectangle.draw();} 定义门面： 12345678910111213141516171819202122232425262728293031323334353637public class ShapeMaker { private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() { circle = new Circle(); rectangle = new Rectangle(); square = new Square(); } /** * 下面定义一堆方法，具体应该调用什么方法，由这个门面来决定 */ public void drawCircle(){ circle.draw(); } public void drawRectangle(){ rectangle.draw(); } public void drawSquare(){ square.draw(); }}//直接调用public static void main(String[] args) { ShapeMaker shapeMaker = new ShapeMaker(); // 客户端调用现在更加清晰了 shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); } 门面模式的优点显而易见，客户端不再需要关注实例化时应该使用哪个实现类，直接调用门面提供的方法就可以了，因为门面类提供的方法的方法名对于客户端来说已经很友好了。 组合模式组合模式用于表示具有层次结构的数据，使得我们对单个对象和组合对象的访问具有一致性。 享元模式 Flyweight Pattern 。Flyweight 是轻量级的意思，享元分开来说就是 共享 元器件，也就是复用已经生成的对象，这种做法当然也就是轻量级的了。 复用对象最简单的方式是，用一个 HashMap 来存放每次新生成的对象。每次需要一个对象的时候，先到 HashMap 中看看有没有，如果没有，再生成新的对象，然后将这个对象放入 HashMap 中。 总结代理模式是做方法增强的，适配器模式是把鸡包装成鸭这种用来适配接口的，桥梁模式做到了很好的解耦，装饰模式从名字上就看得出来，适合于装饰类或者说是增强类的场景，门面模式的优点是客户端不需要关心实例化过程，只要调用需要的方法即可，组合模式用于描述具有层次结构的数据，享元模式是为了在特定的场景中缓存已经创建的对象，用于提高性能。","link":"/2021/08/22/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%BA%94%EF%BC%89%E7%BB%93%E6%9E%84%E5%9E%8B/"},{"title":"设计模式（六）行为型","text":"行为型模式关注的是各个类之间的相互作用，将职责划分清楚，使得我们的代码更加地清晰： 策略模式 观察者模式 责任链模式 模板方法模式 状态模式 策略模式场景：我们需要画一个图形，可选的策略就是用红色笔来画，还是绿色笔来画，或者蓝色笔来画。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//1. 定义一个策略接口public interface Strategy { public void draw(int radius, int x, int y);}//2. 定义几个具体的策略public class RedPen implements Strategy { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用红色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }}public class GreenPen implements Strategy { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用绿色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }}public class BluePen implements Strategy { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用蓝色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }}//3. 写出使用策略的类public class Context { private Strategy strategy; public Context(Strategy strategy){ this.strategy = strategy; } public int executeDraw(int radius, int x, int y){ return strategy.draw(radius, x, y); }}//4. 客户端调用：public static void main(String[] args) { Context context = new Context(new BluePen()); // 使用绿色笔来画 context.executeDraw(10, 0, 0);} 和桥梁模式非常像：桥梁模式在左侧加了一层抽象而已。桥梁模式的耦合更低，结构更复杂一些。 观察者模式观察者订阅自己关心的主题和主题有数据变化后通知观察者们。 定义主题：每个主题都持有观察者列表的引用，用于数据变更时通知观察者 123456789101112131415161718192021222324252627public class Subject { private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); private int state; // 状态 public int getState() { return state; } public void setState(int state) { this.state = state; // 数据已变更，通知观察者们 notifyAllObservers(); } public void attach(Observer observer){ observers.add(observer); } // 通知观察者们 public void notifyAllObservers(){ for (Observer observer : observers) { observer.update(); } } } 定义观察者接口.其实如果只有一个观察者类的话，接口都不用定义了，不过，通常场景下，既然用到了观察者模式，我们就是希望一个事件出来了，会有多个不同的类需要处理相应的信息。比如，订单修改成功事件，我们希望发短信的类得到通知、发邮件的类得到通知、处理物流信息的类得到通知等。 1234public abstract class Observer { protected Subject subject; public abstract void update();} 定义具体的几个观察者类： 1234567891011121314151617181920212223242526272829303132public class BinaryObserver extends Observer { // 在构造方法中进行订阅主题 public BinaryObserver(Subject subject) { this.subject = subject; // 通常在构造方法中将 this 发布出去的操作一定要小心 this.subject.attach(this); } // 该方法由主题类在数据变更的时候进行调用 @Override public void update() { String result = Integer.toBinaryString(subject.getState()); System.out.println(&quot;订阅的数据发生变化，新的数据处理为二进制值为：&quot; + result); }}public class HexaObserver extends Observer { public HexaObserver(Subject subject) { this.subject = subject; this.subject.attach(this); } @Override public void update() { String result = Integer.toHexString(subject.getState()).toUpperCase(); System.out.println(&quot;订阅的数据发生变化，新的数据处理为十六进制值为：&quot; + result); }} 1234567891011public static void main(String[] args) { // 先定义一个主题 Subject subject1 = new Subject(); // 定义观察者 new BinaryObserver(subject1); new HexaObserver(subject1); // 模拟数据变更，这个时候，观察者们的 update 方法将会被调用 subject.setState(11);} jdk 也提供了相似的支持，具体的大家可以参考 java.util.Observable 和 java.util.Observer 这两个类。 实际生产过程中，观察者模式往往用消息中间件来实现，如果要实现单机观察者模式，笔者建议读者使用 Guava 中的 EventBus，它有同步实现也有异步实现，本文主要介绍设计模式，就不展开说了。 责任链模式责任链通常需要先建立一个单向链表，然后调用方只需要调用头部节点就可以了，后面会自动流转下去。比如流程审批就是一个很好的例子，只要终端用户提交申请，根据申请的内容信息，自动建立一条责任链，然后就可以开始流转了。 有这么一个场景，用户参加一个活动可以领取奖品，但是活动需要进行很多的规则校验然后才能放行，比如首先需要校验用户是否是新用户、今日参与人数是否有限额、全场参与人数是否有限额等等。设定的规则都通过后，才能让用户领走奖品。 定义节点的基类： 123456789101112131415public abstract class RuleHandler { // 后继节点 protected RuleHandler successor; public abstract void apply(Context context); public void setSuccessor(RuleHandler successor) { this.successor = successor; } public RuleHandler getSuccessor() { return successor; }} 定义具体的每个节点 1234567891011121314151617181920212223242526272829303132333435363738394041424344//1. 校验是否是新用户public class NewUserRuleHandler extends RuleHandler { public void apply(Context context) { if (context.isNewUser()) { // 如果有后继节点的话，传递下去 if (this.getSuccessor() != null) { this.getSuccessor().apply(context); } } else { throw new RuntimeException(&quot;该活动仅限新用户参与&quot;); } }}//2. 校验用户所在区是否可以参与public class LocationRuleHandler extends RuleHandler { public void apply(Context context) { boolean allowed = activityService.isSupportedLocation(context.getLocation); if (allowed) { if (this.getSuccessor() != null) { this.getSuccessor().apply(context); } } else { throw new RuntimeException(&quot;非常抱歉，您所在的地区无法参与本次活动&quot;); } }}//3. 校验奖品是否已经领完public class LimitRuleHandler extends RuleHandler { public void apply(Context context) { int remainedTimes = activityService.queryRemainedTimes(context); // 查询剩余奖品 if (remainedTimes &gt; 0) { if (this.getSuccessor() != null) { this.getSuccessor().apply(userInfo); } } else { throw new RuntimeException(&quot;您来得太晚了，奖品被领完了&quot;); } }} 客户端调用： 12345678910public static void main(String[] args) { RuleHandler newUserHandler = new NewUserRuleHandler(); RuleHandler locationHandler = new LocationRuleHandler(); RuleHandler limitHandler = new LimitRuleHandler(); // 假设本次活动仅校验地区和奖品数量，不校验新老用户 locationHandler.setSuccessor(limitHandler); locationHandler.apply(context);} 代码其实很简单，就是先定义好一个链表，然后在通过任意一节点后，如果此节点有后继节点，那么传递下去。 模板方法模式在含有继承结构的代码中，模板方法模式是非常常用的。 定义一个抽象类，含有一个模板方法 123456789101112131415161718public abstract class AbstractTemplate { // 这就是模板方法，有几个抽象方法由子类实现是可以选择的。 public void templateMethod(){ init(); apply(); // 这个是重点 end(); // 可以作为钩子方法 } protected void init() { System.out.println(&quot;init 抽象层已经实现，子类也可以选择覆写&quot;); } // 留给子类实现 protected abstract void apply(); protected void end() { } } 写一个实现类： 123456789public class ConcreteTemplate extends AbstractTemplate { public void apply() { System.out.println(&quot;子类实现抽象方法 apply&quot;); } public void end() { System.out.println(&quot;我们可以把 method3 当做钩子方法来使用，需要的时候覆写就可以了&quot;); }} 客户端调用 123456public static void main(String[] args) { AbstractTemplate t = new ConcreteTemplate(); // 调用模板方法 t.templateMethod();} 状态模式用状态模式来写减库存和补库存： 定义状态接口 123public interface State { public void doAction(Context context);} 定义减库存的状态 1234567891011121314151617181920212223242526public class DeductState implements State { public void doAction(Context context) { System.out.println(&quot;商品卖出，准备减库存&quot;); context.setState(this); //... 执行减库存的具体操作 } public String toString(){ return &quot;Deduct State&quot;; }}//定义补库存状态public class RevertState implements State { public void doAction(Context context) { System.out.println(&quot;给此商品补库存&quot;); context.setState(this); //... 执行加库存的具体操作 } public String toString() { return &quot;Revert State&quot;; }} 前面用到了 context.setState(this)，我们来看看怎么定义 Context 类： 123456789101112131415public class Context { private State state; private String name; public Context(String name) { this.name = name; } public void setState(State state) { this.state = state; } public void getState() { return this.state; }} 客户端调用 12345678910111213141516public static void main(String[] args) { // 我们需要操作的是 iPhone X Context context = new Context(&quot;iPhone X&quot;); // 看看怎么进行补库存操作 State revertState = new RevertState(); revertState.doAction(context); // 同样的，减库存操作也非常简单 State deductState = new DeductState(); deductState.doAction(context); // 如果需要我们可以获取当前的状态 // context.getState().toString();} 如果我们不关心当前 context 处于什么状态，那么 Context 就可以不用维护 state 属性了，那样代码会简单很多。 总结行为型模式部分介绍了策略模式、观察者模式、责任链模式、模板方法模式和状态模式，其实，经典的行为型模式还包括备忘录模式、命令模式等，但是它们的使用场景比较有限。","link":"/2021/08/22/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E5%85%AD%EF%BC%89%E8%A1%8C%E4%B8%BA%E5%9E%8B/"},{"title":"设计模式（三）规范和重构实战","text":"aaaa 实战：ID生成器需求背景ID（identity）翻译为标识。在软件开发中，ID 常用来表示一些业务信息的唯一标识，比如订单的单号或者数据库中的唯一主键，比如地址表中的 ID 字段（实际上是没有业务含义的，对用户来说是透明的，不需要关注。 为了方便在请求出错时排查问题，我们在编写代码的时候会在关键路径上打印日志。某个请求出错之后，我们希望能搜索出这个请求对应的所有日志，以此来查找问题的原因。而实际情况是，在日志文件中，不同请求的日志会交织在一起。如果没有东西来标识哪些日志属于同一个请求，我们就无法关联同一个请求的所有日志。 借鉴微服务调用链追踪的实现思路，我们可以给每个请求分配一个唯一 ID，并且保存在请求的上下文（Context）中，比如，处理请求的工作线程的局部变量中。在 Java 语言中，我们可以将 ID 存储在 Servlet 线程的 ThreadLocal 中，或者利用 Slf4j 日志框架的MDC（Mapped Diagnostic Contexts）来实现（实际上底层原理也是基于线程的ThreadLocal）。每次打印日志的时候，我们从请求上下文中取出请求 ID，跟日志一块输出。这样，同一个请求的所有日志都包含同样的请求 ID 信息，我们就可以通过请求 ID 来搜索同一个请求的所有日志了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 初始 id 生成器代码 */public class IdGenerator { private static final Logger logger = LoggerFactory.getLogger(IdGenerator.class); public static String generate(){ String id = &quot;&quot;; try{ //第一部分是本机名的最后一个字段 String hostname = InetAddress.getLocalHost().getHostName(); String[] tokens = hostname.split(&quot;\\\\.&quot;); if (tokens.length &gt; 0) hostname = tokens[tokens.length-1]; //第三部分是 8 位的随机字符串，包含大小写字母和数字 char[] randomChars = new char[8]; int count = 0; Random random = new Random(); while (count &lt; 8){ int randomAscii = random.nextInt(122); if (randomAscii &gt;= 48 &amp;&amp; randomAscii &lt;= 57){ randomChars[count] = (char) ('0' + (randomAscii - 48)); count++; }else if (randomAscii &gt;= 65 &amp;&amp; randomAscii &lt;= 90){ randomChars[count] = (char) ('A' + (randomAscii - 65)); count++; }else if (randomAscii &gt;= 97 &amp;&amp; randomAscii &lt;= 122){ randomChars[count] = (char) ('a' + (randomAscii - 97)); count++; } } //第二部分是当前时间戳，精确到毫秒 id = String.format(&quot;%s-%d-%s&quot;,hostname,System.currentTimeMillis() , new String(randomChars)); }catch (UnknownHostException e) { logger.warn(&quot;Failed to get the host name&quot;,e); e.printStackTrace(); } return id; } public static void main(String[] args) { System.out.println(&quot;IdGenerator.generate() = &quot; + IdGenerator.generate()); //107-1625029281729-ZwlgYXh9 }} 如何分析代码质量问题代码质量角度： 目录设置是否合理、模块划分是否清晰、代码结构是否满足“高内聚、松耦合”？ 是否遵循经典的设计原则和设计思想（SOLID、DRY、KISS、YAGNI、LOD 等）？ 设计模式是否应用得当？是否有过度设计？ 代码是否容易扩展？如果要添加新功能，是否容易实现？ 代码是否可以复用？是否可以复用已有的项目代码或类库？是否有重复造轮子？ 代码是否容易测试？单元测试是否全面覆盖了各种正常和异常的情况？ 代码是否易读？是否符合编码规范（比如命名和注释是否恰当、代码风格是否一致等）？ 问题： 将IdGenerator直接设计成实现类，如果后续项目中需要两种id生成算法，该怎么办呢？是否应该定义成接口比较好，针对不同的生成算法定义不同的生成类 generate为静态函数，会影响代码的可测试性。同时依赖运行环境，没有单元测试代码，是否需要进行补充？ 按照一下的思考顺序： 代码是否实现了预期的业务需求？ 逻辑是否正确？是否处理了各种异常情况？ – 并未处理“hostName 为空” 日志打印是否得当？是否方便 debug 排查问题？ 接口是否易用？是否支持幂等、事务等？ 代码是否存在并发问题？是否线程安全？ 性能是否有优化空间，比如，SQL、算法是否可以优化？–hostName变量不应该被重复使用 是否有安全漏洞？比如输入输出校验是否全面？ 第一轮重构：提高可读性 hostName 变量不应该被重复使用，尤其当这两次使用时的含义还不同的时候； 将获取 hostName 的代码抽离出来，定义为 getLastfieldOfHostName() 函数； 删除代码中的魔法数，比如，57、90、97、122； 将随机数生成的代码抽离出来，定义为 generateRandomAlphameric() 函数； generate() 函数中的三个 if 逻辑重复了，且实现过于复杂，我们要对其进行简化； 对IdGenerator进行重命名并且抽象出相应的接口 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * Id生成器接口 */public interface IdGenerate { String generate();}/** * 日志的随机id生成器 */public interface LogTraceIdGenerator extends IdGenerate{}/** * 随机id生成器 - 第一次重构 */public class RandomIdGenerator implements LogTraceIdGenerator { private static final Logger logger = LoggerFactory.getLogger(RandomIdGenerator.class); @Override public String generate() { String substrOfHostName = getLastFieldOfHostName(); long currentTimeMillis = System.currentTimeMillis(); String randomString = generateRandomAlphameric(8); String id = String.format(&quot;%s-%d-%s&quot; , substrOfHostName ,currentTimeMillis,randomString); return id; } /** * @return 获得主机的最后一位 */ private String getLastFieldOfHostName(){ String substrOfHostName = null; try { String hostName = InetAddress.getLocalHost().getHostName(); String[] tokens = hostName.split(&quot;\\\\.&quot;); substrOfHostName = tokens[tokens.length-1]; return substrOfHostName; } catch (UnknownHostException e) { logger.warn(&quot;Failed to get the host name&quot;,e); } return substrOfHostName; } /** * @param length 随机数字和字母的长度 * @return 返回随机数字和字母的组合 */ private String generateRandomAlphameric(int length){ char[] randomChars = new char[length]; int count = 0; Random random = new Random(); while (count &lt; length){ int maxAscii = 'z'; int randomAscii = random.nextInt(maxAscii); boolean isDigit = randomAscii &gt;= '0' &amp;&amp; randomAscii &lt;= '9'; boolean isUppercase = randomAscii &gt;= 'A' &amp;&amp; randomAscii &lt;= 'Z'; boolean isLowercase = randomAscii &gt;= 'a' &amp;&amp; randomAscii &lt;= 'z'; if (isDigit || isLowercase || isUppercase){ randomChars[count] = (char) randomAscii; count++; } } return new String(randomChars); } } 第二轮重构：提高代码的可测试性 generate() 函数定义为静态函数，会影响使用该函数的代码的可测试性； – 第一轮已经解决了。 generate() 函数的代码实现依赖运行环境（本机名）、时间函数、随机函数，所以generate() 函数本身的可测试性也不好。 重构如下： 从 getLastfieldOfHostName() 函数中，将逻辑比较复杂的那部分代码剥离出来，定义为 getLastSubstrSplittedByDot() 函数。因为 getLastfieldOfHostName() 函数依赖本地主机名，所以，剥离出主要代码之后这个函数变得非常简单，可以不用测试。我们重点测试 getLastSubstrSplittedByDot() 函数即可。 将 generateRandomAlphameric() 和 getLastSubstrSplittedByDot() 这两个函数的访问权限设置为 protected。这样做的目的是，可以直接在单元测试中通过对象来调用两个函数进行测试。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * 随机id生成器 - 第2次重构 */public class RandomIdGenerator implements LogTraceIdGenerator { private static final Logger logger = LoggerFactory.getLogger(RandomIdGenerator.class); @Override public String generate() { String substrOfHostName = getLastFieldOfHostName(); long currentTimeMillis = System.currentTimeMillis(); String randomString = generateRandomAlphameric(8); String id = String.format(&quot;%s-%d-%s&quot; , substrOfHostName ,currentTimeMillis,randomString); return id; } /** * @return 获得主机的最后一位 */ private String getLastFieldOfHostName(){ String substrOfHostName = null; try { String hostName = InetAddress.getLocalHost().getHostName(); substrOfHostName = getLastSubstrSplittedByDot(hostName); } catch (UnknownHostException e) { logger.warn(&quot;Failed to get the host name&quot;,e); } return substrOfHostName; } /** * //Google Guava 的 annotation @VisibleForTesting ,只起到标识的作用，告诉其他人说，这两个函数本该是 private 访问权限的， * 之所以提升访问权限到 protected，只是为了测试，只能用于单元测试中 */ @VisibleForTesting protected String getLastSubstrSplittedByDot(String hostName){ String[] tokens = hostName.split(&quot;\\\\.&quot;); String substrOfHostName = tokens[tokens.length-1]; return substrOfHostName; } /** * @param length 随机数字和字母的长度 * @return 返回随机数字和字母的组合 */ @VisibleForTesting private String generateRandomAlphameric(int length){ char[] randomChars = new char[length]; int count = 0; Random random = new Random(); while (count &lt; length){ int maxAscii = 'z'; int randomAscii = random.nextInt(maxAscii); boolean isDigit = randomAscii &gt;= '0' &amp;&amp; randomAscii &lt;= '9'; boolean isUppercase = randomAscii &gt;= 'A' &amp;&amp; randomAscii &lt;= 'Z'; boolean isLowercase = randomAscii &gt;= 'a' &amp;&amp; randomAscii &lt;= 'z'; if (isDigit || isLowercase || isUppercase){ randomChars[count] = (char) randomAscii; count++; } } return new String(randomChars); }} 第三轮重构：编写单元测试1234567891011121314151617181920212223242526272829303132333435363738394041424344public class RandomIdGeneratorTest { @Test public void testGetLastSubstrSplittedByDot(){ RandomIdGenerator idGenerator = new RandomIdGenerator(); String actualSubstr = idGenerator.getLastSubstrSplittedByDot(&quot;field1.field2.field3&quot;); Assertions.assertEquals(&quot;field3&quot;, actualSubstr); actualSubstr = idGenerator.getLastSubstrSplittedByDot(&quot;field1&quot;); Assertions.assertEquals(&quot;field1&quot;, actualSubstr); actualSubstr = idGenerator.getLastSubstrSplittedByDot(&quot;field1#field2$field3&quot;); Assertions.assertEquals(&quot;field1#field2$field3&quot;, actualSubstr); } @Test //// 此单元测试会失败，因为我们在代码中没有处理hostName为null或空字符串的情况 public void testGetLastSubstrSplittedByDot_nullOrEmpty(){ RandomIdGenerator idGenerator = new RandomIdGenerator(); String actualSubstr = idGenerator.getLastSubstrSplittedByDot(null); Assertions.assertNull(actualSubstr); actualSubstr = idGenerator.getLastSubstrSplittedByDot(&quot;&quot;); Assertions.assertEquals(&quot;&quot;, actualSubstr); } @Test public void testGenerateRandomAlphameric(){ RandomIdGenerator idGenerator = new RandomIdGenerator(); String actualRandomString = idGenerator.generateRandomAlphameric(6); Assertions.assertNotNull(actualRandomString); Assertions.assertEquals(6, actualRandomString.length()); for (char c : actualRandomString.toCharArray()) { Assertions.assertTrue(('0' &lt; c &amp;&amp; c &gt; '9') || ('a' &lt; c &amp;&amp; c &gt; 'z') || ('A' &lt; c &amp;&amp; c &gt; 'Z')); } } @Test // 此单元测试会失败，因为我们在代码中没有处理length&lt;=0的情况 public void testGenerateRandomAlphameric_lengthEqualsOrLessThanZero(){ RandomIdGenerator idGenerator = new RandomIdGenerator(); String actualRandomString = idGenerator.generateRandomAlphameric(0); Assertions.assertEquals(&quot;&quot;, actualRandomString); actualRandomString = idGenerator.generateRandomAlphameric(-1); Assertions.assertNull(actualRandomString); }}","link":"/2021/06/30/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%B8%89%EF%BC%89%E8%A7%84%E8%8C%83%E5%92%8C%E9%87%8D%E6%9E%84%E5%AE%9E%E6%88%98/"},{"title":"设计模式（三）创建型","text":"创建型的设计模式包含： 单例模式 简单工厂模式 工厂方法模式 抽象工厂模式 建造者模式 原型模式 单例模式为什么要用单例模式单例设计模式（Singleton Design Pattern）理解起来非常简单。一个类只允许创建一个对象（或者实例），那这个类就是一个单例类，这种设计模式就叫作单例设计模式，简称单例模式。 处理资源访问冲突举例： 12345678910111213141516171819202122232425262728293031// 日志类：public class Logger { private FileWriter writer; public Logger() { File file = new File(&quot;/Users/wangzheng/log.txt&quot;); writer = new FileWriter(file, true); //true表示追加写入 } public void log(String message) { writer.write(mesasge); } }public class UserController{ private Logger logger = new Logger(); public void login(String username , String password){ //省略业务代码 logger.log(&quot;&quot;); //记录日志 }}public class OrderController{ private Logger logger = new Logger(); public void create(String username , String password){ //省略业务代码 logger.log(&quot;&quot;); //记录日志 }} 问题分析： 这段代码在 UserController和OrderController中都创建了Logger对象，而在Web容器中servlet多线程条件下，如果两个servlet同时执行这个记录日志的操作，可能会发生日志的覆盖的问题。（两个线程同时向共享资源上面写文件） 如果加锁可以解决这个问题嘛？ 12345public void log(String message) { synchronized(this){ writer.write(mesasge); }} 其实不能，应该锁是对象级别的锁（一个对象在不同的线程下同时调用 log() 函数，会被强制要求顺序执行）。而在上面的例子中是在两个类中创建了不同的logger对象，通过不同的对象执行log函数，锁并无作用。 如何解决呢？ — 将对象级别的锁换成类级别的锁即可。让所有的对象都共享一把锁。 12345public void log(String message) { synchronized(Logger.class) { // 类级别的锁 writer.write(mesasge); } } 并发队列（比如 Java 中的 BlockingQueue）也可以解决这个问题：多个线程同时往并发队列里写日志，一个单独的线程负责将并发队列中的数据，写入到日志文件。 将 Logger 设计成一个单例类，程序中只允许创建一个 Logger 对象，所有的线程共享使用的这一个 Logger 对象，共享一个 FileWriter 对象，而 FileWriter 本身是对象级别线程安全的，也就避免了多线程情况下写日志会互相覆盖的问题 表示全局唯一类比如，配置信息类。在系统中，我们只有一个配置文件，当配置文件被加载到内存之后，以对象的形式存在，也理所应当只有一份。 再比如之前的唯一id生成器。 如何实现单例模式 构造函数需要是 private 访问权限的，这样才能避免外部通过 new 创建实例； 考虑对象创建时的线程安全问题； 考虑是否支持延迟加载； 考虑 getInstance() 性能是否高（是否加锁）。 饿汉式12345678910111213141516/** * 饿汉式：在程序启动时就将这个实例初始化好 */public class SingleInstance { private static final SingleInstance instance = new SingleInstance(); //构造器私有化 private SingleInstance(){ } //对外通过一个方法获取单例 public SingleInstance getInstance() { return instance; }} 懒汉式1234567891011121314/** * 懒汉式：对外提供的方法需要加锁，并法度较低 */public class LazySingleInstance { private static LazySingleInstance instance; private LazySingleInstance(){} public static synchronized LazySingleInstance getInstance(){ if (instance == null) instance = new LazySingleInstance(); return instance; }} 双重检测既支持延迟加载、又支持高并发的单例实现方式，也就是双重检测实现方式。 在这种实现方式中，只要 instance 被创建之后，即便再调用 getInstance() 函数也不会再进入到加锁逻辑中了。 12345678910111213141516/** * 双重检测：需要加volatile */public class DoubleCheckSingleInstance { //加上volatile 是为了instance = new DoubleCheckSingleInstance(); 禁止重排序 private static volatile DoubleCheckSingleInstance instance; private DoubleCheckSingleInstance(){} public static DoubleCheckSingleInstance getInstance(){ if (instance == null){ synchronized (DoubleCheckSingleInstance.class){ //此处为类级别的锁 if (instance == null) instance = new DoubleCheckSingleInstance(); } } return instance; }} 静态内部类insance 的唯一性、创建过程的线程安全性，都由JVM 来保证。所以，这种实现方法既保证了线程安全，又能做到延迟加载 12345678910111213141516/** * 静态内部类: SingletonHolder 是一个静态内部类，当外部类 StaticInnerClassSingleInstance 被加载的时候，并不会创建SingletonHolder 实例对象。 * 只有当调用 getInstance() 方法时，SingletonHolder 才会被加载，这个时候才会创建 instance */public class StaticInnerClassSingleInstance { private StaticInnerClassSingleInstance(){} private static class SingletonHolder{ private static final StaticInnerClassSingleInstance instance = new StaticInnerClassSingleInstance(); } public static StaticInnerClassSingleInstance getInstance(){ return SingletonHolder.instance; }} 枚举1234567891011/** * 枚举类实现单例模式 */public enum EnumSingleInstance { INSTANCE; private AtomicLong id = new AtomicLong(0); public long getId(){ return id.incrementAndGet(); }} 单例模式有哪些问题简单工厂模式 定义一个工厂类，根据传入的参数不同返回不同的实例，被创建的实例具有共同的父类或接口。 适用场景 由于只有一个工厂类，所以工厂类中创建的对象不能太多 客户端不关心对象的创建过程。 需求实例创建一个可以绘制不同形状的绘图工具，可以绘制圆形，正方形，三角形，每个图形都会有一个draw()方法用于绘图， 先根据实例圆形，正方形，三角形抽象出共同的父类或接口 123456/** * @description: 圆形，正方形，三角形 共同的父类或接口 */public interface Shape { void draw();} 针对不同的实例，有不同的实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * @description: CircleShape */public class CircleShape implements Shape{ public CircleShape() { System.out.println(&quot;Circle constructor&quot;); } @Override public void draw() { System.out.println(&quot;draw a circle&quot;); }}/** * @description: 正方形 */public class RectShape implements Shape{ public RectShape() { System.out.println(&quot;RectShape constructor&quot;); } @Override public void draw() { System.out.println(&quot;draw a RectShape&quot;); }}/** * @description: 三角形 */public class TriangleShape implements Shape{ public TriangleShape() { System.out.println(&quot;TriangleShape constructor&quot;); } @Override public void draw() { System.out.println(&quot;draw a TriangleShape&quot;); }} 创建一个工厂，对外提供一个获取实例的方法，针对不同的入参提供不同的返回类型： 12345678910111213141516171819/** * @description: 工厂类 */public class ShapeFactory { public static final String TAG = &quot;ShapeFactory&quot;; //根据传入的参数不同返回不同的类型 public static Shape getShape(String type){ Shape shape = null; if (type.equalsIgnoreCase(&quot;circle&quot;)) { shape = new CircleShape(); } else if (type.equalsIgnoreCase(&quot;rect&quot;)) { shape = new RectShape(); } else if (type.equalsIgnoreCase(&quot;triangle&quot;)) { shape = new TriangleShape(); } return shape; }} 直接使用工厂来创建实例： 12345678910public static void main(String[] args) { Shape circle = ShapeFactory.getShape(&quot;circle&quot;); circle.draw(); Shape rect = ShapeFactory.getShape(&quot;rect&quot;); rect.draw(); Shape triangle = ShapeFactory.getShape(&quot;triangle&quot;); triangle.draw(); } 工厂方法模式工厂方法模式是简单工厂的仅一步深化， 在工厂方法模式中，我们不再提供一个统一的工厂类来创建所有的对象，而是针对不同的对象提供不同的工厂。也就是说每个对象都有一个与之对应的工厂。 工厂方法模式(Factory Method Pattern)又称为工厂模式， 工厂父类负责定义创建产品对象的公共接口，而工厂子类则负责生成具体的产品对象，即通过不同的工厂子类来创建不同的产品对象。 简单工厂是由一个代工厂生产不同的产品，而工厂方法是对工厂进行抽象化，不同产品都由专门的具体工厂来生产 适用场景 客户端不需要知道它所创建的对象的类。例子中我们不知道每个图片加载器具体叫什么名，只知道创建它的工厂名就完成了创建过程。 客户端可以通过子类来指定创建对应的对象。 需求实例现在需要设计一个这样的图片加载类，它具有多个图片加载器，用来加载jpg，png，gif格式的图片，每个加载器都有一个read（）方法，用于读取图片。下面我们完成这个图片加载类。 编写一个加载器的公共接口,有加载图片的方法 1234public interface Reader { //加载图片的方法 void read();} 针对不同的图片格式有不同的实现类 123456789101112131415161718192021public class GIFReader implements Reader{ @Override public void read() { System.out.println(&quot;load GIF picture&quot;); }}public class JPGReader implements Reader{ @Override public void read() { System.out.println(&quot;load JPG picture&quot;); }}public class PNGReader implements Reader{ @Override public void read() { System.out.println(&quot;load PNG picture&quot;); }} 如果是简单工厂的话，就一个工厂类，可以提供不同的 Reader。但现在是工厂方法模式，需要先抽象一个加载器工厂出来： 1234public interface ReaderFactory { Reader getReader();} 针对不同格式的图片有对应的加载工厂，返回相对于的加载器 123456789101112131415161718192021public class GIFReaderFactory implements ReaderFactory{ @Override public Reader getReader() { return new GIFReader(); }}public class JPGReaderFactory implements ReaderFactory{ @Override public Reader getReader() { return new JPGReader(); }}public class PNGReaderFactory implements ReaderFactory{ @Override public Reader getReader() { return new PNGReader(); }} 测试使用： 12345678910public static void main(String[] args) { GIFReaderFactory gifReaderFactory = new GIFReaderFactory(); gifReaderFactory.getReader().read(); JPGReaderFactory jpgReaderFactory = new JPGReaderFactory(); jpgReaderFactory.getReader().read(); PNGReaderFactory pngReaderFactory = new PNGReaderFactory(); pngReaderFactory.getReader().read(); } 抽象工厂模式 抽象工厂模式(Abstract Factory Pattern)：提供一个创建一系列相关或相互依赖对象的接口，而无须指定它们具体的类。 抽象工厂和工厂方法不同的地方在于，生产产品的工厂是抽象的。举例，可口可乐公司生产可乐的同时，也需要生产装可乐的瓶子和箱子，瓶子和箱子也是可口可乐专属定制的，同样百事可乐公司也会有这个需求。这个时候我们的工厂不仅仅是生产可乐饮料的工厂，还必须同时生产同一主题的瓶子和箱子，所以它是一个抽象的主题工厂，专门生产同一主题的不同商品。 抽象工厂模式是工厂方法的仅一步深化，在这个模式中的工厂类不单单可以创建一个对象，而是可以创建一组对象。这是和工厂方法最大的不同点。 抽象工厂和工厂方法一样可以划分为4大部分：AbstractFactory（抽象工厂）声明了一组用于创建对象的方法，注意是一组。ConcreteFactory（具体工厂）：它实现了在抽象工厂中声明的创建对象的方法，生成一组具体对象。AbstractProduct（抽象产品）：它为每种对象声明接口，在其中声明了对象所具有的业务方法。ConcreteProduct（具体产品）：它定义具体工厂生产的具体对象。 实例场景现在需要做一款跨平台的游戏，需要兼容Android，Ios，Wp三个移动操作系统，该游戏针对每个系统都设计了一套操作控制器（OperationController）和界面控制器（UIController），下面通过抽象工厂方式完成这款游戏的架构设计。 定义抽象产品：OperationController 和 UIController 1234567public interface OperationController { void control();}public interface UIController { void display();} 实现具体产品： 12345678910111213public class AndroidOperationController implements OperationController{ @Override public void control() { System.out.println(&quot;android OperationController&quot;); }}public class AndroidUIController implements UIController{ @Override public void display() { System.out.println(&quot;android UIController&quot;); }} 1234567891011121314public class IosOperationController implements OperationController{ @Override public void control() { System.out.println(&quot;ios OperationController&quot;); }}public class IosUIController implements UIController{ @Override public void display() { System.out.println(&quot;ios UIController&quot;); }} 12345678910111213public class WpOperationController implements OperationController{ @Override public void control() { System.out.println(&quot;WpOperationController&quot;); }}public class WpUIController implements UIController{ @Override public void display() { System.out.println(&quot;WpInterfaceController&quot;); }} 定义抽象工厂： 123456// 抽象工厂，可以创建两个OperationController 、 UIControllerpublic interface SystemFactory { public OperationController createOperationController(); public UIController createInterfaceController();} 定义具体工厂： 12345678910111213141516171819202122232425262728293031323334353637383940public class AndroidFactory implements SystemFactory { @Override public OperationController createOperationController() { return new AndroidOperationController(); } @Override public UIController createInterfaceController() { return new AndroidUIController(); }}public class IosFactory implements SystemFactory { @Override public OperationController createOperationController() { return new IosOperationController(); } @Override public UIController createInterfaceController() { return new IosUIController(); }} public class WpFactory implements SystemFactory { @Override public OperationController createOperationController() { return new WpOperationController(); } @Override public UIController createInterfaceController() { return new WpUIController(); }} 针对不同平台只通过创建不同的工厂对象就完成了操作和UI控制器的创建 如果这个游戏使用工厂方法模式搭建需要创建多少个工厂类呢？ 适用场景：（1）和工厂方法一样客户端不需要知道它所创建的对象的类。（2）需要一组对象共同完成某种功能时。并且可能存在多组对象完成不同功能的情况。（3）系统结构稳定，不会频繁的增加对象。（因为一旦增加就需要修改原有代码，不符合开闭原则） 这个模式并不符合开闭原则。实际开发还需要做好权衡。 建造者模式简单的说就是先 new 一个 Builder，然后可以链式地调用一堆方法，最后再调用一次 build() 方法，我们需要的对象就有了。 比如： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970//先把所有的属性都设置给 Builder，然后 build() 方法的时候，将这些属性复制给实际产生的对象。class User { // 下面是“一堆”的属性 private String name; private String password; private String nickName; private int age; // 构造方法私有化，不然客户端就会直接调用构造方法了 private User(String name, String password, String nickName, int age) { this.name = name; this.password = password; this.nickName = nickName; this.age = age; } // 静态方法，用于生成一个 Builder，这个不一定要有，不过写这个方法是一个很好的习惯， // 有些代码要求别人写 new User.UserBuilder().a()...build() 看上去就没那么好 public static UserBuilder builder() { return new UserBuilder(); } public static class UserBuilder { // 下面是和 User 一模一样的一堆属性 private String name; private String password; private String nickName; private int age; private UserBuilder() { } // 链式调用设置各个属性值，返回 this，即 UserBuilder public UserBuilder name(String name) { this.name = name; return this; } public UserBuilder password(String password) { this.password = password; return this; } public UserBuilder nickName(String nickName) { this.nickName = nickName; return this; } public UserBuilder age(int age) { this.age = age; return this; } // build() 方法负责将 UserBuilder 中设置好的属性“复制”到 User 中。 // 当然，可以在 “复制” 之前做点检验 public User build() { if (name == null || password == null) { throw new RuntimeException(&quot;用户名和密码必填&quot;); } if (age &lt;= 0 || age &gt;= 150) { throw new RuntimeException(&quot;年龄不合法&quot;); } // 还可以做赋予”默认值“的功能 if (nickName == null) { nickName = name; } return new User(name, password, nickName, age); } }} 客户端调用： 12345678910public class APP { public static void main(String[] args) { User d = User.builder() .name(&quot;foo&quot;) .password(&quot;pAss12345&quot;) .age(25) .build(); }} 当属性很多，而且有些必填，有些选填的时候，这个模式会使代码清晰很多。我们可以在 Builder 的构造方法中强制让调用者提供必填字段，还有，在 build() 方法中校验各个参数比在 User 的构造方法中校验，代码要优雅一些。 用了 lombok 以后，可以用@Builder 注解快速构建建造者模式 原型模式原型模式很简单：有一个原型实例，基于这个原型实例产生新的实例，也就是“克隆”了。 Object 类中有一个 clone() 方法，它用于生成一个新的对象，当然，如果我们要调用这个方法，java 要求我们的类必须先实现 Cloneable 接口，此接口没有定义任何方法，但是不这么做的话，在 clone() 的时候，会抛出 CloneNotSupportedException 异常。 java 的克隆是浅克隆，碰到对象引用的时候，克隆出来的对象和原对象中的引用将指向同一个对象。通常实现深克隆的方法是将对象进行序列化，然后再进行反序列化。 总结创建型模式总体上比较简单，它们的作用就是为了产生实例对象，算是各种工作的第一步了，因为我们写的是面向对象的代码，所以我们第一步当然是需要创建一个对象了。 简单工厂模式最简单；工厂模式在简单工厂模式的基础上增加了选择工厂的维度，需要第一步选择合适的工厂；抽象工厂模式有产品族的概念，如果各个产品是存在兼容性问题的，就要用抽象工厂模式。单例模式就不说了，为了保证全局使用的是同一对象，一方面是安全性考虑，一方面是为了节省资源；建造者模式专门对付属性很多的那种类，为了让代码更优美；原型模式用得最少，了解和 Object 类中的 clone() 方法相关的知识即可。","link":"/2021/07/01/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E5%9B%9B%EF%BC%89%E5%88%9B%E5%BB%BA%E5%9E%8B/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","link":"/tags/ElasticSearch/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"JavaSE","slug":"JavaSE","link":"/tags/JavaSE/"},{"name":"Java并发","slug":"Java并发","link":"/tags/Java%E5%B9%B6%E5%8F%91/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"MapStruct","slug":"MapStruct","link":"/tags/MapStruct/"},{"name":"Netty","slug":"Netty","link":"/tags/Netty/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"SpringBoot","slug":"SpringBoot","link":"/tags/SpringBoot/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"操作系统","slug":"操作系统","link":"/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"消息队列","slug":"消息队列","link":"/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"EffectiveJava","slug":"EffectiveJava","link":"/tags/EffectiveJava/"},{"name":"算法题","slug":"算法题","link":"/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[{"name":"技术","slug":"技术","link":"/categories/%E6%8A%80%E6%9C%AF/"}]}