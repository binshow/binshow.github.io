{"pages":[],"posts":[{"title":"ElasticSearch从入门到实战","text":"本篇主要讲述了elasticsearch入门的一些基本概念（来源：江南一点雨） 简介 安装 十大核心概念 分词器 索引基本操作 文档基本操作 版本控制 倒排索引 ElasticSearch简介LuceneLucene 是一个开源、免费、高性能、纯 Java 编写的全文检索引擎，可以算作是开源领域最好的全文检索工具包。 在实际开发中，Lucene 几乎适用于任何需要全文检索的场景，所以 Lucene 先后发展出好多语言版本，例如 C++、C#、Python 等。 早在 2005 年，Lucene 就升级为 Apache 顶级开源项目。它的作者是 Doug Cutting，有的人可能没听过这这个人，不过你肯定听过他的另一个大名鼎鼎的作品 Hadoop。 不过需要注意的是，Lucene 只是一个工具包，并非一个完整的搜索引擎，开发者可以基于 Lucene 来开发完整的搜索引擎。比较著名的有 Solr、ElasticSearch，不过在分布式和大数据环境下，ElasticSearch 更胜一筹。 Lucene 主要有如下特点： 简单 跨语言 强大的搜索引擎 索引速度快 索引文件兼容不同平台 ElasticSearchElasticSearch 是一个分布式、可扩展、近实时性的高性能搜索与数据分析引擎。ElasticSearch 基于 Java 编写，通过进一步封装 Lucene，将搜索的复杂性屏蔽起来，开发者只需要一套简单的 RESTful API 就可以操作全文检索。 ElasticSearch 在分布式环境下表现优异，这也是它比较受欢迎的原因之一。它支持 PB 级别的结构化或非结构化海量数据处理 整体上来说，ElasticSearch 有三大功能： 数据搜集 数据分析 数据存储 ElasticSearch 的主要特点： 分布式文件存储。 实时分析的分布式搜索引擎。 高可拓展性。 可插拔的插件支持。 ElasticSearch安装（homebrew）首先打开 Es 官网，找到 Elasticsearch： https://www.elastic.co/cn/elasticsearch/ 点击 https://www.elastic.co/guide/en/elasticsearch/reference/7.13/brew.html，按照提示安装 打开终端 123456789101112131415161718192021222324252627282930shengbinbin@192 ~ % brew list //查看所有homebrew安装的软件==&gt; Formulaeautoconf icu4c maven pkg-configbrotli jemalloc mysql protobufc-ares kibana-full nghttp2 rediselasticsearch-full libev node tcl-tkfreetype libpng openjdkgit-gui libuv openjdk@11gradle m4 openssl@1.1shengbinbin@192 ~ % brew info elasticsearch-full //查看安装的elasticsearch-full详细信息elastic/tap/elasticsearch-full: stable 7.13.2Distributed search &amp; analytics enginehttps://www.elastic.co/products/elasticsearchConflicts with: elasticsearch/opt/homebrew/Cellar/elasticsearch-full/7.13.2 (957 files, 504.8MB) * Built from source on 2021-06-29 at 20:46:00From: https://github.com/elastic/homebrew-tap/blob/HEAD/Formula/elasticsearch-full.rb==&gt; CaveatsData: /opt/homebrew/var/lib/elasticsearch/elasticsearch_shengbinbin/Logs: /opt/homebrew/var/log/elasticsearch/elasticsearch_shengbinbin.logPlugins: /opt/homebrew/var/elasticsearch/plugins/Config: /opt/homebrew/etc/elasticsearch/To have launchd start elastic/tap/elasticsearch-full now and restart at login: brew services start elastic/tap/elasticsearch-fullOr, if you don't want/need a background service you can just run: elasticsearch 目录含义如下： 目录 含义 modules 依赖模块目录 lib 第三方依赖库 logs 输出日志目录 plugins 插件目录 bin 可执行文件目录 config 配置文件目录 data 数据存储目录 启动方式：打开终端，直接输入elasticsearch即可启动。看到 started 表示启动成功。 默认监听的端口是 9200，所以浏览器直接输入 localhost:9200 可以查看节点信息。 节点的名字以及集群（默认是 elasticsearch）的名字，我们都可以自定义配置。 打开 config/elasticsearch.yml 文件，可以配置集群名称以及节点名称。配置文件内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.# Before you set out to tweak and tune the configuration, make sure you# understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:#cluster.name: elasticsearch_shengbinbin //可以在这里修改集群名称## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:##node.name: node-1## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#path.data: /opt/homebrew/var/lib/elasticsearch/## Path to log files:#path.logs: /opt/homebrew/var/log/elasticsearch/## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: true## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## By default Elasticsearch is only accessible on localhost. Set a different# address here to expose this node on the network:##network.host: 192.168.0.1## By default Elasticsearch listens for HTTP traffic on the first free port it# finds starting at 9200. Set a specific HTTP port here:##http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when this node is started:# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]##discovery.seed_hosts: [&quot;host1&quot;, &quot;host2&quot;]## Bootstrap the cluster using an initial set of master-eligible nodes:##cluster.initial_master_nodes: [&quot;node-1&quot;, &quot;node-2&quot;]## For more information, consult the discovery and cluster formation module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true HEAD 插件安装Elasticsearch-head 插件，可以通过可视化的方式查看集群信息。 Chrome 直接在 App Store 搜索 Elasticsearch-head，点击安装即可。 2.3 分布式安装假设： 一主二从 master 的端口是 9200，slave 端口分别是 9201 和 9202 首先修改 master 的 config/elasticsearch.yml 配置文件： 12node.master: truenetwork.host: 127.0.0.1 配置完成后，重启 master。 将 es 的压缩包解压两份，分别命名为 slave01 和 slave02，代表两个从机。 分别对其进行配置。 slave01/config/elasticsearch.yml： 123456# 集群名称必须保持一致cluster.name: javaboy-esnode.name: slave01network.host: 127.0.0.1http.port: 9201discovery.zen.ping.unicast.hosts: [&quot;127.0.0.1&quot;] slave02/config/elasticsearch.yml： 123456# 集群名称必须保持一致cluster.name: javaboy-esnode.name: slave02network.host: 127.0.0.1http.port: 9202discovery.zen.ping.unicast.hosts: [&quot;127.0.0.1&quot;] 然后分别启动 slave01 和 slave02。启动后，可以在 head 插件上查看集群信息。 Kibana 安装Kibana 是一个 Elastic 公司推出的一个针对 es 的分析以及数据可视化平台，可以搜索、查看存放在 es 中的数据。 下载 Kibana：https://www.elastic.co/cn/downloads/kibana 使用homebrew安装：https://www.elastic.co/guide/en/kibana/7.13/brew.html 终端输入kibana启动 localhost:5601 查看默认窗口 ElasticSearch 十大核心概念3.1.1 集群（Cluster）一个或者多个安装了 es 节点的服务器组织在一起，就是集群，这些节点共同持有数据，共同提供搜索服务。 一个集群有一个名字，这个名字是集群的唯一标识，该名字成为 cluster name，默认的集群名称是 elasticsearch，具有相同名称的节点才会组成一个集群。 可以在 config/elasticsearch.yml 文件中配置集群名称： 1cluster.name: javaboy-es 在集群中，节点的状态有三种：绿色、黄色、红色： 绿色：节点运行状态为健康状态。所有的主分片、副本分片都可以正常工作。 黄色：表示节点的运行状态为警告状态，所有的主分片目前都可以直接运行，但是至少有一个副本分片是不能正常工作的。 红色：表示集群无法正常工作。 3.1.2 节点（Node）集群中的一个服务器就是一个节点，节点中会存储数据，同时参与集群的索引以及搜索功能。一个节点想要加入一个集群，只需要配置一下集群名称即可。默认情况下，如果我们启动了多个节点，多个节点还能够互相发现彼此，那么它们会自动组成一个集群，这是 es 默认提供的，但是这种方式并不可靠，有可能会发生脑裂现象。所以在实际使用中，建议一定手动配置一下集群信息。 3.1.3 索引（Index）索引可以从两方面来理解： 名词 具有相似特征文档的集合。 动词 索引数据以及对数据进行索引操作。 3.1.4 类型（Type）类型是索引上的逻辑分类或者分区。在 es6 之前，一个索引中可以有多个类型，从 es7 开始，一个索引中，只能有一个类型。在 es6.x 中，依然保持了兼容，依然支持单 index 多个 type 结构，但是已经不建议这么使用。 3.1.5 文档（Document）一个可以被索引的数据单元。例如一个用户的文档、一个产品的文档等等。文档都是 JSON 格式的。 3.1.6 分片（Shards）索引都是存储在节点上的，但是受限于节点的空间大小以及数据处理能力，单个节点的处理效果可能不理想，此时我们可以对索引进行分片。当我们创建一个索引的时候，就需要指定分片的数量。每个分片本身也是一个功能完善并且独立的索引。 默认情况下，一个索引会自动创建 1 个分片，并且为每一个分片创建一个副本。 3.1.7 副本（Replicas）副本也就是备份，是对主分片的一个备份。 3.1.8 Settings集群中对索引的定义信息，例如索引的分片数、副本数等等。 3.1.9 MappingMapping 保存了定义索引字段的存储类型、分词方式、是否存储等信息。 3.1.10 Analyzer字段分词方式的定义。 3.2 ElasticSearch Vs 关系型数据库 内置分词器ElasticSearch 核心功能就是数据检索，首先通过索引将文档写入 es。查询分析则主要分为两个步骤： 词条化：分词器将输入的文本转为一个一个的词条流。 过滤：比如停用词过滤器会从词条中去除不相干的词条（的，嗯，啊，呢）；另外还有同义词过滤器、小写过滤器等。 ElasticSearch 中内置了多种分词器可以供使用。 内置分词器： 4.2 中文分词器在 Es 中，使用较多的中文分词器是 elasticsearch-analysis-ik，这个是 es 的一个第三方插件，代码托管在 GitHub 上： https://github.com/medcl/elasticsearch-analysis-ik 4.2.1 安装两种使用方式： 第一种： 首先打开分词器官网：https://github.com/medcl/elasticsearch-analysis-ik。 在 https://github.com/medcl/elasticsearch-analysis-ik/releases 页面找到最新的正式版，下载下来。我们这里的下载链接是 https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.9.3/elasticsearch-analysis-ik-7.9.3.zip。 将下载文件解压。 在 es/plugins 目录下，新建 ik 目录，并将解压后的所有文件拷贝到 ik 目录下。 重启 es 服务。 第二种： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152shengbinbin@192 plugins % brew list==&gt; Formulaeautoconf icu4c maven pkg-configbrotli jemalloc mysql protobufc-ares kibana-full nghttp2 rediselasticsearch-full libev node tcl-tkfreetype libpng openjdkgit-gui libuv openjdk@11gradle m4 openssl@1.1shengbinbin@192 plugins % brew info elasticsearch-fullelastic/tap/elasticsearch-full: stable 7.13.2Distributed search &amp; analytics enginehttps://www.elastic.co/products/elasticsearchConflicts with: elasticsearch/opt/homebrew/Cellar/elasticsearch-full/7.13.2 (957 files, 504.8MB) * Built from source on 2021-06-29 at 20:46:00From: https://github.com/elastic/homebrew-tap/blob/HEAD/Formula/elasticsearch-full.rb==&gt; CaveatsData: /opt/homebrew/var/lib/elasticsearch/elasticsearch_shengbinbin/Logs: /opt/homebrew/var/log/elasticsearch/elasticsearch_shengbinbin.logPlugins: /opt/homebrew/var/elasticsearch/plugins/Config: /opt/homebrew/etc/elasticsearch/To have launchd start elastic/tap/elasticsearch-full now and restart at login: brew services start elastic/tap/elasticsearch-fullOr, if you don't want/need a background service you can just run: elasticsearchshengbinbin@192 plugins % cd /opt/homebrew/Cellar/elasticsearch-full/7.13.2shengbinbin@192 7.13.2 % lsINSTALL_RECEIPT.json binLICENSE.txt homebrew.mxcl.elasticsearch-full.plistNOTICE.txt libexecREADME.asciidoc //安装命令shengbinbin@192 7.13.2 % ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.13.2/elasticsearch-analysis-ik-7.13.2.zipwarning: usage of JAVA_HOME is deprecated, use ES_JAVA_HOMEFuture versions of Elasticsearch will require Java 11; your Java version from [/Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home/jre] does not meet this requirement. Consider switching to a distribution of Elasticsearch with a bundled JDK. If you are already using a distribution with a bundled JDK, ensure the JAVA_HOME environment variable is not set.-&gt; Installing https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.13.2/elasticsearch-analysis-ik-7.13.2.zip-&gt; Downloading https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.13.2/elasticsearch-analysis-ik-7.13.2.zip[=================================================] 100%@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: plugin requires additional permissions @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@* java.net.SocketPermission * connect,resolveSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.htmlfor descriptions of what these permissions allow and the associated risks.Continue with installation? [y/N]y-&gt; Installed analysis-ik-&gt; Please restart Elasticsearch to activate any plugins installedshengbinbin@192 7.13.2 % 1./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.13.2/elasticsearch-analysis-ik-7.13.2.zip 4.2.2 测试es 重启成功后，首先创建一个名为 test 的索引： 接下来，在该索引中进行分词测试： 4.2.3 自定义扩展词库4.2.3.1 本地自定义在 es/plugins/ik/config 目录下，新建 ext.dic 文件（文件名任意），在该文件中可以配置自定义的词库。 如果有多个词，换行写入新词即可。 然后在 es/plugins/ik/config/IKAnalyzer.cfg.xml 中配置扩展词典的位置： 4.2.3.2 远程词库也可以配置远程词库，远程词库支持热更新（不用重启 es 就可以生效）。 热更新只需要提供一个接口，接口返回扩展词即可。 具体使用方式如下，新建一个 Spring Boot 项目，引入 Web 依赖即可。然后在 resources/stastic 目录下新建 ext.dic 文件，写入扩展词： 接下来，在 es/plugins/ik/config/IKAnalyzer.cfg.xml 文件中配置远程扩展词接口： 配置完成后，重启 es ，即可生效。 热更新，主要是响应头的 Last-Modified 或者 ETag 字段发生变化，ik 就会自动重新加载远程扩展 索引的基本操作新建索引通过 head 插件新建索引在 head 插件中，选择 索引选项卡，然后点击新建索引。新建索引时，需要填入索引名称、分片数以及副本数。 索引创建成功后，如下图： 0、1、2、3、4 分别表示索引的分片，粗框表示主分片，细框表示副本（点一下框，通过 primary 属性可以查看是主分片还是副本）。 通过 kibana 发送请求创建创建索引请求： 1PUT user 创建成功后，返回如下信息： 123456{ &quot;acknowledged&quot; : true, &quot;shards_acknowledged&quot; : true, &quot;index&quot; : &quot;user&quot;} 查看user这个索引的信息： 123456789101112131415161718192021222324252627282930313233343536373839404142{ &quot;version&quot;: 4, &quot;mapping_version&quot;: 1, &quot;settings_version&quot;: 1, &quot;aliases_version&quot;: 1, &quot;routing_num_shards&quot;: 1024, &quot;state&quot;: &quot;open&quot;, &quot;settings&quot;: { &quot;index&quot;: { &quot;routing&quot;: { &quot;allocation&quot;: { &quot;include&quot;: { &quot;_tier_preference&quot;: &quot;data_content&quot;}}}, &quot;number_of_shards&quot;: &quot;1&quot;, &quot;provided_name&quot;: &quot;user&quot;, &quot;creation_date&quot;: &quot;1625048332811&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;IPeNFtZcQ8OEUfNEVryexg&quot;, &quot;version&quot;: { &quot;created&quot;: &quot;7130299&quot;}}}, &quot;mappings&quot;: { }, &quot;aliases&quot;: [ ], &quot;primary_terms&quot;: { &quot;0&quot;: 1}, &quot;in_sync_allocations&quot;: { &quot;0&quot;: [ &quot;ely2hxs1QdiRcTy0YB9Rmw&quot;]}, &quot;rollover_info&quot;: { }, &quot;system&quot;: false, &quot;timestamp_range&quot;: { &quot;unknown&quot;: true}} 索引的要求 索引名称不能有大写字母 123456789101112131415161718{ &quot;error&quot; : { &quot;root_cause&quot; : [ { &quot;type&quot; : &quot;invalid_index_name_exception&quot;, &quot;reason&quot; : &quot;Invalid index name [uSER], must be lowercase&quot;, &quot;index_uuid&quot; : &quot;_na_&quot;, &quot;index&quot; : &quot;uSER&quot; } ], &quot;type&quot; : &quot;invalid_index_name_exception&quot;, &quot;reason&quot; : &quot;Invalid index name [uSER], must be lowercase&quot;, &quot;index_uuid&quot; : &quot;_na_&quot;, &quot;index&quot; : &quot;uSER&quot; }, &quot;status&quot; : 400} 索引名是唯一的，不能重复，重复创建会出错 1234567891011121314151617{ &quot;error&quot; : { &quot;root_cause&quot; : [ { &quot;type&quot; : &quot;resource_already_exists_exception&quot;, &quot;reason&quot; : &quot;index [user/IPeNFtZcQ8OEUfNEVryexg] already exists&quot;, &quot;index_uuid&quot; : &quot;IPeNFtZcQ8OEUfNEVryexg&quot;, &quot;index&quot; : &quot;user&quot; } ], &quot;type&quot; : &quot;resource_already_exists_exception&quot;, &quot;reason&quot; : &quot;index [user/IPeNFtZcQ8OEUfNEVryexg] already exists&quot;, &quot;index_uuid&quot; : &quot;IPeNFtZcQ8OEUfNEVryexg&quot;, &quot;index&quot; : &quot;user&quot; }, &quot;status&quot; : 400} 更新索引索引创建好之后，可以修改其属性。 修改副本数例如修改索引的副本数： 1234PUT user/_settings{ &quot;number_of_replicas&quot;: &quot;2&quot;} 更新分片数也是一样。 向索引中写入文档1234PUT user/_doc/1{ &quot;title&quot;: &quot;三国演义&quot;} 123456789101112131415161718192021222324{ &quot;_index&quot; : &quot;user&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : { &quot;total&quot; : 3, &quot;successful&quot; : 1, &quot;failed&quot; : 0 }, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1}![](https://i.loli.net/2021/06/30/ZLHmCIfcAMgJvKX.png)### 修改索引的读写权限默认情况下，索引是具备读写权限的，当然这个读写权限可以关闭。例如，关闭索引的写权限： PUT user/_settings{ “blocks.write”: “true”} 123关闭之后，就无法添加文档了。关闭了写权限之后，如果想要再次打开，方式如下： PUT user/_settings{ “blocks.write”: “true”} 123456789101112131415其他类似的权限有：- blocks.write- blocks.read- blocks.read_only## 查看索引信息- head 插件查看方式如下- 请求查看方式如下： GET book/_settings 12345678910111213141516171819202122232425262728​```JSON{ &quot;user&quot; : { &quot;settings&quot; : { &quot;index&quot; : { &quot;routing&quot; : { &quot;allocation&quot; : { &quot;include&quot; : { &quot;_tier_preference&quot; : &quot;data_content&quot; } } }, &quot;number_of_shards&quot; : &quot;1&quot;, &quot;blocks&quot; : { &quot;write&quot; : &quot;false&quot; }, &quot;provided_name&quot; : &quot;user&quot;, &quot;creation_date&quot; : &quot;1625048332811&quot;, &quot;number_of_replicas&quot; : &quot;2&quot;, &quot;uuid&quot; : &quot;IPeNFtZcQ8OEUfNEVryexg&quot;, &quot;version&quot; : { &quot;created&quot; : &quot;7130299&quot; } } } }} 也可以同时查看多个索引信息： 1GET user,test/_settings 也可以查看所有索引信息： 1GET _all/_settings 删除索引 head 插件可以删除索引： 请求删除如下： 1DELETE test 删除一个不存在的索引会报错。 索引打开/关闭关闭索引： 1POST book/_close 打开索引： 1POST book/_open 当然，可以同时关闭/打开多个索引，多个索引用 , 隔开，或者直接使用 _all 代表所有索引。 复制索引索引复制，只会复制数据，不会复制索引配置。 12345POST _reindex{ &quot;source&quot;: {&quot;index&quot;:&quot;book&quot;}, &quot;dest&quot;: {&quot;index&quot;:&quot;book_new&quot;}} 复制的时候，可以添加查询条件。 索引别名可以为索引创建别名，如果这个别名是唯一的，该别名可以代替索引名称。 1234567891011POST /_aliases{ &quot;actions&quot;: [ { &quot;add&quot;: { &quot;index&quot;: &quot;book&quot;, &quot;alias&quot;: &quot;book_alias&quot; } } ]} 将 add 改为 remove 就表示移除别名： 1234567891011POST /_aliases{ &quot;actions&quot;: [ { &quot;remove&quot;: { &quot;index&quot;: &quot;book&quot;, &quot;alias&quot;: &quot;book_alias&quot; } } ]} 查看某一个索引的别名： 1GET /book/_alias 查看某一个别名对应的索引（book_alias 表示一个别名）： 1GET /book_alias/_alias 可以查看集群上所有可用别名： 1GET /_alias 文档的基本操作新建文档新建索引blog，在添加文档： 123456PUT blog/_doc/1{ &quot;title&quot;:&quot;文档基本操作&quot;, &quot;date&quot;:&quot;2021-06-30&quot;, &quot;content&quot;:&quot;添加的文档的基本操作&quot;} 123456789101112131415{ &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, //如果更新文档，版本会自动 + 1 &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : { //分片信息 &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 }, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1} 添加文档如果不指定id，系统会默认给一个id，但是需要修改成post请求： 123456POST blog/_doc{ &quot;title&quot;:&quot;666&quot;, &quot;date&quot;:&quot;2021-06-30&quot;, &quot;content&quot;:&quot;添加的文档的基本操作&quot;} 1234567891011121314{ &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1hrCXHoBjgQwlk7iYnof&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : { &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 }, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1} 查询文档get api： 1GET blog/_doc/1 1234567891011121314{ &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : { &quot;title&quot; : &quot;文档基本操作&quot;, &quot;date&quot; : &quot;2021-06-30&quot;, &quot;content&quot; : &quot;添加的文档的基本操作&quot; }} 批量查询文档： 1234GET blog/_mget{ &quot;ids&quot;:[&quot;1&quot;,&quot;1hrCXHoBjgQwlk7iYnof&quot;]} 为什么这里的get请求可以携带请求体呢？ 某些特定的语言比如JavaScript的http请求中不允许get请求有请求体，但是在RFC7231文档中，并没有规定GET请求的请求体如何处理。es为了保证兼容性，get和post都可以 更新文档 文档更新1次，version就+1 1234PUT blog/_doc/1hrCXHoBjgQwlk7iYnof{ &quot;title&quot;:&quot;777&quot;} 注 ： 这种方式更新的文档会覆盖掉原文档 大多数时候我们只是想更新文档中的字段： 12345678910111213141516171819POST blog/_update/1{ &quot;script&quot;: { &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;ctx._source.title=params.title&quot;, &quot;params&quot;: { &quot;title&quot;:&quot;666666&quot; } }}更新的请求格式：POST {index}/_update/{id}{ lang 表示脚本语言 painless 是es中内置的一种脚本语言 source 表示具体执行的脚本 ctx 是一个上下文对象，通过ctx可以访问到_source、_title等} 向文档中添加tag字段： 1234567POST blog/_update/1{ &quot;script&quot;: { &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;ctx._source.tag=[\\&quot;java\\&quot;,\\&quot;php\\&quot;]&quot; }} 1234567POST blog/_update/1{ &quot;script&quot;: { &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;ctx._source.tag.add(\\&quot;js\\&quot;)&quot; }} 用if、else语句：如果tags里面含有Java，就删除这个文档。 1234567POST blog/_update/1{ &quot;script&quot;: { &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;if(ctx._source.tag.contains(\\&quot;java\\&quot;)){ ctx.op = \\&quot;delete\\&quot;}else{ctx.op=\\&quot;none\\&quot;}&quot; }} 查询更新通过条件查询到文档，然后再去更新 12345678910111213讲title中包含666的文档的内容修改为888POST blog/_update_by_query{ &quot;script&quot;: { &quot;source&quot;: &quot;ctx._source.content=\\&quot;888\\&quot;&quot;, &quot;lang&quot;: &quot;painless&quot; }, &quot;query&quot;: { &quot;term&quot;: { &quot;title&quot;:&quot;777&quot; } }} 删除文档1DELETE blog/_doc/1hrCXHoBjgQwlk7iYnof 查询删除123456789删除 title 中包含 666 的文档POST blog/_delete_by_query{ &quot;query&quot;:{ &quot;term&quot;:{ &quot;title&quot;:&quot;666&quot; } }} 123456789删除某一个索引下的所有文档POST blog/_delete_by_query{ &quot;query&quot;:{ &quot;match_all&quot;:{ } }} 批量操作es 中通过 Bulk API 可以执行批量索引、批量删除、批量更新等操作。 首先需要将所有的批量操作写入一个 JSON 文件中，然后通过 POST 请求将该 JSON 文件上传并执行。 1curl -XPOST &quot;http://localhost:9200/user/_bulk&quot; -H &quot;content-type:application/json&quot; --data-binary @aaa.json 执行完成后，就会创建一个名为 user 的索引，同时向该索引中添加一条记录，再修改该记录，最终结果如下： 文档路由你的数据到底存在哪一个分片上？ 1GET _cat/shards/blog?v 可以查看文档在哪个分片上 1234567891011index shard prirep state docs store ip nodeblog 4 p STARTED 0 7.9kb 127.0.0.1 192.168.0.107blog 4 r UNASSIGNED blog 1 p STARTED 2 8.3kb 127.0.0.1 192.168.0.107blog 1 r UNASSIGNED blog 2 p STARTED 1 3.6kb 127.0.0.1 192.168.0.107blog 2 r UNASSIGNED blog 3 p STARTED 1 3.6kb 127.0.0.1 192.168.0.107blog 3 r UNASSIGNED blog 0 p STARTED 1 3.6kb 127.0.0.1 192.168.0.107blog 0 r UNASSIGNED es 中的路由机制是通过哈希算法，将具有相同哈希值的文档放到一个主分片中，分片位置的计算方式如下： shard=hash(routing) % number_of_primary_shards routing 可以是一个任意字符串，es 默认是将文档的 id 作为 routing 值，通过哈希函数根据 routing 生成一个数字，然后将该数字和分片数取余，取余的结果就是分片的位置。 默认的这种路由模式，最大的优势在于负载均衡，这种方式可以保证数据平均分配在不同的分片上。但是他有一个很大的劣势，就是查询时候无法确定文档的位置，此时它会将请求广播到所有的分片上去执行。另一方面，使用默认的路由模式，后期修改分片数量不方便。 然开发者也可以自定义 routing 的值，方式如下： 1234PUT blog/_doc/d?routing=javaboy{ &quot;title&quot;:&quot;d&quot;} 如果文档在添加时指定了 routing，则查询、删除、更新时也需要指定 routing。 1GET blog/_doc/d?routing=javaboy 自定义 routing 有可能会导致负载不均衡，这个还是要结合实际情况选择。 典型场景： 对于用户数据，我们可以将 userid 作为 routing，这样就能保证同一个用户的数据保存在同一个分片中，检索时，同样使用 userid 作为 routing，这样就可以精准的从某一个分片中获取数据。 ES的版本控制当我们使用 es 的 API 去进行文档更新时，它首先读取原文档出来，然后对原文档进行更新，更新完成后再重新索引整个文档。不论你执行多少次更新，最终保存在 es 中的是最后一次更新的文档。但是如果有两个线程同时去更新，就有可能出问题。 要解决问题，就是锁。在 es 中，实际上使用的就是乐观锁。 版本控制es6.7之前 在 es6.7 之前，使用 version+version_type 来进行乐观并发控制。根据前面的介绍，文档每被修改一个，version 就会自增一次，es 通过 version 字段来确保所有的操作都有序进行。 version 分为内部版本控制和外部版本控制。 内部版本es 自己维护的就是内部版本，当创建一个文档时，es 会给文档的版本赋值为 1。 每当用户修改一次文档，版本号就回自增 1。 如果使用内部版本，es 要求 version 参数的值必须和 es 文档中 version 的值相当，才能操作成功。 外部版本也可以维护外部版本。 在添加文档时，就指定版本号： 1234PUT blog/_doc/1?version=200&amp;version_type=external{ &quot;title&quot;:&quot;2222&quot;} 以后更新的时候，版本要大于已有的版本号。 vertion_type=external 或者 vertion_type=external_gt 表示以后更新的时候，版本要大于已有的版本号。 vertion_type=external_gte 表示以后更新的时候，版本要大于等于已有的版本号。 最新方案（Es6.7 之后）现在使用 if_seq_no 和 if_primary_term 两个参数来做并发控制。 seq_no 不属于某一个文档，它是属于整个索引的（version 则是属于某一个文档的，每个文档的 version 互不影响）。现在更新文档时，使用 seq_no 来做并发。由于 seq_no 是属于整个 index 的，所以任何文档的修改或者新增，seq_no 都会自增。 现在就可以通过 seq_no 和 primary_term 来做乐观并发控制。 1234PUT blog/_doc/2?if_seq_no=5&amp;if_primary_term=1{ &quot;title&quot;:&quot;6666&quot;} 倒排索引倒排索引是 es 中非常重要的索引结构，是从文档词项到文档 ID 的一个映射过程。 “正排索引”我们在关系型数据库中见到的索引，就是“正排索引”。 关系型数据库中的索引如下，假设我有一个博客表： id 作者 标题 内容 1 binshow 哈哈哈哈 xxxx 2 zkd 嘻嘻嘻嘻 xxxxx 我们可以针对这个表建立索引（正排索引）： 索引 内容 1 xxxx 2 xxxxx 哈哈哈哈 xxxx 嘻嘻嘻嘻 xxxxx 当我们通过 id 或者标题去搜索文章时，就可以快速搜到。 但是如果我们按照文章内容的关键字去搜索，就只能去内容中做字符匹配了。为了提高查询效率，就要考虑使用倒排索引。 倒排索引倒排索引就是以内容的关键字建立索引，通过索引找到文档 id，再进而找到整个文档。 索引 文档id=1 文档id=2 Java ✅ es ✅ ✅ 索引 ✅ php ✅ 一般来说，倒排索引分为两个部分： 单词词典（记录所有的文档词项，以及词项到倒排列表的关联关系） 倒排列表（记录单词与对应的关系，由一系列倒排索引项组成，倒排索引项指：文档 id、词频（TF）（词项在文档中出现的次数，评分时使用）、位置（Position，词项在文档中分词的位置）、偏移（记录词项开始和结束的位置）） 当我们去索引一个文档时，就回建立倒排索引，搜索时，直接根据倒排索引搜索。","link":"/2021/06/29/ElasticSearch%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E6%88%98/"},{"title":"ElasticSearch从入门到实战（二）映射字段和查询","text":"本篇讲述了； 映射Mapping 字段类型 映射参数 基础查询 复合查询 映射Mapping映射就是 Mapping，它用来定义一个文档以及文档所包含的字段该如何被存储和索引。所以，它其实有点类似于关系型数据库中表的定义。 映射分类动态映射顾名思义，就是自动创建出来的映射。es 根据存入的文档，自动分析出来文档中字段的类型以及存储方式，这种就是动态映射。 举一个简单例子，新建一个索引，然后查看索引信息： 1PUT blog 123456789101112131415161718192021222324252627282930313233343536373839404142{ &quot;version&quot;: 4, &quot;mapping_version&quot;: 1, &quot;settings_version&quot;: 1, &quot;aliases_version&quot;: 1, &quot;routing_num_shards&quot;: 1024, &quot;state&quot;: &quot;open&quot;, &quot;settings&quot;: { &quot;index&quot;: { &quot;routing&quot;: { &quot;allocation&quot;: { &quot;include&quot;: { &quot;_tier_preference&quot;: &quot;data_content&quot;}}}, &quot;number_of_shards&quot;: &quot;1&quot;, &quot;provided_name&quot;: &quot;blog&quot;, &quot;creation_date&quot;: &quot;1625109435679&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;SMZxI91HQMahoEudlSV92Q&quot;, &quot;version&quot;: { &quot;created&quot;: &quot;7130299&quot;}}}, &quot;mappings&quot;: { }, //mappings 为空，这个 mappings 中保存的就是映射信息。 &quot;aliases&quot;: [ ], &quot;primary_terms&quot;: { &quot;0&quot;: 1 }, &quot;in_sync_allocations&quot;: { &quot;0&quot;: [ &quot;L6J-WIOnQTms9nQm7NRhWw&quot; ]}, &quot;rollover_info&quot;: { }, &quot;system&quot;: false, &quot;timestamp_range&quot;: { &quot;unknown&quot;: true }} 现在我们向索引中添加一个文档，如下： 12345PUT blog/_doc/1{ &quot;title&quot;:&quot;1111&quot;, &quot;date&quot;:&quot;2021-07-01&quot;} 文档添加成功后，就会自动生成 Mappings： 12345678910111213141516171819&quot;mappings&quot;: { &quot;_doc&quot;: { &quot;properties&quot;: { &quot;date&quot;: { //date 字段的类型为 date &quot;type&quot;: &quot;date&quot; }, &quot;title&quot;: { //title 的类型有两个，text 和 keyword &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: { &quot;keyword&quot;: { &quot;ignore_above&quot;: 256, &quot;type&quot;: &quot;keyword&quot; } }}}} 可以看到，date 字段的类型为 date，title 的类型有两个，text 和 keyword。 默认情况下，文档中如果新增了字段，mappings 中也会自动新增进来。 有的时候，如果希望新增字段时，能够抛出异常来提醒开发者，这个可以通过 mappings 中 dynamic 属性来配置。 dynamic 属性有三种取值： true，默认即此。自动添加新字段。 false，忽略新字段。 strict，严格模式，发现新字段会抛出异常。 具体配置方式如下，创建索引时指定 mappings（这其实就是静态映射）： 1234567891011121314PUT blog{ &quot;mappings&quot;: { &quot;dynamic&quot;:&quot;strict&quot;, &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;: &quot;text&quot; }, &quot;age&quot;:{ &quot;type&quot;:&quot;long&quot; } } }} 然后向 blog 中索引中添加数据： 123456PUT blog/_doc/2{ &quot;title&quot;:&quot;1111&quot;, &quot;date&quot;:&quot;2020-11-11&quot;, &quot;age&quot;:99} 在添加的文档中，多出了一个 date 字段，而该字段没有预定义，所以这个添加操作就回报错： 12345678910111213{ &quot;error&quot; : { &quot;root_cause&quot; : [ { &quot;type&quot; : &quot;strict_dynamic_mapping_exception&quot;, &quot;reason&quot; : &quot;mapping set to strict, dynamic introduction of [date] within [_doc] is not allowed&quot; } ], &quot;type&quot; : &quot;strict_dynamic_mapping_exception&quot;, &quot;reason&quot; : &quot;mapping set to strict, dynamic introduction of [date] within [_doc] is not allowed&quot; }, &quot;status&quot; : 400} 动态映射还有一个日期检测的问题。 例如新建一个索引，然后添加一个含有日期的文档，如下： 1234PUT blog/_doc/1{ &quot;remark&quot;:&quot;2020-11-11&quot;} 添加成功后，remark 字段会被推断是一个日期类型。 12345678&quot;mappings&quot;: { &quot;_doc&quot;: { &quot;properties&quot;: { &quot;remark&quot;: { &quot;type&quot;: &quot;date&quot; } }} 此时，remark 字段就无法存储其他类型了。 1234PUT blog/_doc/1{ &quot;remark&quot;:&quot;javaboy&quot;} 此时报错如下： 123456789101112131415161718192021{ &quot;error&quot; : { &quot;root_cause&quot; : [ { &quot;type&quot; : &quot;mapper_parsing_exception&quot;, &quot;reason&quot; : &quot;failed to parse field [remark] of type [date] in document with id '1'. Preview of field's value: 'javaboy'&quot; } ], &quot;type&quot; : &quot;mapper_parsing_exception&quot;, &quot;reason&quot; : &quot;failed to parse field [remark] of type [date] in document with id '1'. Preview of field's value: 'javaboy'&quot;, &quot;caused_by&quot; : { &quot;type&quot; : &quot;illegal_argument_exception&quot;, &quot;reason&quot; : &quot;failed to parse date field [javaboy] with format [strict_date_optional_time||epoch_millis]&quot;, &quot;caused_by&quot; : { &quot;type&quot; : &quot;date_time_parse_exception&quot;, &quot;reason&quot; : &quot;Failed to parse with all enclosed parsers&quot; } } }, &quot;status&quot; : 400} 要解决这个问题，可以使用静态映射，即在索引定义时，将 remark 指定为 text 类型。也可以关闭日期检测。 123456PUT blog{ &quot;mappings&quot;: { &quot;date_detection&quot;: false }} 此时日期类型就回当成文本来处理。 静态映射 略。 类型推断es 中动态映射类型推断方式如下： JSON中的数据 自动推断出来的数据类型 null 没有字段被添加 true、false boolean 浮点数字 float 数字 long json对象 object 数组 数组中的第一个非空值来决定 string text、keyword、date、double、long都有可能 字段类型核心类型字符串类型 string：这是一个已经过期的字符串类型。在 es5 之前，用这个来描述字符串，现在的话，它已经被 text 和 keyword 替代了。 text：如果一个字段是要被全文检索的，比如说博客内容、新闻内容、产品描述，那么可以使用 text。用了 text 之后，字段内容会被分析，在生成倒排索引之前，字符串会被分词器分成一个个词项。text 类型的字段不用于排序，很少用于聚合。这种字符串也被称为 analyzed 字段。 keyword：这种类型适用于结构化的字段，例如标签、email 地址、手机号码等等，这种类型的字段可以用作过滤、排序、聚合等。这种字符串也称之为 not-analyzed 字段。 数字类型 在满足需求的情况下，优先使用范围小的字段。字段长度越短，索引和搜索的效率越高。 浮点数，优先考虑使用 scaled_float。 scaled_float 举例： 1234567891011121314PUT product{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;text&quot; }, &quot;price&quot;:{ &quot;type&quot;: &quot;scaled_float&quot;, &quot;scaling_factor&quot;: 100 } } }} 日期类型由于 JSON 中没有日期类型，所以 es 中的日期类型形式就比较多样： 2020-11-11 或者 2020-11-11 11:11:11 一个从 1970.1.1 零点到现在的一个秒数或者毫秒数。 es 内部将时间转为 UTC，然后将时间按照 millseconds-since-the-epoch 的长整型来存储。 自定义日期类型： 12345678910PUT product{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;date&quot;:{ &quot;type&quot;: &quot;date&quot; } } }} 这个能够解析出来的时间格式比较多。 123456789101112131415PUT product/_doc/1{ &quot;date&quot;:&quot;2020-11-11&quot;}PUT product/_doc/2{ &quot;date&quot;:&quot;2020-11-11T11:11:11Z&quot;}PUT product/_doc/3{ &quot;date&quot;:&quot;1604672099958&quot;} 上面三个文档中的日期都可以被解析，内部存储的是毫秒计时的长整型数。 布尔类型（boolean）JSON 中的 “true”、“false”、true、false 都可以。 二进制类型（binary）二进制接受的是 base64 编码的字符串，默认不存储，也不可搜索。 范围类型 integer_range float_range long_range double_range date_range ip_range 定义的时候，指定范围类型即可： 12345678910111213PUT product{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;date&quot;:{ &quot;type&quot;: &quot;date&quot; }, &quot;price&quot;:{ &quot;type&quot;:&quot;float_range&quot; } } }} 插入文档的时候，需要指定范围的界限： 12345678910111213PUT product{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;date&quot;:{ &quot;type&quot;: &quot;date&quot; }, &quot;price&quot;:{ &quot;type&quot;:&quot;float_range&quot; } } }} 指定范围的时，可以使用 gt、gte、lt、lte。 复合类型数组类型es 中没有专门的数组类型。默认情况下，任何字段都可以有一个或者多个值。需要注意的是，数组中的元素必须是同一种类型。 添加数组是，数组中的第一个元素决定了整个数组的类型。 对象类型（object）由于 JSON 本身具有层级关系，所以文档包含内部对象。内部对象中，还可以再包含内部对象。 1234567PUT product/_doc/2{ &quot;date&quot;:&quot;2020-11-11T11:11:11Z&quot;, &quot;ext_info&quot;:{ &quot;address&quot;:&quot;China&quot; }} 嵌套类型（nested）nested 是 object 中的一个特例。 如果使用 object 类型，假如有如下一个文档： 123456789101112{ &quot;user&quot;:[ { &quot;first&quot;:&quot;Zhang&quot;, &quot;last&quot;:&quot;san&quot; }, { &quot;first&quot;:&quot;Li&quot;, &quot;last&quot;:&quot;si&quot; } ]} 由于 Lucene 没有内部对象的概念，所以 es 会将对象层次扁平化，将一个对象转为字段名和值构成的简单列表。即上面的文档，最终存储形式如下： 1234{&quot;user.first&quot;:[&quot;Zhang&quot;,&quot;Li&quot;],&quot;user.last&quot;:[&quot;san&quot;,&quot;si&quot;]} 扁平化之后，用户名之间的关系没了。这样会导致如果搜索 Zhang si 这个人，会搜索到。 此时可以 nested 类型来解决问题，nested 对象类型可以保持数组中每个对象的独立性。nested 类型将数组中的每一饿对象作为独立隐藏文档来索引，这样每一个嵌套对象都可以独立被索引。 123456789{{&quot;user.first&quot;:&quot;Zhang&quot;,&quot;user.last&quot;:&quot;san&quot;},{&quot;user.first&quot;:&quot;Li&quot;,&quot;user.last&quot;:&quot;si&quot;}} 优点 文档存储在一起，读取性能高。 缺点 更新父或者子文档时需要更新更个文档。 地理类型使用场景： 查找某一个范围内的地理位置 通过地理位置或者相对中心点的距离来聚合文档 把距离整个到文档的评分中 通过距离对文档进行排序 geo_pointgeo_point 就是一个坐标点，定义方式如下： 12345678910PUT people{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;location&quot;:{ &quot;type&quot;: &quot;geo_point&quot; } } }} 创建时指定字段类型，存储的时候，有四种方式： 12345678910111213141516171819202122PUT people/_doc/1{ &quot;location&quot;:{ &quot;lat&quot;: 34.27, &quot;lon&quot;: 108.94 }}PUT people/_doc/2{ &quot;location&quot;:&quot;34.27,108.94&quot;}PUT people/_doc/3{ &quot;location&quot;:&quot;uzbrgzfxuzup&quot;}PUT people/_doc/4{ &quot;location&quot;:[108.94,34.27]} 注意，使用数组描述，先经度后纬度。 地址位置转 geo_hash：http://www.csxgame.top/#/ geo_shape 指定 geo_shape 类型： 12345678910PUT people{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;location&quot;:{ &quot;type&quot;: &quot;geo_shape&quot; } } }} 添加文档时需要指定具体的类型： 1234567PUT people/_doc/1{ &quot;location&quot;:{ &quot;type&quot;:&quot;point&quot;, &quot;coordinates&quot;: [108.94,34.27] }} 如果是 linestring，如下： 1234567PUT people/_doc/2{ &quot;location&quot;:{ &quot;type&quot;:&quot;linestring&quot;, &quot;coordinates&quot;: [[108.94,34.27],[100,33]] }} 特殊类型IP存储 IP 地址，类型是 ip： 12345678910PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;address&quot;:{ &quot;type&quot;: &quot;ip&quot; } } }} 添加文档： 1234PUT blog/_doc/1{ &quot;address&quot;:&quot;192.168.91.1&quot;} 搜索文档： 12345678GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;address&quot;: &quot;192.168.0.0/16&quot; } }} token_count用于统计字符串分词后的词项个数。 12345678910111213141516PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: { &quot;length&quot;:{ &quot;type&quot;:&quot;token_count&quot;, &quot;analyzer&quot;:&quot;standard&quot; } } } } }} 相当于新增了 title.length 字段用来统计分词后词项的个数。 添加文档： 1234PUT blog/_doc/1{ &quot;title&quot;:&quot;zhang san&quot;} 可以通过 token_count 去查询： 12345678GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;title.length&quot;: 2 } }} 24种映射参数analyzer定义文本字段的分词器。默认对索引和查询都是有效的。 假设不用分词器，我们先来看一下索引的结果，创建一个索引并添加一个文档： 123456PUT blogPUT blog/_doc/1{ &quot;title&quot;:&quot;定义文本字段&quot;} 查看词条向量（term vectors） 1234GET blog/_termvectors/1{ &quot;fields&quot;: [&quot;title&quot;]} 查看结果如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879{ &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 2, &quot;found&quot; : true, &quot;took&quot; : 19, &quot;term_vectors&quot; : { &quot;title&quot; : { &quot;field_statistics&quot; : { &quot;sum_doc_freq&quot; : 16, &quot;doc_count&quot; : 2, &quot;sum_ttf&quot; : 16 }, &quot;terms&quot; : { &quot;义&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 1, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2 } ] }, &quot;字&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 4, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 5 } ] }, &quot;定&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 0, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1 } ] }, &quot;文&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 2, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3 } ] }, &quot;本&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 3, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 4 } ] }, &quot;段&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 5, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 6 } ] } } } }} 可以看到，默认情况下，中文就是一个字一个字的分，这种分词方式没有任何意义。如果这样分词，查询就只能按照一个字一个字来查，像下面这样： 12345678GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;title&quot;: &quot;定&quot; } }} 无意义！！！ 所以，我们要根据实际情况，配置合适的分词器。 给字段设定分词器： 1234567891011PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot; } } }} 存储文档： 1234PUT blog/_doc/1{ &quot;title&quot;:&quot;定义文本字段的分词器。&quot;} 查看词条向量： 1234GET blog/_termvectors/1{ &quot;fields&quot;: [&quot;title&quot;]} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869{ &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;found&quot; : true, &quot;took&quot; : 2, &quot;term_vectors&quot; : { &quot;title&quot; : { &quot;field_statistics&quot; : { &quot;sum_doc_freq&quot; : 5, &quot;doc_count&quot; : 1, &quot;sum_ttf&quot; : 5 }, &quot;terms&quot; : { &quot;分词器&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 4, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 10 } ] }, &quot;字段&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 2, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 6 } ] }, &quot;定义&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 0, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2 } ] }, &quot;文本&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 1, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 4 } ] }, &quot;的&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 3, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 7 } ] } } } }} 然后就可以通过词去搜索了： 12345678GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;title&quot;: &quot;索引&quot; } }} search_analyzer查询时候的分词器。默认情况下，如果没有配置 search_analyzer，则查询时，首先查看有没有 search_analyzer，有的话，就用 search_analyzer 来进行分词，如果没有，则看有没有 analyzer，如果有，则用 analyzer 来进行分词，否则使用 es 默认的分词器。 normalizernormalizer 参数用于解析前（索引或者查询）的标准化配置。 比如，在 es 中，对于一些我们不想切分的字符串，我们通常会将其设置为 keyword，搜索时候也是使用整个词进行搜索。如果在索引前没有做好数据清洗，导致大小写不一致，例如 javaboy 和 JAVABOY，此时，我们就可以使用 normalizer 在索引之前以及查询之前进行文档的标准化。 先来一个反例，创建一个名为 blog 的索引，设置 author 字段类型为 keyword： 12345678910PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;author&quot;:{ &quot;type&quot;: &quot;keyword&quot; } } }} 添加两个文档： 123456789PUT blog/_doc/1{ &quot;author&quot;:&quot;javaboy&quot;}PUT blog/_doc/2{ &quot;author&quot;:&quot;JAVABOY&quot;} 然后进行搜索： 12345678GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;author&quot;: &quot;JAVABOY&quot; } }} 大写关键字可以搜到大写的文档，小写关键字可以搜到小写的文档。 如果使用了 normalizer，可以在索引和查询时，分别对文档进行预处理。 normalizer 定义方式如下： 123456789101112131415161718192021PUT blog{ &quot;settings&quot;: { &quot;analysis&quot;: { &quot;normalizer&quot;:{ &quot;my_normalizer&quot;:{ &quot;type&quot;:&quot;custom&quot;, &quot;filter&quot;:[&quot;lowercase&quot;] } } } }, &quot;mappings&quot;: { &quot;properties&quot;: { &quot;author&quot;:{ &quot;type&quot;: &quot;keyword&quot;, &quot;normalizer&quot;:&quot;my_normalizer&quot; } } }} 在 settings 中定义 normalizer，然后在 mappings 中引用。 测试方式和前面一致。此时查询的时候，大写关键字也可以查询到小写文档，因为无论是索引还是查询，都会将大写转为小写。 boostboost 参数可以设置字段的权重。 boost 有两种使用思路，一种就是在定义 mappings 的时候使用，在指定字段类型时使用；另一种就是在查询时使用。 实际开发中建议使用后者，前者有问题：如果不重新索引文档，权重无法修改。 mapping 中使用 boost（不推荐）： 1234567891011PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;content&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;boost&quot;: 2 } } }} 另一种方式就是在查询的时候，指定 boost 1234567891011GET blog/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;content&quot;: { &quot;query&quot;: &quot;你好&quot;, &quot;boost&quot;: 2 } } }} coercecoerce 用来清除脏数据，默认为 true。 例如一个数字，在 JSON 中，用户可能写错了： 1{&quot;age&quot;:&quot;99&quot;} 或者 ： 1{&quot;age&quot;:&quot;99.0&quot;} 这些都不是正确的数字格式。 通过 coerce 可以解决该问题。 默认情况下，以下操作没问题，就是 coerce 起作用： 123456789101112131415PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;age&quot;:{ &quot;type&quot;: &quot;integer&quot; } } }}POST blog/_doc{ &quot;age&quot;:&quot;99.0&quot;} 如果需要修改 coerce ，方式如下： 12345678910111213141516PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;age&quot;:{ &quot;type&quot;: &quot;integer&quot;, &quot;coerce&quot;: false } } }}POST blog/_doc{ &quot;age&quot;:99} 当 coerce 修改为 false 之后，数字就只能是数字了，不可以是字符串，该字段传入字符串会报错。 copy_to这个属性，可以将多个字段的值，复制到同一个字段中。 定义方式如下： 123456789101112131415161718192021222324252627282930313233PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;copy_to&quot;: &quot;full_content&quot; }, &quot;content&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;copy_to&quot;: &quot;full_content&quot; }, &quot;full_content&quot;:{ &quot;type&quot;: &quot;text&quot; } } }}PUT blog/_doc/1{ &quot;title&quot;:&quot;你好江南一点雨&quot;, &quot;content&quot;:&quot;当 coerce 修改为 false 之后，数字就只能是数字了，不可以是字符串，该字段传入字符串会报错。&quot;}GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;full_content&quot;: &quot;当&quot; } }} doc_values 和 fielddataes 中的搜索主要是用到倒排索引，doc_values 参数是为了加快排序、聚合操作而生的。当建立倒排索引的时候，会额外增加列式存储映射。 doc_values 默认是开启的，如果确定某个字段不需要排序或者不需要聚合，那么可以关闭 doc_values。 大部分的字段在索引时都会生成 doc_values，除了 text。text 字段在查询时会生成一个 fielddata 的数据结构，fieldata 在字段首次被聚合、排序的时候生成。 doc_values 默认开启，fielddata 默认关闭。 doc_values 演示： 1234567891011121314151617181920212223242526272829303132333435PUT usersPUT users/_doc/1{ &quot;age&quot;:100}PUT users/_doc/2{ &quot;age&quot;:99}PUT users/_doc/3{ &quot;age&quot;:98}PUT users/_doc/4{ &quot;age&quot;:101}GET users/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;sort&quot;:[ { &quot;age&quot;:{ &quot;order&quot;: &quot;desc&quot; } } ]} 由于 doc_values 默认时开启的，所以可以直接使用该字段排序，如果想关闭 doc_values ，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445PUT users{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;age&quot;:{ &quot;type&quot;: &quot;integer&quot;, &quot;doc_values&quot;: false } } }}PUT users/_doc/1{ &quot;age&quot;:100}PUT users/_doc/2{ &quot;age&quot;:99}PUT users/_doc/3{ &quot;age&quot;:98}PUT users/_doc/4{ &quot;age&quot;:101}GET users/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;sort&quot;:[ { &quot;age&quot;:{ &quot;order&quot;: &quot;desc&quot; } } ]} dynamicenabledes 默认会索引所有的字段，但是有的字段可能只需要存储，不需要索引。此时可以通过 enabled 字段来控制： 123456789101112131415161718192021222324PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;enabled&quot;: false } } }}PUT blog/_doc/1{ &quot;title&quot;:&quot;javaboy&quot;}GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;title&quot;: &quot;javaboy&quot; } }} 设置了 enabled 为 false 之后，就可以再通过该字段进行搜索了。 format日期格式。format 可以规范日期格式，而且一次可以定义多个 format。 123456789101112131415161718192021PUT users{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;birthday&quot;:{ &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd||yyyy-MM-dd HH:mm:ss&quot; } } }}PUT users/_doc/1{ &quot;birthday&quot;:&quot;2020-11-11&quot;}PUT users/_doc/2{ &quot;birthday&quot;:&quot;2020-11-11 11:11:11&quot;} 多个日期格式之间，使用 || 符号连接，注意没有空格。 如果用户没有指定日期的 format，默认的日期格式是 strict_date_optional_time||epoch_mills 另外，所有的日期格式，可以在 https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html 网址查看。 ignore_aboveigbore_above 用于指定分词和索引的字符串最大长度，超过最大长度的话，该字段将不会被索引，这个字段只适用于 keyword 类型。 123456789101112131415161718192021222324252627282930PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 10 } } }}PUT blog/_doc/1{ &quot;title&quot;:&quot;javaboy&quot;}PUT blog/_doc/2{ &quot;title&quot;:&quot;javaboyjavaboyjavaboy&quot;}GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;title&quot;: &quot;javaboyjavaboyjavaboy&quot; } }} ignore_malformedignore_malformed 可以忽略不规则的数据，该参数默认为 false。 12345678910111213141516171819202122232425262728293031323334PUT users{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;birthday&quot;:{ &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd||yyyy-MM-dd HH:mm:ss&quot; }, &quot;age&quot;:{ &quot;type&quot;: &quot;integer&quot;, &quot;ignore_malformed&quot;: true } } }}PUT users/_doc/1{ &quot;birthday&quot;:&quot;2020-11-11&quot;, &quot;age&quot;:99}PUT users/_doc/2{ &quot;birthday&quot;:&quot;2020-11-11 11:11:11&quot;, &quot;age&quot;:&quot;abc&quot;}PUT users/_doc/2{ &quot;birthday&quot;:&quot;2020-11-11 11:11:11aaa&quot;, &quot;age&quot;:&quot;abc&quot;} include_in_all这个是针对 _all 字段的，但是在 es7 中，该字段已经被废弃了。 indexindex 属性指定一个字段是否被索引，该属性为 true 表示字段被索引，false 表示字段不被索引。 12345678910111213141516171819202122232425PUT users{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;age&quot;:{ &quot;type&quot;: &quot;integer&quot;, &quot;index&quot;: false } } }}PUT users/_doc/1{ &quot;age&quot;:99}GET users/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;age&quot;: 99 } }} 如果 index 为 false，则不能通过对应的字段搜索。 index_optionsindex_options 控制索引时哪些信息被存储到倒排索引中（用在 text 字段中），有四种取值： normsnorms 对字段评分有用，text 默认开启 norms，如果不是特别需要，不要开启 norms。 null_value在 es 中，值为 null 的字段不索引也不可以被搜索，null_value 可以让值为 null 的字段显式的可索引、可搜索： 1234567891011121314151617181920212223242526PUT users{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;keyword&quot;, &quot;null_value&quot;: &quot;javaboy_null&quot; } } }}PUT users/_doc/1{ &quot;name&quot;:null, &quot;age&quot;:99}GET users/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;javaboy_null&quot; } }} position_increment_gap被解析的 text 字段会将 term 的位置考虑进去，目的是为了支持近似查询和短语查询，当我们去索引一个含有多个值的 text 字段时，会在各个值之间添加一个假想的空间，将值隔开，这样就可以有效避免一些无意义的短语匹配，间隙大小通过 position_increment_gap 来控制，默认是 100。 1234567891011121314151617PUT usersPUT users/_doc/1{ &quot;name&quot;:[&quot;zhang san&quot;,&quot;li si&quot;]}GET users/_search{ &quot;query&quot;: { &quot;match_phrase&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;sanli&quot; } } }} sanli 搜索不到，因为两个短语之间有一个假想的空隙，为 100。 1234567891011GET users/_search{ &quot;query&quot;: { &quot;match_phrase&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;san li&quot;, &quot;slop&quot;: 101 } } }} 可以通过 slop 指定空隙大小。 也可以在定义索引的时候，指定空隙： 123456789101112131415161718192021222324252627PUT users{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;position_increment_gap&quot;: 0 } } }}PUT users/_doc/1{ &quot;name&quot;:[&quot;zhang san&quot;,&quot;li si&quot;]}GET users/_search{ &quot;query&quot;: { &quot;match_phrase&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;san li&quot; } } }} propertiessimilaritysimilarity 指定文档的评分模型，默认有三种： store默认情况下，字段会被索引，也可以搜索，但是不会存储，虽然不会被存储的，但是 _source 中有一个字段的备份。如果想将字段存储下来，可以通过配置 store 来实现。 term_vectorsterm_vectors 是通过分词器产生的信息，包括： 一组 terms 每个 term 的位置 term 的首字符/尾字符与原始字符串原点的偏移量 term_vectors 取值： fieldsfields 参数可以让同一字段有多种不同的索引方式。例如： 1234567891011121314151617181920212223242526272829PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: { &quot;raw&quot;:{ &quot;type&quot;:&quot;keyword&quot; } } } } }}PUT blog/_doc/1{ &quot;title&quot;:&quot;javaboy&quot;}GET blog/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;title.raw&quot;: &quot;javaboy&quot; } }} https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-params.html ElasticSearch 搜索入门搜索分为两个过程： 当向索引中保存文档时，默认情况下，es 会保存两份内容，一份是 _source 中的数据，另一份则是通过分词、排序等一系列过程生成的倒排索引文件，倒排索引中保存了词项和文档之间的对应关系。 搜索时，当 es 接收到用户的搜索请求之后，就会去倒排索引中查询，通过的倒排索引中维护的倒排记录表找到关键词对应的文档集合，然后对文档进行评分、排序、高亮等处理，处理完成后返回文档。 搜索数据导入 在江南一点雨微信公众号后台回复 bookdata.json 下载脚本。 创建索引： 1234567891011121314151617181920212223242526272829PUT books{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;publish&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;type&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;author&quot;:{ &quot;type&quot;: &quot;keyword&quot; }, &quot;info&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;price&quot;:{ &quot;type&quot;: &quot;double&quot; } } }} 在下载的json文件所在目录终端下执行如下脚本导入命令： 1curl -XPOST &quot;http://localhost:9200/books/_bulk?pretty&quot; -H &quot;content-type:application/json&quot; --data-binary @bookdata.json 简单搜索查询所有文档： 123456GET books/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }} 查询结果中hits 中就是查询结果，total 是符合查询条件的文档数。 简单搜索可以简写为： 1GET books/_search 简单搜索默认查询 10 条记录。 词项查询term 查询，就是根据词去查询，查询指定字段中包含给定单词的文档，term 查询不被解析，只有搜索的词和文档中的词精确匹配，才会返回文档。应用场景如：人名、地名等等。 查询 name 字段中包含 材料 的文档。 12345678GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;材料&quot; } }} 分页查询默认返回前 10 条数据，es 中也可以像关系型数据库一样，给一个分页参数： 1234567891011GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;十一五&quot; } }, &quot;size&quot;: 5, &quot;from&quot;: 2 //从第几个查询结果开始展示size个结果} 过滤返回字段如果返回的字段比较多，又不需要这么多字段，此时可以指定返回的字段： 1234567891011GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;十一五&quot; } }, &quot;size&quot;: 10, &quot;from&quot;: 10, &quot;_source&quot;: [&quot;name&quot;,&quot;author&quot;] //返回的字段就只有 name 和 author 了。} 最小评分有的文档得分特别低，说明这个文档和我们查询的关键字相关度很低。我们可以设置一个最低分，只有得分超过最低分的文档才会被返回。 12345678910GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;十一五&quot; } }, &quot;_source&quot;: [&quot;name&quot;, &quot;author&quot;], &quot;min_score&quot;:1.75 //得分低于 1.75 的文档将直接被舍弃。} 高亮查询关键字高亮： 123456789101112131415GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;十一五&quot; } }, &quot;min_score&quot;:1.75, &quot;_source&quot;: [&quot;name&quot;,&quot;author&quot;], &quot;highlight&quot;: { &quot;fields&quot;: { &quot;name&quot;: {} } }} ElasticSearch 全文查询match querymatch query 会对查询语句进行分词，分词后，如果查询语句中的任何一个词项被匹配，则文档就会被索引到。 12345678GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;美术计算机&quot; } }} 这个查询首先会对 美术计算机 进行分词，分词之后，再去查询，只要文档中包含一个分词结果，就回返回文档。换句话说，默认词项之间是 OR 的关系，如果想要修改，也可以改为 AND。 1234567891011GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;美术计算机&quot;, &quot;operator&quot;: &quot;and&quot; } } }} 此时就回要求文档中必须同时包含 美术 和 计算机 两个词。 match_phrase querymatch_phrase query 也会对查询的关键字进行分词，但是它分词后有两个特点： 分词后的词项顺序必须和文档中词项的顺序一致 所有的词都必须出现在文档中 示例如下： 1234567891011GET books/_search{ &quot;query&quot;: { &quot;match_phrase&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;十一五计算机&quot;, &quot;slop&quot;: 7 } } }} query 是查询的关键字，会被分词器进行分解，分解之后去倒排索引中进行匹配。 slop 是指关键字之间的最小距离，但是注意不是关键之间间隔的字数。文档中的字段被分词器解析之后，解析出来的词项都包含一个 position 字段表示词项的位置，查询短语分词之后 的 position 之间的间隔要满足 slop 的要求。 match_phrase_prefix query这个类似于 match_phrase query，只不过这里多了一个通配符，match_phrase_prefix 支持最后一个词项的前缀匹配，但是由于这种匹配方式效率较低，因此大家作为了解即可。 12345678GET books/_search{ &quot;query&quot;: { &quot;match_phrase_prefix&quot;: { &quot;name&quot;: &quot;计&quot; } }} 这个查询过程，会自动进行单词匹配，会自动查找以计开始的单词，默认是 50 个，可以自己控制： 1234567891011GET books/_search{ &quot;query&quot;: { &quot;match_phrase_prefix&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;计&quot;, &quot;max_expansions&quot;: 3 } } }} match_phrase_prefix 是针对分片级别的查询，假设 max_expansions 为 1，可能返回多个文档，但是只有一个词，这是我们预期的结果。有的时候实际返回结果和我们预期结果并不一致，原因在于这个查询是分片级别的，不同的分片确实只返回了一个词，但是结果可能来自不同的分片，所以最终会看到多个词。 multi_match querymatch 查询的升级版，可以指定多个查询域： 123456789GET books/_search{ &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;java&quot;, &quot;fields&quot;: [&quot;name&quot;,&quot;info&quot;] } }} 这种查询方式还可以指定字段的权重： 123456789GET books/_search{ &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;阳光&quot;, &quot;fields&quot;: [&quot;name^4&quot;,&quot;info&quot;] } }} 这个表示关键字出现在 name 中的权重是出现在 info 中权重的 4 倍。 query_string queryquery_string 是一种紧密结合 Lucene 的查询方式，在一个查询语句中可以用到 Lucene 的一些查询语法： 123456789GET books/_search{ &quot;query&quot;: { &quot;query_string&quot;: { &quot;default_field&quot;: &quot;name&quot;, &quot;query&quot;: &quot;(十一五) AND (计算机)&quot; } }} simple_query_string这个是 query_string 的升级，可以直接使用 +、|、- 代替 AND、OR、NOT 等。 123456789GET books/_search{ &quot;query&quot;: { &quot;simple_query_string&quot;: { &quot;fields&quot;: [&quot;name&quot;], &quot;query&quot;: &quot;(十一五) + (计算机)&quot; } }} 查询结果和 query_string。 term query词项查询。词项查询不会分析查询字符，直接拿查询字符去倒排索引中比对。 12345678GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;程序设计&quot; } }} terms query词项查询，但是可以给多个关键词。 12345678GET books/_search{ &quot;query&quot;: { &quot;terms&quot;: { &quot;name&quot;: [&quot;程序&quot;,&quot;设计&quot;,&quot;java&quot;] } }} range query范围查询，可以按照日期范围、数字范围等查询。 range query 中的参数主要有四个： gt lt gte lte 案例： 123456789101112131415161718GET books/_search{ &quot;query&quot;: { &quot;range&quot;: { &quot;price&quot;: { &quot;gte&quot;: 10, &quot;lt&quot;: 20 } } }, &quot;sort&quot;: [ { &quot;price&quot;: { &quot;order&quot;: &quot;desc&quot; } } ]} exists queryexists query 会返回指定字段中至少有一个非空值的文档： 12345678GET books/_search{ &quot;query&quot;: { &quot;exists&quot;: { &quot;field&quot;: &quot;javaboy&quot; } }} 注意，空字符串也是有值。null 是空值。 prefix query前缀查询，效率略低，除非必要，一般不太建议使用。 给定关键词的前缀去查询： 12345678910GET books/_search{ &quot;query&quot;: { &quot;prefix&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;大学&quot; } } }} wildcard querywildcard query 即通配符查询。支持单字符和多字符通配符： ？表示一个任意字符。 * 表示零个或者多个字符。 查询所有姓张的作者的书： 12345678910GET books/_search{ &quot;query&quot;: { &quot;wildcard&quot;: { &quot;author&quot;: { &quot;value&quot;: &quot;张*&quot; } } }} 查询所有姓张并且名字只有两个字的作者的书： 12345678910GET books/_search{ &quot;query&quot;: { &quot;wildcard&quot;: { &quot;author&quot;: { &quot;value&quot;: &quot;张?&quot; } } }} regexp query支持正则表达式查询。 查询所有姓张并且名字只有两个字的作者的书： 12345678GET books/_search{ &quot;query&quot;: { &quot;regexp&quot;: { &quot;author&quot;: &quot;张.&quot; } }} fuzzy query在实际搜索中，有时我们可能会打错字，从而导致搜索不到，在 match query 中，可以通过 fuzziness 属性实现模糊查询。 fuzzy query 返回与搜索关键字相似的文档。怎么样就算相似？以LevenShtein 编辑距离为准。编辑距离是指将一个字符变为另一个字符所需要更改字符的次数，更改主要包括四种： 更改字符（javb–〉java） 删除字符（javva–〉java） 插入字符（jaa–〉java） 转置字符（ajva–〉java） 为了找到相似的词，模糊查询会在指定的编辑距离中创建搜索关键词的所有可能变化或者扩展的集合，然后进行搜索匹配。 12345678GET books/_search{ &quot;query&quot;: { &quot;fuzzy&quot;: { &quot;name&quot;: &quot;javba&quot; } }} ids query根据指定的 id 查询。 12345678GET books/_search{ &quot;query&quot;: { &quot;ids&quot;:{ &quot;values&quot;: [1,2,3] } }} 复合查询constant_score query当我们不关心检索词项的频率（TF）对搜索结果排序的影响时，可以使用 constant_score 将查询语句或者过滤语句包裹起来。 12345678910111213GET books/_search{ &quot;query&quot;: { &quot;constant_score&quot;: { &quot;filter&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;java&quot; } }, &quot;boost&quot;: 1.5 } }} bool querybool query 可以将任意多个简单查询组装在一起，有四个关键字可供选择，四个关键字所描述的条件可以有一个或者多个。 must：文档必须匹配 must 选项下的查询条件。 should：文档可以匹配 should 下的查询条件，也可以不匹配。 must_not：文档必须不满足 must_not 选项下的查询条件。 filter：类似于 must，但是 filter 不评分，只是过滤数据。 例如查询 name 属性中必须包含 java，同时书价不在 [0,35] 区间内，info 属性可以包含 程序设计 也可以不包含程序设计： 123456789101112131415161718192021222324252627282930313233GET books/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;java&quot; } } } ], &quot;must_not&quot;: [ { &quot;range&quot;: { &quot;price&quot;: { &quot;gte&quot;: 0, &quot;lte&quot;: 35 } } } ], &quot;should&quot;: [ { &quot;match&quot;: { &quot;info&quot;: &quot;程序设计&quot; } } ] } }} 这里还涉及到一个关键字，minmum_should_match 参数。 minmum_should_match 参数在 es 官网上称作最小匹配度。在之前学习的 multi_match 或者这里的 should 查询中，都可以设置 minmum_should_match 参数。 假设我们要做一次查询，查询 name 中包含 语言程序设计 关键字的文档： 12345678GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;语言程序设计&quot; } }} 在这个查询过程中，首先会进行分词，分词结果为：语言、程序设计、程序、设计 分词后的 term 会构造成一个 should 的 bool query，每一个 term 都会变成一个 term query 的子句。换句话说，上面的查询和下面的查询等价： 12345678910111213141516171819202122232425262728293031323334353637GET books/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;语言&quot; } } }, { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;程序设计&quot; } } }, { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;程序&quot; } } }, { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;设计&quot; } } } ] } }} 在这两个查询语句中，都是文档只需要包含词项中的任意一项即可，文档就回被返回，在 match 查询中，可以通过 operator 参数设置文档必须匹配所有词项。 如果想匹配一部分词项，就涉及到一个参数，就是 minmum_should_match，即最小匹配度。即至少匹配多少个词。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;语言程序设计&quot;, &quot;operator&quot;: &quot;and&quot; } } }}GET books/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;语言&quot; } } }, { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;程序设计&quot; } } }, { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;程序&quot; } } }, { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;设计&quot; } } } ], &quot;minimum_should_match&quot;: &quot;50%&quot; } }, &quot;from&quot;: 0, &quot;size&quot;: 70} 50% 表示词项个数的 50%。 如下两个查询等价（参数 4 是因为查询关键字分词后有 4 项）： 12345678910111213141516171819202122GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;语言程序设计&quot;, &quot;minimum_should_match&quot;: 4 } } }}GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: { &quot;query&quot;: &quot;语言程序设计&quot;, &quot;operator&quot;: &quot;and&quot; } } }} dis_max query假设现在有两本书： 123456789101112131415161718192021222324252627PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;content&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; } } }}POST blog/_doc{ &quot;title&quot;:&quot;如何通过Java代码调用ElasticSearch&quot;, &quot;content&quot;:&quot;松哥力荐，这是一篇很好的解决方案&quot;}POST blog/_doc{ &quot;title&quot;:&quot;初识 MongoDB&quot;, &quot;content&quot;:&quot;简单介绍一下 MongoDB，以及如何通过 Java 调用 MongoDB，MongoDB 是一个不错 NoSQL 解决方案&quot;} 现在假设搜索 Java解决方案 关键字，但是不确定关键字是在 title 还是在 content，所以两者都搜索： 12345678910111213141516171819GET blog/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ { &quot;match&quot;: { &quot;title&quot;: &quot;java解决方案&quot; } }, { &quot;match&quot;: { &quot;content&quot;: &quot;java解决方案&quot; } } ] } }} 搜索结果如下： 12345678910111213141516171819202122232425262728293031323334353637383940{ &quot;took&quot; : 3, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : { &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; }, &quot;max_score&quot; : 1.1972204, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;YnERZXoBw3hORU3ogqsC&quot;, &quot;_score&quot; : 1.1972204, &quot;_source&quot; : { &quot;title&quot; : &quot;如何通过Java代码调用ElasticSearch&quot;, &quot;content&quot; : &quot;松哥力荐，这是一篇很好的解决方案&quot; } }, { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;Y3ERZXoBw3hORU3ogqsW&quot;, &quot;_score&quot; : 1.1069256, &quot;_source&quot; : { &quot;title&quot; : &quot;初识 MongoDB&quot;, &quot;content&quot; : &quot;简单介绍一下 MongoDB，以及如何通过 Java 调用 MongoDB，MongoDB 是一个不错 NoSQL 解决方案&quot; } } ] }} 肉眼观察，感觉第二个和查询关键字相似度更高，但是实际查询结果并非这样。 要理解这个原因，我们需要来看下 should query 中的评分策略： 首先会执行 should 中的两个查询 对两个查询结果的评分求和 对求和结果乘以匹配语句总数 在对第三步的结果除以所有语句总数 反映到具体的查询中： 前者 title 中 包含 java，假设评分是 1.1 content 中包含解决方案，假设评分是 1.2 有得分的 query 数量，这里是 2 总的 query 数量也是 2 最终结果：（1.1+1.2）*2/2=2.3 后者 title 中 不包含查询关键字，没有得分 content 中包含解决方案和 java，假设评分是 2 有得分的 query 数量，这里是 1 总的 query 数量也是 2 最终结果：2*1/2=1 在这种查询中，title 和 content 相当于是相互竞争的关系，所以我们需要找到一个最佳匹配字段。 为了解决这一问题，就需要用到 dis_max query（disjunction max query，分离最大化查询）：匹配的文档依然返回，但是只将最佳匹配的评分作为查询的评分。 12345678910111213141516171819GET blog/_search{ &quot;query&quot;: { &quot;dis_max&quot;: { &quot;queries&quot;: [ { &quot;match&quot;: { &quot;title&quot;: &quot;java解决方案&quot; } }, { &quot;match&quot;: { &quot;content&quot;: &quot;java解决方案&quot; } } ] } }} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839{ &quot;took&quot; : 7, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : { &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; }, &quot;max_score&quot; : 1.1069256, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;Y3ERZXoBw3hORU3ogqsW&quot;, &quot;_score&quot; : 1.1069256, &quot;_source&quot; : { &quot;title&quot; : &quot;初识 MongoDB&quot;, &quot;content&quot; : &quot;简单介绍一下 MongoDB，以及如何通过 Java 调用 MongoDB，MongoDB 是一个不错 NoSQL 解决方案&quot; } }, { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;YnERZXoBw3hORU3ogqsC&quot;, &quot;_score&quot; : 0.62177753, &quot;_source&quot; : { &quot;title&quot; : &quot;如何通过Java代码调用ElasticSearch&quot;, &quot;content&quot; : &quot;松哥力荐，这是一篇很好的解决方案&quot; } } ] }} 在 dis_max query 中，还有一个参数 tie_breaker（取值在0～1），在 dis_max query 中，是完全不考虑其他 query 的分数，只是将最佳匹配的字段的评分返回。但是，有的时候，我们又不得不考虑一下其他 query 的分数，此时，可以通过 tie_breaker 来优化 dis_max query。tie_breaker 会将其他 query 的分数，乘以 tie_breaker，然后和分数最高的 query 进行一个综合计算。 function_score query场景：例如想要搜索附近的肯德基，搜索的关键字是肯德基，但是我希望能够将评分较高的肯德基优先展示出来。但是默认的评分策略是没有办法考虑到餐厅评分的，他只是考虑相关性，这个时候可以通过 function_score query 来实现。 准备两条测试数据： 1234567891011121314151617181920212223242526PUT blog{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;votes&quot;:{ &quot;type&quot;: &quot;integer&quot; } } }}PUT blog/_doc/1{ &quot;title&quot;:&quot;Java集合详解&quot;, &quot;votes&quot;:100}PUT blog/_doc/2{ &quot;title&quot;:&quot;Java多线程详解，Java锁详解&quot;, &quot;votes&quot;:10} 现在搜索标题中包含 java 关键字的文档： 12345678GET blog/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839{ &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : { &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; }, &quot;max_score&quot; : 0.22534126, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.22534126, &quot;_source&quot; : { &quot;title&quot; : &quot;Java多线程详解，Java锁详解&quot;, &quot;votes&quot; : 10 } }, { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.21799318, &quot;_source&quot; : { &quot;title&quot; : &quot;Java集合详解&quot;, &quot;votes&quot; : 100 } } ] }} 默认情况下，id 为 2 的记录得分较高，因为他的 title 中包含两个 java。 如果我们在查询中，希望能够充分考虑 votes 字段，将 votes 较高的文档优先展示，就可以通过 function_score 来实现。 具体的思路，就是在旧的得分基础上，根据 votes 的数值进行综合运算，重新得出一个新的评分。 具体有几种不同的计算方式： weight random_score script_score field_value_factor weight weight 可以对评分设置权重，就是在旧的评分基础上乘以 weight，他其实无法解决我们上面所说的问题。具体用法如下： 1234567891011121314151617GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;weight&quot;: 10 } ] } }} 查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839{ &quot;took&quot; : 8, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : { &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; }, &quot;max_score&quot; : 2.2534127, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 2.2534127, &quot;_source&quot; : { &quot;title&quot; : &quot;Java多线程详解，Java锁详解&quot;, &quot;votes&quot; : 10 } }, { &quot;_index&quot; : &quot;blog&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 2.1799319, &quot;_source&quot; : { &quot;title&quot; : &quot;Java集合详解&quot;, &quot;votes&quot; : 100 } } ] }} 可以看到，此时的评分，在之前的评分基础上*10 random_score random_score 会根据 uid 字段进行 hash 运算，生成分数，使用 random_score 时可以配置一个种子，如果不配置，默认使用当前时间。 1234567891011121314151617GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;random_score&quot;: {} } ] } }} script_score 自定义评分脚本。假设每个文档的最终得分是旧的分数加上votes。查询方式如下： 12345678910111213141516171819202122GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;script_score&quot;: { &quot;script&quot;: { &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;_score + doc['votes'].value&quot; } } } ] } }} 现在，最终得分是 (oldScore+votes)*oldScore。 如果不想乘以 oldScore，查询方式如下： 1234567891011121314151617181920212223GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;script_score&quot;: { &quot;script&quot;: { &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;_score + doc['votes'].value&quot; } } } ], &quot;boost_mode&quot;: &quot;replace&quot; } }} 通过 boost_mode 参数，可以设置最终的计算方式。该参数还有其他取值： multiply：分数相乘 sum：分数相加 avg：求平均数 max：最大分 min：最小分 replace：不进行二次计算 field_value_factor 这个的功能类似于 script_score，但是不用自己写脚本。 假设每个文档的最终得分是旧的分数乘以votes。查询方式如下： 12345678910111213141516171819GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;field_value_factor&quot;: { &quot;field&quot;: &quot;votes&quot; } } ] } }} 默认的得分就是oldScore*votes。 还可以利用 es 内置的函数进行一些更复杂的运算： 123456789101112131415161718192021GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;field_value_factor&quot;: { &quot;field&quot;: &quot;votes&quot;, &quot;modifier&quot;: &quot;sqrt&quot; } } ], &quot;boost_mode&quot;: &quot;replace&quot; } }} 此时，最终的得分是（sqrt(votes)）。 modifier 中可以设置内置函数，其他的内置函数还有： 另外还有个参数 factor ，影响因子。字段值先乘以影响因子，然后再进行计算。以 sqrt 为例，计算方式为 sqrt(factor*votes)： 12345678910111213141516171819202122GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;field_value_factor&quot;: { &quot;field&quot;: &quot;votes&quot;, &quot;modifier&quot;: &quot;sqrt&quot;, &quot;factor&quot;: 10 } } ], &quot;boost_mode&quot;: &quot;replace&quot; } }} 还有一个参数 max_boost，控制计算结果的范围： 123456789101112131415161718192021GET blog/_search{ &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;java&quot; } }, &quot;functions&quot;: [ { &quot;field_value_factor&quot;: { &quot;field&quot;: &quot;votes&quot; } } ], &quot;boost_mode&quot;: &quot;sum&quot;, &quot;max_boost&quot;: 100 } }} max_boost 参数表示 functions 模块中，最终的计算结果上限。如果超过上限，就按照上线计算。 boosting queryboosting query 中包含三部分： positive：得分不变 negative：降低得分 negative_boost：降低的权重 123456789101112131415161718GET books/_search{ &quot;query&quot;: { &quot;boosting&quot;: { &quot;positive&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;java&quot; } }, &quot;negative&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;2008&quot; } }, &quot;negative_boost&quot;: 0.5 } }} 查询结果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647{ &quot;took&quot; : 15, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : { &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; }, &quot;max_score&quot; : 4.5299835, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;books&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;549&quot;, &quot;_score&quot; : 4.5299835, &quot;_source&quot; : { &quot;name&quot; : &quot;全国计算机等级考试笔试＋上机全真模拟：二级Java语言程序设计（最新版）&quot;, &quot;publish&quot; : &quot;高等教育出版社&quot;, &quot;type&quot; : &quot;考试认证&quot;, &quot;author&quot; : &quot;&quot;, &quot;info&quot; : &quot;为了更好地服务于考生，引导考生尽快掌握考试大纲中要求的知识点和技能，顺利通过计算机等级考试，根据最新的考试大纲，高等教育出版社组织长期从事计算机等级考试命题研究和培训工作的专家编写了这套“笔试+上机考试全真模拟”，全面模拟考试真题，让考生在做题的同时全面巩固复习考点，提前熟悉考试环境，在短时间内冲刺过关。本书内容包括20套笔试模拟题和20套上机模拟题，还给出了参考答案和解析，尤其适合参加计算机等级考试的考生考前实战演练。&quot;, &quot;price&quot; : 30 } }, { &quot;_index&quot; : &quot;books&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;86&quot;, &quot;_score&quot; : 2.5018067, &quot;_source&quot; : { &quot;name&quot; : &quot;全国计算机等级考试2级教程：Java语言程序设计（2008年版）&quot;, &quot;publish&quot; : &quot;高等教育出版社&quot;, &quot;type&quot; : &quot;计算机考试&quot;, &quot;author&quot; : &quot;&quot;, &quot;info&quot; : &quot;由国家教育部考试中心推出的计算机等级考试是一种客观、公正、科学的专门测试计算机应用人员的计算机知识与技能的全国性考试，它面向社会，服务于社会。本书在教育部考试中心组织下、在全国计算机等级考试委员会指导下，由有关专家执笔编写而成。本书按照《全国计算机等级考试二级Java语言程序设计考试大纲（2007年版）》的要求编写，内容包括：Java体系结构、基本数据类型、流程控制语句、类、数组和字符串操作、输入输出及文件操作、图形用户界面编写、线程和串行化技术、A程序设计以及应用开发工具和安装使用等。本书是参加全国计算机等级考试二级Java语言程序设计的考生的良师益友，是教育部考试中心指定教材，也可作为欲学习Java编程的读者的参考书。&quot;, &quot;price&quot; : 37 } } ] }}","link":"/2021/07/01/ElasticSearch%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89%E6%98%A0%E5%B0%84%E5%AD%97%E6%AE%B5%E5%92%8C%E6%9F%A5%E8%AF%A2/"},{"title":"ElasticSearch从入门到实战（三）特殊查询和管道聚合","text":"本篇讲述了； 嵌套查询 地理位置查询 特殊查询 结果高亮和排序 聚合分析 Java客户端操作es 嵌套查询在关系型数据库中有表的关联关系，在 es 中，我们也有类似的需求，例如订单表和商品表，在 es 中，这样的一对多一般来说有两种方式： 嵌套文档（nested） 父子文档 嵌套文档假设：有一个电影文档，每个电影都有演员信息： 12345678910111213141516171819202122232425PUT movies{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;actors&quot;:{ &quot;type&quot;: &quot;nested&quot; } } }}PUT movies/_doc/1{ &quot;name&quot;:&quot;霸王别姬&quot;, &quot;actors&quot;:[ { &quot;name&quot;:&quot;张国荣&quot;, &quot;gender&quot;:&quot;男&quot; }, { &quot;name&quot;:&quot;巩俐&quot;, &quot;gender&quot;:&quot;女&quot; } ]} 注意 actors 类型要是 nested，nested 对象类型可以保持数组中每个对象的独立性（参考前面）。 缺点 查看文档数量： 1GET _cat/indices?v 查看结果如下： 12health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open movies _HXgjxGGQkuZFb_PMAi2kQ 1 1 3 0 5.3kb 5.3kb 这是因为 nested 文档在 es 内部其实也是独立的 lucene 文档，只是在我们查询的时候，es 内部帮我们做了 join 处理，所以最终看起来就像一个独立文档一样。因此这种方案性能并不是特别好。 嵌套查询这个用来查询嵌套文档： 123456789101112131415161718192021222324GET movies/_search{ &quot;query&quot;: { &quot;nested&quot;: { &quot;path&quot;: &quot;actors&quot;, &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match&quot;: { &quot;actors.name&quot;: &quot;张国荣&quot; } }, { &quot;match&quot;: { &quot;actors.gender&quot;: &quot;男&quot; } } ] } } } }} 父子文档相比于嵌套文档，父子文档主要有如下优势： 更新父文档时，不会重新索引子文档 创建、修改或者删除子文档时，不会影响父文档或者其他的子文档。 子文档可以作为搜索结果独立返回。 例如学生和班级的关系： 12345678910111213141516PUT stu_class{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;keyword&quot; }, &quot;s_c&quot;:{ &quot;type&quot;: &quot;join&quot;, &quot;relations&quot;:{ &quot;class&quot;:&quot;student&quot; } } } }} s_c 表示父子文档关系的名字，可以自定义。join 表示这是一个父子文档。relations 里边，class 这个位置是 parent，student 这个位置是 child。 接下来，插入两个父文档： 1234567891011121314PUT stu_class/_doc/1{ &quot;name&quot;:&quot;一班&quot;, &quot;s_c&quot;:{ &quot;name&quot;:&quot;class&quot; }}PUT stu_class/_doc/2{ &quot;name&quot;:&quot;二班&quot;, &quot;s_c&quot;:{ &quot;name&quot;:&quot;class&quot; }} 再来添加三个子文档： 123456789101112131415161718192021222324PUT stu_class/_doc/3?routing=1{ &quot;name&quot;:&quot;zhangsan&quot;, &quot;s_c&quot;:{ &quot;name&quot;:&quot;student&quot;, &quot;parent&quot;:1 }}PUT stu_class/_doc/4?routing=1{ &quot;name&quot;:&quot;lisi&quot;, &quot;s_c&quot;:{ &quot;name&quot;:&quot;student&quot;, &quot;parent&quot;:1 }}PUT stu_class/_doc/5?routing=2{ &quot;name&quot;:&quot;wangwu&quot;, &quot;s_c&quot;:{ &quot;name&quot;:&quot;student&quot;, &quot;parent&quot;:2 }} 首先大家可以看到，子文档都是独立的文档。特别需要注意的地方是，子文档需要和父文档在同一个分片上，所以 routing 关键字的值为父文档的 id。另外，name 属性表明这是一个子文档。 父子文档需要注意的地方： 每个索引只能定义一个 join filed 父子文档需要在同一个分片上（查询，修改需要routing） 可以向一个已经存在的 join filed 上新增关系 has_child query通过子文档查询父文档使用 has_child query。 12345678910111213GET stu_class/_search{ &quot;query&quot;: { &quot;has_child&quot;: { &quot;type&quot;: &quot;student&quot;, &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;wangwu&quot; } } } }} 查询 wangwu 所属的班级。 has_parent query通过父文档查询子文档： 12345678910111213GET stu_class/_search{ &quot;query&quot;: { &quot;has_parent&quot;: { &quot;parent_type&quot;: &quot;class&quot;, &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;二班&quot; } } } }} 查询二班的学生。但是大家注意，这种查询没有评分。 可以使用 parent id 查询子文档： 123456789GET stu_class/_search{ &quot;query&quot;: { &quot;parent_id&quot;:{ &quot;type&quot;:&quot;student&quot;, &quot;id&quot;:1 } }} 通过 parent id 查询，默认情况下使用相关性计算分数。 小结整体上来说： 普通子对象实现一对多，会损失子文档的边界，子对象之间的属性关系丢失。 nested 可以解决第 1 点的问题，但是 nested 有两个缺点：更新主文档的时候要全部更新，不支持子文档属于多个主文档。 父子文档解决 1、2 点的问题，但是它主要适用于写多读少的场景。 地理位置查询数据准备创建一个索引： 12345678910111213PUT geo{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;keyword&quot; }, &quot;location&quot;:{ &quot;type&quot;: &quot;geo_point&quot; } } }} 准备一个 geo.json 文件： 12345678910111213141516{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:1}}{&quot;name&quot;:&quot;西安&quot;,&quot;location&quot;:&quot;34.288991865037524,108.9404296875&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:2}}{&quot;name&quot;:&quot;北京&quot;,&quot;location&quot;:&quot;39.926588421909436,116.43310546875&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:3}}{&quot;name&quot;:&quot;上海&quot;,&quot;location&quot;:&quot;31.240985378021307,121.53076171875&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:4}}{&quot;name&quot;:&quot;天津&quot;,&quot;location&quot;:&quot;39.13006024213511,117.20214843749999&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:5}}{&quot;name&quot;:&quot;杭州&quot;,&quot;location&quot;:&quot;30.259067203213018,120.21240234375001&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:6}}{&quot;name&quot;:&quot;武汉&quot;,&quot;location&quot;:&quot;30.581179257386985,114.3017578125&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:7}}{&quot;name&quot;:&quot;合肥&quot;,&quot;location&quot;:&quot;31.840232667909365,117.20214843749999&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;geo&quot;,&quot;_id&quot;:8}}{&quot;name&quot;:&quot;重庆&quot;,&quot;location&quot;:&quot;29.592565403314087,106.5673828125&quot;} 最后，执行如下命令，批量导入 geo.json 数据： 1curl -XPOST &quot;http://localhost:9200/geo/_bulk?pretty&quot; -H &quot;content-type:application/json&quot; --data-binary @geo.json 可能用到的工具网站： http://geojson.io/#map=6/32.741/116.521 geo_distance query给出一个中心点，查询距离该中心点指定范围内的文档： 1234567891011121314151617181920212223GET geo/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match_all&quot;: {} } ], &quot;filter&quot;: [ { &quot;geo_distance&quot;: { &quot;distance&quot;: &quot;600km&quot;, &quot;location&quot;: { &quot;lat&quot;: 34.288991865037524, &quot;lon&quot;: 108.9404296875 } } } ] } }} 以(34.288991865037524,108.9404296875) 为圆心，以 600KM 为半径，这个范围内的数据。 geo_bounding_box query在某一个矩形内的点，通过两个点锁定一个矩形： 12345678910111213141516171819202122232425262728GET geo/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match_all&quot;: {} } ], &quot;filter&quot;: [ { &quot;geo_bounding_box&quot;: { &quot;location&quot;: { &quot;top_left&quot;: { &quot;lat&quot;: 32.0639555946604, &quot;lon&quot;: 118.78967285156249 }, &quot;bottom_right&quot;: { &quot;lat&quot;: 29.98824461550903, &quot;lon&quot;: 122.20642089843749 } } } } ] } }} 以南京经纬度作为矩形的左上角，以舟山经纬度作为矩形的右下角，构造出来的矩形中，包含上海和杭州两个城市。 geo_polygon query在某一个多边形范围内的查询。 12345678910111213141516171819202122232425262728293031323334GET geo/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match_all&quot;: {} } ], &quot;filter&quot;: [ { &quot;geo_polygon&quot;: { &quot;location&quot;: { &quot;points&quot;: [ { &quot;lat&quot;: 31.793755581217674, &quot;lon&quot;: 113.8238525390625 }, { &quot;lat&quot;: 30.007273923504556, &quot;lon&quot;:114.224853515625 }, { &quot;lat&quot;: 30.007273923504556, &quot;lon&quot;:114.8345947265625 } ] } } } ] } }} 给定多个点，由多个点组成的多边形中的数据。 geo_shape querygeo_shape 用来查询图形，针对 geo_shape，两个图形之间的关系有：相交、包含、不相交。 新建索引： 12345678910111213PUT geo_shape{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;keyword&quot; }, &quot;location&quot;:{ &quot;type&quot;: &quot;geo_shape&quot; } } }} 然后添加一条线： 1234567891011PUT geo_shape/_doc/1{ &quot;name&quot;:&quot;西安-郑州&quot;, &quot;location&quot;:{ &quot;type&quot;:&quot;linestring&quot;, &quot;coordinates&quot;:[ [108.9404296875,34.279914398549934], [113.66455078125,34.768691457552706] ] }} 接下来查询某一个图形中是否包含该线： 12345678910111213141516171819202122232425262728293031323334GET geo_shape/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match_all&quot;: {} } ], &quot;filter&quot;: [ { &quot;geo_shape&quot;: { &quot;location&quot;: { &quot;shape&quot;: { &quot;type&quot;: &quot;envelope&quot;, &quot;coordinates&quot;: [ [ 106.5234375, 36.80928470205937 ], [ 115.33447265625, 32.24997445586331 ] ] }, &quot;relation&quot;: &quot;within&quot; } } } ] } }} relation 属性表示两个图形的关系： within 包含 intersects 相交 disjoint 不相交 特殊查询more_like_this querymore_like_this query 可以实现基于内容的推荐，给定一篇文章，可以查询出和该文章相似的内容。 12345678910111213GET books/_search{ &quot;query&quot;: { &quot;more_like_this&quot;: { &quot;fields&quot;: [ &quot;info&quot; ], &quot;like&quot;: &quot;大学战略&quot;, &quot;min_term_freq&quot;: 1, &quot;max_query_terms&quot;: 12 } }} fields：要匹配的字段，可以有多个 like：要匹配的文本 min_term_freq：词项的最低频率，默认是 2。特别注意，这个是指词项在要匹配的文本中的频率，而不是 es 文档中的频率 max_query_terms：query 中包含的最大词项数目 min_doc_freq：最小的文档频率，搜索的词，至少在多少个文档中出现，少于指定数目，该词会被忽略 max_doc_freq：最大文档频率 analyzer：分词器，默认使用字段的分词器 stop_words：停用词列表 minmum_should_match script query脚本查询，例如查询所有价格大于 200 的图书： 1234567891011121314151617GET books/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;filter&quot;: [ { &quot;script&quot;: { &quot;script&quot;: { &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;if(doc['price'].size()!=0){doc['price'].value &gt; 200}&quot; } } } ] } }} percolate querypercolate query 译作渗透查询或者反向查询。 正常操作：根据查询语句找到对应的文档 query-&gt;document percolate query：根据文档，返回与之匹配的查询语句，document-&gt;query 应用场景： 价格监控 库存报警 股票警告 … 例如阈值告警，假设指定字段值大于阈值，报警提示。 percolate mapping 定义： 12345678910111213141516PUT log{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;threshold&quot;:{ &quot;type&quot;: &quot;long&quot; }, &quot;count&quot;:{ &quot;type&quot;: &quot;long&quot; }, &quot;query&quot;:{ &quot;type&quot;:&quot;percolator&quot; } } }} percolator 类型相当于 keyword、long 以及 integer 等。 插入文档： 123456789101112131415PUT log/_doc/1{ &quot;threshold&quot;:10, &quot;query&quot;:{ &quot;bool&quot;:{ &quot;must&quot;:{ &quot;range&quot;:{ &quot;count&quot;:{ &quot;gt&quot;:10 } } } } }} 最后查询： 12345678910111213141516171819202122232425GET log/_search{ &quot;query&quot;: { &quot;percolate&quot;: { &quot;field&quot;: &quot;query&quot;, &quot;documents&quot;: [ { &quot;count&quot;:3 }, { &quot;count&quot;:6 }, { &quot;count&quot;:90 }, { &quot;count&quot;:12 }, { &quot;count&quot;:15 } ] } }} 查询结果中会列出不满足条件的文档。 查询结果中的 _percolator_document_slot 字段表示文档的 position，从 0 开始计。 结果高亮和排序普通高亮，默认会自动添加 em 标签： 12345678910111213GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;大学&quot; } }, &quot;highlight&quot;: { &quot;fields&quot;: { &quot;name&quot;: {} } }} 查询结果如下： image-20201127170236189 正常来说，我们见到的高亮可能是红色、黄色之类的。 可以自定义高亮标签： 12345678910111213141516GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;大学&quot; } }, &quot;highlight&quot;: { &quot;fields&quot;: { &quot;name&quot;: { &quot;pre_tags&quot;: [&quot;&lt;strong&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/strong&gt;&quot;] } } }} 搜索结果如下： image-20201127170458679 有的时候，虽然我们是在 name 字段中搜索的，但是我们希望 info 字段中，相关的关键字也能高亮： 123456789101112131415161718192021GET books/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;大学&quot; } }, &quot;highlight&quot;: { &quot;require_field_match&quot;: &quot;false&quot;, &quot;fields&quot;: { &quot;name&quot;: { &quot;pre_tags&quot;: [&quot;&lt;strong&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/strong&gt;&quot;] }, &quot;info&quot;: { &quot;pre_tags&quot;: [&quot;&lt;strong&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/strong&gt;&quot;] } } }} 搜索结果如下： image-20201127170726795 排序排序很简单，默认是按照查询文档的相关度来排序的，即（_score 字段）： 12345678910GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;java&quot; } } }} 等价于： 1234567891011121314151617GET books/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;java&quot; } } }, &quot;sort&quot;: [ { &quot;_score&quot;: { &quot;order&quot;: &quot;desc&quot; } } ]} match_all 查询只是返回所有文档，不评分，默认按照添加顺序返回，可以通过 _doc 字段对其进行排序： 1234567891011121314GET books/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;sort&quot;: [ { &quot;_doc&quot;: { &quot;order&quot;: &quot;desc&quot; } } ], &quot;size&quot;: 20} es 同时也支持多字段排序。 12345678910111213141516171819GET books/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;sort&quot;: [ { &quot;price&quot;: { &quot;order&quot;: &quot;asc&quot; } }, { &quot;_doc&quot;: { &quot;order&quot;: &quot;desc&quot; } } ], &quot;size&quot;: 20} 聚合分析指标聚合Max Aggregation 最大值统计最大值。例如查询价格最高的书： 12345678910GET books/_search{ &quot;aggs&quot;: { &quot;max_price&quot;: { &quot;max&quot;: { &quot;field&quot;: &quot;price&quot; } } }} 查询结果如下： 12345&quot;aggregations&quot; : { &quot;max_price&quot; : { &quot;value&quot; : 269.0 }} 1234567891011GET books/_search{ &quot;aggs&quot;: { &quot;max_price&quot;: { &quot;max&quot;: { &quot;field&quot;: &quot;price&quot;, &quot;missing&quot;: 1000 } } }} 如果某个文档中缺少 price 字段，则设置该字段的值为 1000。 也可以通过脚本来查询最大值： 123456789101112GET books/_search{ &quot;aggs&quot;: { &quot;max_price&quot;: { &quot;max&quot;: { &quot;script&quot;: { &quot;source&quot;: &quot;if(doc['price'].size()!=0){doc.price.value}&quot; } } } }} 使用脚本时，可以先通过 doc['price'].size()!=0 去判断文档是否有对应的属性。 Min Aggregation 最小值统计最小值，用法和 Max Aggregation 基本一致： 1234567891011GET books/_search{ &quot;aggs&quot;: { &quot;min_price&quot;: { &quot;min&quot;: { &quot;field&quot;: &quot;price&quot;, &quot;missing&quot;: 1000 } } }} 脚本： 123456789101112GET books/_search{ &quot;aggs&quot;: { &quot;min_price&quot;: { &quot;min&quot;: { &quot;script&quot;: { &quot;source&quot;: &quot;if(doc['price'].size()!=0){doc.price.value}&quot; } } } }} Avg Aggregation统计平均值： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;avg_price&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } }}GET books/_search{ &quot;aggs&quot;: { &quot;avg_price&quot;: { &quot;avg&quot;: { &quot;script&quot;: { &quot;source&quot;: &quot;if(doc['price'].size()!=0){doc.price.value}&quot; } } } }} Sum Aggregation求和： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;sum_price&quot;: { &quot;sum&quot;: { &quot;field&quot;: &quot;price&quot; } } }}GET books/_search{ &quot;aggs&quot;: { &quot;sum_price&quot;: { &quot;sum&quot;: { &quot;script&quot;: { &quot;source&quot;: &quot;if(doc['price'].size()!=0){doc.price.value}&quot; } } } }} Cardinality Aggregationcardinality aggregation 用于基数统计。类似于 SQL 中的 distinct count(0)： text 类型是分析型类型，默认是不允许进行聚合操作的，如果相对 text 类型进行聚合操作，需要设置其 fielddata 属性为 true，这种方式虽然可以使 text 类型进行聚合操作，但是无法满足精准聚合，如果需要精准聚合，可以设置字段的子域为 keyword。 方式一： 重新定义 books 索引： 123456789101112131415161718192021222324252627282930PUT books{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;publish&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;fielddata&quot;: true }, &quot;type&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;author&quot;:{ &quot;type&quot;: &quot;keyword&quot; }, &quot;info&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;price&quot;:{ &quot;type&quot;: &quot;double&quot; } } }} 定义完成后，重新插入数据（参考之前的视频）。 接下来就可以查询出版社的总数量： 12345678910GET books/_search{ &quot;aggs&quot;: { &quot;publish_count&quot;: { &quot;cardinality&quot;: { &quot;field&quot;: &quot;publish&quot; } } }} 查询结果如下： 这种聚合方式可能会不准确。可以将 publish 设置为 keyword 类型或者设置子域为 keyword。 12345678910111213141516171819202122232425262728PUT books{ &quot;mappings&quot;: { &quot;properties&quot;: { &quot;name&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;publish&quot;:{ &quot;type&quot;: &quot;keyword&quot; }, &quot;type&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;author&quot;:{ &quot;type&quot;: &quot;keyword&quot; }, &quot;info&quot;:{ &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; }, &quot;price&quot;:{ &quot;type&quot;: &quot;double&quot; } } }} 查询结果如下： 对比查询结果可知，使用 fileddata 的方式，查询结果不准确。 Stats Aggregation基本统计，一次性返回 count、max、min、avg、sum： 12345678910GET books/_search{ &quot;aggs&quot;: { &quot;stats_query&quot;: { &quot;stats&quot;: { &quot;field&quot;: &quot;price&quot; } } }} 123456789&quot;aggregations&quot; : { &quot;stats_query&quot; : { &quot;count&quot; : 888, &quot;min&quot; : 0.0, &quot;max&quot; : 269.0, &quot;avg&quot; : 29.40765765765766, &quot;sum&quot; : 26114.0 } } Extends Stats Aggregation高级统计，比 stats 多出来：平方和、方差、标准差、平均值加减两个标准差的区间： 12345678910GET books/_search{ &quot;aggs&quot;: { &quot;es&quot;: { &quot;extended_stats&quot;: { &quot;field&quot;: &quot;price&quot; } } }} Percentiles Aggregation百分位统计。 123456789101112131415161718192021GET books/_search{ &quot;aggs&quot;: { &quot;p&quot;: { &quot;percentiles&quot;: { &quot;field&quot;: &quot;price&quot;, &quot;percents&quot;: [ 1, 5, 10, 15, 25, 50, 75, 95, 99 ] } } }} Value Count Aggregation可以按照字段统计文档数量（包含指定字段的文档数量）： 12345678910GET books/_search{ &quot;aggs&quot;: { &quot;count&quot;: { &quot;value_count&quot;: { &quot;field&quot;: &quot;price&quot; } } }} 桶聚合Terms AggregationTerms Aggregation 用于分组聚合，例如，统计各个出版社出版的图书总数量: 1234567891011GET books/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 20 } } }} 统计结果如下： image-20201204200925589 在 terms 分桶的基础上，还可以对每个桶进行指标聚合。 统计不同出版社所出版的图书的平均价格： 123456789101112131415161718GET books/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 20 }, &quot;aggs&quot;: { &quot;avg_price&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } } }} 统计结果如下： image-20201204201400225 Filter Aggregation过滤器聚合。可以将符合过滤器中条件的文档分到一个桶中，然后可以求其平均值。 例如查询书名中包含 java 的图书的平均价格： 12345678910111213141516171819GET books/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;filter&quot;: { &quot;term&quot;: { &quot;name&quot;: &quot;java&quot; } }, &quot;aggs&quot;: { &quot;avg_price&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } } }} Filters Aggregation多过滤器聚合。过滤条件可以有多个。 例如查询书名中包含 java 或者 office 的图书的平均价格： 123456789101112131415161718192021222324252627GET books/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;filters&quot;: { &quot;filters&quot;: [ { &quot;term&quot;:{ &quot;name&quot;:&quot;java&quot; } },{ &quot;term&quot;:{ &quot;name&quot;:&quot;office&quot; } } ] }, &quot;aggs&quot;: { &quot;avg_price&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } } }} Range Aggregation按照范围聚合，在某一个范围内的文档数统计。 例如统计图书价格在 0-50、50-100、100-150、150以上的图书数量： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;range&quot;: { &quot;field&quot;: &quot;price&quot;, &quot;ranges&quot;: [ { &quot;to&quot;: 50 },{ &quot;from&quot;: 50, &quot;to&quot;: 100 },{ &quot;from&quot;: 100, &quot;to&quot;: 150 },{ &quot;from&quot;: 150 } ] } } }} Date Range AggregationRange Aggregation 也可以用来统计日期，但是也可以使用 Date Range Aggregation，后者的优势在于可以使用日期表达式。 造数据： 123456789101112131415PUT blog/_doc/1{ &quot;title&quot;:&quot;java&quot;, &quot;date&quot;:&quot;2018-12-30&quot;}PUT blog/_doc/2{ &quot;title&quot;:&quot;java&quot;, &quot;date&quot;:&quot;2020-12-30&quot;}PUT blog/_doc/3{ &quot;title&quot;:&quot;java&quot;, &quot;date&quot;:&quot;2022-10-30&quot;} 统计一年前到一年后的博客数量： 12345678910111213141516GET blog/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;date_range&quot;: { &quot;field&quot;: &quot;date&quot;, &quot;ranges&quot;: [ { &quot;from&quot;: &quot;now-12M/M&quot;, &quot;to&quot;: &quot;now+1y/y&quot; } ] } } }} 12M/M 表示 12 个月。 1y/y 表示 1年。 d 表示天 Date Histogram Aggregation时间直方图聚合。 例如统计各个月份的博客数量 1234567891011GET blog/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;date_histogram&quot;: { &quot;field&quot;: &quot;date&quot;, &quot;calendar_interval&quot;: &quot;month&quot; } } }} Missing Aggregation空值聚合。 统计所有没有 price 字段的文档： 12345678910GET books/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;missing&quot;: { &quot;field&quot;: &quot;price&quot; } } }} Children Aggregation可以根据父子文档关系进行分桶。 查询子类型为 student 的文档数量： 12345678910GET stu_class/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;children&quot;: { &quot;type&quot;: &quot;student&quot; } } }} Geo Distance Aggregation对地理位置数据做统计。 例如查询(34.288991865037524,108.9404296875)坐标方圆 600KM 和 超过 600KM 的城市数量。 12345678910111213141516171819GET geo/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;geo_distance&quot;: { &quot;field&quot;: &quot;location&quot;, &quot;origin&quot;: &quot;34.288991865037524,108.9404296875&quot;, &quot;unit&quot;: &quot;km&quot;, &quot;ranges&quot;: [ { &quot;to&quot;: 600 },{ &quot;from&quot;: 600 } ] } } }} IP Range AggregationIP 地址范围查询。 12345678910111213141516GET blog/_search{ &quot;aggs&quot;: { &quot;NAME&quot;: { &quot;ip_range&quot;: { &quot;field&quot;: &quot;ip&quot;, &quot;ranges&quot;: [ { &quot;from&quot;: &quot;127.0.0.5&quot;, &quot;to&quot;: &quot;127.0.0.11&quot; } ] } } }} 管道聚合管道聚合相当于在之前聚合的基础上，再次聚合。 Avg Bucket Aggregation计算聚合平均值。例如，统计每个出版社所出版图书的平均值，然后再统计所有出版社的平均值： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;book_count&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 3 }, &quot;aggs&quot;: { &quot;book_avg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } }, &quot;avg_book&quot;:{ &quot;avg_bucket&quot;: { &quot;buckets_path&quot;: &quot;book_count&gt;book_avg&quot; } } }} Max Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值中的最大值： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;book_count&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 3 }, &quot;aggs&quot;: { &quot;book_avg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } }, &quot;avg_book&quot;:{ &quot;max_bucket&quot;: { &quot;buckets_path&quot;: &quot;book_count&gt;book_avg&quot; } } }} Min Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值中的最小值： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;book_count&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 3 }, &quot;aggs&quot;: { &quot;book_avg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } }, &quot;avg_book&quot;:{ &quot;min_bucket&quot;: { &quot;buckets_path&quot;: &quot;book_count&gt;book_avg&quot; } } }} Sum Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值之和： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;book_count&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 3 }, &quot;aggs&quot;: { &quot;book_avg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } }, &quot;avg_book&quot;:{ &quot;sum_bucket&quot;: { &quot;buckets_path&quot;: &quot;book_count&gt;book_avg&quot; } } }} Stats Bucket Aggregation统计每个出版社所出版图书的平均值，然后再统计平均值的各种数据： 1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;book_count&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 3 }, &quot;aggs&quot;: { &quot;book_avg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } }, &quot;avg_book&quot;:{ &quot;stats_bucket&quot;: { &quot;buckets_path&quot;: &quot;book_count&gt;book_avg&quot; } } }} Extended Stats Bucket Aggregation1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;book_count&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 3 }, &quot;aggs&quot;: { &quot;book_avg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } }, &quot;avg_book&quot;:{ &quot;extended_stats_bucket&quot;: { &quot;buckets_path&quot;: &quot;book_count&gt;book_avg&quot; } } }} Percentiles Bucket Aggregation1234567891011121314151617181920212223GET books/_search{ &quot;aggs&quot;: { &quot;book_count&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;publish&quot;, &quot;size&quot;: 3 }, &quot;aggs&quot;: { &quot;book_avg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;price&quot; } } } }, &quot;avg_book&quot;:{ &quot;percentiles_bucket&quot;: { &quot;buckets_path&quot;: &quot;book_count&gt;book_avg&quot; } } }} Java客户端操作ES","link":"/2021/07/02/ElasticSearch%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89%E7%89%B9%E6%AE%8A%E6%9F%A5%E8%AF%A2%E5%92%8C%E7%AE%A1%E9%81%93%E8%81%9A%E5%90%88/"},{"title":"JavaSE(一)基本程序设计结构","text":"基本程序设计结构包括一下入门的概念和实例： 一个简单的Java应用程序 注释 数据类型 变量 运算符 字符串 输入输出 控制流 大数值 数组 一个简单的Java应用程序12345678910111213/** * 1. java区分大小写 * 2. public 为访问修饰符，用于控制程序的其他部分对这段代码的访问级别 * 3. 关键字class表示Java程序中的全部内容都包含在类中 * 4. class 后面跟着类名，表明Java程序中的全部内容都包含在类中。 * 5. 运行已编译的程序时，Java 虚拟机将从指定类中的main方法开始执行 */public class HelloWorld { public static void main(String[] args) { System.out.println(&quot;Hello,World!&quot;); }} 注释123456789101112/** * 注释类 */public class Comments { public static void main(String[] args) { //1. 第一种注释方式 // 一直到本行结束 //2. 第二种注释方式 ：/* ---- */ //3. 第三种注释方式 ：/** ---- */ } } 数据类型 整型 类型 字节数 取值范围 byte 1 -128 ~ 127 short 2 -32678 ~ 32767 int 4 -2147483648 ~ 2147483647（2开头的10位数） long 8 long型数值后面有一个后缀L 浮点类型 类型 字节数 取值范围 float 4 float型数值有一个后缀F double 8 char类型：表示单个字符 boolean类型。 变量与常量字符串Java没有内置的字符串类型，而是在标准Java类库中提供了一个预定义类，很自然地叫做String。每个用双引号括起来的字符串都是String 类的一一个实例。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import java.time.LocalDate;import java.util.Date;/** * 1. java区分大小写 * 2. public 为访问修饰符，用于控制程序的其他部分对这段代码的访问级别 * 3. 关键字class表示Java程序中的全部内容都包含在类中 * 4. class 后面跟着类名，表明Java程序中的全部内容都包含在类中。 * 5. 运行已编译的程序时，Java 虚拟机将从指定类中的main方法开始执行 */public class HelloWorld { public static void main(String[] args) { String s = &quot;abcdef&quot;; //1. 截取子串方法substring String substring = s.substring(0, 2); System.out.println(&quot;substring = &quot; + substring); //substring = ab //2. 字符串拼接，+ String a = &quot;abc&quot;; String b = &quot;def&quot;; System.out.println(a + b); //abcdef //3. 字符串类型是不可变的,下面的改动只是修改字面量，让他去引用另外一个字符串。 /** * 不可变字符串却有一个优点:编译器可以让字符串共享。 * 为了弄清具体的工作方式，可以想象将各种字符串存放在公共的存储池中。字符串变量 * 指向存储池中相应的位置。如果复制一个字符串变量，原始字符串与复制的字符串共享相同的字符. * Java 的设计者认为共享带来的高效率远远胜过于提取、拼接字符串所带来的低效率。 * 查看一下程序会发现:很少需要修改字符串，而是往往需要对字符串进行比较(有 */ s = s.substring(0,2) + &quot;aa&quot;; System.out.println(&quot;s = &quot; + s); //s = abaa //4. 字符串比较是否相等 equals 方法 /** * 一定不要使用==运算符检测两个字符串是否相等!这个运算符只能够确定两个字符串 * 是否放置在同一个位置上。当然，如果字符串放置在同一个位置上，它们必然相等。 */ String c = &quot;abc&quot;; System.out.println(c.equals(a)); // true // 5. 空串和null串// 空串是一个Java对象，有自己的串长度(0)和内容(空)。不过，String 变量还可以存// 放一个特殊的值，名为nul,这表示目前没有任何对象与该变量关联(关于null的更多信息// 请参见第4章)。要检查-一个字符串是否为 null,要使用以下条件: String d = &quot;&quot;; //空串 String e = null; // null }} 输入和输出12345678910111213141516171819202122/** * 输入输出测试 */public class InputTest { public static void main(String[] args) { Scanner scanner = new Scanner(System.in); System.out.println(&quot;What is your name? &quot;); String name = scanner.nextLine(); //读取输入的下一行内容 System.out.println(&quot;How old are you?&quot;); int age = scanner.nextInt(); // 读取下一个整数 System.out.println(&quot;Hello, &quot; + name + &quot;. Next year ,you will be &quot; + (age+1)); // %s 表示字符串替换格式化 // %d 表示十进制整数替换格式化 System.out.printf(&quot;Hello, %s. Next year , you will be %d &quot; , name ,age); }} 大数1234567891011121314151617181920212223/** * 如果基本的整数和浮点数精度不能够满足需求，那么可以使用math包下的两个类： * BigInteger ： 可以实现任意精度的整数计算 * BigDecimal ： 可以实现任意精度的浮点数计算 */public class BigIntegerTest { public static void main(String[] args) { Scanner in = new Scanner(System.in); System.out.println(&quot;How many numbers do you need to draw? &quot;); int k = in.nextInt(); System.out.println(&quot;What is the highest number you can draw?&quot;); int n = in.nextInt(); BigInteger bigInteger = BigInteger.valueOf(1); for (int i = 1; i &lt;= k ; i++) { // 大数不能用常见的加减乘除算术运算符来计算，而是要用指定的方法来。 bigInteger = bigInteger.multiply(BigInteger.valueOf(n - i + 1)).divide(BigInteger.valueOf(i)); } System.out.println(&quot;Your odds are 1 in &quot; + bigInteger + &quot;.Good Luck！&quot;); }} 数组","link":"/2021/08/01/JavaSE-%E4%B8%80-%E5%9F%BA%E6%9C%AC%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%BB%93%E6%9E%84/"},{"title":"JavaSE-七-集合框架","text":"","link":"/2021/08/01/JavaSE-%E4%B8%83-%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6/"},{"title":"JavaSE(三)类的设计和继承","text":"本节讲述了Java中的一个重要特性：继承。并以此来衍生了很多的概念和使用方法： 继承（类、父类、子类） Object类 泛型数组列表 对象包装器（自动装箱和拆箱） 可变参数的方法 枚举类 反射特性 继承的设计技巧 继承继承的意思是： 可以基于已存在的类构造一个新类。继承已存在的类就是复用(继承)这些类的方法和域。在此基础上，还可以添加一些新的方法和域，以满足新的需求。 比如Manager类继承了Employee类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public class Employee implements Comparable&lt;Employee&gt;, Cloneable { // instance fields private String name; private double salary; private LocalDate hireDay; public Employee(String name, double salary) { this.name = name; this.salary = salary; } public Employee(String name, double salary, int year , int mouth , int day) { this.name = name; this.salary = salary; this.hireDay = LocalDate.of(year,mouth,day); } public String getName() { return name; } public double getSalary() { return salary; } public LocalDate getHireDay() { return hireDay; } public void setHireDay(LocalDate hireDay) { this.hireDay = hireDay; } public void raiseSalary(double byPercent) { double raise = this.salary * byPercent / 100; this.salary += raise; } @Override public int compareTo(Employee o) { return Double.compare(salary, o.getSalary()); }}/** * @author shengbinbin * 经理类继承了员工类，获得了员工类的实例域和方法 * 1. 在子类中可以增加域、增加方法、覆盖父类的方法。但是绝不能删除继承的任何域或方法 */public class Manager extends Employee{ private double bonus; // 经理有自己的一个存储奖金信息的域 // 子类构造器 public Manager(String name, double salary, int year, int mouth, int day) { super(name, salary, year, mouth, day); // 通过super实现对超类构造器的调用，必须是子类构造器的第一个语句 bonus = 0; } // 覆盖父类中的返回薪水的方法 public double getSalary(){ double baseSalary = super.getSalary(); // super关键字代表的是父类 return baseSalary + bonus; } public void setBonus(double b){ bonus = b; }}// test Manager boss = new Manager(&quot;binshow&quot;,80000,1997,2,1); boss.setBonus(5000); //奖金5000 Employee[] staff = new Employee[3]; staff[0] = boss; staff[1] = new Employee(&quot;zkd&quot;,30000,1996,10,22); staff[2] = new Employee(&quot;aaa&quot;,10000,1997,7,18); for (Employee e : staff){ System.out.println(&quot;name= &quot; + e.getName() + &quot;, salary= &quot; + e.getSalary()); } 多态有一个用来判断是否应该设计为继承关系的简单规则，这就是“is-a” 规则 它表明子类的每个对象也是超类的对象。例如，每个经理都是雇员，因此，将Manager类设计为Employee类的子类是显而易见的，反之不然，并不是每一名雇员都是经理。 多态的意思是： 可以将一个子类的对象赋给超类变量。 1234567891011// 一个Employee变量即可以引用一个Employee类对象，也可以引用任何一个Employee的子类的对象 Employee e; e = new Employee(&quot;zkd&quot;,30000,1996,10,22); e = new Manager(&quot;binshow&quot;,20000,1996,1,2); Manager boss = new Manager(&quot;binshow&quot;,80000,1997,2,1); Employee[] staff = new Employee[3]; staff[0] = boss; // staff[0] 和 boss 引用的是同一个对象，但是编译器认为 staff[0] 是一个 Employee对象 boss.setBonus(5000); // 可以 // staff[0].setBonus(); // Error， 父类对象不能调用子类的方法。 理解方法调用下面假设要调用x.f(args),隐式参数x声明为类C的一个对象。下面是调用过程的详细过程: 编译器查看对象的声明类型和方法名。假设调用x.f(param)， 且隐式参数x声明为C类的对象。需要注意的是:有可能存在多个名字为f，但参数类型不一样的方法。例如，可能存在方法f(int) 和方法f(String)。 编译器将会一一列举 所有C类中名为f的方法和其超类中访问属性为public且名为f的方法(超类的私有方法不可访问)。至此，编译器已获得所有可能被调用的候选方法。 **接下来，编译器将查看调用方法时提供的参数类型。如果在所有名为f的方法中存在一个与提供的参数类型完全匹配，就选择这个方法。这个过程被称为重载解析( overloadingresolution)**。例如，对于调用x.f(“Hello” )来说，编译器将会挑选f(String), 而不是f(int)。由于允许类型转换，这个过程可能很复杂。如果找到和参数类型相匹配的方法，编译器就获得了需要调用的方法名字和参数类型。 如果是private方法、static 方法、final方法(有关final修饰符的含义将在下一节讲述)或者构造器，那么编译器将可以准确地知道应该调用哪个方法，我们将这种调用方式称为静态绑定( static binding)。与此对应的是，调用的方法依赖于隐式参数的实际类型，并且在运行时实现动态绑定。在我们列举的示例中，编译器采用动态绑定的方式生成一条调用f(String)的指令。 当程序运行，并且采用动态绑定调用方法时，虚拟机一定调用与x所引用对象的实际类型最合适的那个类的方法。假设x的实际类型是D，它是C类的子类。如果D类定义了方法f(String)，就直接调用它;否则，将在D类的超类中寻找f(String)，以此类推。 ps: 每次调用方法都要进行搜索，时间开销相当大。因此，**虚拟机预先为每个类创建了一个方法表( method table)**，其中列出了所有方法的签名和实际调用的方法。这样一来，在真正调用方法的时候只需要查找这个表就行了 12345678910111213141516171819202122232425262728293031323334// test Manager boss = new Manager(&quot;binshow&quot;,80000,1997,2,1); boss.setBonus(5000); //奖金5000 Employee[] staff = new Employee[3]; staff[0] = boss; staff[1] = new Employee(&quot;zkd&quot;,30000,1996,10,22); staff[2] = new Employee(&quot;aaa&quot;,10000,1997,7,18); for (Employee e : staff){ System.out.println(&quot;name= &quot; + e.getName() + &quot;, salary= &quot; + e.getSalary()); }// 以这里的e.getSalary() 为例：1. e是一个 Employee 类型，这个类只有一个getSalary方法且没有参数。所以不会有重载的解析。2. getSalary 不是 private、static、final方法，所以会采用动态绑定3. 虚拟机为 Employee类和Manager类 生成方法表:(省略了Object方法) Employee: getName() -&gt; Employee . getName() getSalary() -&gt; Employee . getSalary() getHireDay() -&gt; Employee . getHireDay() raiseSalary(double) -&gt; Emp1oyee. raiseSalary(doub1e) Manager: getName() -&gt; Employee.getName() getSalary() -&gt; Manager . getSalaryO getHireDay() -&gt; Emp1oyee.getHi reDay( raiseSalary(double) -&gt; Employee. raiseSalary(double) setBonus(double) -&gt; Manager . setBonus (doub1e) 4. 虚拟机提取e的实际类型的方法表，有可能是Employee、Manager的方法表，也可能是Employee的其他子类的方法表5. 最后搜索定义了getSalary 的类，调用该方法 如何阻止继承：final类和方法将类修饰成final表示该类不允许继承。同理将方法修饰成final表示在该类的子类中不允许重写该方法。 在早期的Java中，有些程序员为了避免动态绑定带来的系统开销而使用final 关键字。如果-一个方法没有被覆盖并且很短，编译器就能够对它进行优化处理，这个过程为称为内联( inlining)。例如，内联调用e.getName( )将被替换为访问e.name域。这是一项很有意义的改进，这是由于CPU在处理调用方法的指令时，使用的分支转移会扰乱预取指令的策略，所以，这被视为不受欢迎的。然而，如果getName在另外-一个类中被覆盖，那么编译器就无法:知道覆盖的代码将会做什么操作，因此也就不能对它进行内联处理了。 幸运的是，虚拟机中的即时编译器比传统编译器的处理能力强得多。这种编译器可以准确地知道类之间的继承关系，并能够检测出类中是否真正地存在覆盖给定的方法。如果方法很简短、被频繁调用且没有真正地被覆盖，那么即时编译器就会将这个方法进行内联处理。如果虚拟机加载了另外一个子类，而在这个子类中包含了对内联方法的覆盖，那么将会发生什么情况呢?优化器将取消对覆盖方法的内联。这个过程很慢，但却很少发生。 强制类型转换进行类型转换的 唯一原因是：暂时忽略对象的实际类型之后使用对象的全部功能 12345678910111213141516171819202122232425double x = 3.14; int n = (int) x; // 强制类型转换，将x的值转换成整数类型，舍弃了小数部分。 Manager manager = (Manager) staff[0]; // 将Employee对象 强制转换成 Manager对象。 // staff[0].setBonus(); // Error manager.setBonus(2000); // 暂时忽略对象的实际类型之后使用对象的全部功能 /** * 将一个值存入变量时，编译器将会检查是否允许这个操作。 * 1. 如果是将子类的引用赋给一个超类变量（前面所说的多态），编译器是运行的。 * 2. 但是如果是将一个超类的引用赋给一个子类的变量，就必须进行类型转换了，这样会进行运行时的检查 * 3. 最好在将超类转换成子类之前，使用instanceOf进行检查 */ Employee a = new Employee(&quot;zkd&quot;,30000,1996,10,22); Manager b = new Manager(&quot;binshow&quot;,20000,1996,1,2); a = b; //将子类的引用赋给一个超类变量 b = (Manager) a; //将一个超类的引用赋给一个子类的变量 a.getSalary(); b.getSalary(); //两个类的对象都可以正确的调用 getSalary方法。由于动态绑定 // 只有在使用Manager 类中的特有方法才需要进行类型转换。 b.setBonus(2); 抽象类在继承层次中，位于上层的类更具有通用性和抽象性。比如一个Employee、一个Student都是一个Person类。都有一个name域，就可以将这个name放在Person这个继承层次较高的通用超类中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107/** * @author shengbinbin * 抽象类Person */public abstract class Person { // 实例域 private String name; //构造器 public Person(String name){ this.name = name; } //对外提供的域访问器 public String getName(){ return name; } // 抽象方法，提供给子类去扩展 public abstract String getDescription(); }public class Employee extends Person implements Comparable&lt;Employee&gt;, Cloneable { // instance fields private double salary; private LocalDate hireDay; public Employee(String name, double salary) { super(name); this.salary = salary; } public Employee(String name, double salary, int year , int mouth , int day) { super(name); this.salary = salary; this.hireDay = LocalDate.of(year,mouth,day); } @Override // 必须要重写抽象父类中的抽象方法 public String getDescription() { return String.format(&quot;an employee with a salary of $%.2f&quot;,salary); } public double getSalary() { return salary; } public LocalDate getHireDay() { return hireDay; } public void setHireDay(LocalDate hireDay) { this.hireDay = hireDay; } public void raiseSalary(double byPercent) { double raise = this.salary * byPercent / 100; this.salary += raise; } @Override public int compareTo(Employee o) { return Double.compare(salary, o.getSalary()); }}public class Student extends Person{ private String major; public Student(String name , String major) { super(name); this.major = major; } @Override public String getDescription() { return &quot;a student majoring in &quot; + major; }}public class PersonTest { public static void main(String[] args) { // Person[] people = new Person[2]; people[0] = new Employee(&quot;binshow&quot;,2000,1995,1,2); people[1] = new Student(&quot;zkd&quot; , &quot;math&quot;); // 由于不能构造Person类的对象，所有变量P肯定指的是Person的子类 for (Person p : people){ System.out.println(p.getName() + &quot; , &quot; + p.getDescription()); } // binshow , an employee with a salary of $2000.00 // zkd , a student majoring in math }} Object：所有类的超类Object类是Java中所有类的始祖，在Java中每个类都是由它扩展而来的。 12345678910public static void main(String[] args) { // 1. 可以用Object类型的变量仅作为任意类型的指代符 Object obj = new Employee(); // 2. 但是如果要用到具体某个类型自身的方法，还是需要强制类型转换 Employee e = (Employee) obj; //3. 在Java中只有原始数据类型（数字，字母，boolean值）不能用object指代。数组对象无论是对象数组，还是原始数据类型数组都是Object的子类 Employee[] staff = new Employee[10]; obj = staff; //对象数组 obj = new int[10]; //原始数据类型数组 } equals方法 如果两个对象引用相等，这两个对象就相等。 123456java.lang.Object#equals //比较的是两个对象的引用是否一致。 public boolean equals(Object obj) { return (this == obj); } 在实际使用中经常需要基于状态来比较两个对象是否相等:比如下面比较两个员工对象是否相等 12345678910111213141516171819202122232425public class Employee { private String name; private double salary; private LocalDate hireDay; @Override public boolean equals(Object o) { if (this == o) return true; //1. 如果两个对象所属的类不同，肯定也不相等 if (o == null || getClass() != o.getClass()) return false; Employee employee = (Employee) o; //到这时o是一个不为空的 Employee 对象了 //2. 只有满足两个员工对象的姓名、薪水和入职日期是一样的，才能认为是同一个对象 // PS: 在这里防止 name 和 hireDay 为null，所以需要用equals方法进行比较。equals方法中如果两个参数都为null返回true。其中一个为null返回false //return name.equals(employee.name) &amp;&amp; salary == employee.salary &amp;&amp; hireDay.equals(employee.hireDay); return Objects.equals(name , employee.name) &amp;&amp; salary == employee.salary &amp;&amp; Objects.equals(hireDay,employee.hireDay); } @Override public int hashCode() { return Objects.hash(salary, hireDay); }} 在子类中如果定义equals方法，则首先调用父类的equals方法比较引用地址，如果检测失败，则两个对象肯定不相等。如果父类中的字段都相等，再比较子类中的实例字段。 12345678910111213141516171819public class Manager extends Employee{ private double bonus; @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; //1. 首先调用父类中的equals方法 if (!super.equals(o)) return false; Manager manager = (Manager) o; return Double.compare(manager.bonus, bonus) == 0; } @Override public int hashCode() { return Objects.hash(super.hashCode(), bonus); }} Java语言规范要求equals方法具有一下特性： 自反性： 对称性 传递性 一致性 对于任意非空引用x，x.equals(null)肯定返回false equals方法中继承关系的处理前面的例子中，如果发现两个对象所属的类不匹配，就返回false。但有的情况会这么编写equals方法： 12345678910111213141516@Override public boolean equals(Object o) { if (this == o) return true; if (o == null ) return false; //这种方式就允许了o属于它的子类，但是这样编写会有一些问题 if (!(o instanceof Employee)) return false; Employee employee = (Employee) o; return Objects.equals(name , employee.name) &amp;&amp; salary == employee.salary &amp;&amp; Objects.equals(hireDay,employee.hireDay); }问题： e.equals(m) //其中 e是Employee对象。而 m是 Manager对象。两个对象有相同的姓名，薪水和雇佣日期，此时这个equals方法返回true 但是根据对称性原则 m.equals(e) 必须也要返回true，这样的话就忽略了Manager独有的字段比较 java.util.AbstractSet#equals,有两个子类TreeSet和HashSet 123456789101112131415161718public boolean equals(Object o) { if (o == this) return true; if (!(o instanceof Set)) return false; Collection&lt;?&gt; c = (Collection&lt;?&gt;) o; if (c.size() != size()) return false; try { return containsAll(c); } catch (ClassCastException unused) { return false; } catch (NullPointerException unused) { return false; } } 结论： 如果子类可以有自己的相等性概念，则对称性需求将强制使用getClass检测 如果由父类决定相等性概念，那么可以使用instanceOf来检测，可以在不同的子类对象中进行相等性比较。 编写equals方法的正确步骤： 显示参数为otherObject 检查this和otherObject是否相等。（比较地址比比较字段开销要小很多） 如果otherObject为null，直接返回false 比较this和otherObject的两个类，如果equals的语义可以在子类中改变。则使用getClass进行检测；如果所有的子类都有相同的相等性语义，则使用instanceOf检测。 将otherObject强制转换成this的类 进行字段比较，基础数据类型用==，其他字段使用equals。如果在子类中重新定义equals，则需要包含一个super.equals（other）的调用。 hashCode方法Object默认的hashCode方法会根据对象的存储地址来计算出散列码。 123456789101112131415 String s = &quot;Ok&quot;; StringBuilder sb = new StringBuilder(s); System.out.println(&quot;s的哈希码是：&quot; + s.hashCode()); System.out.println(&quot;sb的哈希码是：&quot; + sb.hashCode()); String t = new String(&quot;Ok&quot;); StringBuilder tb = new StringBuilder(t); System.out.println(&quot;t的哈希码是：&quot; + t.hashCode()); System.out.println(&quot;tb的哈希码是：&quot; + tb.hashCode());// s的哈希码是：2556// sb的哈希码是：685325104// t的哈希码是：2556// tb的哈希码是：460141958 // s 和 t 的哈希码是一样的，是因为String的hashCode是从字符串内容导出的 // sb 和 tb 的哈希码是不一样的，是因为 StringBuilder 的hashCode 是默认的Object的hashCode,也就是从内存地址导出的 12345678910111213java.lang.String#hashCodepublic int hashCode() { int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) { char val[] = value; for (int i = 0; i &lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h; } 如果重新定义了hashCode方法，也必须要重写equals方法。因为如果x.equals(y)为true，那么x.hashcode()和y.hashcode()必须也要相同 toString方法toString方法用来返回表示对象值的字符串。比如： 12345678@Override public String toString() { return &quot;Employee{&quot; + &quot;name='&quot; + name + '\\'' + &quot;, salary=&quot; + salary + &quot;, hireDay=&quot; + hireDay + '}'; } 设计子类的toString方法需要加入子类的字段 123456@Override public String toString() { return &quot;Manager{&quot; + &quot;bonus=&quot; + bonus + &quot;} &quot; + super.toString(); } 如果x是一个任意对象，那么调用System.out.println(x); 就会调用x的toString方法。 123456//数组继承了Object类的toString方法 .int[] nums = {1,2,3,4}; System.out.println(nums); // [I@4554617c(前缀[I表示是一个数组) String numsString = &quot;&quot; + nums; System.out.println(numsString); //[I@4554617c System.out.println(Arrays.toString(nums));//[1, 2, 3, 4] 泛型数组列表ArrayList在C/C++中，必须在编译的时候就要确定整个数组的大小。而在Java中提供了动态数组ArrayList，如果调用add方法而内部数组已经满了，数组就会自动创建一个更大的数组，并将所有对象从较小的数组拷贝到较大的数组中。 ArrayList是一个采用类型参数的泛型类。为了指定数组中保存的元素类型，使用尖括号&lt;&gt;来指定存储的元素类型 1234567891011121314public static void main(String[] args) { // 构造了一个保存Employee对象的数组列表 ArrayList&lt;Employee&gt; staff = new ArrayList&lt;&gt;(); //1. 使用add方法将元素加入到数组列表中 staff.add(new Employee(&quot;binshow&quot;,10000,1997,1,2)); //2. 如果调用add方法且内部数组已经满了，list将自动创建一个更大的数组 //3. 如果已经清楚list数组中可能存储的元素数量，可以在填充数组之前用下面的 方法： staff.ensureCapacity(100); // 直接分配一个可以包含100个对象的内部数组，不用重新分配空间 // ArrayList&lt;Employee&gt; staff = new ArrayList&lt;&gt;(100); 或者构造的时候就传递容量进去 //4. 获取list中第一个位置的元素 Employee employee = staff.get(0); //5. 移除list中第一个位置的元素 Employee remove = staff.remove(0); } 对象包装器和自动装箱 背景：有时候需要将基本类型转换成对象，所以所有的基本对象都有一个与之对应的类，比如int对应于Integer。 定义一个整型数组列表，尖括号的类型参数不允许是基本数据类型，所有就要用到包装类。 12new ArrayList&lt;int&gt;(); //会发生编译错误new ArrayList&lt;Integer&gt;(); // 这样才可以 自动装箱和拆箱： 12345list.add(3); // 等价于 list.add(Integer.valueOf(3)); 自动将基本数据类型转换成包装类型 int n = list.get(0); // 等价于 int i = list.get(0).intValue(); 自动将包装类型转换成基本数据类型 Integer n = 3; n++; //编译器会自动插入一个对象拆箱的指令再进行自增运算 包装类型对象有一个常量缓冲池，比如Integer的缓冲范围为 -128 到 127 123456789101112131415161718192021Integer a = -129; Integer b = -129; System.out.println(a == b); //false Integer a = -128; Integer b = -128; System.out.println(a == b); //true ，都是指向常量池中的对象 Integer a = 100; Integer b = 100; System.out.println(a == b); //true Integer a = 127; Integer b = 127; System.out.println(a == b); //true Integer a = 128; Integer b = 128; System.out.println(a == b); //false 包装类型引用可以为null，所有自动拆箱可能会爆空指针。 12Integer n = null; System.out.println(2 * n); //NullPointerException 自动装箱和拆箱是编译器认可的，也就是说编译器在生成类的字节码时会插入必要的方法调用。虚拟机只是执行这些字节码。 变参方法现在的Java支持用可变的参数数量调用的方法。 print方法 12345// ...省略号表示这个方法可以接受任意数量的对象// 这里接受的是Object数组，来保存所有的参数。public PrintStream printf(Locale l, String format, Object ... args) { return format(l, format, args); } 枚举类 举例: Size枚举类只有4个实例。不可能构造新的对象了，因此在比较两个枚举类型的值时，并不需要equals方法，直接用==就可以了。 123456public enum Size { SMALL, MEDIUM, LARGE, EXTRE_LARGE} 枚举类可以增加构造器、方法和字段。构造器只是在构造枚举常量的时候调用 1234567891011121314151617public enum Size { SMALL(&quot;S&quot;), MEDIUM(&quot;M&quot;), LARGE(&quot;L&quot;), EXTRE_LARGE(&quot;XL&quot;); private String abbreviation; //缩写 //枚举类的构造器总是私有的，因为不可能再构造新的对象了 private Size(String abbreviation){ this.abbreviation = abbreviation; } public String getAbbreviation(){ return abbreviation; }} 每个枚举类型都有一个静态values方法，可以返回包含全部枚举值的数组。 123456789101112131415System.out.println(Arrays.toString(Size.values())); //[SMALL, MEDIUM, LARGE, EXTRE_LARGE]public class EnumTest { public static void main(String[] args) { Scanner scanner = new Scanner(System.in); System.out.println(&quot;enter a size:(SMALL , MEDIUM , LARGE , EXTRE_LARGE)&quot;); String input = scanner.next().toUpperCase(); Size size = Enum.valueOf(Size.class, input); //反向，通过字符串返回枚举类型 System.out.println(&quot;size = &quot; + size); System.out.println(&quot;abbreviation =:&quot; + size.getAbbreviation()); if (size == Size.EXTRE_LARGE) System.out.println(&quot;Good Job!&quot;); }} 反射可以利用反射编写能动态操作Java代码的程序。 在运行时分析类的能力 在运行时查看对象 实现通用的数组操作代码 利用Method对象。 Class类：在程序运行期间，java运行时系统始终为所有的对象维护一个运行时的类型标识。这个信息跟踪着每个对象所属的类，虚拟机可以利用运行时的类型信息选择相应的方法去执行。保存这些信息的类成为Class。Object类中的getClass对象就会返回一个Class类型的实例。 获取Class类对象的三种方式： 1234567891011121314151617181920212223public class Test { // Java运行时系统始终为每个对象维护一个运行时类型标识 ，这个信息保存为Class类。 public static void main(String[] args) throws ClassNotFoundException { Employee e = new Employee(&quot;binshow&quot;,27000,1997,1,8); //1. 通过getClass() 获得 Class 对象会描述一个特定类的属性 Class&lt;? extends Employee&gt; cl = e.getClass(); System.out.println(&quot;cl = &quot; + cl.getName()); //cl = chapter5.Employee //2. 通过Class.forName 来获取类名对于的Class对象 String className = &quot;java.util.Random&quot;; Class&lt;?&gt; aClass = Class.forName(className); System.out.println(&quot;aClass = &quot; + aClass.getName()); //aClass = java.util.Random //3. 直接通过后缀名获取 Class&lt;Random&gt; randomClass = Random.class; System.out.println(&quot;randomClass = &quot; + randomClass.getName()); //randomClass = java.util.Random Class&lt;Integer&gt; integerClass = int.class; System.out.println(&quot;integerClass = &quot; + integerClass.getName()); // integerClass = int Class&lt;int[]&gt; aClass1 = int[].class; System.out.println(&quot;aClass1 = &quot; + aClass1.getName()); //aClass1 = [I 数组类型 }} 通过Class类对象获取新的 对象 1234String className = &quot;java.util.Random&quot;; Class&lt;?&gt; aClass = Class.forName(className); Object o = aClass.getConstructor().newInstance(); //获取Class类的无参构造，再创建一个新的实例。如果没有无参构造则报错 System.out.println(o); //java.util.Random@1b6d3586 通过放射来分析类的能力：Field、Method、Constructors 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import java.lang.reflect.Constructor;import java.lang.reflect.Field;import java.lang.reflect.Method;import java.lang.reflect.Modifier;import java.util.Scanner;/** * 通过反射来分析类的能力 */public class ReflectionTest { public static void main(String[] args) { //java.lang.Double Scanner scanner = new Scanner(System.in); System.out.println(&quot;enter the class name:&quot;); String name = scanner.next(); try{ Class&lt;?&gt; cl = Class.forName(name); Class&lt;?&gt; sc = cl.getSuperclass(); String modifiers = Modifier.toString(cl.getModifiers()); if (modifiers.length() &gt; 0) System.out.println(&quot;modifiers = &quot; + modifiers); System.out.print(&quot;className = &quot; + cl.getName()); if (sc != null &amp;&amp; sc != Object.class) System.out.print(&quot; extends &quot; + sc.getName()); System.out.print(&quot;\\n{\\n&quot;); printConstructor(cl); printMethod(cl); printFields(cl); } catch (ClassNotFoundException e) { e.printStackTrace(); } } // 打印所有的构造器方法及参数类型 public static void printConstructor(Class cl){ Constructor[] constructors = cl.getDeclaredConstructors(); //getDeclared 表示所有的，没有的话就返回公有的 for (Constructor c : constructors){ String name = c.getName(); System.out.print(&quot; &quot;); String modifiers = Modifier.toString(c.getModifiers()); System.out.print(name + &quot;(&quot;); //打印参数类型 Class[] parameterTypes = c.getParameterTypes(); for (int i = 0; i &lt; parameterTypes.length; i++) { if (i &gt; 0) System.out.print(&quot;,&quot;); System.out.print(parameterTypes[i].getName()); } System.out.println(&quot;);&quot;); } } //打印类的所有方法 public static void printMethod(Class cl){ Method[] methods = cl.getDeclaredMethods(); for (Method m : methods){ Class&lt;?&gt; returnType = m.getReturnType(); //返回参数类型 String name = m.getName(); System.out.print(&quot; &quot;); String modifiers = Modifier.toString(m.getModifiers()); if (modifiers.length() &gt; 0) System.out.print(modifiers + &quot; &quot;); System.out.print(returnType.getName() + &quot; &quot; + name + &quot;(&quot;); //打印参数类型 Class&lt;?&gt;[] parameterTypes = m.getParameterTypes(); for (int i = 0; i &lt; parameterTypes.length; i++) { if (i &gt; 0) System.out.print(&quot;, &quot;); System.out.print(parameterTypes[i].getName()); } System.out.println(&quot;);&quot;); } } //打印类的所有字段 public static void printFields(Class cl){ Field[] fields = cl.getDeclaredFields(); for (Field f : fields){ Class type = f.getType(); String name = f.getName(); System.out.print(&quot; &quot;); String modifies = Modifier.toString(f.getModifiers()); if (modifies.length() &gt; 0) System.out.print(modifies + &quot; &quot;); System.out.println(type.getName() + &quot; &quot; + name + &quot;;&quot;); } }} 调用任意方法:和利用Field类的get方法查看对象域的过程类似，Method类中也有一个invoke方法，可以调用包装 1 5.8 继承的设计技巧 将公共操作和字段放在超类里面。比如姓名应该放在Person类中而不是Employee和Student中。 不要使用受保护的字段。 使用继承来实现“is a”的关系。 除非所有继承的方法都有意义，否则就不要使用继承。 在覆盖方法时，不要改变预期的行为。 使用多态而不要使用类型信息。 不要滥用放射。因为编译器无法帮助你查找编程错误，只有在运行时才会发生错误。","link":"/2021/08/02/JavaSE-%E4%B8%89-%E7%B1%BB%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%92%8C%E7%BB%A7%E6%89%BF/"},{"title":"JavaSE(二)对象和类","text":"Java是一门面向对象（OOP）的语言，本节介绍了： oop概述 使用预定义类和用户自定义类 静态字段和方法 方法参数 对象构造 类设计技巧 对象和类OOP设计概述Java是面向对象的。面向对象的程序是由对象组成的，每个对象都包含了对用户公开的特定功能部分和隐藏的实现部分。程序中的很多对象来自标准库，还有一些是自定义的。 OOP将数据放在第一位，然后再考虑操作数据的算法。 类 类是构造对象的模板。由类构造对象的过程称为创建类的实例。 封装( encapsulation,有时称为数据隐藏)是与对象有关的-一个重要概念。从形式上看，封装不过是将数据和行为组合在-一个包中，并对对象的使用者隐藏了数据的实现方式。对象中的数据称为实例域( instance field)，操纵数据的过程称为方法( method)。对于每个特定的类实例(对象)都有一组特定的实例域值。这些值的集合就是这个对象的当前状态( state)。无论何时，只要向对象发送-一个消息，它的状态就有可能发生改变。实现封装的关键在于绝对不能让类中的方法直接地访问其他类的实例域。程序仅通过对象的方法与对象数据进行交互。封装给对象赋予了“黑盒”特征，这是提高重用性和可靠性的关键。这意味着-一个类可以全面地改变存储数据的方式，只要仍旧使用同样的方法操作数据。 对象的三个特性要想使用OOP，- -定要清楚对象的三个主要特性:●对象的行为(behavior)一-可以对对象施加哪些操作，或可以对对象施加哪些方法?●对象的状态(state)一当施加那些方法时，对象如何响应?●对象标识(identity) 一如何辨别具有相同行为与状态的不同对象?同一个类的所有对象实例，由于支持相同的行为而具有家族式的相似性。对象的行为是月可调用的方法定义的。 识别类类与类直接的关系使用预定义的类使用用户自定义的类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package data;import java.time.LocalDate;public class Employee { //instance fields private String name; private double salary; private LocalDate hireDay; public Employee(String name, double salary, int year ,int mouth ,int day) { this.name = name; this.salary = salary; this.hireDay = LocalDate.of(year,mouth,day); } public String getName() { return name; } public double getSalary() { return salary; } public LocalDate getHireDay() { return hireDay; } public void raiseSalary(double byPercent){ double raise = salary * byPercent / 100; salary += raise; }}package data;public class EmployeeTest { public static void main(String[] args) { //1. 构建一个Employee数组 Employee[] employees = new Employee[3]; employees[0] = new Employee(&quot;binshow&quot;,20000,1997,1,8); employees[1] = new Employee(&quot;zkd&quot;,30000,1996,10,22); employees[2] = new Employee(&quot;aaa&quot;,10000,1997,7,18); //2. 将每个员工的工资都提高 for (Employee e : employees){ e.raiseSalary(5); } //3. 打印每个员工的信息 for (Employee e : employees){ System.out.println(&quot;name = &quot; + e.getName() + &quot;,salary = &quot; + e.getSalary() + &quot;,hireday = &quot; + e.getHireDay()); } }} 构造器与类重名，构造Employee类实例的对象时，构造器会运行将实例域初始化为所定义的状态 构造器与类重名 每个类可以有1个或多个构造器 构造器可以有0，1或多个参数 构造器没有返回值 构造器总是伴随着new操作一起调用 隐式参数和显示参数 12345678910 Employee employee = new Employee(&quot;binshow&quot;,20000,1997,1,8)employee.raiseSalary(5); //employee 为隐式参数 、 5 为显示参数//在每个方法中，可以用this表示隐式参数，比如下面：public void raiseSalary(double byPercent){ double raise = this.salary * byPercent / 100; this.salary += raise; } 封装的优点：getName、getSalary 、getHireDay等 12345678910111213141516 //返回实例域值，称为域访问器 public String getName() { return name; } public double getSalary() { return salary; } public LocalDate getHireDay() { return hireDay; }1. name是一个只读域，一旦在构造器中设置完毕，没有任何一个方法可以对其修改，确保了name域不会受到外界的破坏。2. 在本类中 salary不是只读域，但是它只能用raiseSalary方法来修改。一旦这个域出现错误，只要找到这个方法就可以调试了。 不能返回引用可变对象的访问器方法 1234567891011121314151617181920public class Person { // 如果用Date类型的域 private Date birthday; public Person(Date birthday) { this.birthday = birthday; } public Date getBirthday() { return birthday; // 这样是有问题的 }}//test Date date = new Date(); System.out.println(date); Person person = new Person(date); Date d = person.getBirthday(); d.setTime(d.getTime() - 500000); //Date对象是可变的，破坏了封装性。因为 d 和 person.birthday 指向的是同一个对象 System.out.println(person.getBirthday()); 12345678910111213//正确做法是：首先对他进行克隆,克隆完之后的对象是存在另一个位置的副本public class Person { private Date birthday; public Person(Date birthday) { this.birthday = birthday; } public Date getBirthday() { return (Date) birthday.clone(); }} 基于类的访问权限：一个方法可以访问所属类的所有对象的私有数据。 私有方法 final实例域：final修饰实例域表明在构造该类对象的时候必须初始化这样的域。并且在后面的操作中不能被修改。 final修饰符大部分都应用于修饰基本数据类型或者不可变的类（比如String类，类中的每个方法都不会改变其对象） 123456789public class Person { private final String name; //必须要有基于name的构造方法，且不能有对应的set方法 public Person(String name) { this.name = name; }} 静态域和静态方法 静态域： 1234567public class Person { //静态域，是类所共有的，而不是属于单个对象 // 新建100个person实例，每个person都有自己的id，但是它们共有一个personId private static int personId; private int id ;} 静态常量：static + final 共同修饰的域即为常量。 静态方法：可以认为是没有隐式参数的方法。不能对对象实施操作的方法，可以通过类名.方法来实施。 工厂方法： 1 方法参数（值传递和引用传递）将参数传递给方法(或函数)的一些专业术语： 按值调用( call by value)表示方法接收的是调用者提供的值。 按引用调用( call by reference)表示方法接收的是调用者提供的变量地址。 一个方法可以修改传递引用所对应的变量值，而不能修改传递值调用所对应的变量值。 Java程序设计语言总是采用按值调用。也就是说，方法得到的是所有参数值的一个拷贝，特别是，方法不能修改传递给它的任何参数变量的内容。 12double percent = 10; employees[1].raiseSalary(percent); //这个方法调用完之后，percent的值还是10 1234567891011121314151617 double percent = 10; tripleValue(percent); System.out.println(percent); //还是 10 也就是说方法不能改变基本数据类型的参数 private static void tripleValue(double x){ x = 3 * x; }分析具体的执行过程： 1. x被初始化为percent值的一个拷贝(也就是10) 2. x被乘以3后等于30。但是percent仍然是10。 3. 这个方法结束之后，参数变量x不再使用 然而，方法参数共有两种类型:●基本数据类型(数字、布尔值)。●对象引用。 12345678910111213Employee binshow = new Employee(&quot;binshow&quot;, 20000, 1997, 1, 8);System.out.println(&quot;tripleValue方法调用前：&quot; + binshow.getSalary()); // 20000.0tripleValue(binshow);System.out.println(&quot;tripleValue方法调用后：&quot; + binshow.getSalary()); // 60000.0private static void tripleValue(Employee x){ x.raiseSalary(200); }分析具体的执行过程:1. x被初始化为binshow值的拷贝，这里是一个对象的引用2. raiseSalary 方法应用于这个对象引用。x和binshow同时引用的那个Employee对象的薪金提高了200%。3. 方法结束后，参数变量x不再使用。当然，对象变量binshow继续引用那个薪金增至3倍的雇员. 方法得到的是对象引用的拷贝，对象引用和其他的拷贝同时引用同一个对象。 Java是引用调用吗？其实不是 123456789101112Employee binshow = new Employee(&quot;binshow&quot;, 20000, 1997, 1, 8);Employee zkd = new Employee(&quot;zkd&quot;, 30000, 1996, 10, 22);swap(binshow,zkd);//如果java采用的是引用传递，这一步之后binshow和zkd应该会交换数据,实际上并没有System.out.println(binshow.getName()); //binshowSystem.out.println(zkd.getName()); //zkdpublic static void swap(Employee a , Employee b){ Employee tem = a; a = b; b = tem; } 这个过程说明: Java程序设计语言对对象采用的不是引用调用，实际上，对象引用是按值传递的。下面总结一-下Java中方法参数的使用情况:●一个方法不能修改–个基本数据类型的参数(即数值型或布尔型)。●一个方法可以改变-一个对象参数的状态。●一个方法不能让对象参数引用-一个新的对象。 1234567891011121314151617181920212223242526272829303132333435363738/** * 方法参数的测试类 */public class ParamTest { public static void main(String[] args) { System.out.println(&quot;Test tripleValue:方法不能改变基础数据类型&quot;); double precent = 10; System.out.println(&quot;before: precent = &quot; + precent); tripleValue(precent); System.out.println(&quot;after: precent = &quot; + precent); System.out.println(&quot;Test tripleSalary：方法可以改变对象类型的状态&quot;); Employee binshow = new Employee(&quot;binshow&quot;, 20000); System.out.println(&quot;before: binshow的salary = &quot; + binshow.getSalary()); tripleSalary(binshow); System.out.println(&quot;after: binshow的salary = &quot; + binshow.getSalary()); } private static void tripleValue(double x){ x = 3 * x; System.out.println(&quot;End the Method, x = &quot; + x); } private static void tripleSalary(Employee x){ x.raiseSalary(200); System.out.println(&quot;End the Method, salary = &quot; + x.getSalary()); } public static void swap(Employee a , Employee b){ Employee tem = a; a = b; b = tem; }} 对象构造包类路径注释类设计的技巧继承异常异常处理的任务是将控制权从错误产生的地方转移给能够处理这种情况的错误处理器。 异常分类异常对象都是派生于Throwable类的一个实例。如果Java内置的异常类不能满足需求，用户还可以创建自己的异常类。 由程序错误导致的异常属于 RuntimeException。而程序本身没有问题，发生像IO错误这类的异常属于其他异常。 RunTimeException分类： 错误的类型转换 数组访问越界 访问null指针。（使用变量之前需要检测是否为null来避免空指针） 其他异常分类： 试图在文件尾部后面读取数据 试图打开不存在的文件 试图根据给定的字符串查找Class对象，但这个字符串表示的类并不存在。 Error和RunTimeException属于非受检异常。 声明受检异常一个方法不仅需要告诉编译器将要返回什么值，还要告诉编译器有可能发生什么错误。也就是说一个方法必须要声明所有可能抛出的受检异常。而非受检异常要么不可控制Error，要么就应该避免发生RunTimeException 12345java.io.FileInputStream#FileInputStream(java.lang.String) //构造方法会抛出异常：文件找不到异常public FileInputStream(String name) throws FileNotFoundException { this(name != null ? new File(name) : null); } 自定义异常类123456789101112131415161718192021222324252627282930313233public class FileFormatException extends IOException { public FileFormatException(){} //构造一个新的 throwable对象。带有特定的详细描述信息 public FileFormatException(String gripe){ super(gripe); }}public class Test { //main方法也没有处理这个异常，继续往上抛 public static void main(String[] args) throws FileFormatException { read(0); } //方法需要抛出这个异常 public static void read(int x ) throws FileFormatException { if (x == 0){ throw new FileFormatException(&quot;x不能为0，格式错误&quot;); } System.out.println(x); }}运行之后就会发生：Exception in thread &quot;main&quot; exception.FileFormatException: x不能为0，格式错误 at exception.Test.read(Test.java:13) at exception.Test.main(Test.java:6) 捕获异常如果某个异常发生的时候没有任何地方进行捕获。那程序就会终止执行，并在控制台上打印出异常信息。包括异常的类型和堆栈的内容等。如上面代码块所示。 捕获异常需要通过try/catch语句块： 1234567891011121314151617181920public static void main(String[] args) { try { read(0); //业务代码 //如果在这里的任何代码抛出了一个catch子句中说明的异常类，那么程序将会跳过try语句块中的其他代码 // 执行catch子句中的处理器代码 } catch (FileFormatException e) { //handle for this Exception System.out.println(&quot;read的方法参数不能为0&quot;); } } public static void read(int x ) throws FileFormatException { if (x == 0){ throw new FileFormatException(&quot;x不能为0，格式错误&quot;); } System.out.println(x); } finally子句当代码抛出一个异常时就会终止方法中剩余代码的处理。如果方法获得了一些本地资源，又如何保证在退出方法之前可以回收掉所有的资源呢？– finally子句。 不管是否有异常被捕获，finally子句中 的代码都会被执行。 1234567891011121314151617public static void main(String[] args) throws FileNotFoundException { InputStream in = new FileInputStream(&quot;aa&quot;); try{ // ====== 1 ===== // code might throw exceptions // ====== 2 ===== }catch (IOException e){ // ===== 3 ===== // show error message // ===== 4 ===== }finally { // ===== 5 ===== in.close(); } // ===== 6 ===== } 代码无异常，首先执行try语句块的全部代码，然后执行finally子句中的代码，再执行后面的语句。 1256 抛出了一个可以catch子句捕获的异常。程序会执行try语句块的代码直到发生异常，再跳过剩余代码转而执行catch子句中的代码 如果catch子句中没有抛出异常，执行顺序为 13456 如果catch子句中也抛出了异常，异常将会抛回这个方法的调用者。执行顺序为 135 抛出了一个不在catch子句中的异常，程序会执行try语句块的代码直到发生异常，再跳过剩余代码转而执行finally子句中的代码，并将异常抛给这个方法的调用者，执行顺序为15 建议这样写： 1234567891011121314151617181920public static void main(String[] args) throws FileNotFoundException { InputStream in = new FileInputStream(&quot;aa&quot;); try{ try{ //业务代码 } finally { in.close(); } }catch (IOException e){ // show error message } // 内层的 try语句只有一个职责就是确保关闭输入流 // 外层的 try语句也只有一个职责，就是确保报告出现的错误。这样设计还会报告finally子句中出现的错误 } 当finally子句中包含return语句时会出现意外情况。当利用return语句从try语句块中退出，在方法返回前finally子句的内容还是会被执行。如果finally子句中也有一个return语句，就会覆盖掉原来的返回值。 1234567891011System.out.println(f(4)); //16System.out.println(f(2)); //0 public static int f(int x){ try { int r = x * x; return r; }finally { if (x == 2) return 0; }} 断言断言的概念断言机制允许在测试期间向代码中插入一些检查语句。当代码发布时，这些插入的检测语句会被自动移走。","link":"/2021/08/01/JavaSE-%E4%BA%8C-%E5%AF%B9%E8%B1%A1%E5%92%8C%E7%B1%BB/"},{"title":"JavaSE-五-异常处理","text":"程序在运行的过程中肯定会发生各种各样的异常，当发生异常时，我们该如何处理呢？ 异常处理 异常分类 捕获异常 finally子句 异常处理错误用户期望在程序运行在出现错误时，程序能够采用一些理智的行为，比如： 返回到一种安全的状态，并能够让用户执行一些其他的命令 允许用户保存所有操作的结果，并以妥善的方式来终止程序 异常处理的任务就是将控制权从错误产生的地方转移到能够处理这种情况的错误处理器。 在Java中，如果某个方法不能够采用正常的途径完整它的任务，就可以通过另外一个路径退出方法。在这种情况下，方法并不返回任何值，而是抛出( throw) 一个封装了错误信息的对象。需要注意的是，这个方法将会立刻退出，并不返回任何值。此外，调用这个方法的代码也将无法继续执行，取而代之的是，异常处理机制开始搜索能够处理这种异常状况的异常处理器(exceptionhandler)。 异常分类异常具有自己的语法和特定的继承结构，异常对象都是Throwable类的一个实例。如果Java内置的异常类不能满足需求，用户还可以创建自己的异常类。 Throwable类有两个子类，分别是 Error和 Exception： Error类描述了Java运行时系统的内部错误和资源耗尽错误。应用程序不应该抛出这种类型的对象。如果出现了这样的内部错误，除了通告给用户，并尽力使程序安全地终止之外，再也无能为力了。 Exception层次结构又分解为两个分支:一个分支派生于RuntimeException;另一个分支包含其他异常。划分两个分支的规则是:由程序错误导致的异常属于RuntimeException ;而程序本身没有问题，但由于像I/O错误这类问题导致的异常属于其他异常。 一些常见的RuntimeException（如果出现了基本都是你的问题，类似于）: 错误的类型转换 数组访问越界 访问null指针 还有一种说法是 Error和RuntimeException类的所有异常称为 非受检异常（unchecked），所有其他的异常称为受检异常（check）。编译器将核查是否为所有的受检异常提供异常处理器。 声明受检异常在程序运行时，遇到了如下的情况： 读取文件的代码发现读取的文件不存在 处理过程中IO发生了异常 12345//java.io.FileInputStream#FileInputStream(java.io.File)// 根据给定的String参数产生一个FileInputStream对象，但是如果没有这个文件就会抛出异常 FileNotFoundExceptionpublic FileInputStream(File file) throws FileNotFoundException { //...} 自己编写代码的时候，遇到下面4种情况需要抛出异常： 调用一个抛出受查异常的方法，例如，FileInputStream构造器。 程序运行过程中发现错误，并且利用throw语句抛出一个受查异常。 程序出现错误，例如，a[-1]=0 会抛出一个ArrayIndexOutOfBoundsException这样的非受查异常。 Java虚拟机和运行时库出现的内部错误。 编写异常类1234567891011121314151617181920212223242526272829303132public class FileFormatException extends IOException { public FileFormatException(){} //构造一个新的 throwable对象。带有特定的详细描述信息。这样超类的toString方法就会打印出这些详细信息，调试中很有用 public FileFormatException(String gripe){ super(gripe); }}public class Test { //main方法也没有处理这个异常，继续往上抛 public static void main(String[] args) throws FileFormatException { read(0); } //方法需要抛出这个异常 public static void read(int x ) throws FileFormatException { if (x == 0){ throw new FileFormatException(&quot;x不能为0，格式错误&quot;); } System.out.println(x); }}运行之后就会发生：Exception in thread &quot;main&quot; exception.FileFormatException: x不能为0，格式错误 at exception.Test.read(Test.java:13) at exception.Test.main(Test.java:6) 捕获异常如果某个异常发生的时候没有任何地方进行捕获。那程序就会终止执行，并在控制台上打印出异常信息。包括异常的类型和堆栈的内容等。如上面代码块所示。 捕获异常需要通过try/catch语句块(可以对不同类型的异常做不同的处理，多个catch)： 1234567891011121314151617181920public static void main(String[] args) { try { read(0); //业务代码 //如果在这里的任何代码抛出了一个catch子句中说明的异常类，那么程序将会 // 1. 跳过try语句块中的其他代码 // 2. 执行catch子句中的处理器代码 } catch (FileFormatException e) { //handle for this Exception System.out.println(&quot;read的方法参数不能为0&quot;); } } public static void read(int x ) throws FileFormatException { if (x == 0){ throw new FileFormatException(&quot;x不能为0，格式错误&quot;); } System.out.println(x); } finally子句当代码抛出一个异常时就会终止方法中剩余代码的处理。如果方法获得了一些本地资源，又如何保证在退出方法之前可以回收掉所有的资源呢？– finally子句。 不管是否有异常被捕获，finally子句中 的代码都会被执行： 1234567891011121314151617public static void main(String[] args) throws FileNotFoundException { InputStream in = new FileInputStream(&quot;aa&quot;); try{ // ====== 1 ===== // code might throw exceptions // ====== 2 ===== }catch (IOException e){ // ===== 3 ===== // show error message // ===== 4 ===== }finally { // ===== 5 ===== in.close(); } // ===== 6 ===== } 代码无异常，首先执行try语句块的全部代码，然后执行finally子句中的代码，再执行后面的语句。 1256 抛出了一个可以catch子句捕获的异常。程序会执行try语句块的代码直到发生异常，再跳过剩余代码转而执行catch子句中的代码 如果catch子句中没有抛出异常，执行顺序为 13456 如果catch子句中也抛出了异常，异常将会抛回这个方法的调用者。执行顺序为 135 抛出了一个不在catch子句中的异常，程序会执行try语句块的代码直到发生异常，再跳过剩余代码转而执行finally子句中的代码，并将异常抛给这个方法的调用者，执行顺序为15 建议这样写： 1234567891011121314151617181920public static void main(String[] args) throws FileNotFoundException { InputStream in = new FileInputStream(&quot;aa&quot;); try{ try{ //业务代码 } finally { in.close(); } }catch (IOException e){ // show error message } // 内层的 try语句只有一个职责就是确保关闭输入流 // 外层的 try语句也只有一个职责，就是确保报告出现的错误。这样设计还会报告finally子句中出现的错误 } 当finally子句中包含return语句时会出现意外情况。当利用return语句从try语句块中退出，在方法返回前finally子句的内容还是会被执行。如果finally子句中也有一个return语句，就会覆盖掉原来的返回值。 1234567891011System.out.println(f(4)); //16System.out.println(f(2)); //0 public static int f(int x){ try { int r = x * x; return r; }finally { if (x == 2) return 0; }} EffectiveJava中对异常的规范 异常是专门为异常情况设计的。不要用来做普通的流控制 对于可以恢复的情况使用受检异常，编程错误使用运行时异常。 如果期望调用者如果期望调用者可以对异常进行恢复，就应该使用受检异常。抛出一个受检异常，强迫调用者必须在一个catch块里处理这个异常，或者向外抛出这个异常。对于可以恢复的情况，抛出受检异常，而对于程序错误，抛出非受检的异常。无法确定是否可以恢复的时候，抛出非受检异常。不要定义任何的既不是受检异常也不是运行时异常得的可抛出结构。给你的受检异常提供一些方法来帮助恢复异常。 避免受检异常的不必要的使用 优先使用标准的异常 抛出和抽象对应的异常 为每个方法抛出的所有异常建立文档 在细节信息中包含失败-捕获信息 努力保持失败的原子性 不要忽略异常","link":"/2021/08/03/JavaSE-%E4%BA%94-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"},{"title":"JavaSE-四-接口与内部类","text":"接口设计 接口（Interface）技术主要用来描述类具有什么功能，而并不给出每个功能的具体实现。 接口概念如果类遵从某个特定接口，那么就要履行这项服务。比如Arrays类中的sort方法可以对对象数组进行排序，但有一个要求就是对象所属的类必须实现 了Comparable接口。任何实现Comparable接口的类都需要包含compareTo方法，如果没有给出具体的泛型的话，这个方法的参数是一个Object对象，返回一个整型数值。 1234567891011121314java.util.Arrays#sort(T[], java.util.Comparator&lt;? super T&gt;) /**Sorts the specified array of objects according to the order induced by the specified comparator. All elements in the array must be mutually comparable by the specified comparator (that is, c.compare(e1, e2) must not throw a ClassCastException for any elements e1 and e2 in the array).*/ public static &lt;T&gt; void sort(T[] a, Comparator&lt;? super T&gt; c) { if (c == null) { sort(a); } else { if (LegacyMergeSort.userRequested) legacyMergeSort(a, c); else TimSort.sort(a, 0, a.length, c, null, 0, 0); } } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class Employee implements Comparable&lt;Employee&gt; { //instance fields private String name; private double salary; private LocalDate hireDay; public Employee(String name, double salary) { this.name = name; this.salary = salary; } public Employee(String name, double salary, int year , int mouth , int day) { this.name = name; this.salary = salary; this.hireDay = LocalDate.of(year,mouth,day); } public String getName() { return name; } public double getSalary() { return salary; } public LocalDate getHireDay() { return hireDay; } public void raiseSalary(double byPercent){ double raise = this.salary * byPercent / 100; this.salary += raise; } @Override // 按照薪水排序 public int compareTo(Employee o) { return Double.compare(salary , o.getSalary()); }}// Test给Employee排序public class Test { public static void main(String[] args) { Employee[] staff = new Employee[3]; staff[0] = new Employee(&quot;a&quot;,100); staff[1] = new Employee(&quot;b&quot;,200); staff[2] = new Employee(&quot;c&quot;,300); // 如果 Employee 未实现 Comparable 接口，排序的时候就会爆出如下异常： // data.Employee cannot be cast to java.lang.Comparable Arrays.sort(staff); for (Employee e : staff){ System.out.println(&quot;name = &quot; + e.getName() + &quot;, salary = &quot; + e.getSalary()); } }} 接口的特性 接口中的所有方法都自动属于public 接口可以包含多个方法，可以定义常量（不推荐，因为任何实现接口的类都会继承这些常量） 接口中绝不能含有实例域。 接口中可以含有默认的方法实现 接口不能通过new实例化，但是可以声明一个接口的变量引用实现了接口的类对象。 12Comparable&lt;Object&gt; objectComparable = new Comparable&lt;&gt;();//error Comparable x = new Employee(&quot;d&quot;,400); 接口也可以继承。 接口和抽象类为什么不直接把Comparable设计成抽象类呢？ Java中只能实现单继承，但是可以实现多个接口。 静态方法Java8之后允许在接口中增加静态方法了。 目前为止通用的做法都是将静态方法放在伴随类中。比如标准库中的Collection/Collections或Path/Paths。 默认方法实现可以在接口中的方法提供一个默认实现，必须要用default修饰符来标记。 12345//java.util.Collection#stream//Collection 接口的默认对流的实现：default Stream&lt;E&gt; stream() { return StreamSupport.stream(spliterator(), false); } 默认实现的方法可以不在接口实现类中重写。为什么会这样发展呢？ 是为了兼容以前的代码，否则如果在接口中添加新的方法，之前的接口实现类都要重写该方法。 解决默认方法冲突情景：如果在一个接口中将一个方法定义为默认方法，然后又在父类或另一个接口中定义了同样的方法： 超类优先（确保兼容性） 接口冲突 接口实例接口和回调回调( callback)是一种常见的程序设计模式。在这种模式中，可以指出某个特定事件发生时应该采取的动作。例如，可以指出在按下鼠标或选择某个菜单项时应该采取什么行动。然而，由于至此还没有介绍如何实现用户接口，所以只能讨论一些与上述操作类似，但比较简单的情况。 123456789101112131415161718192021222324252627/** * @author shengbinbin * 定时器测试 */public class TimerTest { public static void main(String[] args) { TimePrinter listener = new TimePrinter(); Timer timer = new Timer(1000, listener); timer.start(); //计时器开始 JOptionPane.showMessageDialog(null,&quot;Quit program?&quot;); //展示仪表盘 System.exit(0); }}/** * @author shengbinbin * 打印时间的类,实现了监听事件的接口，重写了事件发生后的回调方法 */public class TimePrinter implements ActionListener { @Override public void actionPerformed(ActionEvent e) { System.out.println(&quot;At the tone , the time is &quot; + new Date()); Toolkit.getDefaultToolkit().beep();//响一声 }} Cloneable接口Cloneable接口指示一个类提供了一个安全的clone方法。但是clone方法是Object类的一个protected方法。 Object类如何实现clone方法的，它对这个对象一无所知，所以只能逐个域的进行拷贝。如果对象中的所有数据域都是数值或其他基本类型，拷贝这些域没有任何问题，但是如果对象包含子对象的引用，拷贝域就会得到相同子对象的另一个引用，这样的话原对象和克隆的对象仍然会共享一些信息。 默认的克隆操作是浅拷贝：并不会克隆对象中引用的其他对象。如果原对象和浅克隆的对象共享的子对象是不可变的（比如String）这种情况下是安全的。 不给通常的子对象都是可变的，必须要重新定义clone方法来实现一个深拷贝，同时克隆出所有的子对象。比如在这个例子中hireDay是一个可变的 Date类，所以也需要克隆（如果是个不可变的LocalDate类的实例就不需要处理了）。 对于每一个类，需要确定： 实现Cloneable接口，指定public修饰符。 默认的clone方法是否满足要求 是否在可变的子对象上调用clone来修补默认的clone方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Employee implements Comparable&lt;Employee&gt;, Cloneable { // instance fields private String name; private double salary; private Date hireDay; public Employee(String name, double salary) { this.name = name; this.salary = salary; } public String getName() { return name; } public double getSalary() { return salary; } public Date getHireDay() { return hireDay; } public void setHireDay(Date hireDay) { this.hireDay = hireDay; } public void raiseSalary(double byPercent) { double raise = this.salary * byPercent / 100; this.salary += raise; } @Override public int compareTo(Employee o) { return Double.compare(salary, o.getSalary()); } @Override public Employee clone() throws CloneNotSupportedException { Employee cloned = (Employee) super.clone(); cloned.hireDay = (Date) hireDay.clone(); // 建立深拷贝 return cloned; }}","link":"/2021/08/02/JavaSE-%E5%9B%9B-%E6%8E%A5%E5%8F%A3%E4%B8%8E%E5%86%85%E9%83%A8%E7%B1%BB/"},{"title":"JavaSE(六)泛型、枚举和注解","text":"本节主要讲述的是泛型、枚举和注解的使用方法和设计原则—来源于Java核心卷1和EffectiveJava 泛型程序设计为什么要使用泛型泛型（generic programming）意味着编写的代码可以对多种不同类型的对象重用。比如说一个ArrayList就可以收集任何类的对象。. 在java5使用泛型前，ArrayList是内部维护了一个Object引用的数组，add方法都是add一个Object类型的元素。这种方式存在两个问题： 获取一个值时必须进行类型转换。 可以向数组列表中添加任何类型的值不会编译错误。 引入类型参数后： 12345之前：ArrayList list = new ArrayList();之后：ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); // 这样一看就知道这个数组列表是String对象。get的时候返回的也是String类型//编译器还会检查你add的时候是不是传入的String类型的参数。 定义简单的范型类123456789101112131415161718192021222324252627282930313233343536373839404142public class Pair&lt;T&gt; { //类型变量T在整个类中可以用于指定方法的返回类型和字段、局部变量的类型。 private T first; private T second; public Pair(){ first = null; second = null; } public Pair(T first , T second){ this.first = first; this.second = second; } public T getFirst(){return first;} public T getSecond(){return second;} public void setFirst(T newValue){ first = newValue; } public void setSecond(T newValue){ second = newValue; }}//可以定义多个类型变量public class Pair&lt;T,U&gt; {}//可以用具体的类型实例化类型变量，比如Pair&lt;String&gt;Pair&lt;String&gt;(String,String) 方法： String getFirst(); String getSecond(); void setFirst(String); void setSecond(String);也就是说：泛型类相当于普通类的工厂。 使用上面定义的泛型类： 123456789101112131415161718192021222324public class PairTest { public static void main(String[] args) { String[] words = {&quot;Mary&quot; , &quot;had&quot; , &quot;a&quot; , &quot;little&quot; , &quot;lamb&quot;}; Pair&lt;String&gt; minax = ArrayAlg.minax(words); System.out.println(&quot;min = &quot; + minax.getFirst()); System.out.println(&quot;max = &quot; + minax.getSecond()); }}class ArrayAlg{ //返回String数组中的最大值和最小值，用Pair对象来进行包装 public static Pair&lt;String&gt; minax(String[] a){ if (a == null || a.length == 0) return null; String min = a[0]; String max = a[0]; for (int i = 1; i &lt; a.length; i++) { if (min.compareTo(a[i]) &gt; 0) min = a[i]; if (max.compareTo(a[i]) &lt; 0) max = a[i]; } return new Pair&lt;&gt;(min , max); }} 泛型方法12345678910111213class ArrayAlg{ //返回中间的元素 public static&lt;T&gt; T getMiddle(T...a){ return a[a.length/2]; }}测试：String mid = ArrayAlg.getMiddle(&quot;a&quot; , &quot;b&quot; , &quot;String&quot;,&quot;c&quot;,&quot;d&quot;);String middle = ArrayAlg.getMiddle(mid);System.out.println(&quot;middle = &quot; + middle); 类型变量的限定12345678910111213141516171819202122232425// 由于T是实现了Comparable接口的类，所以才可以调用compareTo方法。 public static &lt;T extends Comparable&gt; T min(T[] a){ if (a == null || a.length == 0) return null; T smallest = a[0]; for (int i = 1; i &lt; a.length; i++) { if (smallest.compareTo(a[i]) &gt; 0) smallest = a[i]; } return smallest; }一个类型变量可以有多个限定，比如 T extends Comparable &amp; Serializable //测试： LocalDate[] birthdays = { LocalDate.of(1991,12,9), LocalDate.of(1997,1,8), LocalDate.of(1996,10,22), LocalDate.of(2001,7,29), }; LocalDate min1 = ArrayAlg.min(birthdays); System.out.println(&quot;min1 = &quot; + min1.toString()); 泛型代码和虚拟机类型擦除定义的泛型类型，虚拟机都在编译的时候擦除类型参数，提供一个相应的原始类型。 比如：上文的Pair类的原始类型如下（没有限定的类型参数转换为Object） 1234567891011121314151617181920212223242526public class Pair { //类型变量T在整个类中可以用于指定方法的返回类型和字段、局部变量的类型。 private Object first; private Object second; public Pair(){ first = null; second = null; } public Pair(Object first , Object second){ this.first = first; this.second = second; } public Object getFirst(){return first;} public Object getSecond(){return second;} public void setFirst(Object newValue){ first = newValue; } public void setSecond(Object newValue){ second = newValue; }} 原始类型用第一个限定类替换类型变量。 转换泛型表达式当一个泛型方法调用时，如果擦除了返回类型，编译器会插入强制类型转换。 123456Pair&lt;Employee&gt; staff = new Pair&lt;&gt;(); // getFirst方法编译完之后返回的类型是Object，编译器会自动插入Employee的强制类型转换 // 也就是说：编译器将这个方法转换成了两条虚拟机指令 // 1. 对原始方法 Pair.getFirst的调用 // 2. 将放回的 Object类型强制转换为Employee Employee first = staff.getFirst(); 转换泛型方法调用遗留代码限制和局限性大多数限制都是由于类型擦除而引起的 不能用基本类型实例化类型参数。只有Pair，没有Pair. 运行时的类型查询只适用于原始类型。 不能创建参数化类型的数组。 Varargs警告 不能实例化类型变量 不能构造泛型数组。 泛型类的静态上下文中类型变量无效 不能抛出或捕获泛型类的实例 可以取消对检查型异常的检查 范型类型的继承规则通配符类型反射和泛型EffectiveJava中的泛型泛型的需求：在拥有泛型之前，你必须对每个从集合中读出来的对象进行类型转换。如果有人不小心插入了一个错误类型的对象，这个类型转换就会再运行时失败。使用泛型，你就可以告诉编译器，那些类型的对象允许加入这个集合。编译器会自动地为你进行转换，然后当你要插入一个错误类型的时候，在编译地时候就会报错 Item26 不要使用原生类型每一个泛型都包括一组参数类型，其构成方式如下：首先是类或者接口的名字，然后紧跟着用&lt;&gt;括起来的实际类型参数，实际类型参数和泛型中的形式类型参数对应 原生类型指的是没有任何类型参数的泛型类型的名称。 1234567891011121314151617181920212223242526272829// 只是为了兼容之前的代码,不建议直接使用原生类型SetSet set = new Set;// 错误的插入了不同类型的 数据类型，编译时不会报错set.add(1);set.add(&quot;String&quot;);set.add('a');同理：public static void main(String[] args) { List&lt;String&gt; strings = new ArrayList&lt;&gt;(); //1. 添加的时候不会报错，编译也能通过 unsafeAdd(strings , Integer.valueOf(43)); // safeAdd(strings , Integer.valueOf(43)); //2. 取出的时候就会发生类型转换异常 String s = strings.get(0); //java.lang.ClassCastException } //unsafeAdd 方法的参数是一个原生类型List。所以有风险，应该用参数化类型List来代替 private static void unsafeAdd(List list, Object o) { list.add(o); } private static void safeAdd(List&lt;String&gt; list, String o) { // 加上参数化类型之后，这边编译就会报错 list.add(o); } Item27 消除非受检的警告Item28 list优先于数组为什么呢？ 数组是协变的（covariant），泛型是非协变的。 1234567//1. 下面编译不报错，启动才报错// 由于Long是Object的子类，所以 Long[] 也是 Object[]的子类 Object[] objectArray = new Long[1]; objectArray[0] = &quot;abc&quot;;// java.lang.ArrayStoreException //2. 用list编译就报错了 List&lt;Object&gt; list = new ArrayList&lt;Long&gt;(); 数组是具体化的（reified），也就是说在运行的时候数组是知道其元素类型的，并且会强制执行类型限制。相反，泛型是通过泛型擦除来实现的，这就意味着只能在编译器强制执行类型限制，运行的时候是会擦除其元素信息的。类型擦除是为了允许泛型代码可一个之前没有泛型的老代码进行互用 当你在转换到数组类型时遇到泛型数组创建错误或者非受检转换异常的时候，最好的方法是使用集合类型List，而不是数组类型E[]。可能会牺牲一些性能，但是换回的却是更好的类型安全性和互用性。 12345678910111213141516171819202122232425// Chooser - a class badly in need of generics!// 调用这个类的choose方法时，返回的都是Object对象，需要手动进行类型准换public class Chooser { private final Object[] choiceArray; public Chooser(Collection choices) { choiceArray = choices.toArray(); } public Object choose() { Random rnd = ThreadLocalRandom.current(); return choiceArray[rnd.nextInt(choiceArray.length)]; }}public static void main(String[] args) { ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(1); list.add(2); list.add(3); Chooser chooser = new Chooser(list); Integer choose = (Integer) chooser.choose(); System.out.println(&quot;choose = &quot; + choose); } 为了避免进行可能出错的强制类型转换，引入泛型,但还是有点问题： 12345678910111213141516public class Chooser&lt;T&gt; { private final T[] choiceArray; public Chooser(Collection&lt;T&gt; choices) { //它不能确定在运行时的类型转换的安全性，因为程序不知道类型T代表的是什么 //泛型中的元素类型信息在运行时被擦除了 choiceArray = (T[]) choices.toArray(); } public Object choose() { Random rnd = ThreadLocalRandom.current(); return choiceArray[rnd.nextInt(choiceArray.length)]; }} 为了消除这个非受检转换警告，使用list来代替数组： 12345678910111213public class Chooser&lt;T&gt; { private final List&lt;T&gt; choiceArray; public Chooser(Collection&lt;T&gt; choices) { choiceArray = new ArrayList&lt;&gt;(choices); } public T choose() { Random rnd = ThreadLocalRandom.current(); return choiceArray.get(rnd.nextInt(choiceArray.size())); }} 数组和泛型有一些非常不同的类型规则，数组是协变和可具体化的；而泛型是非协变和擦除的。因此，数组可以提供运行时类型安全但是没有贬义是类型安全，而泛型却恰恰相反。一般来说，数组和泛型不能混用。如果你发现你自己混用了他们，并且出现了编译error或者warning。你的第一反应应该是用列表替换数组。 Item29 优先考虑泛型一个例子：未进行参数化的Stack 12345678910111213141516171819202122232425public class Stack { private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { elements = new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public Object pop() { if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} 将这个Stack使用泛型，同时还需要兼容之前的版本： 声明参数类型为E 将所有的Object改成E 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public class Stack&lt;E&gt; { private E[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { // 这里报错 。。 不能创建一个泛型数组。 elements = new E[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public E pop() { if (size == 0) throw new EmptyStackException(); E result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }}// 两种解决方案：1. elementst数组，再进行强制类型转换为E数组 public class Stack&lt;E&gt; { private E[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { // 这里报错 elements = (E[]) new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(E e) { ensureCapacity(); elements[size++] = e; } public E pop() { if (size == 0) throw new EmptyStackException(); E result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} 2. 将elements设为object数组，每次pop访问数组中的元素的时候进行强制类型转换 public class Stack&lt;E&gt; { private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { // 这里报错 elements = new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public E pop() { if (size == 0) throw new EmptyStackException(); E result = (E) elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} 第一种方法可读性更强：数组被声明为E[]，清晰地表示了它只包含E实例；并且更加简洁：在一个典型的泛型类中，你可以以在代码里看到很多次数组，第一中方法只需要进行一次转换（当数组创建的时候）；而第二种方法需要在每次读取数组元素的时候都进行转换。因此第一种方法要更受欢迎一些，在实际情况中，也用得更多些。但是第一种方法确实造成了”堆泄露“（详见Item32）：因为数组的运行时类型和编译时类型不一致（除非E刚刚也是Object）。有一些程序员对这点深恶痛绝，选择使用第二种方法。 Item30 优先考虑泛型方法基于参数化类型进行计算的静态工具方法通常来说都是泛型的。 1234//java.util.Collections#sort(java.util.List&lt;T&gt;, java.util.Comparator&lt;? super T&gt;)public static &lt;T&gt; void sort(List&lt;T&gt; list, Comparator&lt;? super T&gt; c) { list.sort(c); } 写泛型方法和泛型类型差不多： 1234567891011121314151617// 使用了原始类型，是有缺陷的。 // 虽然编译可以通过，但是有很多warning public static Set union(Set s1 , Set s2){ Set res = new HashSet(s1); res.addAll(s2); return res; }// 进行泛型的修改：//这三个set（包括输入参数和返回值）的类型都必须是完全一样的。你可以使用”有限制的通配符类型“来使得这个方法更加灵活public static &lt;E&gt; Set&lt;E&gt; union(Set&lt;E&gt; s1 , Set&lt;E&gt; s2){ Set&lt;E&gt; res = new HashSet&lt;E&gt;(s1); res.addAll(s2); return res; } Item31 使用有限制的通配符来提升API的灵活性参数化类型是非协变的 1234567891011121314151617181920212223public class Stack&lt;E&gt; { public Stack(); public void push(E e); public E pop(); public boolean isEmpty();}//1. 实现将一些元素全部放入到 stcak中。下面这样实现要求 Iterable 中的元素类型必须要和 Stack中的元素类型相同// pushAll method without wildcard type - deficient! public void pushAll(Iterator&lt;E&gt; src) { while (src.hasNext()){ push(src.next()); } }Stack&lt;Number&gt; numberStack = new Stack&lt;&gt;();List&lt;Integer&gt; list = Arrays.asList(1, 2, 3);Iterator&lt;Integer&gt; iterator = list.iterator();numberStack.pushAll(iterator); // Integer 虽然是 Number的子类型，但是这里还是编译不通过 Java语言提供了一种特殊的参数化类型，被称为”有限制通配符类型“，就是用来解决类似这种问题的。 pushAll方法的参数类型不应该是”E的Iterable接口“，而应该是“E的某个子类型的Iterable接口”，然后，这里有一个通配符类型，可以准确的表示这个意思： Iterable&lt;? extends E&gt;（其中extends关键字可能会造成一些误导，重申一下Item29里子类型的定义，每个类型都是自身的子类型，即使它没有继承它自己）。下面是使用这种类型修改后的pushAll方法： 123456// Wildcard type for a parameter that serves as an E producer public void pushAll(Iterator&lt;? extends E&gt; src) { while (src.hasNext()){ push(src.next()); } } 1234567891011121314151617// popAll method without wildcard type - deficient! public void popAll(Collection&lt;E&gt; dst) { while (!dst.isEmpty()) dst.add(pop()); }//同样的Collection&lt;Object&gt; objects = new ArrayList&lt;&gt;(); numberStack.popAll(objects); // 编译报错//可修改如下： super 表示传入的可以是 Collection的父类（包括它自己）public void popAll(Collection&lt;? super E &gt; dst) { while (!dst.isEmpty()) dst.add(pop()); } 这个结论很明显：为了最大化灵活度，对于代表生产者或者消费者的输入参数，使用通配符类型。如果这个输入参数即是生产者又是消费者，这个通配符类型就没什么用了。你需要的是准确的类型匹配，不需要使用任何的通配符。 这个助记符可以帮助你记住应该使用哪些通配符： PECS 表示producer-extends, consumer-super。 换句话说，如果这个参数化类型表示一个T生产者，使用&lt;? extends T&gt;；如果表示的是T消费者，使用 &lt;? super T&gt;。在我们的Stack的例子里，pushAll的src参数产生了E实例供Stack使用；所以src的合适类型是Iterable&lt;? extends T&gt;；popAll的dst参数消费每一个来自Stack的E实例，因此合适的类型是Collection&lt;? super E&gt;。PECS助记符刻画了直到通配符使用的基本原则。Naftalin和Wadler把它称之为“Get and Put Principle”。 12//前面的例子修改：public static &lt;E&gt; Set&lt;E&gt; union(Set&lt;? extends E&gt; s1, Set&lt;? extends E&gt; s2) 不要使用有限制的通配符类型作为返回类型 Item32 谨慎地并用泛型和可变参数Item33 考虑类型安全的异构容器枚举编写一个最简单的枚举类： 123456789101112131415161718192021222324252627/** * 1. 最简单的枚举类，有4个实例，不可能再构造新的对象。 * 2. 比较两个枚举值时，可以直接用 == * 3. 枚举类可以增加构造器、方法和字段。 */public enum Size { SMALL(&quot;S&quot;), MEDIUM(&quot;M&quot;), LARGE(&quot;L&quot;), EXTRA_LARGE(&quot;XL&quot;); private String abbreviation; //构造器总是私有的 private Size(String abbreviation){ this.abbreviation = abbreviation; } public String getAbbreviation(){return abbreviation;}} System.out.println(Size.EXTRA_LARGE.toString()); //EXTRA_LARGE System.out.println(Size.EXTRA_LARGE.getAbbreviation()); //XL System.out.println(Enum.valueOf(Size.class, &quot;SMALL&quot;)); //SMALL枚举类的代表 Size[] values = Size.values(); //返回一个包含 全部枚举值的数组 关于枚举的设计原则使用枚举代替int常量一个枚举类型是指有一组固定的常量组成的合法值的类型，比如一年的季节，太阳系的星星，一副牌的花色。在enum类型被添加到java语言中之前，通常使用一组命名的int常量来表示枚举类型，每一个int值表示一个枚举类型的成员。代码如下： 12345678// The int enum pattern - severely deficient! public static final int APPLE_FUJI = 0; public static final int APPLE_PIPPIN = 1; public static final int APPLE_GRANNY_SMITH = 2; public static final int ORANGE_NAVEL = 0; public static final int ORANGE_TEMPLE = 1; public static final int ORANGE_BLOOD = 2; 这种技术，称为”int枚举模式“，有跟多的缺点。 它完全没有提供类型安全的保证，表达能力也不强。当你把一个apple传递给一个需要orange的方法时，编译器也不会生成警告， 还可以使用==来对apple和orange进行比较，甚至更糟糕： 12// Tasty citrus flavored applesauce!int i = (APPLE_FUJI - ORANGE_TEMPLE) / APPLE_PIPPIN; 使用这种int枚举的程序是非常脆弱的，因为int枚举是编译时常量，他们的int值会被编译到使用他们的程序中。如果一个int枚举相关的值被修改了，它的客户端也必须重新编译。如果客户端没有重新编译而继续执行的话，客户端的行为就是不正确的. java提供了枚举类来解决这些问题： 12public enum Apple { FUJI, PIPPIN, GRANNY_SMITH }public enum Orange { NAVEL, TEMPLE, BLOOD } Java的Enum类型的基本想法很简单：他们就是使用公有静态final域为每一个枚举常量导出一个实例的类。Enum类型由于没有可以访问的构造器，Enum类型是真正的final类。由于客户端既不能穿件enum类型的实例，也不能继承enum类，因为除了它声明的枚举实例外，不会有其他的实例。也就是说，枚举类型是实例受控的（详见Item1）。它们是一组单例的集合（Item3），单例本质上是每个元素的枚举。 Enum类型提供了编译时的类型安全。只要你声明了一个参数的类型是Apple，那么久可以保证传递给这个参数的任何非空的对象引用，一定是这三个有效的Apple值之一。试图传递不同错误类型的值，编译时会出现error，试图把一个类型的表达式赋值给另外一个类型的变量，或者使用==操作符来比较两个不同的enum类型的时候，都会出现编译时错误。 除了完善了enums的不足之处以外，enum类型还允许添加任意多的方法和域，允许实现任意多的接口。Enum还提供了所有的Object方法的高效实现（第三章），还实现了Comparable接口（Item14）和Serializable接口（第12章），以及它的序列化形式可以承受大多数的enum类型的转换形式。 给枚举类添加方法和域 那么我们为什么妖王enum类型里添加方法或者域呢？一开始，可能是想把数据和这些实例关联起来。比如我们的Apple和Orange类型，添加一个可以返回水果颜色的方法，或者返回图片的方法，就很有必要。你可以给enum类型增加任何一个看起来合适的方法。一个Enum类型一开始可能就是一组枚举常量的集合，随着时间推进，就进化成了一个功能齐全的抽象了。 举一个功能齐全的enum类型的例子，比如太阳系里的八大行星。每一个行星都质量和半径，然后根据质量和半径可以计算其表面重力值。然后给定一个对象额质量，就可以计算其在该行星表面所受的重力。下面是这个Enum的代码。每个枚举后面的圆括号内的数字，是要传递给构造器的参数。在这个例子中，就是行星的质量和半径。 要编写一个像Planet这样丰富的Enum类型并不难。为了将枚举常量和数据关联起来，需要声明实例数据域，然后写一个构造器，使用这些数据，并把他们保存在数据域里。Enum实例天生就是不可变的，因此所有的域都应该是final的（Item17）。域可以是公开的，但是最好还是将域设为私有的，然后提供公有的访问方法（Item16）。在Planet这个例子中，构造器还计算并保存了surfaceGravity的值，这仅仅是一个优化。这个surfaceGravity的值也可以在，每次调用surfaceWeight方法时，使用mass和radius重新计算。surfaceWeight方法参数为一个对象的质量，返回这个对象在这个常量代表的星星上的重量。 123456789101112131415161718192021222324252627282930313233343536373839/** * 包含方法和域的枚举类 */public enum Planet { /** * 每个行星都是一个实例 */ //八大行星。每一个行星的质量和半径 MERCURY(3.302e+23, 2.439e6), VENUS (4.869e+24, 6.052e6), EARTH (5.975e+24, 6.378e6), MARS (6.419e+23, 3.393e6), JUPITER(1.899e+27, 7.149e7), SATURN (5.685e+26, 6.027e7), URANUS (8.683e+25, 2.556e7), NEPTUNE(1.024e+26, 2.477e7); // 枚举类的所有域都应该用final来修饰 private final double mass; // In kilograms private final double radius; // In meters private final double surfaceGravity; // In m / s^2 // Universal gravitational constant in m^3 / kg s^2 private static final double G = 6.67300E-11; // 常量 // Constructor 枚举类的构造器，必须和上面罗列的实例参数复合 private Planet(double mass, double radius) { this.mass = mass; this.radius = radius; surfaceGravity = G * mass / (radius * radius); } // 枚举类的域是私有的，对外提供访问的方法 public double mass() { return mass; } public double radius() { return radius; } public double surfaceGravity() { return surfaceGravity; } public double surfaceWeight(double mass) { return mass * surfaceGravity; // F = ma }} 12345678910// 根据某个物体在地球上的重量（任何单位），答应出一个超级棒的表格，显示该物体在8个行星上的重量（相同单位public class WeightTable { public static void main(String[] args) { double earthWeight = Double.parseDouble(args[0]); double mass = earthWeight / Planet.EARTH.surfaceGravity(); //这个Planet，以及所有的枚举，都有一个静态的values方法，可以返回一个枚举值的数组 for (Planet p : Planet.values()) System.out.printf(&quot;Weight on %s is %f%n&quot;, p, p.surfaceWeight(mass)); } } 枚举常量的一些行为可能只需要在定义该枚举的类或者方法中使用，这种行为最好是用私有或者包级私有的方法来实现。每个常量就会有一组隐藏的行为，使得包含该枚举的类型或包在使用常量的时候，可以运作得很好。和其他的类一样，除非有让人难以拒绝的原因，不要把枚举方法暴露给它的客户端，将其声明为私有的，或者有需要的话，声明为包级私有的（Item15）。 如果一个枚举类型有普遍适用性，它就应该是一个顶级类；如果它只是在某个顶级类中使用，那么它应该是这个顶级类的成员类。比如，java.math.RoundingMode枚举类表示十进制小数的舍入模式（rounding mode）。这个舍入模式只在Bigdecimal类里使用，但是RoundingMode提供了一个有用的抽象，不仅仅局限于BigDecimal类。因此类库设计者通过把RoundingMode做成一个顶级类，以鼓励其他需要舍入模式的程序员重用这个枚举，从而增加API之间的一致性。 给枚举类中的每个实例关联不同的行为 有时候，你可能还需要每个实例关联不同的行为。比如，假如你在写一个表示基本四则运算的枚举类型，你想提供一个方法来执行每个实例表示的算数操作。下面是一种通过在枚举的值上使用switch来实现的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// Enum type that switches on its own value - questionablepublic enum Operation { PLUS, MINUS, TIMES, DIVIDE; // Do the arithmetic operation represented by this constant public double apply(double x, double y) { switch(this) { case PLUS: return x + y; case MINUS: return x - y; case TIMES: return x * y; case DIVIDE: return x / y; } throw new AssertionError(&quot;Unknown op: &quot; + this); }}// 问题：新增加一个枚举常量的时候，还需要变动switch中的case，变动较大。// 变体1：在enum类中声明一个抽象的apply方法，然后在“特定于常量的类主体”中，使用具体的方法来覆盖每个常量的抽象方法。这种方法被称为“特定于常量的方法实现”// Enum type with constant-specific method implementationspublic enum Operation { PLUS {public double apply(double x,double y){return x + y;}}, MINUS {public double apply(double x, double y){return x - y;}}, TIMES {public double apply(double x, double y){return x * y;}}, DIVIDE {public double apply(double x, double y){return x / y;}}; // 抽象方法 public abstract double apply(double x, double y);}// 变体2：特定于常量的方法实现可以和特定于常量的数据一起使用// Enum type with constant-specific class bodies and datapublic enum Operation { PLUS(&quot;+&quot;) { public double apply(double x, double y) { return x + y; } }, MINUS(&quot;-&quot;) { public double apply(double x, double y) { return x - y; } }, TIMES(&quot;*&quot;) { public double apply(double x, double y) { return x * y; } }, DIVIDE(&quot;/&quot;) { public double apply(double x, double y) { return x / y; } }; private final String symbol; Operation(String symbol) { this.symbol = symbol; } @Override public String toString() { return symbol; } public abstract double apply(double x, double y);} 这个toString方法的实现使得答应数学表达式变得更容易，比如下面这一小段代码： 1234567891011public static void main(String[] args) { double x = Double.parseDouble(args[0]); double y = Double.parseDouble(args[1]); for (Operation op : Operation.values()) System.out.printf(&quot;%f %s %f = %f%n&quot;,x, op, y, op.apply(x, y));} 2.000000 + 4.000000 = 6.000000 2.000000 - 4.000000 = -2.000000 2.000000 * 4.000000 = 8.000000 2.000000 / 4.000000 = 0.500000 那么什么时候应该使用枚举类型呢？当你需要一组固定的常量集合，并且在编译时就知道其成员的时候，就应该使用枚举。当然，这就包括一些“自然枚举类型”，比如行星们，一周的天数以及棋子的数目。当然还包括一些其他在编译时就知道其所有可能的值的集合，比如，菜单的选项，运算符，和命令行标志。并不需要枚举类型中的常量集合一直保持不变。专门设计的枚举类型的特性可以允许枚举类型随时间演变。 总的来说，枚举类型相对于int常量的优势是让人难以拒绝的。枚举类型可读性更好，更安全，也更加强大。一些枚举类型不需要显示的构造器和成员，也有一些枚举可以从每个常量关联的数据，以及提供的行为受数据影响的方法中获得好处。少数的枚举需要将多个行为和一个方法关联，在这种比较少的情况下，特定于常量的方法实现，比在枚举值上进行switch，要好得多。当有多个（但不是全部）使用需要共用相同的行为的时候，可以考虑使用策略枚举模式。 Item35 用实例代替序数12345678910111213141516171819202122// Abuse of ordinal to derive an associated value - DON'T DO THIS public enum Ensemble { SOLO, DUET, TRIO, QUARTET, QUINTET, SEXTET, SEPTET, OCTET, NONET, DECTET; public int numberOfMusicians() { return ordinal() + 1; } }//不要从枚举的序数（ordinal）中获取关联的值；应该把它保存在一个实例域中// 也就是说，枚举类中的每个实例最好编写一个序号保存起来public enum Ensemble { SOLO(1), DUET(2), TRIO(3), QUARTET(4), QUINTET(5), SEXTET(6), SEPTET(7), OCTET(8), DOUBLE_QUARTET(8), NONET(9), DECTET(10), TRIPLE_QUARTET(12); private final int numberOfMusicians; Ensemble(int size) { this.numberOfMusicians = size; } public int numberOfMusicians(){ return numberOfMusicians; }} Item36 用EnumSet代替位域如果一个枚举类型的元素主要是用在集合中，一般会用int枚举模式 123456789101112// Bit field enumeration constants - OBSOLETE! public class Text { public static final int STYLE_BOLD = 1 &lt;&lt; 0; //1 public static final int STYLE_ITALIC = 1 &lt;&lt; 1; //2 public static final int STYLE_UNDERLINE = 1 &lt;&lt; 2; //4 public static final int STYLE_STRIKETHROUGH = 1 &lt;&lt; 3; // 8 // Parameter is bitwise OR of zero or more STYLE_ constants public void applyStyles(int styles) { ... } } java,util包里提供了EnumSet类来高效的表示从单个枚举类型中取出的一组值的集合。这个类实现了Set接口，提供了丰富的功能、类型安全、以及可以和任何其他Set实现的互用性 12345678910111213//前面的例子使用Enum和EnumSer表示的代码。它更短，更简洁，更安全：// EnumSet - a modern replacement for bit fieldspublic class Text {public enum Style { BOLD, ITALIC, UNDERLINE, STRIKETHROUGH } // Any Set could be passed in, but EnumSet is clearly best public void applyStyles(Set&lt;Style&gt; styles) { ... }}使用： text.applyStyles(EnumSet.of(Style.BOLD, Style.ITALIC)); EnumSet&lt;Planet&gt; neptune = EnumSet.of(Planet.NEPTUNE, Planet.MERCURY, Planet.EARTH); Item37 使用EnumMap代替序数索引12345678910111213141516public class Plant { enum LifeCycle { ANNUAL, PERENNIAL, BIENNIAL } final String name; final LifeCycle lifeCycle; Plant(String name, LifeCycle lifeCycle) { this.name = name; this.lifeCycle = lifeCycle; } @Override public String toString() { return name; }} 12345678910111213141516171819public static void main(String[] args) { Plant a = new Plant(&quot;a&quot;, Plant.LifeCycle.ANNUAL); Plant b = new Plant(&quot;b&quot;, Plant.LifeCycle.ANNUAL); Plant c = new Plant(&quot;c&quot;, Plant.LifeCycle.BIENNIAL); Plant d = new Plant(&quot;d&quot;, Plant.LifeCycle.PERENNIAL); Plant e = new Plant(&quot;e&quot;, Plant.LifeCycle.BIENNIAL); List&lt;Plant&gt; garden = Arrays.asList(a, b, c, d, e); //需求：将这五种植物按照 Plant.LifeCycle 分类： Map&lt;Plant.LifeCycle, Set&lt;Plant&gt;&gt; plantsByLifeCycle = new EnumMap&lt;&gt;(Plant.LifeCycle.class); System.out.println(plantsByLifeCycle); //{} for (Plant.LifeCycle lc : Plant.LifeCycle.values()) plantsByLifeCycle.put(lc, new HashSet&lt;&gt;()); for (Plant p : garden) plantsByLifeCycle.get(p.lifeCycle).add(p); System.out.println(plantsByLifeCycle); //{ANNUAL=[b, a], PERENNIAL=[d], BIENNIAL=[c, e]} } Item38 用接口模拟可以扩展的枚举注解自定义一个注解： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * 自定义注解 MyTest */@Retention(RetentionPolicy.RUNTIME) // 表示这个注解运行时应该被保留@Target(ElementType.METHOD) //表示只有在方法声明上使用时合法的，它不能被用在类声明，域声明，或者其他的程序元素上public @interface MyTest {}// 测试注解的用例// @MyTest被称为”标记注解“，因为它没有参数，只是简单的给被注解元素做了个标记// 注解不会改变被注解代码的语义，但是可以使这段代码可以被一些工具特殊处理public class Sample { @MyTest public static void m1() { } // Test should pass public static void m2() { } @MyTest public static void m3() { // Test should fail ，抛出了一个运行时异常 throw new RuntimeException(&quot;Boom&quot;); } public static void m4() { } @MyTest // 不是静态方法，不能通过放射之后 类名.方法名调用 public void m5() { } // INVALID USE: nonstatic method public static void m6() { } @MyTest public static void m7() { // Test should fail throw new RuntimeException(&quot;Crash&quot;); } public static void m8() { }}// 通过反射的方式来测试public class RunTest { public static void main(String[] args) throws ClassNotFoundException { int tests = 0; int passed = 0; Class&lt;?&gt; testClass = Class.forName(&quot;effectiveJava.chapter4.Sample&quot;); Method[] methods = testClass.getDeclaredMethods(); for (Method m : methods){ // 1. 如果方法 存在 @MyTest 注解 if (m.isAnnotationPresent(MyTest.class)) { tests++; try { //2. 运行这个方法 m.invoke(null); passed++; } catch (InvocationTargetException wrappedExc) { //3. 如果这个方法发生了异常。反射机制会将这个异常包装在一个InvocationTargetException里。 // 然后测试工具会捕获这个异常，并打印一个失败报告 Throwable exc = wrappedExc.getCause(); System.out.println(m + &quot; failed: &quot; + exc); } catch (Exception exc) { //4. 反射调用的方法抛出了不是InvocationTargetException的异常，那就表明这是一个Test注解在编译时没有发现的非法的使用。 System.out.println(&quot;Invalid @Test: &quot; + m); //Invalid @Test: public void effectiveJava.chapter4.Sample.m5() } } } System.out.printf(&quot;Passed: %d, Failed: %d%n&quot;,passed, tests - passed); // Passed: 1, Failed: 3 }} 注解需求改变：只有抛出指定的异常才能通过测试: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface ExceptionTest { // 允许使用这个注解的用户指定一个异常或错误类型 Class&lt;? extends Throwable&gt; value();}public class Sample2 { @ExceptionTest(ArithmeticException.class) public static void m1(){ // Test should pass int i = 0; i = i / i; } @ExceptionTest(ArithmeticException.class) public static void m2(){ // Should fail (wrong exception) int[] a = new int[0]; int i = a[1]; } @ExceptionTest(ArithmeticException.class) public static void m3(){ // Should fail (no exception) }}public static void main(String[] args) throws ClassNotFoundException { int tests = 0; int passed = 0; Class&lt;?&gt; testClass = Class.forName(&quot;effectiveJava.chapter4.Sample2&quot;); Method[] methods = testClass.getDeclaredMethods(); for (Method m : methods) { if (m.isAnnotationPresent(ExceptionTest.class)) { tests++; try { m.invoke(null); System.out.printf(&quot;Test %s failed: no exception%n&quot;, m); //Test public static void effectiveJava.chapter4.Sample2.m3() failed: no exception } catch (InvocationTargetException wrappedEx) { Throwable exc = wrappedEx.getCause(); Class&lt;? extends Throwable&gt; excType = m.getAnnotation(ExceptionTest.class).value(); if (excType.isInstance(exc)) { passed++; } else { //Test public static void effectiveJava.chapter4.Sample2.m2() failed: expected java.lang.ArithmeticException, got java.lang.ArrayIndexOutOfBoundsException: 1 System.out.printf( &quot;Test %s failed: expected %s, got %s%n&quot;, m, excType.getName(), exc); } } catch (Exception exc) { System.out.println(&quot;Invalid @Test: &quot; + m); } } } System.out.printf(&quot;Passed: %d, Failed: %d%n&quot;,passed, tests - passed); // Passed: 1, Failed: 3 } 注解需求改变：当抛出几个特定类型中的任何一个的时候，测试通过 12345678910111213141516171819202122232425262728293031323334353637// Annotation type with an array parameter @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface ExceptionTest { Class&lt;? extends Exception&gt;[] value(); }// Code containing an annotation with an array parameter @ExceptionTest({ IndexOutOfBoundsException.class,NullPointerException.class }) public static void doublyBad() { List&lt;String&gt; list = new ArrayList&lt;&gt;(); // The spec permits this method to throw either // IndexOutOfBoundsException or NullPointerException list.addAll(5, null);}// testif (m.isAnnotationPresent(ExceptionTest.class)) { tests++; try { m.invoke(null); System.out.printf(&quot;Test %s failed: no exception%n&quot;, m); } catch (Throwable wrappedExc) { Throwable exc = wrappedExc.getCause(); int oldPassed = passed; Class&lt;? extends Exception&gt;[] excTypes = m.getAnnotation(ExceptionTest.class).value(); for (Class&lt;? extends Exception&gt; excType : excTypes) { if (excType.isInstance(exc)) { passed++; break; } } if (passed == oldPassed) System.out.printf(&quot;Test %s failed: %s %n&quot;, m, exc); }} 在Java8中，还有一种方法可以处理多值注解：**@Repeatable 来表示这个注解可以在一个单个元素上多次应用** 12345678910111213141516171819202122232425262728293031323334353637383940414243// Repeatable annotation type @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) @Repeatable(ExceptionTestContainer.class) //Repeatable 这个元注解有一个参数，该参数是一个包含注解类型的class对象，这个包含注解类型的唯一的参数就是，被@Repeatable所注解的注解类型 数组 public @interface ExceptionTest { Class&lt;? extends Exception&gt; value(); } @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface ExceptionTestContainer { ExceptionTest[] value(); }// Code containing a repeated annotation @ExceptionTest(IndexOutOfBoundsException.class) @ExceptionTest(NullPointerException.class) public static void doublyBad() { ... }// Processing repeatable annotations if (m.isAnnotationPresent(ExceptionTest.class) || m.isAnnotationPresent(ExceptionTestContainer.class)) { tests++; try { m.invoke(null); System.out.printf(&quot;Test %s failed: no exception%n&quot;, m); } catch (Throwable wrappedExc) { Throwable exc = wrappedExc.getCause(); int oldPassed = passed; ExceptionTest[] excTests = m.getAnnotationsByType(ExceptionTest.class); for (ExceptionTest excTest : excTests) { if (excTest.value().isInstance(exc)) { passed++; break; } } if (passed == oldPassed) System.out.printf(&quot;Test %s failed: %s %n&quot;, m, exc); } } Item40 坚持使用Override注解","link":"/2021/08/03/JavaSE-%E5%85%AD-%E6%B3%9B%E5%9E%8B%E3%80%81%E6%9E%9A%E4%B8%BE%E5%92%8C%E6%B3%A8%E8%A7%A3/"},{"title":"JavaSE-泛型","text":"Java基础之泛型的使用 泛型的需求：在拥有泛型之前，你必须对每个从集合中读出来的对象进行类型转换。如果有人不小心插入了一个错误类型的对象，这个类型转换就会再运行时失败。使用泛型，你就可以告诉编译器，那些类型的对象允许加入这个集合。编译器会自动地为你进行转换，然后当你要插入一个错误类型的时候，在编译地时候就会报错 Item26 不要使用原生类型每一个泛型都包括一组参数类型，其构成方式如下：首先是类或者接口的名字，然后紧跟着用&lt;&gt;括起来的实际类型参数，实际类型参数和泛型中的形式类型参数对应 原生类型指的是没有任何类型参数的泛型类型的名称。 1234567891011121314151617181920212223242526272829// 只是为了兼容之前的代码,不建议直接使用原生类型SetSet set = new Set;// 错误的插入了不同类型的 数据类型，编译时不会报错set.add(1);set.add(&quot;String&quot;);set.add('a');同理：public static void main(String[] args) { List&lt;String&gt; strings = new ArrayList&lt;&gt;(); //1. 添加的时候不会报错，编译也能通过 unsafeAdd(strings , Integer.valueOf(43)); // safeAdd(strings , Integer.valueOf(43)); //2. 取出的时候就会发生类型转换异常 String s = strings.get(0); //java.lang.ClassCastException } //unsafeAdd 方法的参数是一个原生类型List。所以有风险，应该用参数化类型List来代替 private static void unsafeAdd(List list, Object o) { list.add(o); } private static void safeAdd(List&lt;String&gt; list, String o) { // 加上参数化类型之后，这边编译就会报错 list.add(o); } Item27 消除非受检的警告Item28 list优先于数组为什么呢？ 数组是协变的（covariant），泛型是非协变的。 1234567//1. 下面编译不报错，启动才报错// 由于Long是Object的子类，所以 Long[] 也是 Object[]的子类 Object[] objectArray = new Long[1]; objectArray[0] = &quot;abc&quot;;// java.lang.ArrayStoreException //2. 用list编译就报错了 List&lt;Object&gt; list = new ArrayList&lt;Long&gt;(); 数组是具体化的（reified），也就是说在运行的时候数组是知道其元素类型的，并且会强制执行类型限制。相反，泛型是通过泛型擦除来实现的，这就意味着只能在编译器强制执行类型限制，运行的时候是会擦除其元素信息的。类型擦除是为了允许泛型代码可一个之前没有泛型的老代码进行互用 当你在转换到数组类型时遇到泛型数组创建错误或者非受检转换异常的时候，最好的方法是使用集合类型List，而不是数组类型E[]。可能会牺牲一些性能，但是换回的却是更好的类型安全性和互用性。 12345678910111213141516171819202122232425// Chooser - a class badly in need of generics!// 调用这个类的choose方法时，返回的都是Object对象，需要手动进行类型准换public class Chooser { private final Object[] choiceArray; public Chooser(Collection choices) { choiceArray = choices.toArray(); } public Object choose() { Random rnd = ThreadLocalRandom.current(); return choiceArray[rnd.nextInt(choiceArray.length)]; }}public static void main(String[] args) { ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(1); list.add(2); list.add(3); Chooser chooser = new Chooser(list); Integer choose = (Integer) chooser.choose(); System.out.println(&quot;choose = &quot; + choose); } 为了避免进行可能出错的强制类型转换，引入泛型,但还是有点问题： 12345678910111213141516public class Chooser&lt;T&gt; { private final T[] choiceArray; public Chooser(Collection&lt;T&gt; choices) { //它不能确定在运行时的类型转换的安全性，因为程序不知道类型T代表的是什么 //泛型中的元素类型信息在运行时被擦除了 choiceArray = (T[]) choices.toArray(); } public Object choose() { Random rnd = ThreadLocalRandom.current(); return choiceArray[rnd.nextInt(choiceArray.length)]; }} 为了消除这个非受检转换警告，使用list来代替数组： 12345678910111213public class Chooser&lt;T&gt; { private final List&lt;T&gt; choiceArray; public Chooser(Collection&lt;T&gt; choices) { choiceArray = new ArrayList&lt;&gt;(choices); } public T choose() { Random rnd = ThreadLocalRandom.current(); return choiceArray.get(rnd.nextInt(choiceArray.size())); }} 数组和泛型有一些非常不同的类型规则，数组是协变和可具体化的；而泛型是非协变和擦除的。因此，数组可以提供运行时类型安全但是没有贬义是类型安全，而泛型却恰恰相反。一般来说，数组和泛型不能混用。如果你发现你自己混用了他们，并且出现了编译error或者warning。你的第一反应应该是用列表替换数组。 Item29 优先考虑泛型一个例子：未进行参数化的Stack 12345678910111213141516171819202122232425public class Stack { private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { elements = new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public Object pop() { if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} 将这个Stack使用泛型，同时还需要兼容之前的版本： 声明参数类型为E 将所有的Object改成E 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public class Stack&lt;E&gt; { private E[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { // 这里报错 。。 不能创建一个泛型数组。 elements = new E[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public E pop() { if (size == 0) throw new EmptyStackException(); E result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }}// 两种解决方案：1. elementst数组，再进行强制类型转换为E数组 public class Stack&lt;E&gt; { private E[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { // 这里报错 elements = (E[]) new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(E e) { ensureCapacity(); elements[size++] = e; } public E pop() { if (size == 0) throw new EmptyStackException(); E result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} 2. 将elements设为object数组，每次pop访问数组中的元素的时候进行强制类型转换 public class Stack&lt;E&gt; { private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { // 这里报错 elements = new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public E pop() { if (size == 0) throw new EmptyStackException(); E result = (E) elements[--size]; elements[size] = null; // Eliminate obsolete reference return result; return result; } public boolean isEmpty() { return size == 0; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} 第一种方法可读性更强：数组被声明为E[]，清晰地表示了它只包含E实例；并且更加简洁：在一个典型的泛型类中，你可以以在代码里看到很多次数组，第一中方法只需要进行一次转换（当数组创建的时候）；而第二种方法需要在每次读取数组元素的时候都进行转换。因此第一种方法要更受欢迎一些，在实际情况中，也用得更多些。但是第一种方法确实造成了”堆泄露“（详见Item32）：因为数组的运行时类型和编译时类型不一致（除非E刚刚也是Object）。有一些程序员对这点深恶痛绝，选择使用第二种方法。 Item30 优先考虑泛型方法基于参数化类型进行计算的静态工具方法通常来说都是泛型的。 1234//java.util.Collections#sort(java.util.List&lt;T&gt;, java.util.Comparator&lt;? super T&gt;)public static &lt;T&gt; void sort(List&lt;T&gt; list, Comparator&lt;? super T&gt; c) { list.sort(c); } 写泛型方法和泛型类型差不多： 1234567891011121314151617// 使用了原始类型，是有缺陷的。 // 虽然编译可以通过，但是有很多warning public static Set union(Set s1 , Set s2){ Set res = new HashSet(s1); res.addAll(s2); return res; }// 进行泛型的修改：//这三个set（包括输入参数和返回值）的类型都必须是完全一样的。你可以使用”有限制的通配符类型“来使得这个方法更加灵活public static &lt;E&gt; Set&lt;E&gt; union(Set&lt;E&gt; s1 , Set&lt;E&gt; s2){ Set&lt;E&gt; res = new HashSet&lt;E&gt;(s1); res.addAll(s2); return res; } Item31 使用有限制的通配符来提升API的灵活性参数化类型是非协变的 1234567891011121314151617181920212223public class Stack&lt;E&gt; { public Stack(); public void push(E e); public E pop(); public boolean isEmpty();}//1. 实现将一些元素全部放入到 stcak中。下面这样实现要求 Iterable 中的元素类型必须要和 Stack中的元素类型相同// pushAll method without wildcard type - deficient! public void pushAll(Iterator&lt;E&gt; src) { while (src.hasNext()){ push(src.next()); } }Stack&lt;Number&gt; numberStack = new Stack&lt;&gt;();List&lt;Integer&gt; list = Arrays.asList(1, 2, 3);Iterator&lt;Integer&gt; iterator = list.iterator();numberStack.pushAll(iterator); // Integer 虽然是 Number的子类型，但是这里还是编译不通过 Java语言提供了一种特殊的参数化类型，被称为”有限制通配符类型“，就是用来解决类似这种问题的。 pushAll方法的参数类型不应该是”E的Iterable接口“，而应该是“E的某个子类型的Iterable接口”，然后，这里有一个通配符类型，可以准确的表示这个意思： Iterable&lt;? extends E&gt;（其中extends关键字可能会造成一些误导，重申一下Item29里子类型的定义，每个类型都是自身的子类型，即使它没有继承它自己）。下面是使用这种类型修改后的pushAll方法： 123456// Wildcard type for a parameter that serves as an E producer public void pushAll(Iterator&lt;? extends E&gt; src) { while (src.hasNext()){ push(src.next()); } } 1234567891011121314151617// popAll method without wildcard type - deficient! public void popAll(Collection&lt;E&gt; dst) { while (!dst.isEmpty()) dst.add(pop()); }//同样的Collection&lt;Object&gt; objects = new ArrayList&lt;&gt;(); numberStack.popAll(objects); // 编译报错//可修改如下： super 表示传入的可以是 Collection的父类（包括它自己）public void popAll(Collection&lt;? super E &gt; dst) { while (!dst.isEmpty()) dst.add(pop()); } 这个结论很明显：为了最大化灵活度，对于代表生产者或者消费者的输入参数，使用通配符类型。如果这个输入参数即是生产者又是消费者，这个通配符类型就没什么用了。你需要的是准确的类型匹配，不需要使用任何的通配符。 这个助记符可以帮助你记住应该使用哪些通配符： PECS 表示producer-extends, consumer-super。 换句话说，如果这个参数化类型表示一个T生产者，使用&lt;? extends T&gt;；如果表示的是T消费者，使用 &lt;? super T&gt;。在我们的Stack的例子里，pushAll的src参数产生了E实例供Stack使用；所以src的合适类型是Iterable&lt;? extends T&gt;；popAll的dst参数消费每一个来自Stack的E实例，因此合适的类型是Collection&lt;? super E&gt;。PECS助记符刻画了直到通配符使用的基本原则。Naftalin和Wadler把它称之为“Get and Put Principle”。 12//前面的例子修改：public static &lt;E&gt; Set&lt;E&gt; union(Set&lt;? extends E&gt; s1, Set&lt;? extends E&gt; s2) 不要使用有限制的通配符类型作为返回类型 Item32 谨慎地并用泛型和可变参数Item33 考虑类型安全的异构容器","link":"/2021/07/29/JavaSE-%E6%B3%9B%E5%9E%8B/"},{"title":"Java并发理论基础-上","text":"并发编程领域可以抽象成三个核心问题：分配任务、相互协作和互斥： 分配任务：将一个大的任务交给不同的进程/线程来做 相互协作：线程间的协作，比如一个线程执行完了一个任务，如何通知执行后续任务的线程开工 互斥：要实现在同一时刻内只有一个线程访问共享变量 第一部分、并发编程的发展1. 并发编程问题的由来随着CPU 、 内存 、IO设备的不断发展，三者的速度差异一直是存在的（CPU一天 ， 内存一年 ， IO设备十年） 为了合理利用 CPU 的高性能，平衡这三者的速度差异，计算机体系机构、操作系统、编译程序都做出了贡献，主要体现为： CPU 增加了缓存，以均衡与内存的速度差异； 操作系统增加了进程、线程，以分时复用 CPU，进而均衡 CPU 与 I/O 设备的速度差异； 编译程序优化指令执行次序，使得缓存能够得到更加合理地利用 1.1 缓存导致的可见性问题 一个线程对共享变量的修改，另外一个线程能够立刻看到，我们称为可见性。 单核时代，电脑只有1个CPU，所有的线程操作的是同一个CPU的缓存，也就不存在可见性问题。 多核时代，每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了，当多个线程在不同的 CPU 上执行时，这些线程操作的是不同的 CPU 缓存，如下图 1.2 线程切换带来的原子性问题 一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性 在一个时间片内，如果一个进程进行一个 IO 操作，例如读个文件，这个时候该进程可以把自己标记为“休眠状态”并出让 CPU 的使用权，待文件读进内存，操作系统会把这个休眠的进程唤醒，唤醒后的进程就有机会重新获得 CPU 的使用权了。 早期的操作系统基于进程来调度 CPU，不同进程间是不共享内存空间的，所以进程要做任务切换就要切换内存映射地址，而一个进程创建的所有线程，都是共享一个内存空间的，所以线程做任务切换成本就很低了。现代的操作系统都基于更轻量的线程来调度，现在我们提到的“任务切换”都是指“线程切换”。 Java 并发程序都是基于多线程的，自然也会涉及到任务切换，也许你想不到，任务切换竟然也是并发编程里诡异 Bug 的源头之一。任务切换的时机大多数是在时间片结束的时候，我们现在基本都使用高级语言编程，高级语言里一条语句往往需要多条 CPU 指令完成，例如上面代码中的count += 1，至少需要三条 CPU 指令。 指令 1：首先，需要把变量 count 从内存加载到 CPU 的寄存器； 指令 2：之后，在寄存器中执行 +1 操作； 指令 3：最后，将结果写入内存（缓存机制导致可能写入的是 CPU 缓存而不是内存） 1.3 编译优化带来的有序性问题有序性指的是程序按照代码的先后顺序执行。编译器为了优化性能，有时候会进行指令重排序。 重排序分3种类型。 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术(Instruction-Level Parallelism，ILP)来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应 机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上 去可能是在乱序执行 例如程序中：“a=6；b=7；”编译器优化后可能变成“b=7；a=6；”，在这个例子中，编译器调整了语句的顺序，但是不影响程序的最终结果，但是有时会出现问题。 比如单例模式中的双重检验模式： 1234567891011121314public class Singleton { static Singleton instance; public Singleton getInstance(){ if (instance == null){ synchronized (Singleton.class){ if (instance == null) instance = new Singleton(); //在CPU指令上并不是一步操作 } } return instance; }} new一个新的对象分为如下几步（先在内存中初始化对象再赋值给变量）： 分配一块内存 M； 在内存 M 上初始化 Singleton 对象； 然后 M 的地址赋值给 instance 变量。 经过指令重排序变成了下面这种情况（先将内存赋值给变量再进行初始化）： 分配一块内存 M； 将 M 的地址赋值给 instance 变量； 最后在内存 M 上初始化 Singleton 对象。 不安全的情况：假设线程 A 先执行 getInstance() 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance() 方法，那么线程 B 在执行第一个判断时会发现 instance != null ，所以直接返回 instance，而此时的 instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。 2. 并发编程面临的挑战并发编程的目的是为了让程序运行得更快，但是，并不是启动更多的线程就能让程序最大限度地并发执行。 2.1上下文切换所谓的多线程并发执行是通过CPU给每个线程分配CPU时间片来实现的，因为时间片非常短（一般是几十毫秒ms），所以CPU通过不停地切 换线程执行，让我们感觉多个线程是同时执行的。 上下文切换是指：当前任务执行完一个时间片后需要切换到下一个任务，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这 个任务的状态。所以任务从保存到再加载的过程就是一次上下文切换 多线程一定快吗？不一定，因为线程有创建和上下文切换的开销。 如何减少上下文切换： 无锁并发编程。多线程竞争锁时，会引起上下文切换，所以多线程处理数据时，可以用一 些办法来避免使用锁，如将数据的ID按照Hash算法取模分段，不同的线程处理不同段的数据。 CAS算法。Java的Atomic包使用CAS算法来更新数据，而不需要加锁。 使用最少线程。避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样会造成大量线程都处于等待状态。 协程:在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。 2.2 死锁及避免办法 避免一个线程同时获取多个锁。 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 尝试使用定时锁，使用lock.tryLock(timeout)来替代使用内部锁机制。 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。 3. 什么是线程安全？线程安全需要保证几个基本特性： 原子性，简单说就是相关操作不会中途被其他线程干扰，一般通过同步机制实现。 可见性，是一个线程修改了某个共享变量，其状态能够立即被其他线程知晓，通常被解释为将线程本地状态反映到主内存上，volatile就是负责保证可见性的。 有序性，是保证线程内串行语义，避免指令重排等。 第二部分、Java中如何实现并发安全由上一部分可知：解决可见性、有序性最直接的办法就是禁用缓存和编译优化，但是性能上会带来问题。因此需要做到按需禁用。站在程序员的视角看就是 Java 内存模型规范了 JVM 如何提供按需禁用缓存和编译优化的方法。具体来说，这些方法包括 volatile、synchronized 和 final 三个关键字，以及六项 Happens-Before 规则， 1. volatile保证可见性volatile是轻量级的 synchronized，它在多处理器开发中保证了共享变量的“可见性”,不会引起线程上下文的切换和调度。 1volatile int x = 0; //告诉编译器，对这个变量的读写，不能使用 CPU 缓存，必须从内存中读取或者写入 1.1 如何实现的可见性有volatile变量修饰的共享变量进行写操作的时候会多出第二行汇编代码: 0x01a3de1d: movb 0×0,0×1104800(0×0,0×1104800(%esi); 0x01a3de24: lock addl 0×0,0×1104800(0×0,(%esp); Lock前缀的指令： 1)将当前处理器缓存行的数据写回到系统内存。 2)这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。 为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存(L1，L2或其他)后再进行操作，但操作完不知道何时会写到内存。如果对声明了volatile的 变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据 写回到系统内存。但是，就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操 作就会有问题。所以，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一 致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当 处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状 态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存 里。 1.2 volatile的使用优化追加64字节能够提高并发编程的效率？ 处理器的L1、L2或L3缓存的高速缓存行是64个字节宽，如果队列的头节点和尾节点都不足64字节的话，处理器会将 它们都读到同一个高速缓存行中，在多处理器下每个处理器都会缓存同样的头、尾节点，当一 个处理器试图修改头节点时，会将整个缓存行锁定，那么在缓存一致性机制的作用下，会导致 其他处理器不能访问自己高速缓存中的尾节点，而队列的入队和出队操作则需要不停修改头 节点和尾节点，所以在多处理器的情况下将会严重影响到队列的入队和出队效率。Doug lea使 用追加到64字节的方式来填满高速缓冲区的缓存行，避免头节点和尾节点加载到同一个缓存 行，使头、尾节点在修改时不会互相锁定。 2. synchronized保证原子性2.1 应用方法·对于普通同步方法，锁是当前实例对象。 ·对于静态同步方法，锁是当前类的Class对象。 ·对于同步方法块，锁是Synchonized括号里配置的对象。 123456789101112131415161718class X { // 修饰非静态方法,锁定的是当前实例对象 this synchronized void foo() { // 临界区 } // 修饰静态方法,锁定的是当前类的 Class 对象 synchronized static void bar() { // 临界区 } // 修饰代码块 //当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。 Object obj = new Object()； void baz() { synchronized(obj) { // 临界区 } }} 2.2 实现原理 在Java6之前，synchronized完全依靠操作系统的互斥锁来实现，需要进行用户态和内核态的切换，所以开销较大，但随着一系列的锁优化，synchronized的性能也越来越好了 JVM基于进入和退出Monitor对象来实现方法同步和代码块同步，但两者的实现细节不一样。 代码块同步是使用monitorenter 和monitorexit指令实现的，而方法同步是使用另外一种方式实现的 monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。 任何对象都有 一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态。线程执行到monitorenter 指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。 源代码获取： 首先，synchronized的行为是JVM runtime的一部分，所以我们需要先找到Runtime相关的功能实现。通过在代码中查询类似“monitor_enter”或“Monitor Enter”，很直观的就 可以定位到： sharedRuntime.cpp/hpp，它是解释器和编译器运行时的基类。 synchronizer.cpp/hpp，JVM同步相关的各种基础逻辑。 在sharedRuntime.cpp中，下面代码体现了synchronized的主要逻辑。 1234567Handle h_obj(THREAD, obj); if (UseBiasedLocking) { //检查是否开启了偏向锁 // Retry fas entry if bias is revoked to avoid unnecessary infation ObjectSynchronizer::fast_enter(h_obj, lock, true, CHECK); //完整的流程 } else { ObjectSynchronizer::slow_enter(h_obj, lock, CHECK); //直接进入轻量级锁获取逻辑 } 偏斜锁并不适合所有应用场景，撤销操作（revoke）是比较重的行为，只有当存在较多不会真正竞争的synchronized块儿时，才能体现出明显改善。实践中对于偏斜锁的一直是有 争议的，有人甚至认为，当你需要大量使用并发类库时，往往意味着你不需要偏斜锁。从具体选择来看，我还是建议需要在实践中进行测试，根据结果再决定是否使用。 还有一方面是，偏斜锁会延缓JIT 预热的进程，所以很多性能测试中会显式地关闭偏斜锁， -XX:-UseBiasedLocking 12345678910111213141516171819void ObjectSynchronizer::fas_enter(Handle obj, BasicLock* lock, bool attempt_rebias, TRAPS) { if (UseBiasedLocking) { if (!SafepointSynchronize::is_at_safepoint()) { //revoke_and_rebias是获取偏斜锁的入口方法 BiasedLocking::Condition cond = BiasedLocking::revoke_and_rebias(obj, attempt_rebias, THREAD); if (cond == BiasedLocking::BIAS_REVOKED_AND_REBIASED) { return; } } else { assert(!attempt_rebias, &quot;can not rebias toward VM thread&quot;); //revoke_at_safepoint则定义了当检测到安全点时的处理逻辑 BiasedLocking::revoke_at_safepoint(obj); } assert(!obj-&gt;mark()-&gt;has_bias_pattern(), &quot;biases should be revoked by now&quot;); } slow_enter(obj, lock, THREAD);} 1234567891011121314151617181920212223void ObjectSynchronizer::slow_enter(Handle obj, BasicLock* lock, TRAPS) { markOop mark = obj-&gt;mark(); if (mark-&gt;is_neutral()) { // 将目前的Mark Word复制到Displaced Header上 lock-&gt;set_displaced_header(mark); // 利用CAS设置对象的Mark Word if (mark == obj()-&gt;cas_set_mark((markOop) lock, mark)) { TEVENT(slow_enter: release sacklock); return; } // 检查存在竞争 } else if (mark-&gt;has_locker() &amp;&amp; THREAD-&gt;is_lock_owned((address)mark-&gt;locker())) { // 清除 lock-&gt;set_displaced_header(NULL); return; } // 重置Displaced Header lock-&gt;set_displaced_header(markOopDesc::unused_mark()); ObjectSynchronizer::infate(THREAD, obj(), infate_cause_monitor_enter)-&gt;enter(THREAD);} 2.3 锁的优化过程synchronized用的锁是存在Java对象头里的。如果对象是数组类型，还会存数组类型： 锁一共有4种状态，级别从低到高依次是: 无锁状态、 偏向锁状态、 轻量级锁状态、 重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级 2.3.1 偏向锁经验：大多数情况下，锁都是由同一个线程多次获得。 偏向锁的加锁： 当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出 同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否 存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁。如果测试失败，则需 要再测试一下Mark Word中偏向锁的标识是否设置成1(表示当前是偏向锁):如果没有设置，则 使用CAS竞争锁;如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。 偏向锁的撤销： 偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时， 持有偏向锁的线程才会释放锁。偏向锁的撤销，需要等待全局安全点(在这个时间点上没有正 在执行的字节码)。它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着， 如果线程不处于活动状态，则将对象头设置成无锁状态;如果线程仍然活着，拥有偏向锁的栈 会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他 线程，要么恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。 偏向锁的启用： 偏向锁在Java 6和Java 7里是默认启用的，但是它在应用程序启动几秒钟之后才激活 2.3.2 轻量级锁加锁过程： 线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并 将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用 CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁 解锁过程： 轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成 功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁 因为自旋会消耗CPU，为了避免无用的自旋(比如获得锁的线程被阻塞住了)，一旦锁升级 成重量级锁，就不会再恢复到轻量级锁状态。当锁处于这个状态下，其他线程试图获取锁时， 都会被阻塞住，当持有锁的线程释放锁之后会唤醒这些线程，被唤醒的线程就会进行新一轮 的夺锁之争 2.3.3 原子操作的实现原理处理器如何实现： 锁总线：多个处理器同时从各自的缓存中读取变量i，分别进行加1操作，然后分别写入 系统内存中。那么，想要保证读改写共享变量的操作是原子的，就必须保证CPU1读改写共享 变量的时候，CPU2不能操作缓存了该共享变量内存地址的缓存。 处理器使用总线锁就是来解决这个问题的。所谓总线锁就是使用处理器提供的一个 LOCK#信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住，那么该 处理器可以独占共享内存。 锁缓存：总线锁定把CPU和内存之间的通信锁住了，这使得锁定期间，其他处 理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，目前处理器在某些场合下 使用缓存锁定代替总线锁定来进行优化。缓存锁定”是指内存区域如果被缓存在处理器的缓存 行中，并且在Lock操作期间被锁定，那么当它执行锁操作回写到内存时，处理器不在总线上声 言LOCK#信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子 性，因为缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据，当其他处 理器回写已被锁定的缓存行的数据时，会使缓存行无效 Java如何实现： 使用循环CAS实现原子操作：JVM中的CAS操作正是利用了处理器提供的CMPXCHG指令实现的。 从Java 1.5开始，JDK的并发包里提供了一些类来支持原子操作，如AtomicBoolean(用原子 方式更新的boolean值)、AtomicInteger(用原子方式更新的int值)和AtomicLong(用原子方式更 新的long值)。这些原子包装类还提供了有用的工具方法，比如以原子的方式将当前值自增1和 自减1。 CAS实现原子操作的三大问题： 1)ABA问题 2)循环时间长开销大 3)只能保证一个共享变量的原子操作 (3)使用锁机制实现原子操作 2.4 转账为例分析锁保护没有关联关系的多个资源，例如，银行业务中有针对账户余额（余额是一种资源）的取款操作，也有针对账户密码（密码也是一种资源）的更改操作，我们可以为账户余额和账户密码分配不同的锁来解决并发问题，这个还是很简单的。 1234567891011121314151617181920212223242526272829303132333435363738394041//不同的资源用不同的锁保护class Account { // 锁：保护账户余额 private final Object balLock = new Object(); // 账户余额 private Integer balance; // 锁：保护账户密码 private final Object pwLock = new Object(); // 账户密码 private String password; // 取款 void withdraw(Integer amt) { synchronized(balLock) { if (this.balance &gt; amt){ this.balance -= amt; } } } // 查看余额 Integer getBalance() { synchronized(balLock) { return balance; } } // 更改密码 void updatePassword(String pw){ synchronized(pwLock) { this.password = pw; } } // 查看密码 String getPassword() { synchronized(pwLock) { return password; } }} 当然，我们也可以用一把互斥锁来保护多个资源，例如我们可以用 this 这一把锁来管理账户类里所有的资源：账户余额和用户密码。具体实现很简单，示例程序中所有的方法都增加同步关键字 synchronized 就可以了. 但是用一把锁有个问题，就是性能太差，会导致取款、查看余额、修改密码、查看密码这四个操作都是串行的。而我们用两把锁，取款和修改密码是可以并行的。用不同的锁对受保护资源进行精细化管理，能够提升性能。这种锁还有个名字，叫细粒度锁。 保护有关联关系的多个资源 例如银行业务里面的转账操作，账户 A 减少 100 元，账户 B 增加 100 元。这两个账户就是有关联关系的。那对于像转账这种有关联关系的操作，我们应该怎么去解决呢？ 12345678910111213class Account { private int balance; // 转账 //临界区内有两个资源，分别是转出账户的余额 this.balance 和转入账户的余额 target.balance，并且用的是一把锁 this //问题就出在 this 这把锁上，this 这把锁可以保护自己的余额 this.balance，却保护不了别人的余额 target.balance，就像你不能用自家的锁来保护别人家的资产 synchronized void transfer( Account target, int amt){ if (this.balance &gt; amt) { this.balance -= amt; target.balance += amt; } } } ​ 下面我们具体分析一下，假设有 A、B、C 三个账户，余额都是 200 元，我们用两个线程分别执行两个转账操作：账户 A 转给账户 B 100 元，账户 B 转给账户 C 100 元，最后我们期望的结果应该是账户 A 的余额是 100 元，账户 B 的余额是 200 元， 账户 C 的余额是 300 元。 我们假设线程 1 执行账户 A 转账户 B 的操作，线程 2 执行账户 B 转账户 C 的操作。这两个线程分别在两颗 CPU 上同时执行，那它们是互斥的吗？我们期望是，但实际上并不是。因为线程 1 锁定的是账户 A 的实例（A.this），而线程 2 锁定的是账户 B 的实例（B.this），所以这两个线程可以同时进入临界区 transfer()。同时进入临界区的结果是什么呢？线程 1 和线程 2 都会读到账户 B 的余额为 200，导致最终账户 B 的余额可能是 300（线程 1 后于线程 2 写 B.balance，线程 2 写的 B.balance 值被线程 1 覆盖），可能是 100（线程 1 先于线程 2 写 B.balance，线程 1 写的 B.balance 值被线程 2 覆盖），就是不可能是 200。 使用锁的正确姿势 很简单，只要我们的锁能覆盖所有受保护资源就可以了。在上面的例子中，this 是对象级别的锁，所以 A 对象和 B 对象都有自己的锁，如何让 A 对象和 B 对象共享一把锁呢？ 用 Account.class 作为共享的锁, 缺点就是转账操作都成串行了 123456789101112class Account { private int balance; // 转账 void transfer(Account target, int amt){ synchronized(Account.class) { if (this.balance &gt; amt) { this.balance -= amt; target.balance += amt; } } } } 现实世界里，账户转账操作是支持并发的，而且绝对是真正的并行，银行所有的窗口都可以做转账操作。只要我们能仿照现实世界做转账操作，串行的问题就解决了。 我们试想在古代，没有信息化，账户的存在形式真的就是一个账本，而且每个账户都有一个账本，这些账本都统一存放在文件架上。银行柜员在给我们做转账时，要去文件架上把转出账本和转入账本都拿到手，然后做转账。这个柜员在拿账本的时候可能遇到以下三种情况： 文件架上恰好有转出账本和转入账本，那就同时拿走； 如果文件架上只有转出账本和转入账本之一，那这个柜员就先把文件架上有的账本拿到手，同时等着其他柜员把另外一个账本送回来； 转出账本和转入账本都没有，那这个柜员就等着两个账本都被送回来。 上面这个过程在编程的世界里怎么实现呢？其实用两把锁就实现了，转出账本一把，转入账本另一把。在 transfer() 方法内部，我们首先尝试锁定转出账户 this（先把转出账本拿到手），然后尝试锁定转入账户 target（再把转入账本拿到手），只有当两者都成功时，才执行转账操作。 12345678910111213141516class Account { private int balance; // 转账 void transfer(Account target, int amt){ // 锁定转出账户 synchronized(this) { // 锁定转入账户 synchronized(target) { if (this.balance &gt; amt) { this.balance -= amt; target.balance += amt; } } } } } 使用细粒度锁可以提高并行度，是性能优化的一个重要手段,但是会出现死锁的情况，比如下面这种情况： 账户 A 转账户 B 100 元，此时另一个客户找柜员李四也做个转账业务：账户 B 转账户 A 100 元，于是张三和李四同时都去文件架上拿账本，这时候有可能凑巧张三拿到了账本 A，李四拿到了账本 B。张三拿到账本 A 后就等着账本 B（账本 B 已经被李四拿走），而李四拿到账本 B 后就等着账本 A（账本 A 已经被张三拿走），他们要等多久呢？他们会永远等待下去…因为张三不会把账本 A 送回去，李四也不会把账本 B 送回去。我们姑且称为死等吧。 解决死锁在第四节讲。 3. Happens-Before 规则前面一个操作的结果对后续操作是可见的,Happens-Before 约束了编译器的优化行为，虽允许编译器优化，但是要求编译器优化后一定遵守 Happens-Before 规则 3.1 程序的顺序性规则在一个线程中，按照程序顺序，前面的操作 Happens-Before 于后续的任意操作 12345678910111213class VolatileExample { int x = 0; volatile boolean v = false; public void writer() { x = 42; //先发生 v = true; //后发生 } public void reader() { if (v == true) { // 这里 x 会是多少呢？ } }} 3.2 volatile 变量规则对一个 volatile 变量的写操作， Happens-Before 于后续对这个 volatile 变量的读操作 3.3 传递性规则如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens-Before C 3.4 管程中锁的规则对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。 管程是一种通用的同步原语，在 Java 中指的就是 synchronized，synchronized 是 Java 里对管程的实现。 123456synchronized (this) { // 此处自动加锁 // x 是共享变量, 初始值 =10 if (this.x &lt; 12) { this.x = 12; } } // 此处自动解锁 假设 x 的初始值是 10，线程 A 执行完代码块后 x 的值会变成 12（执行完自动释放锁），线程 B 进入代码块时，能够看到线程 A 对 x 的写操作，也就是线程 B 能够看到 x==12。这个也是符合我们直觉的 3.5 线程 start() 规则主线程 A 启动子线程 B 后，子线程 B 能够看到主线程在启动子线程 B 前的操作。 123456789Thread B = new Thread(()-&gt;{ // 主线程调用 B.start() 之前 // 所有对共享变量的修改，此处皆可见 // 此例中，var==77});// 此处对共享变量 var 修改var = 77;// 主线程启动子线程B.start(); 3.6 线程 join() 规则主线程 A 等待子线程 B 完成（主线程 A 通过调用子线程 B 的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），主线程能够看到子线程的操作。当然所谓的“看到”，指的是对共享变量的操作. 换句话说就是，如果在线程 A 中，调用线程 B 的 join() 并成功返回，那么线程 B 中的任意操作 Happens-Before 于该 join() 操作的返回。 123456789101112Thread B = new Thread(()-&gt;{ // 此处对共享变量 var 修改 var = 66;});// 例如此处对共享变量修改，// 则这个修改结果对线程 B 可见// 主线程启动子线程B.start();B.join()// 子线程所有对共享变量的修改// 在主线程调用 B.join() 之后皆可见// 此例中，var==66 4. 如何预防死锁4.1 死锁发生的四个条件 互斥，共享资源 X 和 Y 只能被一个线程占用； 占有且等待，线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X； 不可抢占，其他线程不能强行抢占线程 T1 占有的资源； 循环等待，线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等待。 4.2 破坏死锁条件 对于“占用且等待”这个条件，我们可以一次性申请所有的资源，这样就不存在等待了。 对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。 对于“循环等待”这个条件，可以靠按序申请资源来预防。所谓按序申请，是指资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的，这样线性化后自然就不存在循环了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//利用上面转账的例子class Allocator { private List&lt;Object&gt; als = new ArrayList&lt;&gt;(); // 一次性申请所有资源 synchronized boolean apply( Object from, Object to){ if(als.contains(from) || als.contains(to)){ return false; } else { als.add(from); als.add(to); } return true; } // 归还资源 synchronized void free( Object from, Object to){ als.remove(from); als.remove(to); }} class Account { // actr 应该为单例 private Allocator actr; private int balance; // 转账 void transfer(Account target, int amt){ // 一次性申请转出账户和转入账户，直到成功 while(!actr.apply(this, target))； //while 死循环 try{ // 锁定转出账户 synchronized(this){ // 锁定转入账户 synchronized(target){ if (this.balance &gt; amt){ this.balance -= amt; target.balance += amt; } } } } finally { actr.free(this, target) } } } 破坏不可抢占条件看上去很简单，核心是要能够主动释放它占有的资源，这一点 synchronized 是做不到的。原因是 synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。 1234567891011121314151617181920212223class Account { private int id; private int balance; // 转账 void transfer(Account target, int amt){ Account left = this ① Account right = target; ② if (this.id &gt; target.id) { ③ left = target; ④ right = this; ⑤ } ⑥ // 锁定序号小的账户 synchronized(left){ // 锁定序号大的账户 synchronized(right){ if (this.balance &gt; amt){ this.balance -= amt; target.balance += amt; } } } } } 5. 等待通知模式优化循环等待最好的方案应该是：如果线程要求的条件（转出账本和转入账本同在文件架上）不满足，则线程阻塞自己，进入等待状态；当线程要求的条件（转出账本和转入账本同在文件架上）满足后，通知等待的线程重新执行。其中，使用线程阻塞的方式就能避免循环等待消耗 CPU 的问题。 类比就医环节： 就医流程基本上是这样： 患者先去挂号，然后到就诊门口分诊，等待叫号； 当叫到自己的号时，患者就可以找大夫就诊了； 就诊过程中，大夫可能会让患者去做检查，同时叫下一位患者； 当患者做完检查后，拿检测报告重新分诊，等待叫号； 当大夫再次叫到自己的号时，患者再去找大夫就诊。 下面我们来对比看一下前面都忽视了哪些细节。 患者到就诊门口分诊，类似于线程要去获取互斥锁；当患者被叫到时，类似线程已经获取到锁了。 大夫让患者去做检查（缺乏检测报告不能诊断病因），类似于线程要求的条件没有满足。 患者去做检查，类似于线程进入等待状态；然后大夫叫下一个患者，这个步骤我们在前面的等待 - 通知机制中忽视了，这个步骤对应到程序里，本质是线程释放持有的互斥锁。 患者做完检查，类似于线程要求的条件已经满足；患者拿检测报告重新分诊，类似于线程需要重新获取互斥锁，这个步骤我们在前面的等待 - 通知机制中也忽视了。 5.1 synchronized 实现等待 - 通知机制wait方法原理（会释放锁）： notify方法原理： 为什么说是曾经满足过呢？因为notify() 只能保证在通知时间点，条件是满足的。而被通知线程的执行时间点和通知的时间点基本上不会重合，所以当线程执行的时候，很可能条件已经不满足了（保不齐有其他线程插队）。 上面我们一直强调 wait()、notify()、notifyAll() 方法操作的等待队列是互斥锁的等待队列，所以如果 synchronized 锁定的是 this，那么对应的一定是 this.wait()、this.notify()、this.notifyAll()；如果 synchronized 锁定的是 target，那么对应的一定是 target.wait()、target.notify()、target.notifyAll() 。而且 wait()、notify()、notifyAll() 这三个方法能够被调用的前提是已经获取了相应的互斥锁，所以我们会发现 wait()、notify()、notifyAll() 都是在 synchronized{}内部被调用的。如果在 synchronized{}外部调用，或者锁定的 this，而用 target.wait() 调用的话，JVM 会抛出一个运行时异常：java.lang.IllegalMonitorStateException。 等待 - 通知机制的基本原理搞清楚后，我们就来看看它如何解决一次性申请转出账户和转入账户的问题吧。在这个等待 - 通知机制中，我们需要考虑以下四个要素。 互斥锁：上一篇文章我们提到 Allocator 需要是单例的，所以我们可以用 this 作为互斥锁。 线程要求的条件：转出账户和转入账户都没有被分配过。 何时等待：线程要求的条件不满足就等待。 何时通知：当有线程释放账户时就通知。 ps：因为当 wait() 返回时，有可能条件已经发生变化了，曾经条件满足，但是现在已经不满足了，所以要重新检验条件是否满足。 12345678910111213141516171819202122class Allocator { private List&lt;Object&gt; als; // 一次性申请所有资源 synchronized void apply(Object from, Object to){ // 经典写法 while(als.contains(from) || als.contains(to)){ try{ wait(); //不满足就wait }catch(Exception e){ } } als.add(from); als.add(to); } // 归还资源 synchronized void free( Object from, Object to){ als.remove(from); als.remove(to); notifyAll(); }} notify() 是会随机地通知等待队列中的一个线程，而 notifyAll() 会通知等待队列中的所有线程。 尽量使用 notifyAll()，因为使用notify可能会造成有的线程再也不能被唤醒了 6. 管程Java 采用的是管程技术，synchronized 关键字及 wait()、notify()、notifyAll() 这三个方法都是管程的组成部分。而管程和信号量是等价的，所谓等价指的是用管程能够实现信号量，也能用信号量实现管程。但是管程更容易使用，所以 Java 选择了管程。 管程，对应的英文是 Monitor，很多 Java 领域的同学都喜欢将其翻译成“监视器“。所谓管程，指的是管理共享变量以及对共享变量的操作过程，让他们支持并发。翻译为 Java 领域的语言，就是管理类的成员变量和成员方法，让这个类是线程安全的。那管程是怎么管的呢？ 6.1 MESA 模型在管程的发展史上，先后出现过三种不同的管程模型，分别是：Hasen 模型、Hoare 模型和 MESA 模型。其中，现在广泛应用的是 MESA 模型，并且 Java 管程的实现参考的也是 MESA 模型。所以今天我们重点介绍一下 MESA 模型。 在并发编程领域，有两大核心问题：一个是互斥，即同一时刻只允许一个线程访问共享资源；另一个是同步，即线程之间如何通信、协作。这两大问题，管程都是能够解决的。 6.1.1 解决互斥管程解决互斥问题的思路很简单，就是将共享变量及其对共享变量的操作统一封装起来。 在下图中，管程 X 将共享变量 queue 这个队列和相关的操作入队 enq()、出队 deq() 都封装起来了； 线程 A 和线程 B 如果想访问共享变量 queue，只能通过调用管程提供的 enq()、deq() 方法来实现； enq()、deq() 保证互斥性，只允许一个线程进入管程。不知你有没有发现，管程模型和面向对象高度契合的。 6.1.2 解决同步在管程模型里，共享变量和对共享变量的操作是被封装起来的，图中最外层的框就代表封装的意思。框的上面只有一个入口，并且在入口旁边还有一个入口等待队列。当多个线程同时试图进入管程内部时，只允许一个线程进入，其他线程则在入口等待队列中等待。这个过程类似就医流程的分诊，只允许一个患者就诊，其他患者都在门口等待。 管程里还引入了条件变量的概念，而且每个条件变量都对应有一个等待队列，如下图，条件变量 A 和条件变量 B 分别都有自己的等待队列。 那条件变量和等待队列的作用是什么呢？其实就是解决线程同步问题。你也可以结合上面提到的入队出队例子加深一下理解。 假设有个线程 T1 执行数据出队操作，不过需要注意的是执行出队操作，有个前提条件，就是队列中的数据不能是空的，而队列不空这个前提条件就是管程里的条件变量。 如果线程 T1 进入管程后恰好发现队列是空的，那怎么办呢？等待啊，去哪里等呢？就去条件变量对应的等待队列里面等。此时线程 T1 就去“队列不空”这个条件变量的等待队列中等待。这个过程类似于大夫发现你要去验个血，于是给你开了个验血的单子，你呢就去验血的队伍里排队。线程 T1 进入条件变量的等待队列后，是允许其他线程进入管程的。这和你去验血的时候，医生可以给其他患者诊治，道理都是一样的。 再假设之后另外一个线程 T2 执行数据入队操作，入队操作执行成功之后，“队列不空”这个条件对于线程 T1 来说已经满足了，此时线程 T2 要通知 T1，告诉它需要的条件已经满足了。当线程 T1 得到通知后，会从等待队列里面出来，但是出来之后不是马上执行，而是重新进入到入口等待队列里面。这个过程类似你验血完，回来找大夫，需要重新分诊。 条件变量及其等待队列我们讲清楚了，下面再说说 wait()、notify()、notifyAll() 这三个操作。前面提到线程 T1 发现“队列不空”这个条件不满足，需要进到对应的等待队列里等待。这个过程就是通过调用 wait() 来实现的。如果我们用对象 A 代表“队列不空”这个条件，那么线程 T1 需要调用 A.wait()。同理当“队列不空”这个条件满足时，线程 T2 需要调用 A.notify() 来通知 A 等待队列中的一个线程，此时这个队列里面只有线程 T1。至于 notifyAll() 这个方法，它可以通知等待队列中的所有线程。 这里我还是来一段代码再次说明一下吧。下面的代码实现的是一个阻塞队列，阻塞队列有两个操作分别是入队和出队，这两个方法都是先获取互斥锁，类比管程模型中的入口。 对于入队操作，如果队列已满，就需要等待直到队列不满，所以这里用了notFull.await();。 对于出队操作，如果队列为空，就需要等待直到队列不空，所以就用了notEmpty.await();。 如果入队成功，那么队列就不空了，就需要通知条件变量：队列不空notEmpty对应的等待队列。 如果出队成功，那就队列就不满了，就需要通知条件变量：队列不满notFull对应的等待队列。 123456789101112131415161718192021222324252627282930313233343536373839//实现的是一个阻塞队列，阻塞队列有两个操作分别是入队和出队，这两个方法都是先获取互斥锁public class BlockedQueue&lt;T&gt;{ final Lock lock = new ReentrantLock(); // 条件变量：队列不满 final Condition notFull = lock.newCondition(); // 条件变量：队列不空 final Condition notEmpty = lock.newCondition(); // 入队 void enq(T x) { lock.lock(); try { while (队列已满){ // 等待队列不满 notFull.await(); } // 省略入队操作... // 入队后, 通知可出队 notEmpty.signal(); }finally { lock.unlock(); } } // 出队 void deq(){ lock.lock(); try { while (队列已空){ // 等待队列不空 notEmpty.await(); } // 省略出队操作... // 出队后，通知可入队 notFull.signal(); }finally { lock.unlock(); } }} 6.1.3 wait() 的正确姿势1234//编程范式：用if会造成虚假唤醒while(条件不满足) { wait();} Hasen 模型、Hoare 模型和 MESA 模型的一个核心区别就是当条件满足后，如何通知相关线程。管程要求同一时刻只允许一个线程执行，那当线程 T2 的操作使线程 T1 等待的条件满足时，T1 和 T2 究竟谁可以执行呢？ Hasen 模型里面，要求 notify() 放在代码的最后，这样 T2 通知完 T1 后，T2 就结束了，然后 T1 再执行，这样就能保证同一时刻只有一个线程执行。hasen 是执行完，再去唤醒另外一个线程，能够保证线程的执行。 Hoare 模型里面，T2 通知完 T1 后，T2 阻塞，T1 马上执行；等 T1 执行完，再唤醒 T2，也能保证同一时刻只有一个线程执行。但是相比 Hasen 模型，T2 多了一次阻塞唤醒操作。hoare，是中断当前线程，唤醒另外一个线程，执行玩再去唤醒，也能够保证完成。 MESA 管程里面，T2 通知完 T1 后，T2 还是会接着执行，T1 并不立即执行，仅仅是从条件变量的等待队列进到入口等待队列里面。这样做的好处是 notify() 不用放到代码的最后，T2 也没有多余的阻塞唤醒操作。但是也有个副作用，就是当 T1 再次执行的时候，可能曾经满足的条件，现在已经不满足了，所以需要以循环方式检验条件变量。 6.1.4 notify什么时候使用 所有等待线程拥有相同的等待条件； 所有等待线程被唤醒后，执行相同的操作； 只需要唤醒一个线程。 wait和sleep的区别 相同点： 都是让线程阻塞 都可以接受到中断通知 不同点： 在同步代码块中，sleep不会释放锁，wait会释放锁。所以wait方法必须在synchronized 保护的代码中使用，而sleep没有这个要求。 sleep方法必须定义一个时间，时间到期后自动恢复。而wait可以不设置参数，意味着永久等待 wait是Object类的方法，sleep是Thread的方法。 第三部分、Java中的线程现代操作系统调度的最小单元是线程，也叫轻量级进程(Light Weight Process)，在一个进程里可以创建多个线程，这些线程都拥有各自的计数器、堆栈和局部变量等属性，并且能够访问共享的内存变量。处理器在这些线程上高速切换，让使用者感觉到这些线程在同时执行。 1. 线程的发展路程1.1 操作系统的发展操作系统的发展经历了三个阶段： 手工操作： 单道批处理系统：输入机与主机之间增加了一个存储设备磁带(盘)，单道批处理系统是将作业一个一个加入内存的，那么某一个作业因为等待磁带（盘）或者其他I/O操作而暂停时，那计算机就只能一直阻塞，直到该I/O完成。对于CPU操作密集型的程序，I/O操作相对较少，因此浪费的时间也很少。但是对于I/O操作较多的场景来说，CPU的资源是属于严重浪费的。 多道批处理系统： 为了解决单道批处理系统因为输入/输出（I/O）请求后，导致计算机等待I/O完成而造成的计算机的资源的浪费。接下来又出现了多道批处理系统。多道批处理系统与单道批处理系统的主要区别是在内存中允许一个或多个作业，当一个作业在等待I/O处理时，多批处理系统会通过相应调度算法调度另外一个作业让计算机执行。从而使CPU的利用率得到更大的提高 1.2 进程的由来在多道批处理系统中引申出了一个非常重要的模式，即允许多个作业进入内存并运行。由于在内存中存储了多个作业，那么多个作业如何进行区分？当某个作业因为等待I/O暂停时，怎么恢复到之前的运行状态呢？ 所以这个时候，人们就发明了进程这一概念，用进程来保存每个作业的数据与运行状态，同时对每个进程划分对应的内存地址空间（代码、数据、进程空间、打开的文件），并且要求进程只能使用它自己的内存空间。那么就可以达到作业的区分及恢复。 1.3 线程的由来因为一个进程在一个时间段内只能做一件事情。如果某个程序有多个任务，只能逐个执行这些任务。同时进程中存储了大量信息（数据，进程运行状态信息等）。那么当计算机进行进程切换的时候，必然存在着很大的时间与空间消耗（因为每个进程对应不同内存地址空间，进程的切换，实际是处理器处理不同的地址空间） 为了实现一个进程中任务的切换同时又避免地址空间的切换：发明了线程这一概念，用线程表示进程中的不同任务，同时又将计算机实际调度的单元转到线程。这样就避免了进程的内存地址空间的切换，也达到了多任务的并发执行。 1.4 进程和线程的区别 进程是CPU分配系统资源的基本单位，线程是CPU调度和执行的基本单位。 一个进程可以包含多个线程，进程拥有自己独立的地址空间，而进程中的不同线程共享该进程的地址空间 进程的切换会涉及到虚拟地址空间的切换，开销比较大，线程的切换开销比较小 1.5 为什么要使用多线程 更多的处理器核心：一个 单线程程序在运行时只能使用一个处理器核心，那么再多的处理器核心加入也无法显著提升 该程序的执行效率。相反，如果该程序使用多线程技术，将计算逻辑分配到多个处理器核心 上，就会显著减少程序的处理时间，并且随着更多处理器核心的加入而变得更有效率 更快的响应时间：一笔订单的创建，它包括插入订单数据、生成订单快照、发送邮件通知卖家和记录 货品销售数量等。可以使用多线程技术，即将数据一致性不强的操作派发给其他线程处 理(也可以使用消息队列)，如生成订单快照、发送邮件等。这样做的好处是响应用户请求的线 程能够尽可能快地处理完成，缩短了响应时间，提升了用户体验 更好的编程模型 1.6 线程的优先级现代操作系统基本采用时分的形式调度运行的线程，操作系统会分出一个个时间片，线程会分配到若干时间片，当线程的时间片用完了就会发生线程调度，并等待着下次分配。线程分配到的时间片多少也就决定了线程使用处理器资源的多少，而线程优先级就是决定线程需要多或者少分配一些处理器资源的线程属性。 在Java线程中，通过一个整型成员变量priority来控制优先级，优先级的范围从1~10，在线程构建的时候可以通过setPriority(int)方法来修改优先级，默认优先级是5，优先级高的线程分配时间片的数量要多于优先级低的线程。 1.7 线程的6种状态 NEW（初始化状态） RUNNABLE（可运行 / 运行状态） BLOCKED（阻塞状态） WAITING（无时限等待） TIMED_WAITING（有时限等待） TERMINATED（终止状态） 1.8 Daemon线程Daemon线程是一种支持型线程，因为它主要被用作程序中后台调度以及支持性工作。这 意味着，当一个Java虚拟机中不存在非Daemon线程的时候，Java虚拟机将会退出。可以通过调 用Thread.setDaemon(true)将线程设置为Daemon线程。 2. 启动和终止线程调用线程的start()方法进行启动，随着run()方法的执行完毕，线程也随之终止 2.1 构造线程的三种方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package server.doc.thread;import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;public class ThreadTest { public static void main(String[] args) throws ExecutionException, InterruptedException { A a = new A(); Thread threadA = new Thread(a); threadA.start(); B b = new B(); Thread threadB = new Thread(b); threadB.start(); C c = new C(); FutureTask&lt;Integer&gt; integerFutureTask = new FutureTask&lt;&gt;(c); //FutureTask&lt;V&gt;()是Runnable的实现类 Thread threadC = new Thread(integerFutureTask); threadC.start(); System.out.println(integerFutureTask.get());//可通过get方法获得返回值 }}class A extends Thread{ @Override public void run() { System.out.println(&quot;=======继承Thread类创建线程====&quot;); }}class B implements Runnable{ @Override public void run() { System.out.println(&quot;=======实现runnable接口创建线程====&quot;); }}//实现Callable接口创建线程,Integer就是返回值class C implements Callable&lt;Integer&gt; { @Override public Integer call() throws Exception { System.out.println(&quot;=======实现Callable接口创建线程====&quot;); return 2; }} 2.2 启动线程start源码1234567891011121314151617181920212223242526272829// 该方法可以创建一个新的线程出来public synchronized void start() { // 如果没有初始化，抛异常 if (threadStatus != 0) throw new IllegalThreadStateException(); group.add(this); // started 是个标识符，我们在做一些事情的时候，经常这么写 // 动作发生之前标识符是 false，发生完成之后变成 true boolean started = false; try {// 这里会创建一个新的线程，执行完成之后，新的线程已经在运行了，既 target 的内容已经在运行了 start0(); // 这里执行的还是主线程 started = true; } finally { try { // 如果失败，把线程从线程组中删除 if (!started) { group.threadStartFailed(this); } // Throwable 可以捕捉一些 Exception 捕捉不到的异常，比如说子线程抛出的异常 } catch (Throwable ignore) { /* do nothing. If start0 threw a Throwable then it will be passed up the call stack */ } }}// 开启新线程使用的是 native 方法private native void start0(); 2.3 正确的停止线程中断可以理解为线程的一个标识位属性，它表示一个运行中的线程是否被其他线程进行了中断操作。中断好比其他线程对该线程打了个招呼，其他线程通过调用该线程的interrupt() 方法对其进行中断操作。 从原理上讲应该用 interrupt 来请求中断，而不是强制停止，因为这样可以避免数据错乱，也可以让线程有时间结束收尾工作。 123while (!Thread.currentThread().islnterrupted() &amp;&amp; more work to do) { do more work} 我们一旦调用某个线程的 interrupt() 之后，这个线程的中断标记位就会被设置成 true。每个线程都有这样的标记位，当线程执行时，应该定期检查这个标记位，如果标记位被设置成 true，就说明有程序想终止该线程。回到源码，可以看到在 while 循环体判断语句中，首先通过 Thread.currentThread().isInterrupt() 判断线程是否被中断，随后检查是否还有工作要做。 被 interrupt 的线程，是怎么收到通知的呢？一种是异常，另一种是主动检测。 异常： 当线程 A 处于 WAITING、TIMED_WAITING 状态时，如果其他线程调用线程 A 的 interrupt() 方法，会使线程 A 返回到 RUNNABLE 状态，同时线程 A 的代码会触发 InterruptedException 异常。上面我们提到转换到 WAITING、TIMED_WAITING 状态的触发条件，都是调用了类似 wait()、join()、sleep() 这样的方法，我们看这些方法的签名，发现都会 throws InterruptedException 这个异常。这个异常的触发条件就是：其他线程调用了该线程的 interrupt() 方法。 当线程 A 处于 RUNNABLE 状态时，并且阻塞在 java.nio.channels.InterruptibleChannel 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 会触发 java.nio.channels.ClosedByInterruptException 这个异常；而阻塞在 java.nio.channels.Selector 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 的 java.nio.channels.Selector 会立即返回。 主动监测： 线程通过检查自身是否被中断来进行响应，线程通过方法isInterrupted()来进行判断是否 被中断，也可以调用静态方法Thread.interrupted()对当前线程的中断标识位进行复位。 sleep情况下能否感召到打断位？ 如果 sleep、wait 等可以让线程进入阻塞的方法使线程休眠了，而处于休眠中的线程被中断，那么线程是可以感受到中断信号的，并且会抛出一个 InterruptedException 异常，同时清除中断信号，将中断标记位设置成 false。这样一来就不用担心长时间休眠中线程感受不到中断了，因为即便线程还在休眠，仍然能够响应中断通知，并抛出异常。 3.线程间通信的几种方式 volatile和synchronized关键字 对于同步块的实现使用了monitorenter和monitorexit指令，而同步方法则 是依靠方法修饰符上的ACC_SYNCHRONIZED来完成的。无论采用哪种方式，其本质是对一 个对象的监视器(monitor)进行获取，而这个获取过程是排他的，也就是同一时刻只能有一个 线程获取到由synchronized所保护对象的监视器。 任意一个对象都拥有自己的监视器，当这个对象由同步块或者这个对象的同步方法调用 时，执行方法的线程必须先获取到该对象的监视器才能进入同步块或者同步方法，而没有获 取到监视器(执行该方法)的线程将会被阻塞在同步块和同步方法的入口处，进入BLOCKED 状态。 等待/通知机制（wait / notify） 等待/通知机制，是指一个线程A调用了对象O的wait()方法进入等待状态，而另一个线程B 调用了对象O的notify()或者notifyAll()方法，线程A收到通知后从对象O的wait()方法返回，进而 执行后续操作。上述两个线程通过对象O来完成交互，而对象上的wait()和notify/notifyAll()的 关系就如同开关信号一样，用来完成等待方和通知方之间的交互工作。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package _2不同的生产者消费者模式;/** * 线程之间的通信问题：两个线程交替执行A B操作同一个变量+1，-1* */public class A { public static void main(String[] args) { Data data = new Data(); new Thread(()-&gt;{ for (int i = 0; i &lt; 10; i++) { try { data.increment(); } catch (InterruptedException e) { e.printStackTrace(); } } },&quot;A&quot;).start(); new Thread(()-&gt;{ for (int i = 0; i &lt; 10; i++) { try { data.decrement(); } catch (InterruptedException e) { e.printStackTrace(); } } },&quot;B&quot;).start(); new Thread(()-&gt;{ for (int i = 0; i &lt; 10; i++) { try { data.increment(); } catch (InterruptedException e) { e.printStackTrace(); } } },&quot;C&quot;).start(); new Thread(()-&gt;{ for (int i = 0; i &lt; 10; i++) { try { data.decrement(); } catch (InterruptedException e) { e.printStackTrace(); } } },&quot;D&quot;).start(); }}//1.判断是否需要等待//2.执行业务//3.通知其他线程class Data{ //数字，资源类 private int num = 0; //+1 public synchronized void increment() throws InterruptedException { while (num != 0) { //用if会出现虚假唤醒现象 this.wait(); } num++; System.out.println(Thread.currentThread().getName()+&quot;=====&quot;+num); //通知其他线程，+1完毕 this.notifyAll(); } //-1 public synchronized void decrement() throws InterruptedException { while (num == 0) { this.wait(); } num--; System.out.println(Thread.currentThread().getName()+&quot;=====&quot;+num); this.notifyAll(); }}复制代码 Thread.join()方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package server.doc.thread;public class JoinTest implements Runnable { @Override public void run() { System.out.println(&quot;join thread demo&quot;); } public static void main(String[] args) throws InterruptedException { System.out.println(&quot;main thread start...&quot;); JoinTest joinTest = new JoinTest(); Thread thread = new Thread(joinTest); thread.setName(&quot;joinTest thread&quot;); thread.start(); thread.join(); System.out.println(&quot;main thread end&quot;); }}//没有join的时候：main thread start...main thread endjoin thread demo//有join的时候main thread start...join thread demomain thread end也就是说：当main线程去调用t.join()是，会将自己当前线程阻塞，等到t线程执行完成到达完结状态，main线程才可以继续执行复制代码//join 源码public final synchronized void join(long millis) throws InterruptedException { long base = System.currentTimeMillis(); long now = 0; // 首先校验参数是否合法 if (millis &lt; 0) { throw new IllegalArgumentException(&quot;timeout value is negative&quot;); } // 如果join方法没有参数，则相当于直接调用wait方法 if (millis == 0) { while (isAlive()) { wait(0); } } else { while (isAlive()) {//判断当前的线程是否处于活动状态。什么是活动状态呢？活动状态就是线程已经启动且尚未终止 long delay = millis - now; if (delay &lt;= 0) { break; } wait(delay); now = System.currentTimeMillis() - base; } } }复制代码 ThreadLocal（后续讲解）","link":"/2021/05/06/Java%E5%B9%B6%E5%8F%91%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%B8%8A/"},{"title":"Java并发理论基础-下","text":"Java并发理论基础下章主要描述了以下内容 Lock接口实现的锁 常见的并发容器 常见的并发工具类 线程池 Lock接口synchronized在1.6之后做了很多的优化，效率提高了很多，但是还有很多问题是synchronized无法解决的，因此Lock接口及其实现方法就出现了： 能够响应中断。synchronized 的问题是，持有锁 A 后，如果尝试获取锁 B 失败，那么线程就进入阻塞状态，一旦发生死锁，就没有任何机会来唤醒阻塞的线程。但如果阻塞状态的线程能够响应中断信号，也就是说当我们给阻塞的线程发送中断信号的时候，能够唤醒它，那它就有机会释放曾经持有的锁 A。这样就破坏了不可抢占条件了。 支持超时。如果线程在一段时间之内没有获取到锁，不是进入阻塞状态，而是返回一个错误，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。 非阻塞地获取锁。如果尝试获取锁失败，并不进入阻塞状态，而是直接返回，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。 API接口方法123456void lock(); //获取锁，调用该方法的线程会获得锁，获得锁之后从该方法返回void lockInterruptibly() throws InterruptedException; //可中断的获得锁boolean tryLock(); //尝试非阻塞的获取锁，调用该方法后立即返回，如果能获取返回true，否则返回falseboolean tryLock(long time, TimeUnit unit) throws InterruptedException; //超时的获取锁void unlock(); //释放锁Condition newCondition(); //获取等待通知组件，该组件和当前锁绑定，当前线程获得了锁之后才能调用组件的wait方法释放锁 Lock的一般使用实例1234567Lock lock = new ReentrantLock(); lock.lock(); try { //业务逻辑 }finally { lock.unlock();//在finally块中释放锁，目的是保证在获取到锁之后，最终能够被释放。 } synchronized和ReentrantLock的区别 synchronized是JVM内建的同步机制，是一个关键字，ReentrantLock是一个类。 ReentrantLock可以实现公平锁，可以自定义条件，可以定义超时时间，需要显式的释放锁，而synchronized只能是非公平锁。 每一个lock操作，为了保证锁的释放，最好在finally中显式的unlock lock只适用于代码块，而synchronized可以用来修饰方法，代码块 在Java6之前，synchronized完全依靠操作系统的互斥锁来实现，需要进行用户态和内核态的切换，所以开销较大，但随着一系列的锁优化，synchronized的性能也越来越好了 队列同步器AQSAQS的全称是AbstractQueuedSynchronizer，它的定位是为Java中几乎所有的锁和同步器提供一个基础框架。 AQS是基于FIFO的队列实现的，并且内部维护了一个volatile修饰的状态变量state，通过原子更新这个状态变量state即可以实现加锁解锁操作。 AQS的源码解析 主要内部类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647static final class Node { //初始化两个节点引用 static final Node SHARED = new Node(); static final Node EXCLUSIVE = null; static final int CANCELLED = 1; // 标识线程已取消 static final int SIGNAL = -1; // 标识后继节点需要唤醒 static final int CONDITION = -2; // 标识线程等待在一个条件上 static final int PROPAGATE = -3; // 标识后面的共享锁需要无条件的传播（共享锁需要连续唤醒读的线程） volatile int waitStatus; //// 当前节点保存的线程对应的等待状态 volatile Node prev; volatile Node next; volatile Thread thread; // 当前节点保存的线程 Node nextWaiter; final boolean isShared() { return nextWaiter == SHARED; } final Node predecessor() throws NullPointerException { Node p = prev; if (p == null) throw new NullPointerException(); else return p; } Node() { // Used to establish initial head or SHARED marker } Node(Thread thread, Node mode) { // Used by addWaiter this.nextWaiter = mode; this.thread = thread; } Node(Thread thread, int waitStatus) { // Used by Condition this.waitStatus = waitStatus; this.thread = thread; } } 主要属性 123456789101112131415161718 private transient volatile Node head; //维护一个头节点和尾节点的引用 private transient volatile Node tail; private volatile int state; //同步状态，用volatile修饰 //获取当前同步状态 protected final intgetState() { return state; } //设置新的同步状态 protected final void setState(int newState) { state = newState;} //通过unsafe类的CAS修改同步状态 protected final boolean compareAndSetState(int expect, int update) { // See below for intrinsics setup to support this return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 子类需要实现的方法–模版方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495//提供给子类重写，独占式的获取同步状态，实现该方法需要查询当前状态并判断是否符合预期，然后用CAS来设置同步状态 protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException(); } //提供给子类重写，独占式的释放同步状态，等待的线程将有机会获取同步状态 protected boolean tryRelease(int arg) { throw new UnsupportedOperationException(); } //共享式的获取同步状态，返回值大于0表示成功 protected int tryAcquireShared(int arg) { throw new UnsupportedOperationException(); } //共享式的释放同步状态 protected boolean tryReleaseShared(int arg) { throw new UnsupportedOperationException(); } //当前同步器是否在独占模式下被线程占用 protected boolean isHeldExclusively() { throw new UnsupportedOperationException(); } //独占式获取同步状态，获取成功则返回，否则进入同步队列等待 public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } //和上面这个方法相同，但是响应中断 public final void acquireInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); if (!tryAcquire(arg)) doAcquireInterruptibly(arg); } //在上面的方法中增加了时间限制 public final boolean tryAcquireNanos(int arg, long nanosTimeout) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); return tryAcquire(arg) || doAcquireNanos(arg, nanosTimeout); } //独占式释放同步状态，释放后唤醒同步队列中的第一个节点 public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } //共享式的获取同步状态，主要区别是同一时间可以有多个线程获取到同步状态 public final void acquireShared(int arg) { if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg); } //和上面相同，响应中断 public final void acquireSharedInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg); } public final boolean tryAcquireSharedNanos(int arg, long nanosTimeout) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); return tryAcquireShared(arg) &gt;= 0 || doAcquireSharedNanos(arg, nanosTimeout); } //共享式的释放同步状态 public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } 节点加入等待队列流程同步器将节点加入到同步队列的过程：加入队列的过程必须要保证线程安全，因此同步器提供了一个基于CAS的设置尾节点的方法:**compareAndSetTail(Node expect,Node update)**，它需要传递当前线程“认为”的尾节点和当前节点，只有设置成功后，当前节点才正式 与之前的尾节点建立关联。 1234private final boolean compareAndSetTail(Node expect, Node update) { return unsafe.compareAndSwapObject(this, tailOffset, expect, update);} 设置首节点的过程设置首节点的过程：同步队列遵循FIFO，首节点是获取同步状态成功的节点，首节点的线程在释放同步状态时，将会唤醒后继节点，而后继节点将会在获取同步状态成功时将自己设置为首节点。 设置首节点是通过获取同步状态成功的线程来完成的，由于只有一个线程能够成功获取到同步状态，因此设置头节点的方法并不需要使用CAS来保证，它只需要将首节 点设置成为原首节点的后继节点并断开原首节点的next引用即可。 acquire流程分析123456AQS ----&gt; acquire()public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 123456789101112131415161718192021222324252627282930313233343536373839404142tryAcquire 方法针对公平锁和非公平锁有着不同的实现，总的来说是保证线程安全的获取同步状态 //Fair version of tryAcquire. Don't grant access unless recursive call or no waiters or is first. protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (!hasQueuedPredecessors() &amp;&amp; //hasQueuedPredecessors 是公平锁和非公平锁的区别 compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } //可重入锁的实现 else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } 如果tryAcquire不能获取锁： 123456789101112131415161718192021222324252627282930313233343536373839404142434445//构造新的尾节点，通过CAS来放入队列尾部//Creates and enqueues node for current thread and given mode.Params://mode – Node.EXCLUSIVE for exclusive, Node.SHARED for sharedReturns://the new nodeprivate Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node); ////如果多个线程获取同步状态失败，并发的添加到list，也许会顺序混乱，通过CAS变 得“串行化”了 return node; }/*Inserts node into queue, initializing if necessary. See picture above.Params:node – the node to insertReturns:node's predecessor*/private Node enq(final Node node) { for (;;) { ////通过“死循环”来保证节点的正确添加 Node t = tail; if (t == null) { // Must initialize if (compareAndSetHead(new Node())) tail = head; } else { node.prev = t; //只有通过CAS将节点设置成为尾节点之后，当前线程才能从该方法返回，否则，当前线 程不断地尝试设置 if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } 12345678910111213141516171819202122//acquireQueued --- 节点进入同步队列之后，就进入了一个自旋的过程，每个节点(或者说每个线程)都在自 省地观察，当条件满足，获取到了同步状态，就可以从这个自旋过程中退出，否则依旧留在这 个自旋过程中(并会阻塞节点的线程):final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { //死循环自旋的过程 final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return interrupted; } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } acquire方法调用流程： 前驱节点为头节点且能够获取同步状态的判断条件和线程进入等待状态是获 取同步状态的自旋过程。当同步状态获取成功之后，当前线程从acquire(int arg)方法返回，如果 对于锁这种并发组件而言，代表着当前线程获取了锁。 当前线程获取同步状态并执行了相应逻辑之后，就需要释放同步状态，使得后续节点能 够继续获取同步状态。通过调用同步器的release(int arg)方法可以释放同步状态，该方法在释 放了同步状态之后，会唤醒其后继节点(进而使后继节点重新尝试获取同步状态)。 12345678910@ReservedStackAccess public final boolean release(int arg) { if (tryRelease(arg)) { //tryRelease针对公平锁和非公平锁也有不同的实现 Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } 重入锁重进入是指任意线程在获取到锁之后能够再次获取该锁而不会被锁所阻塞，该特性的实现需要解决以下两个问题。 线程再次获取锁。锁需要去识别获取锁的线程是否为当前占据锁的线程，如果是，则再次成功获取。 锁的最终释放。线程重复n次获取了锁，随后在第n次释放该锁后，其他线程能够获取到该锁。锁的最终释放要求锁对于获取进行计数自增，计数表示当前锁被重复获取的次数，而锁被释放时，计数自减，当计数等于0时表示锁已经成功释放。 12345678910111213141516171819protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { //如果当前线程就是拥有锁的线程 int nextc = c + acquires; //则共享变量++ if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } 读写锁读写锁，并不是 Java 语言特有的，而是一个广为使用的通用技术，所有的读写锁都遵守以下三条基本原则： 允许多个线程同时读共享变量； 只允许一个线程写共享变量； 如果一个写线程正在执行写操作，此时禁止读线程读共享变量。 读写锁在同一时刻可以允许多个读线程访问，但是在写线程访问时，所有的读线程和其他写线程均被阻塞。读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。 LockSupport类LockSupport定义了一组以park开头的方法用来阻塞当前线程，以及unpark(Thread thread) 方法来唤醒一个被阻塞的线程： 1234567891011public static void park(Object blocker) { //阻塞当前线程 Thread t = Thread.currentThread(); setBlocker(t, blocker); UNSAFE.park(false, 0L); setBlocker(t, null); }public static void unpark(Thread thread) { //唤醒当前线程 if (thread != null) UNSAFE.unpark(thread); } Condition接口等待通知模式：任意一个Java对象，都拥有一组监视器方法（定义在java.lang.Object上），主要包括wait()、 wait(long timeout)、notify()以及notifyAll()方法，这些方法与synchronized同步关键字配合，可以实现等待/通知模式 Condition接口也提供了类似Object的监视器方法，与Lock配合可以实现等待/通知模式 新版生产者和消费者一般都会将Condition对象作为成员变量。当调用await()方法后，当前线程会释放锁并在此等待，而其他线程调用Condition对象的signal()方法，通知当前线程后，当前线程才从await()方法返回，并且在返回前已经获取了锁 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class SharedDate{ //共享资源类 private int num = 0; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void increment(){ lock.lock(); try { while (num != 0) condition.await(); //1. 判断释放满足条件，注意用while num++; //2. 业务逻辑 System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); condition.signal(); //3. 唤醒其他线程 } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } public void decrement(){ lock.lock(); try { while (num == 0) condition.await(); num--; System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); condition.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } }}/** 需求：两个线程操作一个初始值为0的变量，一个线程操作变量+1，另一个线程操作变量-1。操作10次后变量依旧为0 * */public class ConditionDemo { public static void main(String[] args) { // 线程操作资源类 SharedDate sharedDate = new SharedDate(); Thread a = new Thread(()-&gt;{ for (int i = 0; i &lt; 5; i++) { sharedDate.increment(); } },&quot;A&quot;); Thread b = new Thread(()-&gt;{ for (int i = 0; i &lt; 5; i++) { sharedDate.decrement(); } },&quot;B&quot;); a.start(); b.start(); } } 精确通知不同的等待者12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485class SharedDate{ //共享资源类 private int num = 1; //1 A ，2 B ， 3 C private Lock lock = new ReentrantLock(); private Condition c1 = lock.newCondition(); //多个条件实现精确通知 private Condition c2 = lock.newCondition(); private Condition c3 = lock.newCondition(); public void print2(int a){ lock.lock(); try { while (num != 1) c1.await(); for (int i = 0; i &lt; a; i++) { System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); } num = 2; //要修改状态位，以此来唤醒不同的线程 c2.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } public void print4(int a){ lock.lock(); try { while (num != 2) c2.await(); for (int i = 0; i &lt; a; i++) { System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); } num = 3;//要修改状态位，以此来唤醒不同的线程 c3.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } public void print8(int a){ lock.lock(); try { while (num != 3) c3.await(); for (int i = 0; i &lt; a; i++) { System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); } num = 1;//要修改状态位，以此来唤醒不同的线程 c1.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } }}/** 需求：三个线程依次打印1，2，3。其中A线程打印2次，B线程打印4次，C线程打印6次** */public class ConditionDemo { public static void main(String[] args) { // 线程操作资源类 SharedDate sharedDate = new SharedDate(); Thread a = new Thread(()-&gt;{ sharedDate.print2(2); },&quot;A&quot;); Thread b = new Thread(()-&gt;{ sharedDate.print4(4); },&quot;B&quot;); Thread c = new Thread(()-&gt;{ sharedDate.print8(8); },&quot;C&quot;); a.start(); b.start(); c.start(); }} Java并发容器CopyOnWriteArrayListCopyOnWrite，顾名思义就是写的时候会将共享变量新复制一份出来，这样做的好处是读操作完全无锁 CopyOnWriteArrayList 内部维护了一个数组，成员变量 array 就指向这个内部数组，所有的读操作都是基于 array 进行的。 如果在遍历 array 的同时，还有一个写操作，例如增加元素，CopyOnWriteArrayList 是如何处理的呢？CopyOnWriteArrayList 会将 array 复制一份，然后在新复制处理的数组上执行增加元素的操作，执行完之后再将 array 指向这个新的数组。通过下图你可以看到，读写是可以并行的，遍历操作一直都是基于原 array 执行，而写操作则是基于新 array 进行。 ConcurrentHashMap1.1 为什么要使用ConcurrentHashMap hashMap线程不安全，hashtable效率低下 ConcurrentHashMap的锁分段技术可有效提升并发访问率，首先将数据分成一段一段地存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问 1.2 结构![image-20210502103840847](/Users/shengbinbin/Library/Application Support/typora-user-images/image-20210502103840847.png) ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁（ReentrantLock），在ConcurrentHashMap里扮演锁的角色；HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组。Segment的结构和HashMap类似，是一种数组和链表结构。一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得与它对应的Segment锁 1.3 初始化1.4 定位segment1.5 常用的方法操作2. ConcurrentLinkedQueue3. Java中的阻塞队列3.1 什么是阻塞队列阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作支持阻塞的插入和移除方法。 1）支持阻塞的插入方法：意思是当队列满时，队列会阻塞插入元素的线程，直到队列不满。 2）支持阻塞的移除方法：意思是在队列为空时，获取元素的线程会等待队列变为非空。 阻塞队列常用于生产者和消费者的场景，生产者是向队列里添加元素的线程，消费者是从队列里取元素的线程。阻塞队列就是生产者用来存放元素、消费者用来获取元素的容器 3.2 常见的阻塞队列种类JDK 7提供了7个阻塞队列，如下。 ·ArrayBlockingQueue：一个由数组结构组成的有界阻塞队列。 ·LinkedBlockingQueue：一个由链表结构组成的有界阻塞队列。 ·PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列。 ·DelayQueue：一个使用优先级队列实现的无界阻塞队列。 ·SynchronousQueue：一个不存储元素的阻塞队列。 ·LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。 ·LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。 3.3 阻塞队列的实现原理ArrayBlockingQueue使用了Condition来实现 1234567891011121314151617181920212223242526272829303132333435363738394041/** Condition for waiting takes */private fnal Condition notEmpty;/** Condition for waiting puts */private fnal Condition notFull;public ArrayBlockingQueue(int capacity, boolean fair) { if (capacity &lt;= 0) throw new IllegalArgumentException(); this.items = new Object[capacity]; lock = new ReentrantLock(fair); notEmpty = lock.newCondition(); notFull = lock.newCondition(); }//put方法public void put(E e) throws InterruptedException { checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { while (count == items.length) notFull.await(); enqueue(e); } finally { lock.unlock(); } }//take public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { while (count == 0) notEmpty.await(); return dequeue(); } finally { lock.unlock(); } } 第七部分、13个原子操作类1. 原子更新基本类型3个·AtomicBoolean：原子更新布尔类型。 ·AtomicInteger：原子更新整型。 ·AtomicLong：原子更新长整型。 1.1 实现原理基于CAS（compare-and-swap）技术来实现的，所谓CAS，表征的是一些列操作的集合，获取当前数值，进行一些运算，利用CAS指令试图进行更新。如果当前数值未变，代表没有其他线程进行并发修改，则成功更新。否则，可能出现不同的选择，要么进行重试，要么就返回一个成功或者失败的结果。 1234567891011121314151617181920public class AtomicInteger extends Number implements java.io.Serializable { private static final long serialVersionUID = 6214790243416807050L; // setup to use Unsafe.compareAndSwapInt for updates private static final Unsafe unsafe = Unsafe.getUnsafe(); //使用了unsafe类 private static final long valueOffset; private volatile int value; //volatile修饰的变量 static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(&quot;value&quot;)); } catch (Exception ex) { throw new Error(ex); } } public AtomicInteger(int initialValue) { value = initialValue; } public AtomicInteger() { } 12345678910111213141516public final int getAndIncrement() { return unsafe.getAndAddInt(this, valueOffset, 1); //调用的是unsafe类中的方法实现原子自增的 }public final int getAndAddInt(Object o, long offset, int delta) { int v; do { v = getIntVolatile(o, offset); } while (!compareAndSwapInt(o, offset, v, v + delta)); return v; }//cas底层是unsafe类中的本地方法，依赖于CPU提供的特定指令public final native boolean compareAndSwapInt(Object o, long offset, int expected, int x); 1.2 CAS的问题 CAS也并不是没有副作用，试想，其常用的失败重试机制，隐含着一个假设，即竞争情况是短暂的。大多数应用场景中，确实大部分重试只会发生一次就获得了成功，但是总是有意外情况，所以在有需要的时候，还是要考虑限制自旋的次数，以免过度消耗CPU。 另外一个就是著名的ABA问题，这是通常只在lock-free算法下暴露的问题。我前面说过CAS是在更新时比较前值，如果对方只是恰好相同，例如期间发生了 A -&gt; B -&gt; A的更新，仅仅判断数值是A，可能导致不合理的修改操作。针对这种情况，Java提供了AtomicStampedReference工具类，通过为引用建立类似版本号（stamp）的方式，来保证CAS的正确性 2. 原子更新数组·AtomicIntegerArray：原子更新整型数组里的元素。 ·AtomicLongArray：原子更新长整型数组里的元素。 ·AtomicReferenceArray：原子更新引用类型数组里的元素。 ·AtomicIntegerArray类主要是提供原子的方式更新数组里的整型，其常用方法如下。 3.原子更新引用类型4. 原子更新字段类第八部分、并发工具类1.倒计时器CountDownLatchCountDownLatch允许一个或多个线程等待其他线程完成操作。 运动员进行跑步比赛时，假设有 6 个运动员参与比赛，裁判员在终点会为这 6 个运动员分别计时，可以想象没当一个运动员到达终点的时候，对于裁判员来说就少了一个计时任务。直到所有运动员都到达终点了，裁判员的任务也才完成。这 6 个运动员可以类比成 6 个线程，当线程调用 CountDownLatch.countDown 方法时就会对计数器的值减一，直到计数器的值为 0 的时候，裁判员（调用 await 方法的线程）才能继续往下执行。 1.1 方法解析12345//整型数 N，之后调用 CountDownLatch 的countDown方法会对 N 减一，知道 N 减到 0 的时候，当前调用await方法的线程继续执行。public CountDownLatch(int count) { if (count &lt; 0) throw new IllegalArgumentException(&quot;count &lt; 0&quot;); this.sync = new Sync(count); } public void await() throws InterruptedException：调用await()方法的线程会被挂起，等待直到count值为0再继续执行。 public boolean await(long timeout, TimeUnit unit) throws InterruptedException：同await()，若等待timeout时长后，count值还是没有变为0，不再等待，继续执行。时间单位如下常用的毫秒、天、小时、微秒、分钟、纳秒、秒。 public void countDown()： count值递减1. public long getCount()：获取当前count值。 public String toString()：重写了toString()方法，多打印了count值，具体参考源码。 1.2 使用实例 创建CountDownLatch并设置计数器值。 启动多线程并且调用CountDownLatch实例的countDown()方法。 主线程调用 await() 方法，这样主线程的操作就会在这个方法上阻塞，直到其他线程完成各自的任务，count值为0，停止阻塞，主线程继续执行。 123456789101112131415161718192021222324252627282930313233343536373839404142public class CountDownLatchDemo { //线程数 private static int N = 10; // 单位：min private static int countDownLatchTimeout = 5; public static void main(String[] args) { //创建CountDownLatch并设置计数值，该count值可以根据线程数的需要设置 CountDownLatch countDownLatch = new CountDownLatch(N); //创建线程池 ExecutorService cachedThreadPool = Executors.newCachedThreadPool(); for (int i = 0; i &lt; N; i++) { cachedThreadPool.execute(() -&gt; { try { System.out.println(Thread.currentThread().getName() + &quot; do something!&quot;); } catch (Exception e) { System.out.println(&quot;Exception: do something exception&quot;); } finally { //该线程执行完毕-1 countDownLatch.countDown(); } }); } System.out.println(&quot;main thread do something-1&quot;); try { countDownLatch.await(countDownLatchTimeout, TimeUnit.MINUTES); } catch (InterruptedException e) { System.out.println(&quot;Exception: await interrupted exception&quot;); } finally { System.out.println(&quot;countDownLatch: &quot; + countDownLatch.toString()); } System.out.println(&quot;main thread do something-2&quot;); //若需要停止线程池可关闭;// cachedThreadPool.shutdown(); }} 2.循环栅栏：CyclicBarrier 开运动会时，会有跑步这一项运动，我们来模拟下运动员入场时的情况，假设有 6 条跑道，在比赛开始时，就需要 6 个运动员在比赛开始的时候都站在起点了，裁判员吹哨后才能开始跑步。跑道起点就相当于“barrier”，是临界点，而这 6 个运动员就类比成线程的话，就是这 6 个线程都必须到达指定点了，意味着凑齐了一波，然后才能继续执行，否则每个线程都得阻塞等待，直至凑齐一波即可。cyclic 是循环的意思，也就是说 CyclicBarrier 当多个线程凑齐了一波之后，仍然有效，可以继续凑齐下一波 2.1 常用方法12345678910//等到所有的线程都到达指定的临界点await() throws InterruptedException, BrokenBarrierException//与上面的await方法功能基本一致，只不过这里有超时限制，阻塞等待直至到达超时时间为止await(long timeout, TimeUnit unit) throws InterruptedException,BrokenBarrierException, TimeoutException//获取当前有多少个线程阻塞等待在临界点上int getNumberWaiting()//用于查询阻塞等待的线程是否被中断boolean isBroken() 2.2 使用实例123456789101112131415161718192021222324public class CyclicBarrierDemo { //指定必须有6个运动员到达才行 private static CyclicBarrier barrier = new CyclicBarrier(6, () -&gt; { System.out.println(&quot;所有运动员入场，裁判员一声令下！！！！！&quot;); }); public static void main(String[] args) { System.out.println(&quot;运动员准备进场，全场欢呼............&quot;); ExecutorService service = Executors.newFixedThreadPool(6); for (int i = 0; i &amp;lt; 6; i++) { service.execute(() -&amp;gt; { try { System.out.println(Thread.currentThread().getName() + &quot; 运动员，进场&quot;); barrier.await(); System.out.println(Thread.currentThread().getName() + &quot; 运动员出发&quot;); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } }); }}} 2.3 CyclicBarrier 和CountDownLatch 的区别 CountDownLatch是不可以重置的，所以无法重用；而CyclicBarrier则没有这种限制，可以重用。 CountDownLatch的基本操作组合是countDown/await。调用await的线程阻塞等待countDown足够的次数，不管你是在一个线程还是多个线程里countDown，只要次数足够即可。所以就像Brain Goetz说过的，CountDownLatch操作的是事件。 CyclicBarrier的基本操作组合，则就是await，当所有的伙伴（parties）都调用了await，才会继续进行任务，并自动进行重置。注意，正常情况下，CyclicBarrier的重置都是自动发生的，如果我们调用reset方法，但还有线程在等待，就会导致等待线程被打扰，抛出BrokenBarrierException异常。CyclicBarrier侧重点是线程，而不是调用事件，它的典型应用场景是用来等待并发线程结束。 3.控制并发线程数的SemaphoreSemaphore（信号量）是用来控制同时访问特定资源的线程数量，它通过协调各个线程，以保证合理的使用公共资源 第九部分、线程池1. 为什么要使用线程池第一：降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 第二：提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 第三：提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。但是，要做到合理利用线程池，必须对其实现原理了如指掌。 2. 线程池的实现原理2.1五种不同的标准线程池Executors目前提供了5种不同的线程池创建配置： newCachedThreadPool()，它是一种用来处理大量短时间工作任务的线程池，具有几个鲜明特点：它会试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；如果线程闲置的时间超过60秒，则被终止并移出缓存；长时间闲置时，这种线程池，不会消耗什么资源。其内部使用SynchronousQueue作为工作队列。 newFixedThreadPool(int nThreads)，重用指定数目（nThreads）的线程，其背后使用的是无界的工作队列，任何时候最多有nThreads个工作线程是活动的。这意味着，如果任务数量超过了活动队列数目，将在工作队列中等待空闲线程出现；如果有工作线程退出，将会有新的工作线程被创建，以补足指定的数目nThreads。 newSingleThreadExecutor()，它的特点在于工作线程数目被限制为1，操作一个无界的工作队列，所以它保证了所有任务的都是被顺序执行，最多会有一个任务处于活动状态，并且不允许使用者改动线程池实例，因此可以避免其改变线程数目。 newSingleThreadScheduledExecutor()和newScheduledThreadPool(int corePoolSize)，创建的是个ScheduledExecutorService，可以进行定时或周期性的工作调度，区别在于单一工作线程还是多个工作线程。 newWorkStealingPool(int parallelism)，这是一个经常被人忽略的线程池，Java 8才加入这个创建方法，其内部会构建ForkJoinPool，利用Work-Stealing算法，并行地处理任务，不保证处理顺序。 2.2 创建七大参数 corePoolSize：核心线程数 maximumPoolSize：最大线程数 keepAliveTime：线程存活时间 TimeUnit：存活时间的单位 workQueue：工作队列，必须是阻塞队列 ThreadFactory ：创建线程的工厂 RejectedExecutionHandler：拒绝执行的策略 2.3 四大拒绝策略·AbortPolicy：直接抛出异常。 ·CallerRunsPolicy：只用调用者所在线程来运行任务。 ·DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务。 ·DiscardPolicy：不处理，丢弃掉。 Ps：可以根据应用场景需要来实现RejectedExecutionHandler接口自定义策略。如记录日志或持久化存储不能处理的任务。 2.4 任务处理流程![image-20210502114404422](/Users/shengbinbin/Library/Application Support/typora-user-images/image-20210502114404422.png) 2.5 提交任务submit和execute execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功。 execute()方法输入的任务是一个Runnable类的实例。 submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 2.6 线程池的大小选择策略 如果是计算型任务，说明CPU是一种稀缺的资源，线程太多会导致上下文切换，所以线程数一般为按照CPU核的数目N或者N+1； 如果是IO密集型任务，线程数 = CPU核数 × （1 + 平均等待时间/平均工作时间） 第十部分、Executor框架1. Executor框架简介1.1Executor框架的两级调度模型 在HotSpot VM的线程模型中，Java线程（java.lang.Thread）被一对一映射为本地操作系统线 程。Java线程启动时会创建一个本地操作系统线程；当该Java线程终止时，这个操作系统线程也会被回收。操作系统会调度所有线程并将它们分配给可用的CPU。 在上层，Java多线程程序通常把应用分解为若干个任务，然后使用用户级的调度器（Executor框架）将这些任务映射为固定数量的线程；在底层，操作系统内核将这些线程映射到硬件处理器上: ![image-20210503091113831](/Users/shengbinbin/Library/Application Support/typora-user-images/image-20210503091113831.png) 1.2 三大组成部分 任务。包括被执行任务需要实现的接口：Runnable接口或Callable接口。 任务的执行。包括任务执行机制的核心接口Executor，以及继承自Executor的 ExecutorService接口。Executor框架有两个关键类实现了ExecutorService接口 （ThreadPoolExecutor和ScheduledThreadPoolExecutor）。 异步计算的结果。包括接口Future和实现Future接口的FutureTask类。 ![image-20210503091241378](/Users/shengbinbin/Library/Application Support/typora-user-images/image-20210503091241378.png) 2. ThreadPoolExecutor详解Executor框架最核心的类是ThreadPoolExecutor，它是线程池的实现类，主要由下列4个组件构成。 ·corePool：核心线程池的大小。 ·maximumPool：最大线程池的大小。 ·BlockingQueue：用来暂时保存任务的工作队列。 ·RejectedExecutionHandler：当ThreadPoolExecutor已经关闭或ThreadPoolExecutor已经饱和 时（达到了最大线程池大小且工作队列已满），execute()方法将要调用的Handler。 2.1 FixedThreadPoolFixedThreadPool的corePoolSize和maximumPoolSize都被设置为创建FixedThreadPool时指定的参数nThreads。 123456public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory); } keepAliveTime设置为0L，意味着多余的空闲线程会被立即终止 FixedThreadPool使用无界队列LinkedBlockingQueue作为线程池的工作队列（队列的容量为 Integer.MAX_VALUE）。使用无界队列作为工作队列会对线程池带来如下影响: 1）当线程池中的线程数达到corePoolSize后，新任务将在无界队列中等待，因此线程池中 的线程数不会超过corePoolSize。 2）由于1，使用无界队列时maximumPoolSize将是一个无效参数。 3）由于1和2，使用无界队列时keepAliveTime将是一个无效参数。 4）由于使用无界队列，运行中的FixedThreadPool（未执行方法shutdown()或 shutdownNow()）不会拒绝任务（不会调用RejectedExecutionHandler.rejectedExecution方法）。 2.2 SingleThreadExecutor123456public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); } SingleThreadExecutor的corePoolSize和maximumPoolSize被设置为1。其他参数与 FixedThreadPool相同。SingleThreadExecutor使用无界队列LinkedBlockingQueue作为线程池的工 作队列（队列的容量为Integer.MAX_VALUE）。SingleThreadExecutor使用无界队列作为工作队列 对线程池带来的影响与FixedThreadPool相同. 2.3 CachedThreadPool12345public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); } CachedThreadPool的corePoolSize被设置为0，即corePool为空；maximumPoolSize被设置为 Integer.MAX_VALUE，即maximumPool是无界的。这里把keepAliveTime设置为60L，意味着 CachedThreadPool中的空闲线程等待新任务的最长时间为60秒，空闲线程超过60秒后将会被 终止。 如果主线程提交任务的速度高于 maximumPool中线程处理任务的速度时，CachedThreadPool会不断创建新线程。极端情况下， CachedThreadPool会因为创建过多线程而耗尽CPU和内存资源。","link":"/2021/05/07/Java%E5%B9%B6%E5%8F%91%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%B8%8B/"},{"title":"Java并发编程（七）并发工具类","text":"","link":"/2021/10/08/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%EF%BC%88%E4%B8%83%EF%BC%89%E5%B9%B6%E5%8F%91%E5%B7%A5%E5%85%B7%E7%B1%BB/"},{"title":"Java并发编程（五）并发容器","text":"java中提供了线程安全的并发容器 ConcurrentHashMap ConcurrentLinkedQueue 阻塞队列 ConcurrentHashMap为什么要使用ConcurrentHashMap hashMap线程不安全，hashtable效率低下 ConcurrentHashMap的锁分段技术可有效提升并发访问率，首先将数据分成一段一段地存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问 结构 ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁（ReentrantLock），在ConcurrentHashMap里扮演锁的角色；HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组。Segment的结构和HashMap类似，是一种数组和链表结构。一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得与它对应的Segment锁 初始化定位segment常用的方法操作ConcurrentLinkedQueue阻塞队列什么是阻塞队列阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作支持阻塞的插入和移除方法。 1）支持阻塞的插入方法：意思是当队列满时，队列会阻塞插入元素的线程，直到队列不满。 2）支持阻塞的移除方法：意思是在队列为空时，获取元素的线程会等待队列变为非空。 阻塞队列常用于生产者和消费者的场景，生产者是向队列里添加元素的线程，消费者是从队列里取元素的线程。阻塞队列就是生产者用来存放元素、消费者用来获取元素的容器 常见的阻塞队列种类JDK 7提供了7个阻塞队列，如下。 ·ArrayBlockingQueue：一个由数组结构组成的有界阻塞队列。 ·LinkedBlockingQueue：一个由链表结构组成的有界阻塞队列。 ·PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列。 ·DelayQueue：一个使用优先级队列实现的无界阻塞队列。 ·SynchronousQueue：一个不存储元素的阻塞队列。 ·LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。 ·LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。 阻塞队列的实现原理ArrayBlockingQueue使用了Condition来实现 1234567891011121314151617181920212223242526272829303132333435363738394041/** Condition for waiting takes */private fnal Condition notEmpty;/** Condition for waiting puts */private fnal Condition notFull;public ArrayBlockingQueue(int capacity, boolean fair) { if (capacity &lt;= 0) throw new IllegalArgumentException(); this.items = new Object[capacity]; lock = new ReentrantLock(fair); notEmpty = lock.newCondition(); notFull = lock.newCondition(); }//put方法public void put(E e) throws InterruptedException { checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { while (count == items.length) notFull.await(); enqueue(e); } finally { lock.unlock(); } }//take public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { while (count == 0) notEmpty.await(); return dequeue(); } finally { lock.unlock(); } }","link":"/2021/10/08/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%EF%BC%88%E4%BA%94%EF%BC%89%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8/"},{"title":"Java并发编程（八）线程池","text":"本节主要围绕下面几个问题来讲述： 为什么要使用线程池 如何正确的创建线程池 如何获取线程池的执行结果 正确的使用CompletableFuture 正确的使用CompletionService 线程池为什么要使用线程池第一：降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 第二：提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 第三：提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。但是，要做到合理利用线程池，必须对其实现原理了如指掌。 线程池（Thread Pool）是一种基于池化思想管理线程的工具，经常出现在多线程服务器中，如MySQL。 “池化”思想不仅仅能应用在计算机领域，在金融、设备、人员管理、工作管理等领域也有相关的应用。 在计算机领域中的表现为：统一管理IT资源，包括服务器、存储、和网络资源等等。通过共享资源，使用户在低投入中获益。除去线程池，还有其他比较典型的几种使用策略包括： 内存池(Memory Pooling)：预先申请内存，提升申请内存速度，减少内存碎片。 连接池(Connection Pooling)：预先申请数据库连接，提升申请连接的速度，降低系统的开销。 实例池(Object Pooling)：循环使用对象，减少资源在初始化和释放时的昂贵损耗。 线程池的实现原理五种不同的标准线程池Executors目前提供了5种不同的线程池创建配置： newCachedThreadPool()，它是一种用来处理大量短时间工作任务的线程池，具有几个鲜明特点：它会试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；如果线程闲置的时间超过60秒，则被终止并移出缓存；长时间闲置时，这种线程池，不会消耗什么资源。其内部使用SynchronousQueue作为工作队列。 newFixedThreadPool(int nThreads)，重用指定数目（nThreads）的线程，其背后使用的是无界的工作队列，任何时候最多有nThreads个工作线程是活动的。这意味着，如果任务数量超过了活动队列数目，将在工作队列中等待空闲线程出现；如果有工作线程退出，将会有新的工作线程被创建，以补足指定的数目nThreads。 newSingleThreadExecutor()，它的特点在于工作线程数目被限制为1，操作一个无界的工作队列，所以它保证了所有任务的都是被顺序执行，最多会有一个任务处于活动状态，并且不允许使用者改动线程池实例，因此可以避免其改变线程数目。 newSingleThreadScheduledExecutor()和newScheduledThreadPool(int corePoolSize)，创建的是个ScheduledExecutorService，可以进行定时或周期性的工作调度，区别在于单一工作线程还是多个工作线程。 newWorkStealingPool(int parallelism)，这是一个经常被人忽略的线程池，Java 8才加入这个创建方法，其内部会构建ForkJoinPool，利用Work-Stealing算法，并行地处理任务，不保证处理顺序。 创建七大参数 corePoolSize：核心线程数 maximumPoolSize：最大线程数 keepAliveTime：线程存活时间 TimeUnit：存活时间的单位 workQueue：工作队列，必须是阻塞队列 ThreadFactory ：创建线程的工厂 RejectedExecutionHandler：拒绝执行的策略 四大拒绝策略·AbortPolicy：直接抛出异常。 ·CallerRunsPolicy：只用调用者所在线程来运行任务。 ·DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务。 ·DiscardPolicy：不处理，丢弃掉。 Ps：可以根据应用场景需要来实现RejectedExecutionHandler接口自定义策略。如记录日志或持久化存储不能处理的任务。 任务处理流程 提交任务submit和execute execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功。 execute()方法输入的任务是一个Runnable类的实例。 submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 线程池的大小选择策略 如果是计算型任务，说明CPU是一种稀缺的资源，线程太多会导致上下文切换，所以线程数一般为按照CPU核的数目N或者N+1； 如果是IO密集型任务，线程数 = CPU核数 × （1 + 平均等待时间/平均工作时间） 如何正确的创建线程池线程池的结果获取FutureThreadPoolExecutor的execute方法并没有返回值，那我们如何获取任务的执行结果呢？ 123456789101112131415161718192021222324//java.util.concurrent.ThreadPoolExecutor#executepublic void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); //1. 如果正在运行的线程小于 corePoolSize ，那么就尝试启动一个新的线程使用给定的命令作为第一个任务 if (workerCountOf(c) &lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } //2. 如果任务可以成功的排队，我们仍然需要检测是否需要一个新的线程（可能自从上次检测后线程就死亡了或者线程池关闭了） if (isRunning(c) &amp;&amp; workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } //3. 如果不能成功的排队任务，我们需要添加一个新的线程。如果添加失败就拒绝这个任务。 else if (!addWorker(command, false)) reject(command); } ExecutorService提供了三个重载submit方法，返回值都是Future接口 123456789//这个方法的参数是一个 Runnable 接口，Runnable 接口的 run() 方法是没有返回值的，所以 `submit(Runnable task)` 这个方法返回的 Future 仅可以用来断言任务已经结束了，类似于 Thread.join()。Future&lt;?&gt; submit(Runnable task);//这个方法的参数是一个 Callable 接口，它只有一个 call() 方法，并且这个方法是有返回值的，所以这个方法返回的 Future 对象可以通过调用其 get() 方法来获取任务的执行结果。&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task);//这个方法很有意思，假设这个方法返回的 Future 对象是 f，f.get() 的返回值就是传给 submit() 方法的参数 result&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); Future接口有5个方法： 123456789101112// 取消任务boolean cancel()boolean mayInterruptIfRunning);// 判断任务是否已取消 boolean isCancelled();// 判断任务是否已结束boolean isDone();// 获得任务执行结果get();// 获得任务执行结果，支持超时get(long timeout, TimeUnit unit); FutureTask工具类FutureTask 实现了 Runnable 和 Future 接口： 12345678910111213// 有两个构造函数public FutureTask(Callable&lt;V&gt; callable) { if (callable == null) throw new NullPointerException(); this.callable = callable; this.state = NEW; // ensure visibility of callable }public FutureTask(Runnable runnable, V result) { this.callable = Executors.callable(runnable, result); this.state = NEW; // ensure visibility of callable } 由于实现了 Runnable 接口，所以可以将 FutureTask 对象作为任务提交给 ThreadPoolExecutor 去执行，也可以直接被 Thread 执行； 又因为实现了 Future 接口，所以也能用来获得任务的执行结果。 123456789101112131415public static void main(String[] args) throws ExecutionException, InterruptedException { // 创建 FutureTask FutureTask futureTask = new FutureTask&lt;&gt;(() -&gt; 1+2); // 创建线程池 ExecutorService threadPool = newCachedThreadPool(); // 提交 FutureTask threadPool.submit(futureTask); //通过 FutureTask的get方法获取结果 System.out.println(futureTask.get()); //3 // 也可以直接将FutureTask 作为线程构造函数的参数 new Thread(futureTask).start(); System.out.println(futureTask.get()); } 利用 FutureTask 对象可以很容易获取子线程的执行结果！ 用多线程实现烧水泡茶 用两个线程 T1 和 T2 来完成烧水泡茶程序，T1 负责洗水壶、烧开水、泡茶这三道工序，T2 负责洗茶壶、洗茶杯、拿茶叶三道工序，其中 T1 在执行泡茶这道工序时需要等待 T2 完成拿茶叶的工序。对于 T1 的这个等待动作，你应该可以想出很多种办法，例如 Thread.join()、CountDownLatch，甚至阻塞队列都可以解决，不过今天我们用 Future 特性来实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;import java.util.concurrent.TimeUnit;/** * @author shengbinbin * @description: 使用FutureTask 来实现烧水泡茶 * @date 2021/8/2510:40 下午 */public class TestFuture { public static void main(String[] args) throws ExecutionException, InterruptedException { // 创建任务 T2 的 FutureTask FutureTask&lt;String&gt; ft2 = new FutureTask&lt;&gt;(new T2task()); // 创建任务 T1 的 FutureTask FutureTask&lt;String&gt; ft1 = new FutureTask&lt;&gt;(new T1task(ft2)); // 线程 T1 执行任务 ft1 Thread T1 = new Thread(ft1); T1.start(); // 线程 T2 执行任务 ft2 Thread T2 = new Thread(ft2); T2.start(); // 等待线程 T1 执行结果 System.out.println(ft1.get()); } // T1Task 需要执行的任务： // 洗水壶、烧开水、泡茶 static class T1task implements Callable&lt;String&gt;{ FutureTask&lt;String&gt; ft2; // T1 任务需要 T2 任务的 FutureTask T1task(FutureTask&lt;String&gt; ft2){ this.ft2 = ft2; } @Override public String call() throws Exception { System.out.println(&quot;T1: 洗水壶..&quot;); TimeUnit.SECONDS.sleep(1); // 需要1分钟 System.out.println(&quot;T1: 烧开水...&quot;); TimeUnit.SECONDS.sleep(15); // 获取 T2 线程的茶叶,需要有线程的交互 String tf = ft2.get(); System.out.println(&quot;T1: 拿到茶叶:&quot;+tf); System.out.println(&quot;T1: 泡茶...&quot;); return &quot; 上茶:&quot; + tf; } } // T2Task 需要执行的任务: // 洗茶壶、洗茶杯、拿茶叶 static class T2task implements Callable&lt;String&gt;{ @Override public String call() throws Exception { System.out.println(&quot;T2: 洗茶壶...&quot;); TimeUnit.SECONDS.sleep(1); System.out.println(&quot;T2: 洗茶杯...&quot;); TimeUnit.SECONDS.sleep(2); System.out.println(&quot;T2: 拿茶叶...&quot;); TimeUnit.SECONDS.sleep(1); return &quot; 龙井 &quot;; } }} 利用 Java 并发包提供的 Future 可以很容易获得异步任务的执行结果，无论异步任务是通过线程池 ThreadPoolExecutor 执行的，还是通过手工创建子线程来执行的。Future 可以类比为现实世界里的提货单，比如去蛋糕店订生日蛋糕，蛋糕店都是先给你一张提货单，你拿到提货单之后，没有必要一直在店里等着，可以先去干点其他事，比如看场电影；等看完电影后，基本上蛋糕也做好了，然后你就可以凭提货单领蛋糕了。 利用多线程可以快速将一些串行的任务并行化，从而提高性能；如果任务之间有依赖关系，比如当前任务依赖前一个任务的执行结果，这种问题基本上都可以用 Future 来解决。在分析这种问题的过程中，建议你用有向图描述一下任务之间的依赖关系，同时将线程的分工也做好，类似于烧水泡茶最优分工方案那幅图。对照图来写代码，好处是更形象，且不易出错。 CompletableFuture用多线程优化性能，其实不过就是将串行操作变成并行操作: 12345678910// 以下两个方法都是耗时操作doBizA();doBizB();//异步化：主线程无需等待 doBizA() 和 doBizB() 的执行结果，也就是说 doBizA() 和 doBizB() 两个操作已经被异步化了。new Thread(()-&gt;doBizA()) .start();new Thread(()-&gt;doBizB()) .start(); 用CompletableFuture实现烧水泡茶123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.concurrent.CompletableFuture;import java.util.concurrent.TimeUnit;/** * @author shengbinbin * @description: CompletableFuture 使用 * @date 2021/8/2510:52 下午 */public class TestCompletableFuture { public static void main(String[] args) { // 任务 1：洗水壶 -&gt; 烧开水 CompletableFuture&lt;Void&gt; f1 = CompletableFuture.runAsync(()-&gt;{ System.out.println(&quot;T1: 洗水壶...&quot;); sleep(1, TimeUnit.SECONDS); System.out.println(&quot;T1: 烧开水...&quot;); sleep(15, TimeUnit.SECONDS); }); // 任务 2：洗茶壶 -&gt; 洗茶杯 -&gt; 拿茶叶 CompletableFuture&lt;String&gt; f2 = CompletableFuture.supplyAsync(()-&gt;{ System.out.println(&quot;T2: 洗茶壶...&quot;); sleep(1, TimeUnit.SECONDS); System.out.println(&quot;T2: 洗茶杯...&quot;); sleep(2, TimeUnit.SECONDS); System.out.println(&quot;T2: 拿茶叶...&quot;); sleep(1, TimeUnit.SECONDS); return &quot; 龙井 &quot;; }); // 任务 3：任务 1 和任务 2 完成后执行：泡茶 CompletableFuture&lt;String&gt; f3 = f1.thenCombine(f2, (__, tf)-&gt;{ System.out.println(&quot;T1: 拿到茶叶:&quot; + tf); System.out.println(&quot;T1: 泡茶...&quot;); return &quot; 上茶:&quot; + tf; }); // 等待任务 3 执行结果 System.out.println(f3.join()); } static void sleep(int t, TimeUnit u) { try { u.sleep(t); }catch(InterruptedException e){} }} 创建 CompletableFuture 对象主要靠下面4个静态方法： 1234567// 使用默认线程池static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable) //Runnable 接口的 run() 方法没有返回值static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier) //Supplier 接口的 get() 方法是有返回值的。// 可以指定线程池 static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable, Executor executor)static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier, Executor executor) 默认情况下 CompletableFuture 会使用公共的 ForkJoinPool 线程池，这个线程池默认创建的线程数是 CPU 的核数（也可以通过 JVM option:-Djava.util.concurrent.ForkJoinPool.common.parallelism 来设置 ForkJoinPool 线程池的线程数）。如果所有 CompletableFuture 共享一个线程池，那么一旦有任务执行一些很慢的 I/O 操作，就会导致线程池中所有线程都阻塞在 I/O 操作上，从而造成线程饥饿，进而影响整个系统的性能。所以，强烈建议你要根据不同的业务类型创建不同的线程池，以避免互相干扰。 创建完 CompletableFuture 对象之后，会自动地异步执行 runnable.run() 方法或者 supplier.get() 方法。 对于一个异步操作，你需要关注两个问题： 一个是异步操作什么时候结束， 另一个是如何获取异步操作的执行结果。 因为 CompletableFuture 类实现了 Future 接口，所以这两个问题你都可以通过 Future 接口来解决。另外，CompletableFuture 类还实现了 CompletionStage 接口， CompletionStage 接口任务是有时序关系的，比如有串行关系、并行关系、汇聚关系等。这样说可能有点抽象，这里还举前面烧水泡茶的例子，其中洗水壶和烧开水就是串行关系，洗水壶、烧开水和洗茶壶、洗茶杯这两组任务之间就是并行关系，而烧开水、拿茶叶和泡茶就是汇聚关系。 例如前面提到的 f3 = f1.thenCombine(f2, ()-&gt;{}) 描述的就是一种汇聚关系。烧水泡茶程序中的汇聚关系是一种 AND 聚合关系，这里的 AND 指的是所有依赖的任务（烧开水和拿茶叶）都完成后才开始执行当前任务（泡茶）。既然有 AND 聚合关系，那就一定还有 OR 聚合关系，所谓 OR 指的是依赖的任务只要有一个完成就可以执行当前任务。 描述串行关系主要是 thenApply、thenAccept、thenRun 和 thenCompose 这四个系列的接口。 1234567public &lt;U&gt; CompletionStage&lt;U&gt; thenApply(Function&lt;? super T,? extends U&gt; fn);public CompletionStage&lt;Void&gt; thenAccept(Consumer&lt;? super T&gt; action);public CompletionStage&lt;Void&gt; thenRun(Runnable action);public &lt;U&gt; CompletionStage&lt;U&gt; thenCompose(Function&lt;? super T, ? extends CompletionStage&lt;U&gt;&gt; fn); thenApply 系列函数里参数 fn 的类型是接口 Function&lt;T, R&gt;，这个接口里与 CompletionStage 相关的方法是 R apply(T t)，这个方法既能接收参数也支持返回值，所以 thenApply 系列方法返回的是CompletionStage&lt;R&gt;。 而 thenAccept 系列方法里参数 consumer 的类型是接口Consumer&lt;T&gt;，这个接口里与 CompletionStage 相关的方法是 void accept(T t)，这个方法虽然支持参数，但却不支持回值，所以 thenAccept 系列方法返回的是CompletionStage&lt;Void&gt;。 thenRun 系列方法里 action 的参数是 Runnable，所以 action 既不能接收参数也不支持返回值，所以 thenRun 系列方法返回的也是CompletionStage&lt;Void&gt;。 这些方法里面 Async 代表的是异步执行 fn、consumer 或者 action。其中，需要你注意的是 thenCompose 系列方法，这个系列的方法会新创建出一个子流程，最终结果和 thenApply 系列是相同的。 12345678CompletionStage&lt;R&gt; thenApply(fn);CompletionStage&lt;R&gt; thenApplyAsync(fn);CompletionStage&lt;Void&gt; thenAccept(consumer);CompletionStage&lt;Void&gt; thenAcceptAsync(consumer);CompletionStage&lt;Void&gt; thenRun(action);CompletionStage&lt;Void&gt; thenRunAsync(action);CompletionStage&lt;R&gt; thenCompose(fn);CompletionStage&lt;R&gt; thenComposeAsync(fn); thenApply的使用： 12345678public static void main(String[] args) throws ExecutionException, InterruptedException { CompletableFuture&lt;String&gt; completableFuture = CompletableFuture .supplyAsync(() -&gt; &quot;Hello,World&quot;) // 1 是一个异步流程 .thenApply(s -&gt; s + &quot;QQ&quot;) // 2 .thenApply(String :: toUpperCase); //3 和 2是一个串行的 // join方法的作用是使主线程等待 completableFuture 的任务执行完 System.out.println(completableFuture.join()); } 描述 AND 汇聚关系主要是 thenCombine、thenAcceptBoth 和 runAfterBoth 系列的接口，这些接口的区别也是源自 fn、consumer、action 这三个核心参数不同。它们的使用你可以参考上面烧水泡茶的实现程序，这里就不赘述了 123456CompletionStage&lt;R&gt; thenCombine(other, fn);CompletionStage&lt;R&gt; thenCombineAsync(other, fn);CompletionStage&lt;Void&gt; thenAcceptBoth(other, consumer);CompletionStage&lt;Void&gt; thenAcceptBothAsync(other, consumer);CompletionStage&lt;Void&gt; runAfterBoth(other, action);CompletionStage&lt;Void&gt; runAfterBothAsync(other, action); 描述 OR 汇聚关系主要是 applyToEither、acceptEither 和 runAfterEither 系列的接口，这些接口的区别也是源自 fn、consumer、action 这三个核心参数不同。 123456CompletionStage applyToEither(other, fn);CompletionStage applyToEitherAsync(other, fn);CompletionStage acceptEither(other, consumer);CompletionStage acceptEitherAsync(other, consumer);CompletionStage runAfterEither(other, action);CompletionStage runAfterEitherAsync(other, action); 使用： 123456789101112131415161718CompletableFuture&lt;String&gt; f1 = CompletableFuture.supplyAsync(()-&gt;{ int t = getRandom(5, 10); sleep(t, TimeUnit.SECONDS); return String.valueOf(t);}); CompletableFuture&lt;String&gt; f2 = CompletableFuture.supplyAsync(()-&gt;{ int t = getRandom(5, 10); sleep(t, TimeUnit.SECONDS); return String.valueOf(t);}); CompletableFuture&lt;String&gt; f3 = f1.applyToEither(f2,s -&gt; s); System.out.println(f3.join()); 异常处理虽然上面我们提到的 fn、consumer、action 它们的核心方法都不允许抛出可检查异常，但是却无法限制它们抛出运行时异常，例如下面的代码，执行 7/0 就会出现除零错误这个运行时异常。非异步编程里面，我们可以使用 try{}catch{}来捕获并处理异常，那在异步编程里面，异常该如何处理呢？ 12345CompletableFuture&lt;Integer&gt; f0 = CompletableFuture. .supplyAsync(()-&gt;(7/0)) .thenApply(r-&gt;r*10);System.out.println(f0.join()); CompletionStage 接口给我们提供的方案非常简单，比 try{}catch{}还要简单，下面是相关的方法，使用这些方法进行异常处理和串行操作是一样的，都支持链式编程方式。 12345CompletionStage exceptionally(fn);CompletionStage&lt;R&gt; whenComplete(consumer);CompletionStage&lt;R&gt; whenCompleteAsync(consumer);CompletionStage&lt;R&gt; handle(fn);CompletionStage&lt;R&gt; handleAsync(fn); 下面的示例代码展示了如何使用 exceptionally() 方法来处理异常，exceptionally() 的使用非常类似于 try{}catch{}中的 catch{}，但是由于支持链式编程方式，所以相对更简单。既然有 try{}catch{}，那就一定还有 try{}finally{}，whenComplete() 和 handle() 系列方法就类似于 try{}finally{}中的 finally{}，无论是否发生异常都会执行 whenComplete() 中的回调函数 consumer 和 handle() 中的回调函数 fn。whenComplete() 和 handle() 的区别在于 whenComplete() 不支持返回结果，而 handle() 是支持返回结果的。 12345678public static void main(String[] args) throws ExecutionException, InterruptedException { CompletableFuture&lt;Integer&gt; completableFuture = CompletableFuture .supplyAsync(() -&gt; 7/0) // 1 是一个异步流程 .thenApply(r -&gt; r * 10) // 2 .exceptionally(e -&gt; 0); // 如果发生运行时异常就这样 // join方法的作用是使主线程等待 completableFuture 的任务执行完 System.out.println(completableFuture.join()); } 线程池的一些思考总体设计先看一下ThreadPoolExecutor类的继承关系: ThreadPoolExecutor实现的顶层接口是Executor， 顶层接口Executor提供了一种思想：将任务提交和任务执行进行解耦。用户无需关注如何创建线程，如何调度线程来执行任务，用户只需提供Runnable对象，将任务的运行逻辑提交到执行器(Executor)中，由Executor框架完成线程的调配和任务的执行部分。 ExecutorService接口增加了一些能力： （1）扩充执行任务的能力，补充可以为一个或一批异步任务生成Future的方法； （2）提供了管控线程池的方法，比如停止线程池的运行。 AbstractExecutorService则是上层的抽象类，将执行任务的流程串联了起来，保证下层的实现只需关注一个执行任务的方法即可。 最下层的实现类ThreadPoolExecutor实现最复杂的运行部分，ThreadPoolExecutor将会一方面维护自身的生命周期，另一方面同时管理线程和任务，使两者良好的结合从而执行并行任务。 ThreadPoolExecutor是如何运行，如何同时维护线程和执行任务的呢？其运行机制如下图所示 线程池在内部实际上构建了一个生产者消费者模型，将线程和任务两者解耦，并不直接关联，从而良好的缓冲任务，复用线程。线程池的运行主要分成两部分：任务管理、线程管理。任务管理部分充当生产者的角色，当任务提交后，线程池会判断该任务后续的流转：（1）直接申请线程执行该任务；（2）缓冲到队列中等待线程执行；（3）拒绝该任务。线程管理部分是消费者，它们被统一维护在线程池内，根据任务请求进行线程的分配，当线程执行完任务后则会继续获取新的任务去执行，最终当线程获取不到任务的时候，线程就会被回收。 线程池的生命周期线程池运行的状态，并不是用户显式设置的，而是伴随着线程池的运行，由内部来维护。线程池内部使用一个变量维护两个值：**运行状态(runState)和线程数量 (workerCount)**。在具体实现中，线程池将运行状态(runState)、线程数量 (workerCount)两个关键参数的维护放在了一起，如下代码所示： 1private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); ctl这个AtomicInteger类型，是对线程池的运行状态和线程池中有效线程的数量进行控制的一个字段， 它同时包含两部分的信息：线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount)，高3位保存runState，低29位保存workerCount，两个变量之间互不干扰。用一个变量去存储两个值，可避免在做相关决策时，出现不一致的情况，不必为了维护两者的一致，而占用锁资源。通过阅读线程池源代码也可以发现，经常出现要同时判断线程池运行状态和线程数量的情况。线程池也提供了若干方法去供用户获得线程池当前的运行状态、线程个数。这里都使用的是位运算的方式，相比于基本运算，速度也会快很多。 关于内部封装的获取生命周期状态、获取线程池线程数量的计算方法如以下代码所示： 123private static int runStateOf(int c) { return c &amp; ~CAPACITY; } //计算当前运行状态private static int workerCountOf(int c) { return c &amp; CAPACITY; } //计算当前线程数量private static int ctlOf(int rs, int wc) { return rs | wc; } //通过状态和线程数生成ctl ThreadPoolExecutor的运行状态有5种，分别为： 任务执行机制任务调度 任务缓冲任务缓冲模块是线程池能够管理任务的核心部分。线程池的本质是对任务和线程的管理，而做到这一点最关键的思想就是将任务和线程两者解耦，不让两者直接关联，才可以做后续的分配工作。 线程池中是以生产者消费者模式，通过一个阻塞队列来实现的。阻塞队列缓存任务，工作线程从阻塞队列中获取任务 阻塞队列(BlockingQueue)是一个支持两个附加操作的队列。这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素 阻塞队列的分类: 任务申请任务的执行有两种可能： 一种是任务直接由新创建的线程执行,基本只出现在线程池刚启动的时候。 另一种是线程从任务队列中获取任务然后执行，执行完任务的空闲线程会再次去从队列中申请任务再去执行。 线程获取任务的流程如下： getTask这部分进行了多次判断，为的是控制线程的数量，使其符合线程池的状态。如果线程池现在不应该持有那么多线程，则会返回null值。工作线程Worker会不断接收新任务去执行，而当工作线程Worker接收不到任务的时候，就会开始被回收。 任务拒绝任务拒绝模块是线程池的保护部分，线程池有一个最大的容量，当线程池的任务缓存队列已满，并且线程池中的线程数目达到maximumPoolSize时，就需要拒绝掉该任务，采取任务拒绝策略，保护线程池。 线程池的业务实践 快速响应用户的请求： 快速处理批量任务： 实际问题如何正确的配置线程池的线程数美团的动态线程池设计","link":"/2021/10/08/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%EF%BC%88%E5%85%AB%EF%BC%89%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"title":"Java并发编程（六）原子操作类","text":"","link":"/2021/10/08/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%EF%BC%88%E5%85%AD%EF%BC%89%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C%E7%B1%BB/"},{"title":"Java并发（一）内存模型和锁","text":"主要讲述了 并发编程的发展历程 解决可见性和有序性 解决原子性 死锁 等待通知 并发编程的发展历程并发编程问题的由来随着CPU 、 内存 、IO设备的不断发展，三者的速度差异一直是存在的（CPU一天 ， 内存一年 ， IO设备十年） 为了合理利用 CPU 的高性能，平衡这三者的速度差异，计算机体系机构、操作系统、编译程序都做出了贡献，主要体现为： CPU 增加了缓存，以均衡与内存的速度差异； 操作系统增加了进程、线程，以分时复用 CPU，进而均衡 CPU 与 I/O 设备的速度差异； 编译程序优化指令执行次序，使得缓存能够得到更加合理地利用 1.1 缓存导致的可见性问题 一个线程对共享变量的修改，另外一个线程能够立刻看到，我们称为可见性。 单核时代，电脑只有1个CPU，所有的线程操作的是同一个CPU的缓存，也就不存在可见性问题。 多核时代，每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了，当多个线程在不同的 CPU 上执行时，这些线程操作的是不同的 CPU 缓存，如下图线程A对变量V的操作线程B就看不到了。 比如下面这个代码：两个线程同时对变量count 进行2000次++操作。最后count的值一定&lt;=4000（由于线程操作的不可见性带来的操作覆盖） 1234567891011121314151617181920212223242526272829303132public class HelloWorld { private int count = 0; private void add() { int idx = 0; while(idx++ &lt; 2000) { count += 1; } } public static int calc() throws Exception { final HelloWorld test = new HelloWorld(); Thread th1 = new Thread(()-&gt;{ test.add(); }); Thread th2 = new Thread(()-&gt;{ test.add(); }); th1.start(); th2.start(); // 等待两个线程执行结束 th1.join(); th2.join(); return test.count; } public static void main(String[] args) throws Exception { long c =calc(); System.out.println(c); }} 1.2 线程切换带来的原子性问题 一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性 在一个时间片内，如果一个进程进行一个 IO 操作，例如读个文件，这个时候该进程可以把自己标记为“休眠状态”并出让 CPU 的使用权，待文件读进内存，操作系统会把这个休眠的进程唤醒，唤醒后的进程就有机会重新获得 CPU 的使用权了。 早期的操作系统基于进程来调度 CPU，不同进程间是不共享内存空间的，所以进程要做任务切换就要切换内存映射地址，而一个进程创建的所有线程，都是共享一个内存空间的，所以线程做任务切换成本就很低了。现代的操作系统都基于更轻量的线程来调度，现在我们提到的“任务切换”都是指“线程切换”。 Java并发程序都是基于多线程的，也就会涉及到任务切换。一般来说任务切换的时机大多数都是在一个时间片结束的时候，但是在Java这种高级语言中，一个语句往往需要多个CPU指令来执行。比如上面代码中的count += 1，至少需要三条 CPU 指令。 指令 1：首先，需要把变量 count 从内存加载到 CPU 的寄存器； 指令 2：之后，在寄存器中执行 +1 操作； 指令 3：最后，将结果写入内存（缓存机制导致可能写入的是 CPU 缓存而不是内存） 操作系统进行任务切换的时候，发生在任何一个CPU指令执行完之后，而不是可见的Java语言后。如下这种情况线程A就会把线程B执行的count+=1覆盖了。 编译优化带来的有序性问题有序性指的是程序按照代码的先后顺序执行。编译器为了优化性能，有时候会进行指令重排序。 重排序分3种类型。 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术(Instruction-Level Parallelism，ILP)来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应 机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上 去可能是在乱序执行 例如程序中：“a=6；b=7；”编译器优化后可能变成“b=7；a=6；”，在这个例子中，编译器调整了语句的顺序，但是不影响程序的最终结果，但是有时会出现问题。 比如单例模式中的双重检验模式： 1234567891011121314151617181920public class Singleton { static Singleton instance; public Singleton getInstance(){ if (instance == null){ // a synchronized (Singleton.class){ if (instance == null) instance = new Singleton(); //在CPU指令上并不是一步操作 } } return instance; }}/**解释一下这个双重校验模式：假设有两个线程 A、B 同时调用 getInstance() 方法，他们会同时发现 instance == null (同时执行到a处代码) ，于是同时对 Singleton.class 加锁，此时 JVM 保证只有一个线程能够加锁成功（假设是线程 A），另外一个线程则会处于等待状态（假设是线程 B）； 线程 A 会创建一个Singleton 实例，之后释放锁，锁释放后，线程 B 被唤醒，线程 B 再次尝试加锁，此时是可以加锁成功的，加锁成功后，线程 B 检查 instance == null 时会发现，已经创建过Singleton 实例了，所以线程 B 不会再创建一个 Singleton 实例。 看起来是没问题，但实际上还存在瑕疵，因为new一个新的对象分为如下几步（先在内存中初始化对象再赋值给变量）： 分配一块内存 M； 在内存 M 上初始化 Singleton 对象； 然后 M 的地址赋值给 instance 变量。 经过指令重排序变成了下面这种情况（先将内存赋值给变量再进行初始化）： 分配一块内存 M； 将 M 的地址赋值给 instance 变量； 最后在内存 M 上初始化 Singleton 对象。 不安全的情况：假设线程 A 先执行 getInstance() 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance() 方法，那么线程 B 在执行第一个判断时会发现 instance != null ，所以直接返回 instance，而此时的 instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。 解决可见性和有序性导致可见性的原因是缓存，导致有序性的原因是编译优化，那解决可见性、有序性最直接的办法就是禁用缓存和编译优化。虽然可以解决问题，但是程序的性能会受到很大影响，因此，最合理的方式应该是按需禁用缓存以及编译优化。何谓按需，按照程序的要求来进行禁用。 使用volatilevolatile在C语言中也存在，它的原始意义就是禁用CPU缓存。 声明一个 volatile 变量 volatile int x = 0，它表达的是：告诉编译器，对这个变量的读写，不能使用 CPU 缓存，必须从内存中读取或者写入。 Happens-Before 规则它想表达的是：前面一个操作的结果对后续的操作是可见的。Happens-Before 约束了编译器的优化行为，虽允许编译器优化，但是要求编译器优化后一定遵守 Happens\u0002Before 规则。 1234567891011121314class VolatileExample { int x = 0; volatile boolean v = false; public void writer() { x = 42; // a v = true; // b } public void reader() { if (v == true) { // c // 这里 x 会是多少呢？ } }} 假设线程 A 执行 writer() 方法，按照 volatile 语义，会把变量“v=true” 写入内存；假设线程 B 执行 reader() 方法，同样按照 volatile 语义，线程 B会从内存中读取变量 v，如果线程 B 看到 “v == true” 时，那么线程 B 看到的变量 x 是多少呢？ 同一线程的顺序性规则指在一个线程中，按照程序顺序，前面的操作 Happens-Before 于后续的任意操作。比如上面的代码 x = 42 Happens-Before 于代码 “v = true;”. volatile 变量规则对一个 volatile 变量的写操作， Happens-Before 于后续对这个 volatile 变量的读操作。 传递性指如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens\u0002-Before C。将这个规则和上面的volatile规则结合就能得到问题的答案： a 对b 可见 ， b是对volatile变量的写操作，所有b对c可见。 — 也就是 a 对c可见，所以 读出来是42。 管程中锁的规则指对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。 管程是一种通用的同步原语，在Java 中指的就是 synchronized，synchronized 是 Java 里对管程的实现。 线程 start() 规则主线程A启动子线程B后，子线程B可以看到主线程在启动子线程B之前的操作。 12345678910Thread B = new Thread(()-&gt;{ // 主线程调用 B.start() 之前 // 所有对共享变量的修改，此处皆可见 // 此例中，var==77 }); // 此处对共享变量 var 修改 var = 77; // 主线程启动子线程 B.start(); 线程Join()规则指主线程 A 等待子线程 B 完成（主线程 A 通过调用子线程 B的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），主线程能够看到子线程的操作。当然所谓的“看到”，指的是对共享变量的操作。 换句话说：如果在线程 A 中，调用线程 B 的 join() 并成功返回，那么线程 B 中的任意操作 Happens-Before 于该 join() 操作的返回。 12345678910111213Thread B = new Thread(()-&gt;{ // 此处对共享变量 var 修改 var = 66;}); // 例如此处对共享变量修改， // 则这个修改结果对线程 B 可见 // 主线程启动子线程 B.start(); B.join() // 子线程所有对共享变量的修改 // 在主线程调用 B.join() 之后皆可见 // 此例中，var==66 使用final关键字final 修饰变量时，初衷是告诉编译器：这个变量生而不变，可以可劲儿优化。Java 编译器在 1.5 以前的版本的确优化得很努力，以至于都优化错了。 解决原子性原子性问题的源头是线程切换，如果能够禁用线程切换那不就能解决这个问题了吗？而操作系统做线程切换是依赖 CPU 中断的，所以禁止 CPU 发生中断就能够禁止线程切换。在早期单核 CPU 时代，这个方案的确是可行的。但是在多核场景下，同一时刻，有可能有两个线程同时在执行，一个线程执行在 CPU-1上，一个线程执行在 CPU-2 上，此时禁止 CPU 中断，只能保证 CPU 上的线程连续执行，并不能保证同一时刻只有一个线程执行，如果这两个线程同时写 long 型变量高 32 位的话，那就有可能出现我们开头提及的诡异 Bug 了。 同一时刻只有一个线程执行”这个条件非常重要，我们称之为互斥 简易锁模型： 改进后：锁是用来保护资源的： synchronizedsynchronized既可以修饰方法，也可以修饰代码块。Java 编译器会在 synchronized 修饰的方法或代码块前后自动加上加锁 lock() 和解锁 unlock()，这样做的好处就是加锁 lock() 和解锁 unlock() 一定是成对出现的。 当修饰静态方法的时候，锁定的是当前类的 Class 对象，在上面的例子中就是 Class X； 当修饰非静态方法的时候，锁定的是当前实例对象 this。 修饰代码块： 1234567891011121314151617181920class X { // 修饰非静态方法 synchronized void foo() { // 临界区 } // 修饰静态方法 synchronized static void bar() { // 临界区 } // 修饰代码块 Object obj = new Object()； void baz() { synchronized(obj) { // 临界区 } }} 被 synchronized 修饰后，无论是单核CPU 还是多核 CPU，只有一个线程能够执行addOne方法,也就说如果有 1000 个线程执行 addOne() 方法，最终结果一定是 value 的值增加了 1000。 1234567891011public class HelloWorld { long value = 0L; long get(){ return value; } synchronized void addOne(){ value +=1; }} 执行 addOne() 方法后，value 的值对 get() 方法是可见的吗？这个可见性是没法保证的。管程中锁的规则，是只保证后续对这个锁的加锁的可见性，而 get() 方法并没有加锁操作，所以可见性没法保证。那如何解决呢？很简单，就是 get() 方法也 synchronized 一下， 1234567891011public class HelloWorld { long value = 0L; synchronized long get(){ return value; } synchronized void addOne(){ value +=1; }} 这样的话 get 和addone方法也是互斥的。 这个模型更像现实世界里面球赛门票的管理，一个座位只允许一个人使用，这个座位就是“受保护资源”，球场的入口就是 Java 类里的方法，而门票就是用来保护资源的“锁”，Java 里的检票工作是由 synchronized 解决的。 锁和受保护资源的关系受保护资源和锁之间的关联关系是 N:1 的关系.可以用一把锁来保护多个资源，但是不能用多把锁来保护一个资源 1234567891011public class HelloWorld { static long value = 0L; synchronized long get(){ return value; } synchronized static void addOne(){ value +=1; }} 这样的话就是两个锁锁的是同一个资源：两个锁分别是 this 和 SafeCalc.class。由于临界区 get() 和 addOne() 是用两个锁保护的，因此这两个临界区没有互斥 关系，临界区 addOne() 对 value 的修改对临界区 get() 也没有可见性保证，这就导致并发问题了。 锁，一定有一个要锁定的对象，至于这个锁定的对象要保护的资源以及在哪里加锁 / 解锁，就属于设计层面的事情了。 如何用一把锁来保护多个资源保护没有关联关系的多个资源例如，银行业务中有针对账户余额（余额是一种资源）的取款操作，也有针对账户密码（密码也是一种资源）的更改操作，我们可以为账户余额和账户密码分配不同的锁来解决并发问题： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * @author shengbinbin * @description: 保护没有关联关系的多个资源 */public class Account { // 锁：保护账户余额 private final Object balLock = new Object(); //账户余额 private Integer balance; //锁：保护账户密码 private final Object pwLock = new Object(); // 账户密码 private String password; //取款 void withdraw(Integer amt){ synchronized (balLock){ if (this.balance &gt; amt) { this.balance -= amt; } } } //查看余额 Integer getBalance(){ synchronized (balLock){ return balance; } } //更改密码 void updatePassword(String pw){ synchronized (pwLock){ this.password = pw; } } //查看密码 String getPassword(){ synchronized (pwLock){ return password; } } } 我们也可以用一把互斥锁来保护多个资源，例如我们可以用 this 这一把锁来管理账户类里所有的资源：账户余额和用户密码.就是性能太差，会导致取款、查看余额、修改密码、查看密码这四个操作都是串行的。 我们用两把锁，取款和修改密码是可以并行的。用不同的锁对受保护资源进行精细化管理，能够提升性能。这种锁还有个名字，叫细粒度锁。 保护有关联关系的多个资源例如银行业务里面的转账操作，账户 A 减少 100 元，账户 B 增加 100 元。这两个账户就是有关联关系的。那对于像转账这种有关联关系的操作，我们应该怎么去解决呢？ 1234567891011public class Account { private int balance; //怎么保证转账操作 transfer() 没有并发问题呢 void transfer(Account target , int amt){ if (this.balance &gt; amt){ this.balance -= amt; target.balance += amt; } }} 如果简单的用synchronize来修饰这个方法并不能解决并发问题，为什么呢？ 临界区内有两个资源，分别是转出账户的余额 this.balance 和转入账户的余额 target.balance，并且用的是一把锁 this。问题是这把锁可以保护自己的 余额，怎么保护别人的余额呢？ 下面我们具体分析一下，假设有 A、B、C 三个账户，余额都是 200 元，我们用两个线程分别执行两个转账操作： 账户 A 转给账户 B 100 元，账户 B 转给账户 C 100 元， 最后我们期望的结果应该是账户 A 的余额是 100 元，账户 B 的余额是 200 元， 账户 C 的余额是300元。 我们假设线程 1 执行账户 A 转账户 B 的操作，线程 2 执行账户 B 转账户 C 的操作。这两个线程分别在两颗 CPU 上同时执行，那它们是互斥的吗？ 我们期望是，但实际上并不是。 因为线程 1 锁定的是账户 A 的实例（A.this），而线程 2 锁定的是账户 B 的实例（B.this），所以这两个线程可以同时进入临界区 transfer()。同时进入临界区的结果是什么呢？ 线程 1 和线程 2 都会读到账户 B 的余额为 200，导致最终账户 B 的余额可能是300（线程 1 后于线程 2 写 B.balance，线程 2 写的 B.balance 值被线程 1 覆盖）， 可能是 100（线程 1 先于线程 2 写 B.balance，线程 1 写的 B.balance 值被线程 2 覆盖），就是不可能是 200。 如何解决这个问题呢？ 只要我们的锁能覆盖所有受保护资源就可以了 this 是对象级别的锁，所以 A 对象和 B 对象都有自己的锁，如何让 A 对象和 B 对象共享一把锁呢？ 方案1：把 Account 默认构造函数变为 private，同时增加一个带 Object lock 参数的构造函数，创建 Account 对象时，传入相同的 lock，这样所有的 Account 对象都会共享这个lock 了。 12345678910111213141516171819class Account { private Object lock； private int balance; private Account(); // 创建 Account 时传入同一个 lock 对象 public Account(Object lock) { this.lock = lock;} // 转账 void transfer(Account target, int amt){ // 此处检查所有对象共享的锁 synchronized(lock) { if (this.balance &gt; amt) { this.balance -= amt; target.balance += amt;}}}} 它要求在创建 Account 对象的时候必须传入同一个对象，如果创建 Account 对象时，传入的 lock 不是同一个对象，会出现锁自家门来保护他家资产的荒唐事 方案2：用Account.class 作为共享的锁。Account.class 是所有 Account 对象共享的，而且这个对象是 Java 虚拟机在加载 Account 类的时候创建的，所以我们不用担心它的唯一性。使用Account.class 作为共享的锁，我们就无需在创建 Account 对象时传入了，代码更简单 1234567891011121314public class Account { private int balance; //怎么保证转账操作 transfer() 没有并发问题呢 void transfer(Account target , int amt){ synchronized (Account.class){ if (this.balance &gt; amt){ this.balance -= amt; target.balance += amt; } } }} 缺点：会导致所有转账都是串行的。 总结：对如何保护多个资源已经很有心得了，关键是要分析多个资源之间的关系。如果资源之间没有关系，很好处理，每个资源一把锁就可以了。如果资源之间有关联关系，就要选择一个粒度更大的锁，这个锁应该能够覆盖所有相关的资源。 死锁在古代的时候，账户的存在形式真的就是一个账本，而且每个账户都有一个账本，这些账本都统一存放在文件架上。在给我们做转账时，要去文件架上把转出账本和转入账本都拿到手，然后做转账。这个柜员在拿账本的时候可能遇到以下三种情况： 文件架上恰好有转出账本和转入账本，那就同时拿走； 如果文件架上只有转出账本和转入账本之一，那这个柜员就先把文件架上有的账本拿到手，同时等着其他柜员把另外一个账本送回来； 转出账本和转入账本都没有，那这个柜员就等着两个账本都被送回来。 类比到现在的模型，其实就是两把锁：转出账本 和 转入账本。在 transfer() 方法内部，我们首先尝试锁定转出账户 this（先把转出账本拿到手），然后尝试锁定转入账户 target（再把转入账本拿到手），只有当两者都成功时，才执行转账操作。 代码实现： 123456789101112131415161718192021/** * @author shengbinbin * @description: 转账 */public class Account { private int balance; void transfer(Account target , int amt){ //1. 先锁定转出账号 synchronized (this){ //① //2. 再锁定转入的账号 synchronized (target){ //② if (this.balance &gt; amt){ this.balance -= amt; target.balance += amt; } } } }} 这样的话，我们只需要锁定两个账户就行了，而不是锁定Account.class。锁的粒度更小了，并发度更高了。但是我们需要警惕死锁的发生： 出现死锁如果有客户找柜员张三做个转账业务：账户 A 转账户 B 100 元 此时另一个客户找柜员李四也做个转账业务：账户 B转账户 A 100 元， 于是张三和李四同时都去文件架上拿账本，这时候有可能凑巧张三拿到了账本 A，李四拿到了账本 B。张三拿到账本 A 后就等着账本 B（账本 B 已经被李四拿走），而李四拿到账本 B 后就等着账本 A（账本 A 已经被张三拿走），他们要等多久呢？ 他们会永远等待下去…因为张三不会把账本 A 送回去，李四也不会把账本 B 送回去。这就是死锁。 死锁的一个比较专业的定义是：一组互相竞争资源的线程因互相等待，导致“永久”阻塞的现象。 比较上面的代码可知： 当 T1 和 T2 同时执行完①处的代码时，T1 获得了账户 A 的锁（对于 T1，this 是账户A），而 T2 获得了账户 B 的锁（对于 T2，this 是账户 B）。之后 T1 和 T2 在执行②处的代码时，T1 试图获取账户 B 的锁时，发现账户 B 已经被锁定（被 T2 锁定），所以 T1 开始等待；T2 则试图获取账户 A 的锁时，发现账户 A 已经被锁定（被 T1 锁定），所以 T2也开始等待。于是 T1 和 T2 会无期限地等待下去，也就是我们所说的死锁了。 死锁的条件及预防死锁发生的四个条件： 互斥，共享资源 X 和 Y 只能被一个线程占用； 占有且等待，线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X； 不可抢占，其他线程不能强行抢占线程 T1 占有的资源； 循环等待，线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等待。 破坏这4个条件： 对于“占用且等待”这个条件，我们可以一次性申请所有的资源，这样就不存在等待了。 对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。 对于“循环等待”这个条件，可以靠按序申请资源来预防。所谓按序申请，是指资源是有线性顺序的 如何实践： 可以增加一个账本管理员，只有账本 A 和 B 都在的时候才会给张三，也就是一次性申请所有的资源。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * @author shengbinbin * @description: 账本管理员 * @date 2021/8/292:39 下午 */public class Allocator { private List&lt;Object&gt; als = new ArrayList&lt;&gt;(); // 一次性申请所有的资源 synchronized boolean apply(Object from , Object to){ if (als.contains(from) || als.contains(to)){ return false; }else { als.add(from); als.add(to); } return true; } // 归还资源 synchronized void free(Object from , Object to){ als.remove(from); als.remove(to); }}public class Account { private int balance; private Allocator allocator; // 必须为单例 // 转账 void transfer(Account target , int amt){ while(!allocator.apply(target,this)){ // 一次性申请转出账户和转入账户，直到成功 // 死循环 ; } try { //1. 先锁定转出账号 synchronized (this){ //2. 再锁定转入的账号 synchronized (target){ if (this.balance &gt; amt){ this.balance -= amt; target.balance += amt; } } } }finally { allocator.free(this,target); } }} 破坏不可抢占条件: 这一点synchronized 是做不到的。原因是 synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。在java.util.concurrent 这个包下面提供的 Lock 是可以轻松解决这个问题的。 破坏循环等待条件：破坏这个条件，需要对资源进行排序，然后按序申请资源。申请的时候，我们可以按照从小到大的顺序来申请: 12345678910111213141516171819202122232425public class Account { private int id; private int balance; // 转账 void transfer(Account target , int amt){ Account left = this; Account right = target; if (this.id &gt; target.id){ left = target; right = this; } // 先锁序号小的账户 synchronized (left){ // 再锁序号大的账户 synchronized (right){ if (this.balance &gt; amt){ this.balance -= amt; target.balance += amt; } } } }} 例如上面转账那个例子，我们破坏占用且等待条件的成本就比破坏循环等待条件的成本高，破坏占用且等待条件，我们也是锁了所有的账户，而且还是用了死循环 while(!actr.apply(this, target));方法，不过好在 apply() 这个方法基本不耗时。 在转账这个例子中，破坏循环等待条件就是成本最低的一个方案。 等待通知机制现实中的等待通知例子去医院就诊： 患者先去挂号，然后到就诊门口分诊，等待叫号。这一步就是线程要去获取互斥锁，叫到号意味着线程已经获得锁了。 就诊过程中，大夫可能会让患者去做检查。这一步类似的就是线程的条件没有得到满足。 患者去做检查。这一步就是线程进入等待状态。 大夫叫下一个患者。这一步就是上一个线程释放了互斥锁。 患者做完检查。 这一步就是线程的条件重新满足了。 患者拿到检查报告重新去分诊等待较好。这一步就是这个线程要重新去获取锁。 综合一下，就可以得出一个完整的等待 - 通知机制：线程首先获取互斥锁，当线程要求的条件不满足时，释放互斥锁，进入等待状态；当要求的条件 满足时，通知等待的线程，重新获取互斥锁。 用synchronized来实现等待通知 这个等待队列和互斥锁是一对一的关系，每个互斥锁都有自己独立的等待队列。 因为notify() 只能保证在通知时间点，条件是满足的。而被通知线程的执行时间点和通知的时间点基本上不会重合，所以当线程执行的时候，很可能条件已经不满足了（保不齐有其他线程插队）。 12345678910111213141516171819202122232425262728293031323334353637public class Allocator { private List&lt;Object&gt; als = new ArrayList&lt;&gt;(); // 一次性申请所有的资源 synchronized void apply(Object from , Object to){ // 这里用while可以避免虚假唤醒 while (als.contains(from) || als.contains(to)){ try { wait(); } catch (InterruptedException e) { e.printStackTrace(); } } als.add(from); als.add(to); } // 归还资源 synchronized void free(Object from , Object to){ als.remove(from); als.remove(to); /** * 注意这里尽量用notifyAll 唤醒线程而不是notify * 因为notify会随机的通知等待队列中的一个线程，这样的话会蕴含一些风险：可能导致某些线程永远不会被通知到。 * * 假设我们有资源 A、B、C、D，线程 1 申请到了 AB，线程 2 申请到了 CD，此时线程 3 申 * 请 AB，会进入等待队列（AB 分配给线程 1，线程 3 要求的条件不满足），线程 4 申请 * CD 也会进入等待队列。我们再假设之后线程 1 归还了资源 AB，如果使用 notify() 来通知 * 等待队列中的线程，有可能被通知的是线程 4，但线程 4 申请的是 CD，所以此时线程 4 还 * 是会继续等待，而真正该唤醒的线程 3 就再也没有机会被唤醒了。 */ notifyAll(); }}","link":"/2021/08/28/Java%E5%B9%B6%E5%8F%91%EF%BC%88%E4%B8%80%EF%BC%89%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%94%81/"},{"title":"Java并发（三）并发工具类","text":"并发工具类 Lock和Condition前面说过了管程的概念，Java关键字synchronized本身就是管程的体现之一。初次之外，JavaSDK并发包中还提供了Lock和Condition两个接口来实现管程，Lock来解决互斥问题，而Condition用来解决同步问题。 为什么不直接用synchronized呢在预防死锁时，有一个方案叫”破坏不可抢占的条件“。而synchronized是没办法实现的，因为synchronized去申请资源时，如果没有申请到，线程就直接阻塞了，啥也干不了，也释放不了已经占有的资源了。 对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。 如何解决synchronized的这个弊端呢？ 响应中断：synchronized的问题就是一旦发生死锁，就没有机会去唤醒阻塞的线程了。但如果阻塞的线程可以响应中断信号，这样就有机会释放它自己占用的资源了，这样就可以避免死锁。 支持超时：如果线程在一段时间内没有获取到锁，不是进入到阻塞状态，而是返回一个错误。这样也可以避免死锁。 非阻塞的获取锁：如果线程获取锁失败，并不是进入阻塞状态，而是直接返回。 以上的三种方法对应的就是Lock接口下的三个方法： 12345678// 支持中断的 APIvoid lockInterruptibly() throws InterruptedException;// 支持超时的 APIboolean tryLock(long time, TimeUnit unit) throws InterruptedException;// 支持非阻塞获取锁的 APIboolean tryLock(); 如何保证可见性的原理简单说就是利用了volatile相关的happens-before规则。比如说ReentrantLock类中有一个抽象类Sync继承了AbstractQueuedSynchronizer，内部持有一个volatile的成员变量state，每次获取锁和解锁的时候都回去读写state中的值。 也就是说在执行lock 和unlock中间的业务代码之前和之后都会读写volatile变量state。根据happens-before的相关规则： 顺序性规则：线程T1，执行业务代码 happens-before 释放锁的操作 unlock。 volatile变量规则：由于state的读取，线程T1的unlock操作 happens-before 线程T2的lock操作。 传递性规则：线程T1的业务代码 happens-before 线程T2的lock操作。 这样的话，后续线程T2可以看到业务代码执行之后的正确结果。 什么是可重入锁ReentrantLock的意思就是可重入的锁。顾名思义，指的就是线程可以重复的获取同一把锁。例如下面的代码： 12345678910111213141516171819202122232425262728/** * @author shengbinbin * @description: 可重入锁测试 */public class ReentrantTest { private final Lock lock = new ReentrantLock(); int value; public int getValue(){ lock.lock(); try { return value; }finally { lock.unlock(); } } public void addOne(){ lock.lock(); try { value = 1 + getValue(); // 线程执行到这里时调用getValue方法时会再次加锁 }finally { lock.unlock(); // 保证锁可释放 } }} 公平锁和非公平锁实现原理：锁都对应着一个等待队列，如果一个线程没有获得锁就会进入等待队列。当有线程释放锁的时候，就需要从等待队列中唤醒一个等待的线程。这时如果是公平锁，唤醒的策略就是谁等的时间长就唤醒谁。而非公平就不能保证这个。 一些实践建议出自 《Java 并发编程：设计原则与模式》 永远只在更新对象的成员变量时加锁 永远只在访问可变的成员变量时加锁 永远不在调用其他对象的方法时加锁 Condition支持了多个条件变量在很多并发场景下，支持多个条件变量能够让我们的并发程序可读性更好，实现起来也更容易。例如，实现一个阻塞队列，就需要两个条件变量。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * @author shengbinbin * @description: 实现一个阻塞队列 */public class BlockedQueue&lt;T&gt; { final Lock lock = new ReentrantLock(); // 条件变量：队列不满/不空 final Condition notFull = lock.newCondition(); final Condition notEmpty = lock.newCondition(); // 入队 void enq(T x){ lock.lock(); try { while (队列已满){ notFull.await(); } //省略入队操作. // 入队后, 通知可出队 notEmpty.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } // 出队 void deq(){ lock.lock(); try { while (队列已空){ // 等待队列不空 notEmpty.await(); } // 省略出队操作... // 出队后，通知可入队 notFull.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } }} 同步和异步同步和异步的区别到底是什么呢？ 通俗点来讲就是调用方是否需要等待结果，如果需要等待结果，就是同步；如果不需要等待结果，就是异步。 如果想让你的程序支持异步，有两种方法： 调用方创建一个子线程，在子线程中执行方法调用，这种方法称为异步调用。 方法实现的时候，创建一个新的线程来执行主要逻辑，主线程直接return。这种方法称为异步方法。 在 TCP 协议层面，发送完 RPC 请求后，线程是不会等待 RPC 的响应结果的。但是我们平时工作中的RPC调用大多数是同步的 这说明一定有人做了异步转同步的事情，本来发送请求是异步的，但是调用线程却阻塞了，说明Dubbo 帮我们做了异步转同步的事情。通过调用栈，你能看到线程是阻塞在DefaultFuture.get() 方法上，所以可以推断：Dubbo 异步转同步的功能应该是通过DefaultFuture 这个类实现的。 需求是：当 RPC 返回结果之前，阻塞调用线程，让调用线程等待；当 RPC 返回结果后，唤醒调用线程，让调用线程重新执行 这不就是一个经典的等待通知机制吗？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import javax.xml.ws.Response;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * @author shengbinbin * @description: TODO * @date 2021/9/510:07 下午 */public class AsyncToSyn { private final Lock lock = new ReentrantLock(); private final Condition done = lock.newCondition(); // 调用方通过该方法等待结果 Object get(int timeout) throws TimeoutException { long start = System.nanoTime(); lock.lock(); try { while (!isDone()){ done.await(timeout , TimeUnit.SECONDS); long cur = System.nanoTime(); if (isDone() || cur - start &gt; timeout){ break; } } } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } if (!isDone()){ throw new TimeoutException(); } return response; } //判断结果是否返回 private boolean isDone() { return response != null; } // RPC 结果返回时调用这个方法 private void doReceived(Response response){ lock.unlock(); try { response = res; if (done != null){ done.signal(); } }finally { lock.unlock(); } }} 调用线程通过调用 get() 方法等待 RPC 返回结果，这个方法里面，你看到的都是熟悉的“面孔”：调用 lock() 获取锁，在 finally 里面调用 unlock() 释放锁；获取锁后，通过经典的在循环中调用 await() 方法来实现等待。 当 RPC 结果返回时，会调用 doReceived() 方法，这个方法里面，调用 lock() 获取锁，在finally 里面调用 unlock() 释放锁，获取锁后通过调用 signal() 来通知调用线程，结果已经返回，不用继续等待了。 Semaphore信号量用来控制同时访问某个特定资源的操作数量，或者同时执行某个指定操作的数量。信号量还可以用来实现某种资源池，或者对容器施加边界。 Semaphore管理着一组许可（permit）,许可的初始数量可以通过构造函数设定，操作时首先要获取到许可，才能进行操作，操作完成后需要释放许可。如果没有获取许可，则阻塞到有许可被释放。如果初始化了一个许可为1的Semaphore，那么就相当于一个不可重入的互斥锁（Mutex）。 核心方法： Semaphore(int premits,boolean fair)： 构造器方法。permits为信号量初始化数量，第二个参数fair可以设置是否需要公平策略，如果传入true，那么Semaphore会把等待的线程放入FIFO队列中，以便许可证被释放后，可以分配给等待时间最长的线程； acquire()： 试图获取许可证，如果当前没有可用的，就会进入阻塞等待状态； tryAcquire()： 试图获取许可证，如果是否能够获取，都不会进入阻塞。 tryAcquire(long timeout, TimeUnit unit)： 和tryAcquire一样，只是多了一个超时时间，等待指定时间还获取不到许可证，就会停止等待； availablePermits: 获取可用许可证数量； release()： 释放一个许可证； release(int permits)： 释放指定数量的许可证。 使用案例： 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.util.concurrent.Executor;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Semaphore;/** * @author shengbinbin * @description: 保证同一时间内只有3个线程可以拿到许可证执行业务代码 * @date 2021/9/510:28 下午 */public class SemaphoreDemo { static Semaphore semaphore = new Semaphore(3,true); public static void main(String[] args) { ExecutorService service = Executors.newFixedThreadPool(5); for (int i = 0; i &lt; 100; i++) { service.submit(new Runnable() { @Override public void run() { try { semaphore.acquire(1); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot;：获取到许可证&quot;); try { // 执行业务代码 Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot;：释放许可证&quot;); semaphore.release(); } }); } service.shutdown(); }} 注意事项： 获取和释放的许可证数量必须一致，否则随着时间的推移，最后许可证数量不够用，会导致线程卡死。 Semaphore设置是否公平性时，一般设置为true比较合理，因为Semaphore使用场景就是用在耗时较长的操作，如果被反复插队，线程就会持续陷入等待。 获取和释放的许可证不要求为同一个线程，只要满足我们业务需要，可以由A线程获取许可证，让B线程来释放。 ReadWriteLock读写锁： 允许多个线程同时读共享变量 只允许一个线程写共享变量 如果一个线程正在写共享变量，此时会禁止别的线程读取共享变量 StampedLockCountDownLatchCyclicBarrier","link":"/2021/08/24/Java%E5%B9%B6%E5%8F%91%EF%BC%88%E4%B8%89%EF%BC%89%E5%B9%B6%E5%8F%91%E5%B7%A5%E5%85%B7%E7%B1%BB/"},{"title":"Java并发（二）管程和线程","text":"什么是管程 线程 线程数的设置 管程什么是管程Java 采用的是管程技术，synchronized 关键字及 wait()、notify()、notifyAll() 这三个方法都是管程的组成部分。而管程和信号量是等价的，所谓等价指的是用管程能够实现信号量，也能用信号量实现管程。但是管程更容易使用，所以 Java 选择了管程。 管程，对应的英文是 Monitor，指的是管理共享变量以及对共享变量的操作过程，让他们支持并发。翻译为Java 领域的语言，就是管理类的成员变量和成员方法，让这个类是线程安全的。 在并发编程领域，有两大核心问题： 一个是互斥，即同一时刻只允许一个线程访问共享资源； 另一个是同步，即线程之间如何通信、协作。这两大问题，管程都是能够解决的。 如何解决互斥管程解决互斥问题的思路很简单，就是将共享变量及其对共享变量的操作统一封装起来。 比如：管程 X 将共享变量 queue 这个队列和相关的操作入队 enq()、出队 deq() 都封装起来了；线程 A 和线程 B 如果想访问共享变量 queue，只能通过调用管程提供的 enq()、deq() 方法来实现；enq()、deq() 保证互斥性，只允许一个线程进入管程。管程的模型和面向对象的思想高度契合。 如何解决同步在管程模型里，共享变量和对共享变量的操作是被封装起来的，图中最外层的框就代表封装的意思。框的上面只有一个入口，并且在入口旁边还有一个入口等待队列。当多个线程同时试图进入管程内部时，只允许一个线程进入，其他线程则在入口等待队列中等待。这个过程类似就医流程的分诊，只允许一个患者就诊，其他患者都在门口等待。 管程里还引入了条件变量的概念，而且每个条件变量都对应有一个等待队列，如下图，条件变量 A 和条件变量 B 分别都有自己的等待队列。 假设有个线程 T1 执行出队操作，不过需要注意的是执行出队操作，有个前提条件，就是队列不能是空的，而队列不空这个前提条件就是管程里的条件变量。 如果线程 T1 进入管程后恰好发现队列是空的，那怎么办呢？等待啊，去哪里等呢？就去条件变量对应的等待队列里面等。此时线程 T1 就去“队列不空”这个条件变量的等待队列中等待。这个过程类似于大夫发现你要去验个血，于是给你开了个验血的单子，你呢就去验血的队伍里排队。线程T1 进入条件变量的等待队列后，是允许其他线程进入管程的。这和你去验血的时候，医生可以给其他患者诊治，道理都是一样的。 再假设之后另外一个线程 T2 执行入队操作，入队操作执行成功之后，“队列不空”这个条件对于线程 T1 来说已经满足了，此时线程 T2 要通知 T1，告诉它需要的条件已经满足了。当线程 T1 得到通知后，会从等待队列里面出来，但是出来之后不是马上执行，而是重新进入到入口等待队列里面。这个过程类似你验血完，回来找大夫，需要重新分诊。 Java 参考了 MESA 模型，语言内置的管程（synchronized）对 MESA 模型进行了精简。MESA 模型中，条件变量可以有多个，Java 语言内置的管程里只有一个条件变量。 线程Java 语言里的线程本质上就是操作系统的线程，它们是一一对应的。 通用的线程生命周期 初始状态，指的是线程已经被创建，但是还不允许分配 CPU 执行。这个状态属于编程语言特有的，不过这里所谓的被创建，仅仅是在编程语言层面被创建，而在操作系统层面，真正的线程还没有创建。 可运行状态，指的是线程可以分配 CPU 执行。在这种状态下，真正的操作系统线程已经被成功创建了，所以可以分配 CPU 执行。 当有空闲的 CPU 时，操作系统会将其分配给一个处于可运行状态的线程，被分配到CPU 的线程的状态就转换成了运行状态。 运行状态的线程如果调用一个阻塞的 API（例如以阻塞方式读文件）或者等待某个事件（例如条件变量），那么线程的状态就会转换到休眠状态，同时释放 CPU 使用权，休眠状态的线程永远没有机会获得 CPU 使用权。当等待的事件出现了，线程就会从休眠状态转换到可运行状态。 线程执行完或者出现异常就会进入终止状态，终止状态的线程不会切换到其他任何状态，进入终止状态也就意味着线程的生命周期结束了。 Java 语言里则把可运行状态和运行状态合并了，这两个状态在操作系统调度层面有用，而 JVM 层面不关心这两个状态，因为 JVM 把线程调度交给操作系统处理了。 Java中线程的生命周期六种状态： NEW（初始化状态） RUNNABLE（可运行 / 运行状态） BLOCKED（阻塞状态） WAITING（无时限等待） TIMED_WAITING（有时限等待） TERMINATED（终止状态） 状态转换： RUNNABLE 与 BLOCKED 的状态转换：只有一种场景，就是就是线程等待 synchronized 的隐式锁。synchronized 修饰的方法、代码块同一时刻只允许一个线程执行，其他线程只能等待，这种情况下，等待的线程就会从 RUNNABLE 转换到 BLOCKED 状态。而当等待的线程获得 synchronized 隐式锁时，就又会从 BLOCKED 转换到 RUNNABLE 状态。 如果你熟悉操作系统线程的生命周期的话，可能会有个疑问：线程调用阻塞式 API 时，是否会转换到 BLOCKED 状态呢？在操作系统层面，线程是会转换到休眠状态的，但是在JVM 层面，Java 线程的状态不会发生变化，也就是说 Java 线程的状态会依然保持RUNNABLE 状态。JVM 层面并不关心操作系统调度相关的状态，因为在 JVM 看来，等待CPU 使用权（操作系统层面此时处于可执行状态）与等待 I/O（操作系统层面此时处于休眠状态）没有区别，都是在等待某个资源，所以都归入了 RUNNABLE 状态。 RUNNABLE 与 WAITING 的状态转换: 第一种场景，获得 synchronized 隐式锁的线程，调用无参数的 Object.wait() 方法。 第二种场景，调用无参数的 Thread.join() 方法。其中的 join() 是一种线程同步方法，例如有一个线程对象 thread A，当调用 A.join() 的时候，执行这条语句的线程会等待 thread A执行完，而等待中的这个线程，其状态会从 RUNNABLE 转换到 WAITING。当线程thread A 执行完，原来等待它的线程又会从 WAITING 状态转换到 RUNNABLE。 第三种场景，调用 LockSupport.park() 方法。其中的 LockSupport 对象，也许你有点陌生，其实 Java 并发包中的锁，都是基于它实现的。调用 LockSupport.park() 方法，当前线程会阻塞，线程的状态会从 RUNNABLE 转换到 WAITING。调用LockSupport.unpark(Thread thread) 可唤醒目标线程，目标线程的状态又会从WAITING 状态转换到 RUNNABLE。 RUNNABLE 与 TIMED_WAITING 的状态转换: 调用带超时参数的 Thread.sleep(long millis) 方法； 获得 synchronized 隐式锁的线程，调用带超时参数的 Object.wait(long timeout) 方法； 调用带超时参数的 Thread.join(long millis) 方法； 调用带超时参数的 LockSupport.parkNanos(Object blocker, long deadline) 方法； 调用带超时参数的 LockSupport.parkUntil(long deadline) 方法。 TIMED_WAITING 和 WAITING 状态的区别，仅仅是触发条件多了超时参数。 从 NEW 到 RUNNABLE 状态:只要调用线程对象的start() 方法就可以了 从 RUNNABLE 到 TERMINATED 状态:线程执行完 run() 方法后，会自动转换到 TERMINATED 状态，当然如果执行 run() 方法的时候异常抛出，也会导致线程终止。 stop() 和 interrupt() 方法的区别 stop() 方法会真的杀死线程，不给线程喘息的机会，如果线程持有 ReentrantLock 锁，被stop() 的线程并不会自动调用 ReentrantLock 的 unlock() 去释放锁，那其他线程就再也没机会获得 ReentrantLock 锁，这实在是太危险了。所以该方法就不建议使用了，类似的方法还有 suspend() 和 resume() 方法，这两个方法同样也都不建议使用了。 interrupt() 方法仅仅是通知线程，线程有机会执行一些后续操作，同时也可以无视这个通知。被 interrupt 的线程，是怎么收到通知的呢？一种是异常，另一种是主动检测。 当线程 A 处于 WAITING、TIMED_WAITING 状态时，如果其他线程调用线程 A 的interrupt() 方法，会使线程 A 返回到 RUNNABLE 状态，同时线程 A 的代码会触发InterruptedException 异常。上面我们提到转换到 WAITING、TIMED_WAITING 状态的触发条件，都是调用了类似 wait()、join()、sleep() 这样的方法，我们看这些方法的签名，发现都会 throws InterruptedException 这个异常。这个异常的触发条件就是：其他线程调用了该线程的 interrupt() 方法。 当线程 A 处于 RUNNABLE 状态时，并且阻塞在 java.nio.channels.InterruptibleChannel上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 会触发java.nio.channels.ClosedByInterruptException 这个异常；而阻塞在java.nio.channels.Selector 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A的 java.nio.channels.Selector 会立即返回。 上面这两种情况属于被中断的线程通过异常的方式获得了通知。 还有一种是主动检测，如果线程处于 RUNNABLE 状态，并且没有阻塞在某个 I/O 操作上，例如中断计算圆周率的线程 A，这时就得依赖线程 A 主动检测中断状态了。如果其他线程调用线程 A 的 interrupt()方法，那么线程 A 可以通过 isInterrupted() 方法，检测是不是自己被中断了。 线程数的设置多少才是合理的为什么要用多线程本质上是为了提升性能。那么如何衡量性能的好坏呢？主要有下面两个指标： 延迟：指的是发出请求到收到响应这个过程的时间 吞吐量：指的是在单位时间内能处理请求的数量；吞吐量越大，意味着程序能处理的请求越多，性能也就越好。 那么如何降低延迟，增加吞吐量呢？ 优化算法。 发挥硬件的性能：主要就是IO和CPU两个方面的利用率。 创建多少线程合适呢我们的程序一般都是CPU计算和IO操作交叉执行的。由于IO设备的速度肯定要比CPU计算的速度慢的多，所以不部分情况下，IO操作执行的时间都要比CPU计算的时间长，这种场景我们成为IO密集型。相对应的如果大部分是纯CPU计算的场景我们称为CPU密集型。 对于IO密集型的场景，本线程本质上就是提升多核CPU的效率，最佳的线程数是与程序中的IO操作和CPU计算的耗时比相关的：线程数 = 1 + （IO耗时 / CPU耗时）。这样的话当线程 A 执行 IO 操作时，另外 R 个线程正好执行完各自的 CPU 计算。这样 CPU 的利用率就达到了 100%。至于多核 CPU，也很简单，只需要等比扩大就可以了。 对于 CPU 密集型的计算场景，理论上“线程的数量 =CPU 核数”就是最合适的。不过在工程上，线程的数量一般会设置为“CPU 核数 +1”，这样的话，当线程因为偶尔的内存页失效或其他原因导致阻塞时，这个额外的线程可以顶上，从而保证CPU 的利用率。 为什么局部变量是安全的局部变量是不存在数据竞争的，但是至于原因嘛，就说不清楚了。那它背后的原因到底是怎样的呢？要弄清楚这个，你需要一点编译原理的知识。你知道在 CPU 层面，是没有方法概念的，CPU 的眼里，只有一条条的指令。编译程序，负责把高级 语言里的方法转换成一条条的指令。所以你可以站在编译器实现者的角度来思考“怎么完成 方法到指令的转换”。 方法是如何被执行的代码如下： 123int a = 7；int[] b = fibonacci(a);int[] c = b; 调用fibonacci方法时，CPU会先找到这个方法的地址，然后跳转到这个地址再去执行代码，最后执行完这个方法之后，要能够返回到话首先需要找到调用方法的下一个语句的地址，再跳转到这个地址继续去执行。 那么CPU是如何找到调用方法的参数和返回地址呢？ —- 通过CPU的堆栈寄存器 比如A调用B，B调用C。那么在运行的时候就会出现下面这样的调用栈，每个方法在调用栈中都有自己的独立空间，称为栈帧。而每个栈帧中都有对应方法需要的参数和返回地址。而局部变量就是存放在栈帧中，随着方法同生共死。 所以每个线程都有自己的调用栈，局部变量存放在线程各自的调用栈里，不会共享，也就不会有线程安全的问题了。","link":"/2021/08/24/Java%E5%B9%B6%E5%8F%91%EF%BC%88%E4%BA%8C%EF%BC%89%E7%AE%A1%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/"},{"title":"Java并发（六）线程池","text":"本节主要围绕下面几个问题来讲述： 为什么要使用线程池 如何正确的创建线程池 如何获取线程池的执行结果 正确的使用CompletableFuture 正确的使用CompletionService 为什么要使用线程池第一：降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 第二：提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 第三：提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。但是，要做到合理利用线程池，必须对其实现原理了如指掌。 如何正确的创建线程池线程池的结果获取FutureThreadPoolExecutor的execute方法并没有返回值，那我们如何获取任务的执行结果呢？ 123456789101112131415161718192021222324//java.util.concurrent.ThreadPoolExecutor#executepublic void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); //1. 如果正在运行的线程小于 corePoolSize ，那么就尝试启动一个新的线程使用给定的命令作为第一个任务 if (workerCountOf(c) &lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } //2. 如果任务可以成功的排队，我们仍然需要检测是否需要一个新的线程（可能自从上次检测后线程就死亡了或者线程池关闭了） if (isRunning(c) &amp;&amp; workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } //3. 如果不能成功的排队任务，我们需要添加一个新的线程。如果添加失败就拒绝这个任务。 else if (!addWorker(command, false)) reject(command); } ExecutorService提供了三个重载submit方法，返回值都是Future接口 123456789//这个方法的参数是一个 Runnable 接口，Runnable 接口的 run() 方法是没有返回值的，所以 `submit(Runnable task)` 这个方法返回的 Future 仅可以用来断言任务已经结束了，类似于 Thread.join()。Future&lt;?&gt; submit(Runnable task);//这个方法的参数是一个 Callable 接口，它只有一个 call() 方法，并且这个方法是有返回值的，所以这个方法返回的 Future 对象可以通过调用其 get() 方法来获取任务的执行结果。&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task);//这个方法很有意思，假设这个方法返回的 Future 对象是 f，f.get() 的返回值就是传给 submit() 方法的参数 result&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); Future接口有5个方法： 123456789101112// 取消任务boolean cancel()boolean mayInterruptIfRunning);// 判断任务是否已取消 boolean isCancelled();// 判断任务是否已结束boolean isDone();// 获得任务执行结果get();// 获得任务执行结果，支持超时get(long timeout, TimeUnit unit); FutureTask工具类FutureTask 实现了 Runnable 和 Future 接口： 12345678910111213// 有两个构造函数public FutureTask(Callable&lt;V&gt; callable) { if (callable == null) throw new NullPointerException(); this.callable = callable; this.state = NEW; // ensure visibility of callable }public FutureTask(Runnable runnable, V result) { this.callable = Executors.callable(runnable, result); this.state = NEW; // ensure visibility of callable } 由于实现了 Runnable 接口，所以可以将 FutureTask 对象作为任务提交给 ThreadPoolExecutor 去执行，也可以直接被 Thread 执行； 又因为实现了 Future 接口，所以也能用来获得任务的执行结果。 123456789101112131415public static void main(String[] args) throws ExecutionException, InterruptedException { // 创建 FutureTask FutureTask futureTask = new FutureTask&lt;&gt;(() -&gt; 1+2); // 创建线程池 ExecutorService threadPool = newCachedThreadPool(); // 提交 FutureTask threadPool.submit(futureTask); //通过 FutureTask的get方法获取结果 System.out.println(futureTask.get()); //3 // 也可以直接将FutureTask 作为线程构造函数的参数 new Thread(futureTask).start(); System.out.println(futureTask.get()); } 利用 FutureTask 对象可以很容易获取子线程的执行结果！ 用多线程实现烧水泡茶 用两个线程 T1 和 T2 来完成烧水泡茶程序，T1 负责洗水壶、烧开水、泡茶这三道工序，T2 负责洗茶壶、洗茶杯、拿茶叶三道工序，其中 T1 在执行泡茶这道工序时需要等待 T2 完成拿茶叶的工序。对于 T1 的这个等待动作，你应该可以想出很多种办法，例如 Thread.join()、CountDownLatch，甚至阻塞队列都可以解决，不过今天我们用 Future 特性来实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;import java.util.concurrent.TimeUnit;/** * @author shengbinbin * @description: 使用FutureTask 来实现烧水泡茶 * @date 2021/8/2510:40 下午 */public class TestFuture { public static void main(String[] args) throws ExecutionException, InterruptedException { // 创建任务 T2 的 FutureTask FutureTask&lt;String&gt; ft2 = new FutureTask&lt;&gt;(new T2task()); // 创建任务 T1 的 FutureTask FutureTask&lt;String&gt; ft1 = new FutureTask&lt;&gt;(new T1task(ft2)); // 线程 T1 执行任务 ft1 Thread T1 = new Thread(ft1); T1.start(); // 线程 T2 执行任务 ft2 Thread T2 = new Thread(ft2); T2.start(); // 等待线程 T1 执行结果 System.out.println(ft1.get()); } // T1Task 需要执行的任务： // 洗水壶、烧开水、泡茶 static class T1task implements Callable&lt;String&gt;{ FutureTask&lt;String&gt; ft2; // T1 任务需要 T2 任务的 FutureTask T1task(FutureTask&lt;String&gt; ft2){ this.ft2 = ft2; } @Override public String call() throws Exception { System.out.println(&quot;T1: 洗水壶..&quot;); TimeUnit.SECONDS.sleep(1); // 需要1分钟 System.out.println(&quot;T1: 烧开水...&quot;); TimeUnit.SECONDS.sleep(15); // 获取 T2 线程的茶叶,需要有线程的交互 String tf = ft2.get(); System.out.println(&quot;T1: 拿到茶叶:&quot;+tf); System.out.println(&quot;T1: 泡茶...&quot;); return &quot; 上茶:&quot; + tf; } } // T2Task 需要执行的任务: // 洗茶壶、洗茶杯、拿茶叶 static class T2task implements Callable&lt;String&gt;{ @Override public String call() throws Exception { System.out.println(&quot;T2: 洗茶壶...&quot;); TimeUnit.SECONDS.sleep(1); System.out.println(&quot;T2: 洗茶杯...&quot;); TimeUnit.SECONDS.sleep(2); System.out.println(&quot;T2: 拿茶叶...&quot;); TimeUnit.SECONDS.sleep(1); return &quot; 龙井 &quot;; } }} 利用 Java 并发包提供的 Future 可以很容易获得异步任务的执行结果，无论异步任务是通过线程池 ThreadPoolExecutor 执行的，还是通过手工创建子线程来执行的。Future 可以类比为现实世界里的提货单，比如去蛋糕店订生日蛋糕，蛋糕店都是先给你一张提货单，你拿到提货单之后，没有必要一直在店里等着，可以先去干点其他事，比如看场电影；等看完电影后，基本上蛋糕也做好了，然后你就可以凭提货单领蛋糕了。 利用多线程可以快速将一些串行的任务并行化，从而提高性能；如果任务之间有依赖关系，比如当前任务依赖前一个任务的执行结果，这种问题基本上都可以用 Future 来解决。在分析这种问题的过程中，建议你用有向图描述一下任务之间的依赖关系，同时将线程的分工也做好，类似于烧水泡茶最优分工方案那幅图。对照图来写代码，好处是更形象，且不易出错。 CompletableFuture用多线程优化性能，其实不过就是将串行操作变成并行操作: 12345678910// 以下两个方法都是耗时操作doBizA();doBizB();//异步化：主线程无需等待 doBizA() 和 doBizB() 的执行结果，也就是说 doBizA() 和 doBizB() 两个操作已经被异步化了。new Thread(()-&gt;doBizA()) .start();new Thread(()-&gt;doBizB()) .start(); 用CompletableFuture实现烧水泡茶123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.concurrent.CompletableFuture;import java.util.concurrent.TimeUnit;/** * @author shengbinbin * @description: CompletableFuture 使用 * @date 2021/8/2510:52 下午 */public class TestCompletableFuture { public static void main(String[] args) { // 任务 1：洗水壶 -&gt; 烧开水 CompletableFuture&lt;Void&gt; f1 = CompletableFuture.runAsync(()-&gt;{ System.out.println(&quot;T1: 洗水壶...&quot;); sleep(1, TimeUnit.SECONDS); System.out.println(&quot;T1: 烧开水...&quot;); sleep(15, TimeUnit.SECONDS); }); // 任务 2：洗茶壶 -&gt; 洗茶杯 -&gt; 拿茶叶 CompletableFuture&lt;String&gt; f2 = CompletableFuture.supplyAsync(()-&gt;{ System.out.println(&quot;T2: 洗茶壶...&quot;); sleep(1, TimeUnit.SECONDS); System.out.println(&quot;T2: 洗茶杯...&quot;); sleep(2, TimeUnit.SECONDS); System.out.println(&quot;T2: 拿茶叶...&quot;); sleep(1, TimeUnit.SECONDS); return &quot; 龙井 &quot;; }); // 任务 3：任务 1 和任务 2 完成后执行：泡茶 CompletableFuture&lt;String&gt; f3 = f1.thenCombine(f2, (__, tf)-&gt;{ System.out.println(&quot;T1: 拿到茶叶:&quot; + tf); System.out.println(&quot;T1: 泡茶...&quot;); return &quot; 上茶:&quot; + tf; }); // 等待任务 3 执行结果 System.out.println(f3.join()); } static void sleep(int t, TimeUnit u) { try { u.sleep(t); }catch(InterruptedException e){} }} 创建 CompletableFuture 对象主要靠下面4个静态方法： 1234567// 使用默认线程池static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable) //Runnable 接口的 run() 方法没有返回值static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier) //Supplier 接口的 get() 方法是有返回值的。// 可以指定线程池 static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable, Executor executor)static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier, Executor executor) 默认情况下 CompletableFuture 会使用公共的 ForkJoinPool 线程池，这个线程池默认创建的线程数是 CPU 的核数（也可以通过 JVM option:-Djava.util.concurrent.ForkJoinPool.common.parallelism 来设置 ForkJoinPool 线程池的线程数）。如果所有 CompletableFuture 共享一个线程池，那么一旦有任务执行一些很慢的 I/O 操作，就会导致线程池中所有线程都阻塞在 I/O 操作上，从而造成线程饥饿，进而影响整个系统的性能。所以，强烈建议你要根据不同的业务类型创建不同的线程池，以避免互相干扰。 创建完 CompletableFuture 对象之后，会自动地异步执行 runnable.run() 方法或者 supplier.get() 方法。 对于一个异步操作，你需要关注两个问题： 一个是异步操作什么时候结束， 另一个是如何获取异步操作的执行结果。 因为 CompletableFuture 类实现了 Future 接口，所以这两个问题你都可以通过 Future 接口来解决。另外，CompletableFuture 类还实现了 CompletionStage 接口， CompletionStage 接口任务是有时序关系的，比如有串行关系、并行关系、汇聚关系等。这样说可能有点抽象，这里还举前面烧水泡茶的例子，其中洗水壶和烧开水就是串行关系，洗水壶、烧开水和洗茶壶、洗茶杯这两组任务之间就是并行关系，而烧开水、拿茶叶和泡茶就是汇聚关系。 例如前面提到的 f3 = f1.thenCombine(f2, ()-&gt;{}) 描述的就是一种汇聚关系。烧水泡茶程序中的汇聚关系是一种 AND 聚合关系，这里的 AND 指的是所有依赖的任务（烧开水和拿茶叶）都完成后才开始执行当前任务（泡茶）。既然有 AND 聚合关系，那就一定还有 OR 聚合关系，所谓 OR 指的是依赖的任务只要有一个完成就可以执行当前任务。 描述串行关系主要是 thenApply、thenAccept、thenRun 和 thenCompose 这四个系列的接口。 1234567public &lt;U&gt; CompletionStage&lt;U&gt; thenApply(Function&lt;? super T,? extends U&gt; fn);public CompletionStage&lt;Void&gt; thenAccept(Consumer&lt;? super T&gt; action);public CompletionStage&lt;Void&gt; thenRun(Runnable action);public &lt;U&gt; CompletionStage&lt;U&gt; thenCompose(Function&lt;? super T, ? extends CompletionStage&lt;U&gt;&gt; fn); thenApply 系列函数里参数 fn 的类型是接口 Function&lt;T, R&gt;，这个接口里与 CompletionStage 相关的方法是 R apply(T t)，这个方法既能接收参数也支持返回值，所以 thenApply 系列方法返回的是CompletionStage&lt;R&gt;。 而 thenAccept 系列方法里参数 consumer 的类型是接口Consumer&lt;T&gt;，这个接口里与 CompletionStage 相关的方法是 void accept(T t)，这个方法虽然支持参数，但却不支持回值，所以 thenAccept 系列方法返回的是CompletionStage&lt;Void&gt;。 thenRun 系列方法里 action 的参数是 Runnable，所以 action 既不能接收参数也不支持返回值，所以 thenRun 系列方法返回的也是CompletionStage&lt;Void&gt;。 这些方法里面 Async 代表的是异步执行 fn、consumer 或者 action。其中，需要你注意的是 thenCompose 系列方法，这个系列的方法会新创建出一个子流程，最终结果和 thenApply 系列是相同的。 12345678CompletionStage&lt;R&gt; thenApply(fn);CompletionStage&lt;R&gt; thenApplyAsync(fn);CompletionStage&lt;Void&gt; thenAccept(consumer);CompletionStage&lt;Void&gt; thenAcceptAsync(consumer);CompletionStage&lt;Void&gt; thenRun(action);CompletionStage&lt;Void&gt; thenRunAsync(action);CompletionStage&lt;R&gt; thenCompose(fn);CompletionStage&lt;R&gt; thenComposeAsync(fn); thenApply的使用： 12345678public static void main(String[] args) throws ExecutionException, InterruptedException { CompletableFuture&lt;String&gt; completableFuture = CompletableFuture .supplyAsync(() -&gt; &quot;Hello,World&quot;) // 1 是一个异步流程 .thenApply(s -&gt; s + &quot;QQ&quot;) // 2 .thenApply(String :: toUpperCase); //3 和 2是一个串行的 // join方法的作用是使主线程等待 completableFuture 的任务执行完 System.out.println(completableFuture.join()); } 描述 AND 汇聚关系主要是 thenCombine、thenAcceptBoth 和 runAfterBoth 系列的接口，这些接口的区别也是源自 fn、consumer、action 这三个核心参数不同。它们的使用你可以参考上面烧水泡茶的实现程序，这里就不赘述了 123456CompletionStage&lt;R&gt; thenCombine(other, fn);CompletionStage&lt;R&gt; thenCombineAsync(other, fn);CompletionStage&lt;Void&gt; thenAcceptBoth(other, consumer);CompletionStage&lt;Void&gt; thenAcceptBothAsync(other, consumer);CompletionStage&lt;Void&gt; runAfterBoth(other, action);CompletionStage&lt;Void&gt; runAfterBothAsync(other, action); 描述 OR 汇聚关系主要是 applyToEither、acceptEither 和 runAfterEither 系列的接口，这些接口的区别也是源自 fn、consumer、action 这三个核心参数不同。 123456CompletionStage applyToEither(other, fn);CompletionStage applyToEitherAsync(other, fn);CompletionStage acceptEither(other, consumer);CompletionStage acceptEitherAsync(other, consumer);CompletionStage runAfterEither(other, action);CompletionStage runAfterEitherAsync(other, action); 使用： 123456789101112131415161718CompletableFuture&lt;String&gt; f1 = CompletableFuture.supplyAsync(()-&gt;{ int t = getRandom(5, 10); sleep(t, TimeUnit.SECONDS); return String.valueOf(t);}); CompletableFuture&lt;String&gt; f2 = CompletableFuture.supplyAsync(()-&gt;{ int t = getRandom(5, 10); sleep(t, TimeUnit.SECONDS); return String.valueOf(t);}); CompletableFuture&lt;String&gt; f3 = f1.applyToEither(f2,s -&gt; s); System.out.println(f3.join()); 异常处理虽然上面我们提到的 fn、consumer、action 它们的核心方法都不允许抛出可检查异常，但是却无法限制它们抛出运行时异常，例如下面的代码，执行 7/0 就会出现除零错误这个运行时异常。非异步编程里面，我们可以使用 try{}catch{}来捕获并处理异常，那在异步编程里面，异常该如何处理呢？ 12345CompletableFuture&lt;Integer&gt; f0 = CompletableFuture. .supplyAsync(()-&gt;(7/0)) .thenApply(r-&gt;r*10);System.out.println(f0.join()); CompletionStage 接口给我们提供的方案非常简单，比 try{}catch{}还要简单，下面是相关的方法，使用这些方法进行异常处理和串行操作是一样的，都支持链式编程方式。 12345CompletionStage exceptionally(fn);CompletionStage&lt;R&gt; whenComplete(consumer);CompletionStage&lt;R&gt; whenCompleteAsync(consumer);CompletionStage&lt;R&gt; handle(fn);CompletionStage&lt;R&gt; handleAsync(fn); 下面的示例代码展示了如何使用 exceptionally() 方法来处理异常，exceptionally() 的使用非常类似于 try{}catch{}中的 catch{}，但是由于支持链式编程方式，所以相对更简单。既然有 try{}catch{}，那就一定还有 try{}finally{}，whenComplete() 和 handle() 系列方法就类似于 try{}finally{}中的 finally{}，无论是否发生异常都会执行 whenComplete() 中的回调函数 consumer 和 handle() 中的回调函数 fn。whenComplete() 和 handle() 的区别在于 whenComplete() 不支持返回结果，而 handle() 是支持返回结果的。 12345678public static void main(String[] args) throws ExecutionException, InterruptedException { CompletableFuture&lt;Integer&gt; completableFuture = CompletableFuture .supplyAsync(() -&gt; 7/0) // 1 是一个异步流程 .thenApply(r -&gt; r * 10) // 2 .exceptionally(e -&gt; 0); // 如果发生运行时异常就这样 // join方法的作用是使主线程等待 completableFuture 的任务执行完 System.out.println(completableFuture.join()); }","link":"/2021/08/24/Java%E5%B9%B6%E5%8F%91%EF%BC%88%E5%85%AD%EF%BC%89%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"title":"Java并发编程（四）锁","text":"Java中的锁除了synchronize之外，在1.6之后还引入了Lock接口来实现锁 Lock AQS LockSupport Condition Lock接口的由来synchronized在1.6之后做了很多的优化，效率提高了很多，但是还有很多问题是synchronized无法解决的，因此Lock接口及其实现方法就出现了： 能够响应中断。synchronized 的问题是，持有锁 A 后，如果尝试获取锁 B 失败，那么线程就进入阻塞状态，一旦发生死锁，就没有任何机会来唤醒阻塞的线程。但如果阻塞状态的线程能够响应中断信号，也就是说当我们给阻塞的线程发送中断信号的时候，能够唤醒它，那它就有机会释放曾经持有的锁 A。这样就破坏了不可抢占条件了。 支持超时。如果线程在一段时间之内没有获取到锁，不是进入阻塞状态，而是返回一个错误，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。 非阻塞地获取锁。如果尝试获取锁失败，并不进入阻塞状态，而是直接返回，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。 API接口方法123456void lock(); //获取锁，调用该方法的线程会获得锁，获得锁之后从该方法返回void lockInterruptibly() throws InterruptedException; //可中断的获得锁boolean tryLock(); //尝试非阻塞的获取锁，调用该方法后立即返回，如果能获取返回true，否则返回falseboolean tryLock(long time, TimeUnit unit) throws InterruptedException; //超时的获取锁void unlock(); //释放锁Condition newCondition(); //获取等待通知组件，该组件和当前锁绑定，当前线程获得了锁之后才能调用组件的wait方法释放锁 Lock的一般使用注意一个lock方法必须要有对应的unlock，且unlock方法一般都放在finally代码块中避免中间的业务代码发生异常而没有释放锁的情况。 1234567Lock lock = new ReentrantLock(); lock.lock(); try { //业务逻辑 }finally { lock.unlock();//在finally块中释放锁，目的是保证在获取到锁之后，最终能够被释放。 } synchronized和ReentrantLock的区别 synchronized是JVM内建的同步机制，是一个关键字，ReentrantLock是一个类。 ReentrantLock可以实现公平锁，可以自定义条件，可以定义超时时间，需要显式的释放锁，而synchronized只能是非公平锁。 每一个lock操作，为了保证锁的释放，最好在finally中显式的unlock lock只适用于代码块，而synchronized可以用来修饰方法，代码块 在Java6之前，synchronized完全依靠操作系统的互斥锁来实现，需要进行用户态和内核态的切换，所以开销较大，但随着一系列的锁优化，synchronized的性能也越来越好了 队列同步器AQSAQS的全称是AbstractQueuedSynchronizer，它的定位是为Java中几乎所有的锁和同步器提供一个基础框架。 AQS是基于FIFO的队列实现的，并且内部维护了一个volatile修饰的状态变量state，通过原子更新这个状态变量state即可以实现加锁解锁操作。 AQS它的所有子类中，要么实现并使用了它的独占锁的api，要么使用了共享锁的功能，而不会同时使用两套api，即便是最有名的子类ReentrantReadWriteLock也是通过两个内部类读锁和写锁分别实现了两套api来实现的 AQS的源码解析 主要内部类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647static final class Node { //初始化两个节点引用 static final Node SHARED = new Node(); static final Node EXCLUSIVE = null; static final int CANCELLED = 1; // 标识该线程节点已释放（超时、中断），已取消的节点不会再阻塞。 static final int SIGNAL = -1; // 标识后继节点需要唤醒 static final int CONDITION = -2; // 标识线程等待在一个条件上 static final int PROPAGATE = -3; // 标识后面的共享锁需要无条件的传播（共享锁需要连续唤醒读的线程） volatile int waitStatus; // 当前节点保存的线程对应的等待状态 volatile Node prev; volatile Node next; volatile Thread thread; // 当前节点保存的线程 Node nextWaiter; final boolean isShared() { return nextWaiter == SHARED; } final Node predecessor() throws NullPointerException { Node p = prev; if (p == null) throw new NullPointerException(); else return p; } Node() { // Used to establish initial head or SHARED marker } Node(Thread thread, Node mode) { // Used by addWaiter this.nextWaiter = mode; this.thread = thread; } Node(Thread thread, int waitStatus) { // Used by Condition this.waitStatus = waitStatus; this.thread = thread; } } 主要属性 123456789101112131415161718 private transient volatile Node head; //维护一个头节点和尾节点的引用 private transient volatile Node tail; private volatile int state; //同步状态，用volatile修饰 //获取当前同步状态 protected final int getState() { return state; } //设置新的同步状态 protected final void setState(int newState) { state = newState;} //通过unsafe类的CAS修改同步状态 protected final boolean compareAndSetState(int expect, int update) { // See below for intrinsics setup to support this return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 子类需要实现的方法–模版方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899//提供给子类重写，独占式的获取同步状态，实现该方法需要查询当前状态并判断是否符合预期，然后用CAS来设置同步状态 protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException(); } //提供给子类重写，独占式的释放同步状态，等待的线程将有机会获取同步状态 protected boolean tryRelease(int arg) { throw new UnsupportedOperationException(); } //共享式的获取同步状态，返回值大于0表示成功 protected int tryAcquireShared(int arg) { throw new UnsupportedOperationException(); } //共享式的释放同步状态 protected boolean tryReleaseShared(int arg) { throw new UnsupportedOperationException(); } //当前同步器是否在独占模式下被线程占用 protected boolean isHeldExclusively() { throw new UnsupportedOperationException(); } //独占式获取同步状态，获取成功则返回，否则进入同步队列等待 public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } //和上面这个方法相同，但是响应中断 public final void acquireInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); if (!tryAcquire(arg)) doAcquireInterruptibly(arg); } //在上面的方法中增加了时间限制 public final boolean tryAcquireNanos(int arg, long nanosTimeout) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); return tryAcquire(arg) || doAcquireNanos(arg, nanosTimeout); } //独占式释放同步状态，释放后唤醒同步队列中的第一个节点 public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } //共享式的获取同步状态，主要区别是同一时间可以有多个线程获取到同步状态 public final void acquireShared(int arg) { if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg); } //和上面相同，响应中断 public final void acquireSharedInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg); } public final boolean tryAcquireSharedNanos(int arg, long nanosTimeout) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); return tryAcquireShared(arg) &gt;= 0 || doAcquireSharedNanos(arg, nanosTimeout); } //共享式的释放同步状态 public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } 节点加入等待队列流程队列结构如下图所示：同步队列，是一个双向链表。包括head节点和tail节点。head节点主要用作后续的调度。 只有前驱节点是head节点的节点才能被首先唤醒去进行同步状态的获取。当该节点获取到同步状态时，它会清除自己的值，将自己作为head节点，以便唤醒下一个节点。 同步器将节点加入到同步队列的过程：加入队列的过程必须要保证线程安全，因此同步器提供了一个基于CAS的设置尾节点的方法:**compareAndSetTail(Node expect,Node update)**，它需要传递当前线程“认为”的尾节点和当前节点，只有设置成功后，当前节点才正式 与之前的尾节点建立关联。 123private final boolean compareAndSetTail(Node expect, Node update) { return unsafe.compareAndSwapObject(this, tailOffset, expect, update);} 设置首节点的过程设置首节点的过程：同步队列遵循FIFO，首节点是获取同步状态成功的节点，首节点的线程在释放同步状态时，将会唤醒后继节点，而后继节点将会在获取同步状态成功时将自己设置为首节点。 设置首节点是通过获取同步状态成功的线程来完成的，由于只有一个线程能够成功获取到同步状态，因此设置头节点的方法并不需要使用CAS来保证，它只需要将首节 点设置成为原首节点的后继节点并断开原首节点的next引用即可。 acquire流程分析123456AQS ----&gt; acquire()public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } tryAcquire方法123456789101112131415161718192021222324252627282930313233343536373839404142tryAcquire 方法针对公平锁和非公平锁有着不同的实现，总的来说是保证线程安全的获取同步状态 //Fair version of tryAcquire. Don't grant access unless recursive call or no waiters or is first. protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (!hasQueuedPredecessors() &amp;&amp; //公平锁会先判断队列中是否有后继节点，如果没有再尝试修改共享变量 compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } //可重入锁的实现 else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { // 非公平锁直接就尝试用cas来修改共享变量了 setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } 如果tryAcquire不能获取锁： addWaiter12345678910111213141516171819202122232425262728293031323334353637383940//构造新的尾节点，通过CAS来放入队列尾部//Creates and enqueues node for current thread and given mode.private Node addWaiter(Node mode) { //将当前线程以指定的模式创建节点node Node node = new Node(Thread.currentThread(), mode); // 获取当前同队列的尾节点 Node pred = tail; // 如果存在尾节点，则将新的node加入等待队列中 if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { // 注意是通过CAS的方式来加入到尾节点的 pred.next = node; return node; } } //当队列为empty或者CAS失败时会调用enq方法处理 enq(node); return node; }//1. 当前队列为空//2. cas 加入到尾节点失败之后 死循环来重试，保证尾节点的插入成功private Node enq(final Node node) { for (;;) { ////通过“死循环”来保证节点的正确添加 Node t = tail; if (t == null) { //当前队列为empty，需要进行初始化。头结点中不放数据，只是作为起始标记，lazy-load， if (compareAndSetHead(new Node())) tail = head; } else { node.prev = t; //只有通过CAS将节点设置成为尾节点之后，当前线程才能从该方法返回，否则，当前线 程不断地尝试设置 if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } acquireQueued方法用于已在队列中的线程以独占且不间断模式获取state状态，直到获取锁后返回 123456789101112131415161718192021222324252627//acquireQueued --- 节点进入同步队列之后，就进入了一个自旋的过程，每个节点(或者说每个线程)都在自 省地观察，当条件满足，获取到了同步状态，就可以从这个自旋过程中退出，否则依旧留在这 个自旋过程中(并会阻塞节点的线程):final boolean acquireQueued(final Node node, int arg) { //是否已获取锁的标志，默认为true 即为尚未 boolean failed = true; try { boolean interrupted = false; for (;;) { //死循环自旋的过程 ////获取节点的前一个节点 final Node p = node.predecessor(); //如果前节点已经成为头结点，尝试获取锁（tryAcquire）成功，然后返回 if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return interrupted; } //shouldParkAfterFailedAcquire根据对当前节点的前一个节点的状态进行判断，对当前节点做出不同的操作 //parkAndCheckInterrupt让线程进入等待状态，并检查当前线程是否被可以被中断 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } acquire方法调用流程： 前驱节点为头节点且能够获取同步状态的判断条件和线程进入等待状态是获 取同步状态的自旋过程。当同步状态获取成功之后，当前线程从acquire(int arg)方法返回，如果 对于锁这种并发组件而言，代表着当前线程获取了锁。 当前线程获取同步状态并执行了相应逻辑之后，就需要释放同步状态，使得后续节点能 够继续获取同步状态。通过调用同步器的release(int arg)方法可以释放同步状态，该方法在释 放了同步状态之后，会唤醒其后继节点(进而使后继节点重新尝试获取同步状态)。 12345678910@ReservedStackAccess public final boolean release(int arg) { if (tryRelease(arg)) { //tryRelease针对公平锁和非公平锁也有不同的实现 Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } ReentrantLock-独占式 默认是非公平锁，也可以在构造的时候指定参数设为公平锁 1234567public ReentrantLock() { sync = new NonfairSync(); } public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } 非公平锁的实现： 123456789101112131415161718static final class NonfairSync extends Sync { private static final long serialVersionUID = 7316153563782823691L; /** * Performs lock. Try immediate barge, backing up to normal * acquire on failure. */ @ReservedStackAccess final void lock() { // 如果将 共享遍历 state 通过原子方法从0设置为1 ，说明当前线程获取到了资源，将当前线程设置为 独占资源的线程 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); // 如果获取资源没有成功，则 进入 acquire方法 } protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } } ​ 独占模式下的AQS是不响应中断的，指的是加入到同步队列中的线程，如果因为中断而被唤醒的话，不会立即返回，并且抛出InterruptedException。而是再次去判断其前驱节点是否为head节点，决定是否争抢同步状态。如果其前驱节点不是head节点或者争抢同步状态失败，那么再次挂起。 acquire 流程解析 调用自定义同步器的tryAcquire()尝试直接去获取资源，如果成功则直接返回； 没成功，则addWaiter()将该线程加入等待队列的尾部，并标记为独占模式； acquireQueued()使线程在等待队列中休息，有机会时（轮到自己，会被unpark()）会去尝试获取资源。获取到资源后才返回。如果在整个等待过程中被中断过，则返回true，否则返回false。 如果线程在等待过程中被中断过，它是不响应的。只是获取资源后才再进行自我中断selfInterrupt()，将中断补上。 CountDownLatch-共享式CountDownLatch，任务分为N个子线程去执行，同步状态state也初始化为N（注意N要与线程个数一致）： 共享模式下线程获取共享资源： 1234public final void acquireShared(int arg) { if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg); } 尝试获取资源 负值代表获取失败； 0代表获取成功，但没有剩余资源； 正数表示获取成功，还有剩余资源，其他线程还可以去获取。 123protected int tryAcquireShared(int acquires) { return (getState() == 0) ? 1 : -1;} doAcquireShared()用于将当前线程加入等待队列尾部休息，直到其他线程释放资源唤醒自己，自己成功拿到相应量的资源后才返回： 1234567891011121314151617181920212223242526272829303132private void doAcquireShared(int arg) { //以共享加入队列尾部 final Node node = addWaiter(Node.SHARED); //是否成功标志 boolean failed = true; try { //等待过程中是否被中断过的标志 boolean interrupted = false; for (;;) { final Node p = node.predecessor();//获取前驱节点 if (p == head) {//如果到head的下一个，因为head是拿到资源的线程，此时node被唤醒，很可能是head用完资源来唤醒自己的 int r = tryAcquireShared(arg);//尝试获取资源 if (r &gt;= 0) {//成功 setHeadAndPropagate(node, r);//将head指向自己，还有剩余资源可以再唤醒之后的线程 p.next = null; // help GC if (interrupted)//如果等待过程中被打断过，此时将中断补上。 selfInterrupt(); failed = false; return; } } //判断状态，队列寻找一个适合位置，进入waiting状态，等着被unpark()或interrupt() if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } 重入锁重进入是指任意线程在获取到锁之后能够再次获取该锁而不会被锁所阻塞，该特性的实现需要解决以下两个问题。 线程再次获取锁。锁需要去识别获取锁的线程是否为当前占据锁的线程，如果是，则再次成功获取。 锁的最终释放。线程重复n次获取了锁，随后在第n次释放该锁后，其他线程能够获取到该锁。锁的最终释放要求锁对于获取进行计数自增，计数表示当前锁被重复获取的次数，而锁被释放时，计数自减，当计数等于0时表示锁已经成功释放。 12345678910111213141516171819protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { //如果当前线程就是拥有锁的线程 int nextc = c + acquires; //则共享变量++ if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } 读写锁读写锁，并不是 Java 语言特有的，而是一个广为使用的通用技术，所有的读写锁都遵守以下三条基本原则： 允许多个线程同时读共享变量； 只允许一个线程写共享变量； 如果一个写线程正在执行写操作，此时禁止读线程读共享变量。 读写锁在同一时刻可以允许多个读线程访问，但是在写线程访问时，所有的读线程和其他写线程均被阻塞。读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。 LockSupport类LockSupport定义了一组以park开头的方法用来阻塞当前线程，以及unpark(Thread thread) 方法来唤醒一个被阻塞的线程： 1234567891011public static void park(Object blocker) { //阻塞当前线程 Thread t = Thread.currentThread(); setBlocker(t, blocker); UNSAFE.park(false, 0L); setBlocker(t, null); }public static void unpark(Thread thread) { //唤醒当前线程 if (thread != null) UNSAFE.unpark(thread); } Condition接口等待通知模式：任意一个Java对象，都拥有一组监视器方法（定义在java.lang.Object上），主要包括wait()、 wait(long timeout)、notify()以及notifyAll()方法，这些方法与synchronized同步关键字配合，可以实现等待/通知模式 Condition接口也提供了类似Object的监视器方法，与Lock配合可以实现等待/通知模式 新版生产者和消费者一般都会将Condition对象作为成员变量。当调用await()方法后，当前线程会释放锁并在此等待，而其他线程调用Condition对象的signal()方法，通知当前线程后，当前线程才从await()方法返回，并且在返回前已经获取了锁 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class SharedDate{ //共享资源类 private int num = 0; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void increment(){ lock.lock(); try { while (num != 0) condition.await(); //1. 判断释放满足条件，注意用while num++; //2. 业务逻辑 System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); condition.signal(); //3. 唤醒其他线程 } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } public void decrement(){ lock.lock(); try { while (num == 0) condition.await(); num--; System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); condition.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } }}/** 需求：两个线程操作一个初始值为0的变量，一个线程操作变量+1，另一个线程操作变量-1。操作10次后变量依旧为0 * */public class ConditionDemo { public static void main(String[] args) { // 线程操作资源类 SharedDate sharedDate = new SharedDate(); Thread a = new Thread(()-&gt;{ for (int i = 0; i &lt; 5; i++) { sharedDate.increment(); } },&quot;A&quot;); Thread b = new Thread(()-&gt;{ for (int i = 0; i &lt; 5; i++) { sharedDate.decrement(); } },&quot;B&quot;); a.start(); b.start(); } } 精确通知不同的等待者12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485class SharedDate{ //共享资源类 private int num = 1; //1 A ，2 B ， 3 C private Lock lock = new ReentrantLock(); private Condition c1 = lock.newCondition(); //多个条件实现精确通知 private Condition c2 = lock.newCondition(); private Condition c3 = lock.newCondition(); public void print2(int a){ lock.lock(); try { while (num != 1) c1.await(); for (int i = 0; i &lt; a; i++) { System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); } num = 2; //要修改状态位，以此来唤醒不同的线程 c2.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } public void print4(int a){ lock.lock(); try { while (num != 2) c2.await(); for (int i = 0; i &lt; a; i++) { System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); } num = 3;//要修改状态位，以此来唤醒不同的线程 c3.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } public void print8(int a){ lock.lock(); try { while (num != 3) c3.await(); for (int i = 0; i &lt; a; i++) { System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); } num = 1;//要修改状态位，以此来唤醒不同的线程 c1.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } }}/** 需求：三个线程依次打印1，2，3。其中A线程打印2次，B线程打印4次，C线程打印6次** */public class ConditionDemo { public static void main(String[] args) { // 线程操作资源类 SharedDate sharedDate = new SharedDate(); Thread a = new Thread(()-&gt;{ sharedDate.print2(2); },&quot;A&quot;); Thread b = new Thread(()-&gt;{ sharedDate.print4(4); },&quot;B&quot;); Thread c = new Thread(()-&gt;{ sharedDate.print8(8); },&quot;C&quot;); a.start(); b.start(); c.start(); }} 总结问题 为什么有了Synchronize还要引入Lock接口？ AQS是什么，如何实现的？ ReentrantLock 和 CountDownLatch 是如何基于AQS实现的 公平锁和非公平锁的实现？重入锁的实现？ 读写锁的实现？ Condition接口的作用？如何在AQS中实现作用的","link":"/2021/10/08/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%EF%BC%88%E5%9B%9B%EF%BC%89%E9%94%81/"},{"title":"Java并发（四）并发集合","text":"","link":"/2021/08/24/Java%E5%B9%B6%E5%8F%91%EF%BC%88%E5%9B%9B%EF%BC%89%E5%B9%B6%E5%8F%91%E9%9B%86%E5%90%88/"},{"title":"Java虚拟机（一）内存管理机制","text":"主要讲述： Java内存区域划分 垃圾收集 内存分配策略 对象详解 Java内存区域运行时数据区域虚拟机在执行Java程序的过程中会将他管理的内存区域划分为不同的数据区域，有的区域随着进程的启动而存在，有的区域则依赖用户线程的启动和结束而建立和销毁。 程序计数器程序计数器（Program Counter Register）也被称为PC寄存器，是一块较小的内存空间。 可以看作是当前线程所执行的字节码的行号指示器，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 线程私有的，在任意时刻，一条 Java 虚拟机线程只会执行一个方法的代码，这个正在被线程执行的方法称为该线程的当前方法。为了线程切换后能恢复到正确的执行位置，每个线程都有一个独立的程序计数器。 虚拟机栈Java虚拟机栈（Java Virtual Machine Stack）也是线程私有的，它的生命周期与线程相同。 每个方法被执行的时候，Java虚拟机都会同步创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态连接、方法出口等信息。每一个方法被调用直至执行完毕的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 局部变量表存放了编译期可知的各种Java虚拟机基本数据类型（boolean、byte、char、short、int、 float、long、double）、对象引用（reference类型，它并不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress 类型（指向了一条字节码指令的地址）。 这些数据类型在局部变量表中的存储空间以局部变量槽（Slot）来表示，其中64位长度的long和 double类型的数据会占用两个变量槽，其余的数据类型只占用一个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在栈帧中分配多大的局部变量空间是完全确定 的，在方法运行期间不会改变局部变量表的大小。 虚拟机栈会发生以下两种异常： 如果线程请求分配的栈容量超过 Java 虚拟机栈允许的最大容量时，Java 虚拟机将会抛出一个 StackOverflowError 异常。 如果 Java 虚拟机栈可以动态扩展，并且扩展的动作已经尝试过，但是目前无法申请到足够的内存去完成扩展，或者在建立新的线程时没有足够的内存去创建对应的虚拟机栈，那 Java 虚拟机将会抛出一个 OutOfMemoryError 异常。 本地方法栈本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别只是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的本地（Native）方法服务。 会发生的异常和虚拟机栈相同 Java堆Java堆（Java Heap）是虚拟机所管理的内存中最大的一块。堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，Java里“几乎”所有的对象实例都在这里分配内存。 Java堆是垃圾收集器管理的内存区域，因此一些资料中它也被称作“GC堆”（Garbage Collected Heap，）。从回收内存的角度看，由于现代垃圾收集器大部分都是基于分代收集理论设计的，所以Java堆中经常会出现“新生代”“老年代”“永久代”“Eden空间”“From Survivor空间”“To Survivor空间”等名词，需要注意的是这些区域划分仅仅是一部分垃圾收集器的共同特性或者说设计风格而已，而非某个Java虚拟机具体 实现的固有内存布局，更不是《Java虚拟机规范》里对Java堆的进一步细致划分。 如果从分配内存的角度看，所有线程共享的Java堆中可以划分出多个线程私有的分配缓冲区 （Thread Local Allocation Buffer，TLAB），以提升对象分配时的效率。不过无论从什么角度，无论如何划分，都不会改变Java堆中存储内容的共性，无论是哪个区域，存储的都只能是对象的实例，将Java 堆细分的目的只是为了更好地回收内存，或者更快地分配内存。 方法区方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。 这区域的内存回收目标主要是针对常量池的回收和对类型的卸载 运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池表（Constant Pool Table），用于存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量 一定只有编译期才能产生，也就是说，并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可以将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法 直接内存在JDK 1.4中新加入了NIO（New Input/Output）类，引入了一种基于通道（Channel）与缓冲区 （Buffer）的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆里面的 DirectByteBuffer对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了 在Java堆和Native堆中来回复制数据。 内存区域演变 JDK1.6时期和我们上面讲的JVM内存区域是一致的： JDK1.7时发生了一些变化，将字符串常量池、静态变量，存放在堆上 在JDK1.8时彻底干掉了方法区，而在直接内存中划出一块区域作为元空间，运行时常量池、类常量池都移动到元空间。 思考一下，为什么使用元空间替换永久代？ 表面上看是为了避免OOM异常。因为通常使用PermSize和MaxPermSize设置永久代的大小就决定了永久代的上限，但是不是总能知道应该设置为多大合适, 如果使用默认值很容易遇到OOM错误。 当使用元空间时，可以加载多少类的元数据就不再由MaxPermSize控制, 而由系统的实际可用空间来控制。 更深层的原因还是要合并HotSpot和JRockit的代码，JRockit从来没有所谓的永久代，也不需要开发运维人员设置永久代的大小，但是运行良好。同时也不用担心运行性能问题了,在覆盖到的测试中, 程序启动和运行速度降低不超过1%，但是这点性能损失换来了更大的安全保障。 哪些地方会发生OOM 堆内存不足是最常见的OOM原因之一，抛出的错误信息是“java.lang.OutOfMemoryError:Java heap space”，原因可能千奇百怪，例如，可能存在内存泄漏问题；也很有可能就是堆的大小不合理，比如我们要处理比较可观的数据量，但是没有显式指定JVM堆大小或者指定数值偏小；或者出现JVM处理引用不及时，导致堆积起来，内存无法释放等。 对于Java虚拟机栈和本地方法栈，这里要稍微复杂一点。如果我们写一段程序不断的进行递归调用，而且没有退出条件，就会导致不断地进行压栈。类似这种情况，JVM实际会抛出StackOverFlowError；当然，如果JVM试图去扩展栈空间的的时候失败，则会抛出OutOfMemoryError。 对于老版本的Oracle JDK，因为永久代的大小是有限的，并且JVM对永久代垃圾回收（如，常量池回收、卸载不再需要的类型）非常不积极，所以当我们不断添加新类型的时候，永久代出现OutOfMemoryError也非常多见，尤其是在运行时存在大量动态类型生成的场合；类似Intern字符串缓存占用太多空间，也会导致OOM问题。对应的异常信息，会标记出来和永久代相关：“java.lang.OutOfMemoryError: PermGen space”。随着元数据区的引入，方法区内存已经不再那么窘迫，所以相应的OOM有所改观，出现OOM，异常信息则变成了：“java.lang.OutOfMemoryError: Metaspace”。 直接内存不足，也会导致OOM。 对象的创建都是在堆上吗我注意到有一些观点，认为通过逃逸分析，JVM会在栈上分配那些不会逃逸的对象，这在理论上是可行的，但是取决于JVM设计者的选择。据我所知，Oracle Hotspot JVM中并未这么做，这一点在逃逸分析相关的文档里已经说明，所以可以明确所有的对象实例都是创建在堆上。 目前很多书籍还是基于JDK 7以前的版本，JDK已经发生了很大变化，Intern字符串的缓存和静态变量曾经都被分配在永久代上，而永久代已经被元数据区取代。但是，Intern字符串缓存和静态变量并不是被转移到元数据区，而是直接在堆上分配，所以这一点同样符合前面一点的结论：对象实例都是分配在堆上。 对象访问的两种方式在Java语言中，对象访问是如何进行的？即使是最简单的访问，也会涉及到Java堆、栈和方法区三个内存区域。 比如在方法体重出现： 1Object obj = new Object(); Object obj 这部分会放在栈的本地变量表中，作为一个reference类型数据出现。 new Object() 这部分语义会放到堆中，形成了一块存储了Object类型的所有实例类型值的结构化内存。 在方法区中会存放对象类型数据，比如对象类型、父类、实现的接口和方法等。 Java程序会通过栈上的reference数据来操作堆上的具体对象。由于reference类型在《Java虚拟机规范》里面只规定了它是一个指向对象的引用，并没有定义这个引用应该通过什么方式去定位、访问到堆中对象的具体位置，所以对象访问方式也是由虚拟机实现而定的，主流的访问方式主要有使用句柄和直接指针两种： 如果使用句柄访问的话，Java堆中将可能会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自具体的地址信息，其结构如图所示： 如果使用直接指针访问的话，Java堆中对象的内存布局就必须考虑如何放置访问类型数据的相关信息，reference中存储的直接就是对象地址，如果只是访问对象本身的话，就不需要多一次间接访问的开销，如图所示： 这两种对象访问方式各有优势，使用句柄来访问的最大好处就是reference中存储的是稳定句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而reference本身不需要被修改。 使用直接指针来访问最大的好处就是速度更快，它节省了一次指针定位的时间开销，由于对象访问在Java中非常频繁，因此这类开销积少成多也是一项极为可观的执行成本。 HotSpot虚拟机主要使用直接指针来进行对象访问。 垃圾收集如何判断对象已死 引用计数法：在对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加一；当引用失效时，计数器值就减一；任何时刻计数器为零的对象就是不可能再被使用的。 需要额外的空间来存储计数器，以及繁琐的更新操作，引用计数法还有一个重大的漏洞，那便是无法处理循环引用对象。 12345678910111213141516public static void main(String[] args) { Person father = new Person(); Person son = new Person(); father.setSon(son); son.setFather(father); father = null; son = null; /** * 调用此方法表示希望进行一次垃圾回收。但是它不能保证垃圾回收一定会进行， * 而且具体什么时候进行是取决于具体的虚拟机的，不同的虚拟机有不同的对策。 */ System.gc();} 可达性分析算法:通过一系列名为“GC Roots” 的对象作为终点，当一个对象到GC Roots 之间无法通过引用到达时，便可以进行回收了 在Java技术体系里面，固定可作为GC Roots的对象包括以下几种： 在虚拟机栈（栈帧中的本地变量表）中引用的对象 在方法区中类静态属性引用的对象，譬如Java类的引用类型静态变量。 在方法区中常量引用的对象，譬如字符串常量池（String Table）里的引用。 在本地方法栈中JNI（即通常所说的Native方法）引用的对象。 再谈引用为了描述这样一类对象：当内存空间还足够时，则能保留在内存之中。如果内存进行垃圾回收之后还是非常紧张，则可以抛弃这些对象。 因此对引用的概念做了扩充：分为强引用（Strongly Reference）、软引用（Soft Reference）、弱引用（Weak Reference）和虚引用（Phantom Reference）4种，这4种引用强度依次逐渐减弱。 强引用是最传统的“引用”的定义，是指在程序代码之中普遍存在的引用赋值，无论任何情况下，只要强引用关系还存在，垃圾收集器就永远不会回 收掉被引用的对象。 1Object obj =new Object(); 软引用是用来描述一些还有用，但非必须的对象。只被软引用关联着的对象，在系统将要发生内存溢出异常前，会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存， 才会抛出内存溢出异常。在JDK 1.2版之后提供了SoftReference类来实现软引用。 12345Object obj = new Object();ReferenceQueue queue = new ReferenceQueue();SoftReference reference = new SoftReference(obj, queue);//强引用对象滞空，保留软引用obj = null; 弱引用也是用来描述那些非必须对象，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生为止。当垃圾收集器开始工作，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。在JDK 1.2版之后提供了WeakReference类来实现弱引用。 12345Object obj = new Object();ReferenceQueue queue = new ReferenceQueue();WeakReference reference = new WeakReference(obj, queue);//强引用对象滞空，保留软引用obj = null; 虚引用也称为“幽灵引用”或者“幻影引用”，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的只是为了能在这个对象被收集器回收时收到一个系统通知。在JDK 1.2版之后提供了PhantomReference类来实现虚引用。 12345Object obj = new Object();ReferenceQueue queue = new ReferenceQueue();PhantomReference reference = new PhantomReference(obj, queue);//强引用对象滞空，保留软引用obj = null; 方法区的垃圾回收主要回收两种：废弃常量和废弃类 废弃常量：以常量池中的字面量回收为例，假如一个字符串”abc”已经进入了常量池中，而当前系统中并没有任何一个String对象引用常量池中的“abc”常量。如果这时候发送内存回收，这个字符串常量就会被请出去。 废弃类：需要满足下面的三种条件： 该类的所有实例都已经被回收了 加载该类的类加载器已经被回收了 该类对应的Class对象没有任何地方被引用 垃圾回收算法标记清除标记-清除（Mark-Sweep）算法分为两个阶段： 标记 : 标记出所有需要回收的对象 清除：回收所有被标记的对象 标记-清除算法比较基础，但是主要存在两个缺点： 执行效率不稳定，如果Java堆中包含大量对象，而且其中大部分是需要被回收的，这时必须进行大量标记和清除的动作，导致标记和清除两个过程的执行效率都随对象数量增长而降低。 内存空间的碎片化问题，标记、清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 标记-清除算法主要用于老年代，因为老年代可回收的对象比较少。 复制标记-复制算法解决了标记-清除算法面对大量可回收对象时执行效率低的问题。 过程也比较简单：将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 这种算法存在一个明显的缺点：一部分空间没有使用，存在空间的浪费。 新生代垃圾收集主要采用这种算法，因为新生代的存活对象比较少，每次复制的只是少量的存活对象。 一般虚拟机的具体实现不会采用1:1的比例划分，以HotSpot为例，HotSpot虚拟机将内存分为一块较大的Eden空间和两块较小的 Survivor空间，每次分配内存只使用Eden和其中一块Survivor。发生垃圾搜集时，将Eden和Survivor中仍然存活的对象一次性复制到另外一块Survivor空间上，然后直接清理掉Eden和已用过的那块Survivor空间。默认Eden和Survivor的大小比例是8∶1。 标记整理为了降低内存的消耗，引入一种针对性的算法：标记-整理（Mark-Compact）算法。 其中的标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存。 标记-整理算法主要用于老年代，在老年代这种大量对象存活的区域，移动对象是个很大的负担，而且这种对象移动操作必须全程暂停用户应用程序（Stop The World）才能进行。 垃圾处理器分类 Serial收集器它是一个单线程工作的收集器，使用一个处理器或一条收集线程去完成垃圾收集工作。并且进行垃圾收集时，必须暂停其他所有工作线程，直到垃圾收集结束——这就是所谓的“Stop The World”。 Serial/Serial Old收集器的运行过程如图： ParNew收集器ParNew收集器实质上是Serial收集器的多线程并行版本，使用多条线程进行垃圾收集。 ParNew收集器的工作过程如图所示： Parallel Scavenge收集器Parallel Scavenge收集器是一款新生代收集器，基于标记-复制算法实现，也能够并行收集。和ParNew有些类似，但Parallel Scavenge主要关注的是垃圾收集的吞吐量。 所谓吞吐量指的是运行用户代码的时间与处理器总消耗时间的比值。这个比例越高，证明垃圾收集占整个程序运行的比例越小。 Parallel Scavenge收集器提供了两个参数用于精确控制吞吐量: -XX：MaxGCPauseMillis，最大垃圾回收停顿时间。这个参数的原理是空间换时间，收集器会控制新生代的区域大小，从而尽可能保证回收少于这个最大停顿时间。简单的说就是回收的区域越小，那么耗费的时间也越小。 所以这个参数并不是设置得越小越好。设太小的话，新生代空间会太小，从而更频繁的触发GC。 -XX：GCTimeRatio，垃圾收集时间与总时间占比。这个是吞吐量的倒数，原理和MaxGCPauseMillis相同。 由于与吞吐量关系密切，Parallel Scavenge收集器也经常被称作“吞吐量优先收集器”。 Serial Old收集器Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法。 Parallel Old收集器Parallel Old是Parallel Scavenge收集器的老年代版本，支持多线程并发收集，基于标记-整理算法实现。 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器，同样是老年代的收集齐，采用标记-清除算法。 垃圾收集器： 初始标记（CMS initial mark）：单线程运行，需要Stop The World，标记GC Roots能直达的对象。 并发标记（（CMS concurrent mark）：无停顿，和用户线程同时运行，从GC Roots直达对象开始遍历整个对象图。 重新标记（CMS remark）：多线程运行，需要Stop The World，标记并发标记阶段产生对象。 并发清除（CMS concurrent sweep）：无停顿，和用户线程同时运行，清理掉标记阶段标记的死亡的对象。 Concurrent Mark Sweep收集器运行示意图如下： 优点：CMS最主要的优点在名字上已经体现出来——并发收集、低停顿。 缺点：CMS同样有三个明显的缺点。 Mark Sweep算法会导致内存碎片比较多 CMS的并发能力比较依赖于CPU资源，并发回收时垃圾收集线程可能会抢占用户线程的资源，导致用户程序性能下降。 并发清除阶段，用户线程依然在运行，会产生所谓的理“浮动垃圾”（Floating Garbage），本次垃圾收集无法处理浮动垃圾，必须到下一次垃圾收集才能处理。如果浮动垃圾太多，会触发新的垃圾回收，导致性能降低。 Garbage First收集器G1把连续的Java堆划分为多个大小相等的独立区域（Region），每一个Region都可以根据需要，扮演新生代的Eden空间、Survivor空间，或者老年代空间。收集器能够对扮演不同角色的Region采用不同的策略去处理。 这样就避免了收集整个堆，而是按照若干个Region集进行收集，同时维护一个优先级列表，跟踪各个Region回收的“价值，优先收集价值高的Region。 G1收集器的运行过程大致可划分为以下四个步骤： 初始标记（initial mark），标记了从GC Root开始直接关联可达的对象。STW（Stop the World）执行。 并发标记（concurrent marking），和用户线程并发执行，从GC Root开始对堆中对象进行可达性分析，递归扫描整个堆里的对象图，找出要回收的对象、 最终标记（Remark），STW，标记再并发标记过程中产生的垃圾。 筛选回收（Live Data Counting And Evacuation），制定回收计划，选择多个Region 构成回收集，把回收集中Region的存活对象复制到空的Region中，再清理掉整个旧 Region的全部空间。需要STW。 相比CMS，G1的优点有很多，可以指定最大停顿时间、分Region的内存布局、按收益动态确定回收集。 只从内存的角度来看，与CMS的“标记-清除”算法不同，G1从整体来看是基于“标记-整理”算法实现的收集器，但从局部（两个Region 之间）上看又是基于“标记-复制”算法实现，无论如何，这两种算法都意味着G1运作期间不会产生内存空间碎片，垃圾收集完成之后能提供规整的可用内存。 MinorGC/MajorGC/FullGC的区别 ①、Minor GC 也叫Young GC，指的是新生代 GC，发生在新生代（Eden区和Survivor区）的垃圾回收。因为Java对象大多是朝生夕死的，所以 Minor GC 通常很频繁，一般回收速度也很快。 ②、Major GC 也叫Old GC，指的是老年代的 GC，发生在老年代的垃圾回收，该区域的对象存活时间比较长，通常来讲，发生 Major GC时，会伴随着一次 Minor GC，而 Major GC 的速度一般会比 Minor GC 慢10倍。 ③、Full GC 指的是全区域（整个堆）的垃圾回收，通常来说和 Major GC 是等价的。 内存分配内存溢出和内存泄漏的区别 **内存溢出（*Out Of Memory*） ：就是申请内存时，JVM没有足够的内存空间。通俗说法就是去蹲坑发现坑位满了。 内存泄露 （Memory Leak）：就是申请了内存，但是没有释放，导致内存空间浪费。通俗说法就是有人占着茅坑不拉屎。 内存分配的策略 对象优先在Eden区分配，当 Eden 区没有足够的空间进行分配时，虚拟机将会发起一次 Minor GC(新生代GC)。 大对象直接分配在老年代，比较典型的就是那种很长的字符串以及数组。 长期存活的对象将进入老年代，新生代对象每熬过一次 Minor GC，年龄就增加1，当它的年龄增加到一定阈值时（默认是15岁），就会被晋升到老年代中。 新生代Survivor 区相同年龄所有对象之和大于 Survivor 所有对象之和的一半，大于等于该年龄的对象进入老年代 空间分配担保原则： 新生代内存分为一块 Eden区，和两块 Survivor 区域，当发生一次 Minor GC时，虚拟机会将Eden和一块Survivor区域的所有存活对象复制到另一块Survivor区域，通常情况下，Java对象朝生夕死，一块 Survivor 区域是能够存放GC后剩余的对象的，但是极端情况下，GC后仍然有大量存活的对象，那么一块 Survivor 区域就会存放不下这么多的对象，那么这时候就需要老年代进行分配担保，让无法放入 Survivor 区域的对象直接进入到老年代，当然前提是老年代还有空间能够存放这些对象。但是实际情况是在完成GC之前，是不知道还有多少对象能够存活下来的，所以老年代也无法确认是否能够存放GC后新生代转移过来的对象，那么这该怎么办呢? 前面我们介绍的都是Minor GC,那么何时会发生 Full GC？ 在发生 Minor GC 时，虚拟机会检测之前每次晋升到老年代的平均大小是否大于老年代的剩余空间，如果大于，则改为 Full GC。如果小于，则查看 HandlePromotionFailure 设置是否允许担保失败，如果允许，那只会进行一次 Minor GC，如果不允许，则也要进行一次 Full GC。 1-XX:-HandlePromotionFailure 回到第一个问题，老年代也无法确认是否能够存放GC后新生代转移过来的对象，那么这该怎么办呢? 也就是取之前每一次回收晋升到老年代对象容量的平均大小作为经验值，然后与老年代剩余空间进行比较，来决定是否进行 Full GC，从而让老年代腾出更多的空间。 通常情况下，我们会将 HandlePromotionFaile 设置为允许担保失败，这样能够避免频繁的发生 Full GC。 Java对象详解创建对象的4种方式 new 关键字： 在方法区的常量池中查看是否有new 后面参数（也就是类名）的符号引用，并检查是否有类的加载信息也就是是否被加载解析和初始化过。如果已经加载过了就不在加载，否则执行类的加载全过程。 给实例分配内存：此内存中存放对象自己的实例变量和从父类继承过来的实例变量（即使这些从超类继承过来的实例变量有可能被隐藏也会被分配空间），同时这些实例变量被赋予默认值（零值）； 调用构造函数，初始化成员字段：在Java对象初始化过程中，主要涉及三种执行对象初始化的结构，分别是实例变量初始化、实例代码块初始化以及构造函数初始化； user对象指向分配的内存空间： 注意：new操作不是原子操作，b和c的顺序可能会调换。 clone方法创建对象： 要想让一个对象支持clone，必须让这个对象对应的类实现Cloneable接口（标识接口），同时此类中也要重写clone方法 clone()方法是属于Object类的，clone是在堆内存中用二进制的方式进行拷贝，重新分配给对象一块内存 反射创建对象： 获取类的Class对象实例 1234获取方式如下：Class.forName(&quot;类全路径&quot;);类名.class; 如：Animal.class;对象名.getClass(); 通过反射创建类对象的实例对象 1Class.newInstance()：调用无参的构造方法，必需确保类中有无参数的可见的构造函数，否则将会抛出异常； 强制转换成用户所需类型 Java 反射机制是指在程序运行时，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性。这种动态的获取信息以及动态调用对象的方法的功能称为java 的反射机制。 反射机制很重要的一点就是“运行时”，其使得我们可以在程序运行时加载、探索以及使用编译期间完全未知的 .class 文件。换句话说，Java 程序可以加载一个运行时才得知名称的 .class 文件，然后获悉其完整构造，并生成其对象实体、或对其 fields（变量）设值、或调用其 methods（方法） 反序列化创建对象 Java中要序列化的类必须实现Serializable接口； 所有可在网络上传输的对象都必须是可序列化的；如RMI（remote method invoke，即远程方法调用），传入的参数或返回的对象都是可序列化的，否则会出错； 所有需要保存到磁盘的java对象都必须是可序列化的；通常建议：程序创建的每个JavaBean类都实现Serializeable接口； 对象创建过程我们以虚拟机遇到一个new指令开始： 首先检查这个指令的参数是否能在常量池中定位到一个类的符号引用 检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，就先执行相应的类加载过程 类加载检查通过后，接下来虚拟机将为新生对象分配内存。 内存分配有两种方式，指针碰撞（Bump The Pointer）、空闲列表（Free List） 指针碰撞：假设Java堆中内存是绝对规整的，所有被使用过的内存都被放在一边，空闲的内存被放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间方向挪动一段与对象大小相等的距离，这种分配方式称为“指针碰撞” 如果Java堆中的内存并不是规整的，已被使用的内存和空闲的内存相互交错在一起，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为“空闲列表” 两种方式的选择由Java堆是否规整决定 Java堆规整由所采用的垃圾收集器是否带有空间压缩整理（Compact）的能力决定 内存分配完成之后，虚拟机将分配到的内存空间（但不包括对象头）都初始化为零值。 设置对象头，请求头里包含了对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。 从虚拟机角度来看，设置完对象头信息以后初始化就已经完成了，但是对于Java程序而言，new指令之后会接着执行 ()方法，对对象进行初始化，这样一个真正可用的对象才算完全被构造出来。 分配对象内存的时候如何保证线程安全分配内存线程安全问题：对象创建在虚拟机中是非常频繁的行为，即使仅仅修改一个指针所指向的位置，在并发情况下也并不是线程安全的，可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。 线程安全问题有两种解可选方案： 一种是对分配内存空间的动作进行同步处理——实际上虚拟机是采用CAS配上失败重试的方式保证更新操作的原子性 另外一种是把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer，TLAB），哪个线程要分配内存，就在哪个线程的本地缓冲区中分配，只有本地缓冲区用完了，分配新的缓存区时才需要同步锁定。 对象的内存布局在HotSpot虚拟机里，对象在堆内存中的存储布局可以划分为三个部分：对象头（Header）、实例数据（Instance Data）和对齐填充（Padding） 在64位的HotSpot虚拟机中，如对象未被同步锁锁定的状态下，Mark Word的64个比特存储空间中的31个比特用于存储对象哈希码，4个比特用于存储对象分代年龄，2个比特用于存储锁标志位，在其他状态（轻量级锁、重量级锁、偏向锁）下对象的存储内容变化如图示。 对象头的另外一部分是类型指针，即对象指向它的类型元数据的指针，Java虚拟机通过这个指针来确定该对象是哪个类的实例。并不是所有的虚拟机实现都必须在对象数据上保留类型指针，查找对象的元数据信息并不一定要经过对象本身， 如果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据，因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小，但是如果数组的长度是不确定的，将无法通过元数据中的信息推断出数组的大小。 总结 Java内存区域是怎么划分的？每一块都干了啥？ 哪些区域会发生OOM？ 对象的创建都是在堆上吗？ 堆上的内存是怎么划分的啊 如何判断对象已经死亡了 Java中引用的种类有哪些，分别干啥用的？为什么要分这4种引用 垃圾回收算法有哪些？ 方法区这里有垃圾吗？哪些垃圾 垃圾处理器有哪些啊 可以说说CMS垃圾收集器吗 可以说说G1垃圾收集器吗 MinorGC、MajorGC、和 FullGC的区别是啥呀 内存溢出和内存泄漏啥意思啊 内存分配的策略有哪些啊 对象创建方式有哪些啊 对象创建的过程是怎么样的啊 对象分配内存时如果有并发分配怎么办啊 对象到底长啥样啊？","link":"/2021/10/08/Java%E8%99%9A%E6%8B%9F%E6%9C%BA%EF%BC%88%E4%B8%80%EF%BC%89%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%9C%BA%E5%88%B6/"},{"title":"Java虚拟机（三）编译优化和代码优化","text":"主要是讲虚拟机的效率上的优化 编译期优化 运行期优化 编译器的种类： 前端编译器：比如javac，将 ***.java 文件 转变成 *.class 文件**的过程 JIT编译器：把字节码转换成机器码的过程 AOT编译器： 直接把***.java** 文件 转变成 机器码的过程。 编译期优化运行期优化当虚拟机发现某个方法或代码块运行的特别频繁的时候，就会把这些代码认为是热点代码。为了提高这些热点代码的运行效率，会","link":"/2021/10/08/Java%E8%99%9A%E6%8B%9F%E6%9C%BA%EF%BC%88%E4%B8%89%EF%BC%89%E7%BC%96%E8%AF%91%E4%BC%98%E5%8C%96%E5%92%8C%E4%BB%A3%E7%A0%81%E4%BC%98%E5%8C%96/"},{"title":"Java虚拟机（二）类文件和类加载","text":"Java语言的特性：一次编译、到处运行。 类文件结构 类加载机制 java跨平台的实现是基于JVM虚拟机的，编写的java源码，编译后会生成一种.class文件，称为字节码文件。 java虚拟机就是负责将字节码文件翻译成特定平台下的机器码然后运行。 为了保证Class文件在多个平台的通用性，java官方制定了严格的Class文件格式。 了解Class文件结构，有利于我们反编译 .class 文件或在程序编译期间修改字节码做代码注入。 类文件结构Class文件是一组8位字节为基础单元的二进制流，当遇到超过8位字节以上的数据项时，就会按照高位在前的方式分割存储。 中包含了Java虚拟机指令集、符号表以及若干其他辅助信息。 由于 Class 文件结构没有任何分隔符，所以无论是每个数据项的的顺序还是数量，都是严格限定的，哪个字节代表什么含义，长度多少，先后顺序如何，都是不允许改变的 魔数将class文件用16进制打开的话 第一行中有一串特殊的字符 cafebabe，它就是一个魔数，是 JVM 识别 class 文件的标志，JVM 会在验证阶段检查 class 文件是否以该魔数开头，如果不是则会抛出 ClassFormatError。 版本号第 5 和第 6 个字节是次版本号（Minor Version），第 7 和第 8 个字节是主版本号（Major Version）。高版本的 JDK 能向下兼容以前版本的 Class 文件，但不能运行以后版本的 Class 文件，即使文件格式未发生变化。 常量池常量池是Class文件中占有空间最大的数据项目。 常量池中主要存放两大类常量： 字面量：字面量比较接近于Java语言层面的常量概念，如文本字符串、被声明为final的常量值等 符号引用：主要包括： 类或接口的权限定名 + 字段的名称和描述符 + 方法的名称和描述符 虚拟机运行的时候需要从常量池中拿到对应的符号引用，再在类创建的时候解析翻译到具体的内存地址当中。 这17类常量结构只有一个相同之处，表结构起始的第一位是个u1类型的标志位（tag），代表着当前常量属于哪种常量类型。 17种常量类型所代表的具体含义如表所示： 类型 标志 描述 CONSTANT_Utf8_info 1 UTF-8 编码的字符串 CONSTANT_Integer_info 3 整型字面量 CONSTANT_Float_info 4 浮点型字面量 CONSTANT_Long_info 5 长整型型字面量 CONSTANT_Double_info 6 双精度浮点型字面量 CONSTANT_Class_info 7 类或接口的符号引用 CONSTANT_String_info 8 字符串类型字面量 CONSTANT_Fieldref_info 9 字段的符号引用 CONSTANT_Methodref_info 10 类中方法的符号引用 CONSTANT_InterfaceMethodref_info 11 接口中方法的符号引用 CONSTANT_NameAndType_info 12 字段或方法的部分符号引用 CONSTANT_MethodHandle_info 15 表示方法句柄 CONSTANT_MethodType_info 16 表示方法类型 CONSTANT_Dynamic_info 17 表示一个动态计算常量 CONSTANT_InvokeDynamic_info 18 表示一个动态方法调用点 CONSTANT_Moudle_info 19 表示一个模块 CONSTANT_Package_info 20 表示一个模块中开放或者导出的包 常量池非常繁琐，17种常量类型各自有着完全独立的数据结构，彼此之间没有什么共性和联系。 访问标志在常量池结束之后，紧接着的2个字节代表访问标志（access_flags），这个标志用于识别一些类或者接口层次的访问信息， 包括：这个Class是类还是接口；是否定义为public类型；是否定义为abstract类型；如果是类的话，是否被声明为final等等。 具体的标志位以及标志的含义如表： 标志名称 标志值 含义 ACC_PUBLIC 0x0001 是否为 Public 类型 ACC_FINAL 0x0010 是否被声明为 final，只有类可以设置 ACC_SUPER 0x0020 是否允许使用 invokespecial 字节码指令的新语义 ACC_INTERFACE 0x0200 标志这是一个接口 ACC_ABSTRACT 0x0400 是否为 abstract 类型，对于接口或者抽象类来说，次标志值为真，其他类型为假 ACC_SYNTHETIC 0x1000 标志这个类并非由用户代码产生 ACC_ANNOTATION 0x2000 标志这是一个注解 ACC_ENUM 0x4000 标志这是一个枚举 access_flags中一共有16个标志位可以使用，当前只定义了其中9个，没有使用到的标志位要求一 律为零。 类索引、父类索引、接口索引这三者用来确定类的继承关系。 类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名。由于Java语言不允许多重继承，所以父类索引只有一个，除了java.lang.Object之外，所有的Java类都有父类，因此除了 java.lang.Object外，所有Java类的父类索引都不为0。 接口索引集合就用来描述这个类实现了哪些接口，这些被实现的接口将按implements关键字（后的接口顺序从左到右排列在接口索引集合中。 字段表集合接口索引结束后，接着是字段表（field_info），它用于描述接口或者类中声明的变量——这里的字段（Field）只包括类级变量以及实例级变量，不包括在方法内部声明的局部变量。 描述的主要信息包括： ①、字段的作用域（public，protected，private修饰） ②、是类级变量还是实例级变量（static修饰） ③、是否可变（final修饰） ④、并发可见性（volatile修饰，是否强制从主从读写） ⑤、是否可序列化（transient修饰） ⑥、字段数据类型（8种基本数据类型，对象，数组等引用类型） ⑦、字段名称 字段表的结构如下： 类型 名称 数量 u2 access_flags 1 u2 name_index 1 u2 descriptor_index 1 u2 attributes_count 1 attribute_info attributes attributes_count access_flags是该字段的的访问标志，它和类中的访问标志很类似，用以描述该字段的权限类型：private、protected、public；并发可见性：volatile；可变性：final； 访问标志详情如下图所示： 由于Java语法规则的约束，ACC_PUBLIC、ACC_PRIVATE、ACC_PROTECTED三个标志最多只能选择其一，ACC_FINAL、ACC_VOLATILE不能同时选择。接口之中的字段必须有ACC_PUBLIC、ACC_STATIC、ACC_FINAL标志。 方法表集合方法表的结构如同字段表一样，依次包括访问标志（access_flags）、名称索引（name_index）、描述符索引（descriptor_index）、属性表集合（attributes）几项，如表所示： 有区别的部分只有方法访问标志 access_flag, 因为volatile关键字和transient关键字不能修饰方法。 方法表标志位及其取值如下： 属性表集合接下来终于到了最后一项：属性表集合。 前面提到的Class文件、字段表、方法表都可以携带自己的属性表集合，就是引用的这里。 属性表集合中的属性如下所示： 与Class文件中其他的数据项目要求严格的顺序、长度和内容不同，属性表集合的限制宽松一些，不再要求各个属性表具有严格顺序，并且《Java虚拟机规范》允许只要不与已有属性名重复，任何人实现的编译器都可以向属性表中写入自己定义的属性信息，Java虚拟机运行时会忽略掉它不认识的属性。 举例查看12345public class Hello { public static void main(String[] args) { System.out.println(&quot;Hello World&quot;); }} 12javac Hello.java //javac 命令编译成 jvm 能识别的 class 文件xxd Hello.class //以 16 进制的方式查看这个 class 文件 16进制如下： 1234567891011121314151617181920212223242526272800000000: cafe babe 0000 0034 001d 0a00 0600 0f09 .......4........ //cafe babe为魔数00000010: 0010 0011 0800 120a 0013 0014 0700 1507 ................00000020: 0016 0100 063c 696e 6974 3e01 0003 2829 .....&lt;init&gt;...()00000030: 5601 0004 436f 6465 0100 0f4c 696e 654e V...Code...LineN00000040: 756d 6265 7254 6162 6c65 0100 046d 6169 umberTable...mai00000050: 6e01 0016 285b 4c6a 6176 612f 6c61 6e67 n...([Ljava/lang00000060: 2f53 7472 696e 673b 2956 0100 0a53 6f75 /String;)V...Sou00000070: 7263 6546 696c 6501 000a 4865 6c6c 6f2e rceFile...Hello.00000080: 6a61 7661 0c00 0700 0807 0017 0c00 1800 java............00000090: 1901 000b 4865 6c6c 6f20 576f 726c 6407 ....Hello World.000000a0: 001a 0c00 1b00 1c01 0021 636f 6d2f 7869 .........!com/xi000000b0: 6173 6d2f 6173 6d64 656d 6f2f 636c 6173 asm/asmdemo/clas000000c0: 7374 6573 742f 4865 6c6c 6f01 0010 6a61 stest/Hello...ja000000d0: 7661 2f6c 616e 672f 4f62 6a65 6374 0100 va/lang/Object..000000e0: 106a 6176 612f 6c61 6e67 2f53 7973 7465 .java/lang/Syste000000f0: 6d01 0003 6f75 7401 0015 4c6a 6176 612f m...out...Ljava/00000100: 696f 2f50 7269 6e74 5374 7265 616d 3b01 io/PrintStream;.00000110: 0013 6a61 7661 2f69 6f2f 5072 696e 7453 ..java/io/PrintS00000120: 7472 6561 6d01 0007 7072 696e 746c 6e01 tream...println.00000130: 0015 284c 6a61 7661 2f6c 616e 672f 5374 ..(Ljava/lang/St00000140: 7269 6e67 3b29 5600 2100 0500 0600 0000 ring;)V.!.......00000150: 0000 0200 0100 0700 0800 0100 0900 0000 ................00000160: 1d00 0100 0100 0000 052a b700 01b1 0000 .........*......00000170: 0001 000a 0000 0006 0001 0000 0003 0009 ................00000180: 000b 000c 0001 0009 0000 0025 0002 0001 ...........%....00000190: 0000 0009 b200 0212 03b6 0004 b100 0000 ................000001a0: 0100 0a00 0000 0a00 0200 0000 0500 0800 ................000001b0: 0600 0100 0d00 0000 0200 0e ........... 1javap -c xxx 是用来对class文件进行反编译 12345678910111213141516171819xiasmdeMacBook-Pro:test xiasm$ javap -c Hello警告: 二进制文件Hello包含com.xiasm.asmdemo.classtest.Hello1 Compiled from &quot;Hello.java&quot;2 public class com.xiasm.asmdemo.classtest.Hello {3 public com.xiasm.asmdemo.classtest.Hello();4 Code:5 0: aload_06 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V7 4: return89 public static void main(java.lang.String[]);10 Code:11 0: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream;12 3: ldc #3 // String Hello World13 5: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V14 8: return15 } 第5行：aload_x 操作码用来把 对象引用 加载到 操作数栈，非静态的函数都有第一个默认参数，那就是 this，这里的 aload_0 就是把 this 入栈 第6行：invokespecial #1 invokespecial指令调用实例初始化方法、私有方法、父类方法，#1 指的是常量池中的第一个，这里是方法引用java/lang/Object.””:()V，也即构造器函数 第7行：return，这个操作码属于 ireturn、lreturn、freturn、dreturn、areturn 和 return 操作码组中的一员，其中 i 表示 int，返回整数，同类的还有 l 表示 long，f 表示 float，d 表示 double，a 表示 对象引用。没有前缀类型字母的 return 表示返回 void 到此，构造器函数就结束了，接下来是 main 函数： 第11行：getstatic #2 getstatic获取指定类的静态域，并将其值压入栈顶，#2 代表常量池中的第 2 个，这里表示的是java/lang/System.out:Ljava/io/PrintStream;，其实就是java.lang.System 类的静态变量 out（类型是 PrintStream） 第12行：ldc #3 ldc表示将int, float或String型常量值从常量池中推送至栈顶，#3 代表常量池的第三个（字符串 Hello, World） 第13行：invokevirtual #4 invokevirutal 指令调用一个对象的实例方法，#4表示 PrintStream.println(String) 函数引用，并把栈顶两个元素出栈 类加载机制 加载就是把字节码文件从IO或内存加载到内存中的过程； 初始化就是使用()进行类初始化的过程，这不同于调用构造函数； 使用就是字面意思； 卸载就是从方法区移除类型。 解析在一些情况下可以在初始化之后再开始，这是为了支持Java语言的动态绑定 类加载过程加载 1. 通过一个类的全限定名来获取定义此类的二进制字节流。 2. 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 3. 在Java堆中生成一个代表这个类的java.lang.Class对象，作为方法区这些数据的访问入口 它是Java将字节码数据从不同的数据源读取到JVM中，并映射为JVM认可的数据结构（Class对象），这里的数据源可能是各种各样的形态，如jar文件、class文件，甚至是网络数据源等；如果输入数据不是ClassFile的结构，则会抛出ClassFormatError。 加载阶段是用户参与的阶段，我们可以自定义类加载器，去实现自己的类加载过程。 定义此类的二进制流的获取方式有多种： 1、从 ZIP 包中读取。这称为后面的 JAR、EAR、WAR 格式的基础。 2、从网络中获取。比较典型的应用就是 Applet。 3、运行时计算生成。这就是动态代理技术。 4、由其它文件生成。比如 JSP 应用。 5、从数据库中读取。 加载阶段完成后，虚拟机外部的二进制字节流就按照虚拟机所需的格式存储在方法区中，然后在Java堆中实例化一个 java.lang.Class 类的对象，这个对象将作为程序访问方法区中这些类型数据的外部接口。 注意，加载阶段与连接阶段的部分内容（如一部分字节码文件的格式校验）是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始了。 验证作用是为了确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 ①、文件格式验证 ②、元数据验证：是否有父类、是否实现了父类或接口中要求实现的所有方法等。 ③、字节码验证 ④、符号引用验证：检查通过符号引用是否可以找到唯一确定的类。 确保后续的解析动作能顺利执行 准备准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存是在方法区中进行分配。 注意： 一、上面说的是类变量，也就是被 static 修饰的变量，不包括实例变量。实例变量会在对象实例化时随着对象一起分配在堆中。 二、初始值，指的是一些数据类型的默认值。基本的数据类型初始值如下（引用类型的初始值为null）： 解析解析阶段是虚拟机将常量池中的符号引用替换为直接引用的过程。 符号引用（Symbolic References）：符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义的定位到目标即可。符号引用与虚拟机实现的内存布局无关，引用的目标不一定已经加载到内存中。 直接引用（Direct References）：直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用是与虚拟机实现内存布局相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那么引用的目标必定已经在内存中存在。 解析动作主要针对类或接口、字段、类方法、接口方法四类符号引用， 分别对应于常量池的 CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info、CONSTANTS_InterfaceMethodref_info四种类型常量。 初始化到了初始化阶段，才开始真正的执行类中定义的Java程序代码。 初始化阶段是执行类构造器() 方法的过程。 这一步真正去执行类初始化的代码逻辑，包括静态字段赋值的动作，以及执行类定义中的静态初始化块内的逻辑，编译器在编译阶段就会把这部分逻辑整理好，父类型的初始化逻辑优先于当前类型的逻辑。 () 方法 是由编译器自动收集类中的所有类变量的赋值动作和静态语句块（static{}）中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序所决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块中可以赋值，但是不能访问。 比如如下代码会报错： 但是你把第 14 行代码放到 static 静态代码块的上面就不会报错了。或者不改变代码顺序，将第 11 行代码移除，也不会报错。 () 方法与类的构造函数（或者说是实例构造器()方法）不同，它不需要显示的调用父类构造器，虚拟机会保证在子类的()方法执行之前，父类的()方法已经执行完毕。因此虚拟机中第一个被执行的()方法的类肯定是 java.lang.Object。 由于父类的() 方法先执行，所以父类中定义的静态语句块要优先于子类的变量赋值操作。 () 方法对于接口来说并不是必须的，如果一个类中没有静态语句块，也没有对变量的赋值操作，那么编译器可以不为这个类生成() 方法。 接口中不能使用静态语句块，但仍然有变量初始化的赋值操作，因此接口与类一样都会生成() 方法。但接口与类不同的是，执行接口中的() 方法不需要先执行父接口的() 方法。只有当父接口中定义的变量被使用时，父接口才会被初始化。 接口的实现类在初始化时也一样不会执行接口的() 方法。 虚拟机会保证一个类的() 方法在多线程环境中被正确的加锁和同步。如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的() 方法，其他的线程都需要阻塞等待，直到活动线程执行() 方法完毕。如果在一个类的() 方法中有很耗时的操作，那么可能造成多个进程的阻塞。 类加载器分类 启动类加载器（Bootstrap ClassLoader）：负责将存放在 /lib目录中的类库加载。 扩展类加载器（Extension ClassLoader）：负责将存放在 /lib/ext目录中的类库加载。开发者可以直接使用扩展类加载器。 应用程序类加载器（Application ClassLoader）：它负责加载用户类路径ClassPath上所指定的类库，开发者可以直接使用这个类加载器。如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 自定义类加载器（User ClassLoader）：由用户自己定义的类加载器 12345678910public class ClassLoadTest { public static void main(String[] args) { ClassLoader classLoader1 = ClassLoadTest.class.getClassLoader(); ClassLoader classLoader2 = classLoader1.getParent(); ClassLoader classLoader3 = classLoader2.getParent(); System.out.println(classLoader1); // sun.misc.Launcher$AppClassLoader@18b4aac2 System.out.println(classLoader2); // sun.misc.Launcher$ExtClassLoader@7ea987ac System.out.println(classLoader3); // null ,用户拿不到启动类加载器 }} 双亲委派模型如果自己写了一个String类？会发生什么呢？ 双亲委派机制就是如果一个类加载器收到了类加载请求，它首先不会自己尝试去加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有父类加载器反馈到无法完成这个加载请求（它的搜索范围没有找到这个类），子加载器才会尝试自己去加载。 双亲委派机制有什么好处呢? 回到上面提出的问题，如果你自定义了一个 java.lang.String类，你会发现这个自定义的String.java可以正常编译，但是永远无法被加载运行。因为加载这个类的加载器，会一层一层的往上推，最终由启动类加载器来加载，而启动类加载的会是源码包下的String类，不是你自定义的String类。 实现源码： 12345678910111213141516171819202122232425262728293031323334353637protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) { c = parent.loadClass(name, false); } else { c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // ClassNotFoundException thrown if class not found // from the non-null parent class loader } if (c == null) { // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; } } 自定义类加载器先说说我们为什么要自定义类加载器？ ①、加密 我们知道Java字节码是可以进行反编译的，在某些安全性高的场景，是不允许这种情况发生的。那么我们可以将编译后的代码用某种加密算法进行加密，加密后的文件就不能再用常规的类加载器去加载类了。而我们自己可以自定义类加载器在加载的时候先解密，然后在加载。 ②、动态创建 比如很有名的动态代理。 ③、从非标准的来源加载代码 我们不用非要从class文件中获取定义此类的二进制流，还可以从数据库，从网络中，或者从zip包等。 明白了为什么要自定义类加载器，接下来我们再来详述如何自定义类加载器。 如何自定义类加载器： 类加载时根据双亲委派模型会先一层层找到父加载器，如果加载失败，则会调用当前加载器的 findClass() 方法来完成加载。因此我们自定义类加载器： 1、继承 ClassLoader 2、覆写 findClass() 方法 破坏双亲委派模型的情况 重写 loadClass() 方法 逆向使用类加载器，引入线程上下文类加载器，如果 API 中的基础类想要调用用户的代码(JNDI/JDBC 等),此时双亲委派模型就不能完成.为了解决这个问题,java 设计团队只好 使用一个不优雅的设计方案:Thread 的上下文类加载器,默认就是应用程序的类加载器。 追求程序的动态性：代码热替换、模块热部署等技术，希望应用程序不用重启就可以加载最新的字节码文件.此时就需要破坏双亲委派模型 动态代理的实现对于一个普通的Java动态代理，其实现过程可以简化成为： 提供一个基础的接口，作为被调用类型（com.mycorp.HelloImpl）和代理类之间的统一入口，如com.mycorp.Hello。 实现InvocationHandler，对代理对象方法的调用，会被分派到其invoke方法来真正实现动作。 通过Proxy类，调用其newProxyInstance方法，生成一个实现了相应基础接口的代理类实例，可以看下面的方法签名。 123public satic Object newProxyInsance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) 我们分析一下，动态代码生成是具体发生在什么阶段呢？ 不错，就是在newProxyInstance生成代理类实例的时候。我选取了JDK自己采用的ASM作为示例，一起来看看用ASM实现的简要过程，请参考下面的示例代码片段。 第一步，生成对应的类，其实和我们去写Java代码很类似，只不过改为用ASM方法和指定参数，代替了我们书写的源码。 123456789101112131415161718ClassWriter cw = new ClassWriter(ClassWriter.COMPUTE_FRAMES);cw.visit(V1_8, // 指定Java版本ACC_PUBLIC, // 说明是public类型&quot;com/mycorp/HelloProxy&quot;, // 指定包和类的名称null, // 签名，null表示不是泛型&quot;java/lang/Object&quot;, // 指定父类new String[]{ &quot;com/mycorp/Hello&quot; }); // 指定需要实现的接口更进一步，我们可以按照需要为代理对象实例，生成需要的方法和逻辑。MethodVisitor mv = cw.visitMethod(ACC_PUBLIC, // 声明公共方法&quot;sayHello&quot;, // 方法名称&quot;()Ljava/lang/Object;&quot;, // 描述符null, // 签名，null表示不是泛型null); // 可能抛出的异常，如果有，则指定字符串数组mv.visitCode();// 省略代码逻辑实现细节cw.visitEnd(); // 结束类字节码生成 上面的代码虽然有些晦涩，但总体还是能多少理解其用意，不同的visitX方法提供了创建类型，创建各种方法等逻辑。ASM API，广泛的使用了Visitor模式，如果你熟悉这个模式， 就会知道它所针对的场景是将算法和对象结构解耦，非常适合字节码操纵的场合，因为我们大部分情况都是依赖于特定结构修改或者添加新的方法、变量或者类型等。 按照前面的分析，字节码操作最后大都应该是生成byte数组，ClassWriter提供了一个简便的方法。 cw.toByteArray();然后，就可以进入我们熟知的类加载过程了， 总结自测 Class文件中有哪些东西？ 类加载机制是什么样的？每一步都干了啥 类加载器有哪些呀？ 什么是双亲委派机制？ 为什么有时候要破坏双亲委派机制？","link":"/2021/10/08/Java%E8%99%9A%E6%8B%9F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E7%B1%BB%E6%96%87%E4%BB%B6%E5%92%8C%E7%B1%BB%E5%8A%A0%E8%BD%BD/"},{"title":"MapStruct使用详解","text":"MapStruct的使用需求： 随着模块的划分越来越细，尤其是现在的分布式系统中，应用与应用之间，还有单独的应用细分模块之后，DO（domain） 一般不会让外部依赖，这时候需要在提供对外接口的模块里放 DTO 用于对象传输，也即是 DO 对象对内，DTO对象对外，DTO 可以根据业务需要变更，并不需要映射 DO 的全部属性。 是什么MapStruct 就是一个属性映射工具，只需要定义一个 Mapper 接口，MapStruct 就会自动实现这个映射接口，避免了复杂繁琐的映射实现。 MapStruct官网地址： http://mapstruct.org/ 那么为啥不用 BeanUtils 的 copyProperties 方法呢？不也照样可以实现属性的映射么？BeanUtils 只能同属性映射，或者在属性相同的情况下，允许被映射的对象属性少；但当遇到被映射的属性数据类型被修改或者被映射的字段名被修改，则会导致映射失败。 怎么用需求描述： 有这么个场景：从数据库查询出来了一个user对象（包含id，用户名，密码，手机号，邮箱，角色这些字段）和一个对应的角色对象role（包含id，角色名，角色描述这些字段），现在在controller需要用到user对象的id，用户名，和角色对象的角色名三个属性。 一种方式是直接把两个对象传递到controller层，但是这样会多出很多没用的属性。 更通用的方式是需要用到的属性封装成一个类(DTO)，通过传输这个类的实例来完成数据传输。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * @author shengbinbin * @description: User DO 实体类 * @date 2021/8/1110:29 下午 */@Data@Builder@NoArgsConstructor@AllArgsConstructorpublic class User { private Long id; private String username; private String password; private String phoneNum; private String email; private Role role;}/** * @author shengbinbin * @description: Role实体类 * @date 2021/8/1110:36 下午 */@Data@Builder@NoArgsConstructor@AllArgsConstructorpublic class Role { private Long id; private String roleName; private String description;}/** * @author shengbinbin * @description: UserRoleDTO 传输对象实体类 * @date 2021/8/1110:36 下午 */@Data@Builder@NoArgsConstructor@AllArgsConstructorpublic class UserRoleDTO { /**用户id*/ private Long userId; /**用户名*/ private String name; /**角色名*/ private String roleName;} 模拟转换类： 1234567891011121314151617181920212223242526public class ConvertTest { User user = null; /** * 模拟从数据库中查出user对象 */ @Before public void before() { Role role = new Role(2L, &quot;binshow&quot;, &quot;超级管理员&quot;); user = new User(1L, &quot;zkd&quot;, &quot;12345&quot;, &quot;8888888&quot;, &quot;123456@qq.com&quot;, role); } /** * 模拟把user对象转换成UserRoleDto对象 */ @Test public void test1() { UserRoleDTO userRoleDto = new UserRoleDTO(); userRoleDto.setUserId(user.getId()); userRoleDto.setName(user.getUsername()); userRoleDto.setRoleName(user.getRole().getRoleName()); System.out.println(userRoleDto); }} 使用mapstruct之后： 引入依赖： 1234567891011121314151617181920212223242526&lt;dependencies&gt; &lt;!--记录小坑：lombok 1.18.16 之后必须使用 lombok-mapstruct-binding 插件--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;!-- jdk8以下就使用mapstruct --&gt; &lt;artifactId&gt;mapstruct-jdk8&lt;/artifactId&gt; &lt;version&gt;1.2.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct-processor&lt;/artifactId&gt; &lt;version&gt;1.2.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.22&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 编写一个Mapper接口（测试类也可）：定义映射规则 1234567891011121314151617181920212223242526272829303132/** * @author shengbinbin * @description: MapStruct的映射工具类 * @date 2021/8/1110:44 下午 * * @Mapper 定义这是一个MapStruct对象属性转换接口，在这个类里面规定转换规则 * * 在项目构建时，会自动生成改接口的实现类，这个实现类将实现对象属性值复制 * */@Mapperpublic interface UserRoleMapper { /** * 获取该类自动生成的实现类的实例 * 接口中的属性都是 public static final 的 方法都是public abstract的 */ UserRoleMapper INSTANCES = Mappers.getMapper(UserRoleMapper.class); /** * 这个方法就是用于实现对象属性复制的方法 * * @Mapping 用来定义属性复制规则 source 指定源对象属性 target指定目标对象属性 * * @param user 这个参数就是源对象，也就是需要被复制的对象 * @return 返回的是目标对象，就是最终的结果对象 */ @Mappings({ @Mapping(source = &quot;id&quot;, target = &quot;userId&quot;), @Mapping(source = &quot;username&quot;, target = &quot;name&quot;), @Mapping(source = &quot;role.roleName&quot;, target = &quot;roleName&quot;) }) UserRoleDTO toUserRoleDto(User user);} Build一下项目，就会发现target编译之后的Class有这个接口的实现类： 1234567891011121314151617181920212223242526272829303132333435// MapStruct 接口自动生成public class UserRoleMapperImpl implements UserRoleMapper { public UserRoleMapperImpl() { } public UserRoleDTO toUserRoleDto(User user) { if (user == null) { return null; } else { UserRoleDTO userRoleDTO = new UserRoleDTO(); userRoleDTO.setName(user.getUsername()); String roleName = this.userRoleRoleName(user); if (roleName != null) { userRoleDTO.setRoleName(roleName); } userRoleDTO.setUserId(user.getId()); return userRoleDTO; } } private String userRoleRoleName(User user) { if (user == null) { return null; } else { Role role = user.getRole(); if (role == null) { return null; } else { String roleName = role.getRoleName(); return roleName == null ? null : roleName; } } }} 测试类中测试： 12345678910111213141516171819202122232425262728/** * @author shengbinbin * @description: 测试将 DO 实体类转换成 DTO 传输对象 * @date 2021/8/1110:38 下午 */public class ConvertTest { User user = null; /** * 模拟从数据库中查出user对象 */ @Before public void before() { Role role = new Role(2L, &quot;manager&quot;, &quot;超级管理员&quot;); user = new User(1L, &quot;zkd&quot;, &quot;12345&quot;, &quot;8888888&quot;, &quot;123456@qq.com&quot;, role); } /** * 模拟把user对象转换成UserRoleDto对象 */ @Test public void test1() { UserRoleDTO userRoleDTO = UserRoleMapper.INSTANCES.toUserRoleDto(user); System.out.println(&quot;userRoleDTO = &quot; + userRoleDTO); }} 可以在接口和测试类中提供默认方法来进行其他的操作。 可以绑定多个对象的属性到模板对象中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * @author shengbinbin * @description: MapStruct的映射工具类 * @date 2021/8/1110:44 下午 * * @Mapper 定义这是一个MapStruct对象属性转换接口，在这个类里面规定转换规则 * * 在项目构建时，会自动生成改接口的实现类，这个实现类将实现对象属性值复制 * */@Mapperpublic interface UserRoleMapper { /** * 获取该类自动生成的实现类的实例 * 接口中的属性都是 public static final 的 方法都是public abstract的 */ UserRoleMapper INSTANCES = Mappers.getMapper(UserRoleMapper.class); /** * 这个方法就是用于实现对象属性复制的方法 * * @Mapping 用来定义属性复制规则 source 指定源对象属性 target指定目标对象属性 * * @param user 这个参数就是源对象，也就是需要被复制的对象 * @return 返回的是目标对象，就是最终的结果对象 */ @Mappings({ @Mapping(source = &quot;id&quot;, target = &quot;userId&quot;), @Mapping(source = &quot;username&quot;, target = &quot;name&quot;), @Mapping(source = &quot;role.roleName&quot;, target = &quot;roleName&quot;) }) UserRoleDTO toUserRoleDto(User user); /** * 多个参数中的值绑定 * @param user 源1 * @param role 源2 * @return 从源1、2中提取出的结果 */ @Mappings({ @Mapping(source = &quot;user.id&quot;, target = &quot;userId&quot;), // 把user中的id绑定到目标对象的userId属性中 @Mapping(source = &quot;user.username&quot;, target = &quot;name&quot;), // 把user中的username绑定到目标对象的name属性中 @Mapping(source = &quot;role.roleName&quot;, target = &quot;roleName&quot;) // 把role对象的roleName属性值绑定到目标对象的roleName中 }) UserRoleDTO toUserRoleDto(User user, Role role); /** * 直接使用参数作为值 * @param user * @param myRoleName * @return */ @Mappings({ @Mapping(source = &quot;user.id&quot;, target = &quot;userId&quot;), // 把user中的id绑定到目标对象的userId属性中 @Mapping(source = &quot;user.username&quot;, target = &quot;name&quot;), // 把user中的username绑定到目标对象的name属性中 @Mapping(source = &quot;myRoleName&quot;, target = &quot;roleName&quot;) // 把role对象的roleName属性值绑定到目标对象的roleName中 }) UserRoleDTO useParameter(User user, String myRoleName);} 生成的实现类： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class UserRoleMapperImpl implements UserRoleMapper { public UserRoleMapperImpl() { } public UserRoleDTO toUserRoleDto(User user) { if (user == null) { return null; } else { UserRoleDTO userRoleDTO = new UserRoleDTO(); userRoleDTO.setName(user.getUsername()); String roleName = this.userRoleRoleName(user); if (roleName != null) { userRoleDTO.setRoleName(roleName); } userRoleDTO.setUserId(user.getId()); return userRoleDTO; } } public UserRoleDTO toUserRoleDto(User user, Role role) { if (user == null &amp;&amp; role == null) { return null; } else { UserRoleDTO userRoleDTO = new UserRoleDTO(); if (user != null) { userRoleDTO.setName(user.getUsername()); userRoleDTO.setUserId(user.getId()); } if (role != null) { userRoleDTO.setRoleName(role.getRoleName()); } return userRoleDTO; } } public UserRoleDTO useParameter(User user, String myRoleName) { if (user == null &amp;&amp; myRoleName == null) { return null; } else { UserRoleDTO userRoleDTO = new UserRoleDTO(); if (user != null) { userRoleDTO.setName(user.getUsername()); userRoleDTO.setUserId(user.getId()); } if (myRoleName != null) { userRoleDTO.setRoleName(myRoleName); } return userRoleDTO; } } private String userRoleRoleName(User user) { if (user == null) { return null; } else { Role role = user.getRole(); if (role == null) { return null; } else { String roleName = role.getRoleName(); return roleName == null ? null : roleName; } } }} 注解关键词123456789101112@Mapper 只有在接口加上这个注解， MapStruct 才会去实现该接口 @Mapper 里有个 componentModel 属性，主要是指定实现类的类型，一般用到两个 default：默认，可以通过 Mappers.getMapper(Class) 方式获取实例对象 spring：在接口的实现类上自动添加注解 @Component，可通过 @Autowired 方式注入@Mapping：属性映射，若源对象属性与目标对象名字一致，会自动映射对应属性 source：源属性 target：目标属性 dateFormat：String 到 Date 日期之间相互转换，通过 SimpleDateFormat，该值为 SimpleDateFormat 的日期格式 ignore: 忽略这个字段@Mappings：配置多个@Mapping@MappingTarget 用于更新已有对象@InheritConfiguration 用于继承配置 原理解析相比于反射获取对象进行拷贝的方法，这种更贴近于原生 get/set 方法的框架显得更为高效。这个文件是通过在 mapper 中的注解，使用生成映射器的注解处理器从而自动生成了这段代码","link":"/2021/08/11/MapStruct%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/"},{"title":"MySQL（一）语法和数据类型","text":"DDL，英文叫做 Data Definition Language，也就是数据定义语言，它用来定义我们的数据库对象，包括数据库、数据表和列。通过使用 DDL，我们可以创建，删除和修改数据库和表结构。 DML，英文叫做 Data Manipulation Language，数据操作语言，我们用它操作和数据库相关的记录，比如增加、删除、修改数据表中的记录。 DCL，英文叫做 Data Control Language，数据控制语言，我们用它来定义访问权限和安全级别。 DQL，英文叫做 Data Query Language，数据查询语言，我们用它查询想要的记录，它是 SQL 语言的重中之重。 DDL数据定义语言123456789101112131415shengbinbin@192 ~ % mysql -uroot -p //登陆数据库Enter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 8Server version: 8.0.23 MySQL Community Server - GPLCopyright (c) 2000, 2021, Oracle and/or its affiliates.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; 12345678910111213141516171819mysql&gt; CREATE DATABASE test; //创建数据库Query OK, 1 row affected (0.00 sec)mysql&gt; CREATE DATABASE test;ERROR 1007 (HY000): Can't create database 'test'; database existsmysql&gt; show databases; //查看所有数据库+--------------------+| Database |+--------------------+| binshow || information_schema | // 存储了系统中的一些数据库对象信息，比如用户表信息，列信息，权限信息等等| mybatis || mysql | // mysql 存储了系统的用户权限信息| performance_schema || sys || test |+--------------------+7 rows in set (0.02 sec) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253mysql&gt; use test //选择数据库Database changedmysql&gt; show tables; //展示数据库中的表Empty set (0.00 sec)mysql&gt; use mysql;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+------------------------------------------------------+| Tables_in_mysql |+------------------------------------------------------+| columns_priv || component || db || default_roles || engine_cost || func || general_log || global_grants || gtid_executed || help_category || help_keyword || help_relation || help_topic || innodb_index_stats || innodb_table_stats || password_history || plugin || procs_priv || proxies_priv || replication_asynchronous_connection_failover || replication_asynchronous_connection_failover_managed || role_edges || server_cost || servers || slave_master_info || slave_relay_log_info || slave_worker_info || slow_log || tables_priv || time_zone || time_zone_leap_second || time_zone_name || time_zone_transition || time_zone_transition_type || user |+------------------------------------------------------+35 rows in set (0.01 sec)mysql&gt; 1234567891011121314151617mysql&gt; drop database test; //删除数据库Query OK, 0 rows affected (0.01 sec)mysql&gt; show databases;+--------------------+| Database |+--------------------+| binshow || information_schema || mybatis || mysql || performance_schema || sys |+--------------------+6 rows in set (0.01 sec)mysql&gt; 12345678910111213141516mysql&gt; create table emp(ename varchar(10),hiredate date,sal decimal(10,2),deptno int(2)); //创建表Query OK, 0 rows affected, 1 warning (0.01 sec)mysql&gt; desc emp; //查看表的结构+----------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------------+------+-----+---------+-------+| ename | varchar(10) | YES | | NULL | || hiredate | date | YES | | NULL | || sal | decimal(10,2) | YES | | NULL | || deptno | int | YES | | NULL | |+----------+---------------+------+-----+---------+-------+4 rows in set (0.01 sec)mysql&gt; drop table emp; //删除表Query OK, 0 rows affected (0.01 sec) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788mysql&gt; desc emp;+----------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------------+------+-----+---------+-------+| ename | varchar(10) | YES | | NULL | || hiredate | date | YES | | NULL | || sal | decimal(10,2) | YES | | NULL | || deptno | int | YES | | NULL | |+----------+---------------+------+-----+---------+-------+4 rows in set (0.00 sec)mysql&gt; alter table emp modify ename varchar(20); //修改表的结构类型Query OK, 0 rows affected (0.01 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc emp;+----------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------------+------+-----+---------+-------+| ename | varchar(20) | YES | | NULL | || hiredate | date | YES | | NULL | || sal | decimal(10,2) | YES | | NULL | || deptno | int | YES | | NULL | |+----------+---------------+------+-----+---------+-------+4 rows in set (0.01 sec)mysql&gt; alter table emp add column age int(3); //增加字段Query OK, 0 rows affected, 1 warning (0.01 sec)Records: 0 Duplicates: 0 Warnings: 1mysql&gt; desc emp;+----------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------------+------+-----+---------+-------+| ename | varchar(20) | YES | | NULL | || hiredate | date | YES | | NULL | || sal | decimal(10,2) | YES | | NULL | || deptno | int | YES | | NULL | || age | int | YES | | NULL | |+----------+---------------+------+-----+---------+-------+5 rows in set (0.00 sec)mysql&gt; alter table emp drop column age; //删除字段Query OK, 0 rows affected (0.01 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc emp;+----------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------------+------+-----+---------+-------+| ename | varchar(20) | YES | | NULL | || hiredate | date | YES | | NULL | || sal | decimal(10,2) | YES | | NULL | || deptno | int | YES | | NULL | |+----------+---------------+------+-----+---------+-------+4 rows in set (0.01 sec)mysql&gt; alter table emp add column age int(3);Query OK, 0 rows affected, 1 warning (0.01 sec)Records: 0 Duplicates: 0 Warnings: 1mysql&gt; alter table emp change age age1 int(4); //字段改名Query OK, 0 rows affected, 1 warning (0.01 sec)Records: 0 Duplicates: 0 Warnings: 1mysql&gt; desc emp;+----------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------------+------+-----+---------+-------+| ename | varchar(20) | YES | | NULL | || hiredate | date | YES | | NULL | || sal | decimal(10,2) | YES | | NULL | || deptno | int | YES | | NULL | || age1 | int | YES | | NULL | |+----------+---------------+------+-----+---------+-------+5 rows in set (0.00 sec)mysql&gt; alter table emp rename emp1; //修改表名称Query OK, 0 rows affected (0.01 sec)mysql&gt; show tables;+-------------------+| Tables_in_binshow |+-------------------+| emp1 |+-------------------+1 row in set (0.00 sec) DML数据操控语言插入数据1234567891011121314151617181920212223242526272829303132mysql&gt; insert into emp(ename , hiredate , sal ,deptno) values('binshow','2020-02-02','2000',2); //插入语句Query OK, 1 row affected (0.01 sec)mysql&gt; insert into emp(ename,sal) values('wb','1000'); //部分列显示插入数据Query OK, 1 row affected (0.00 sec)mysql&gt; select * from emp;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 2000.00 | 2 | NULL || wb | NULL | 1000.00 | NULL | NULL |+---------+------------+---------+--------+------+2 rows in set (0.00 sec)mysql&gt; insert into emp values('zkd','2020-02-04','3000',3,4); //不加要插入的列名称，但是要一一对应Query OK, 1 row affected (0.00 sec)mysql&gt; insert into emp values('zkd','2020-02-04','3000',3,4),('aaa','2020-01-04','4000',3,4); //直接插入多个数据Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; select * from emp;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 2000.00 | 2 | NULL || wb | NULL | 1000.00 | NULL | NULL || zkd | 2020-02-04 | 3000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 || aaa | 2020-01-04 | 4000.00 | 3 | 4 |+---------+------------+---------+--------+------+5 rows in set (0.00 sec)mysql&gt; 更新数据1234567891011121314151617181920212223242526272829mysql&gt; select * from emp;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 2000.00 | 2 | NULL || wb | NULL | 1000.00 | NULL | NULL || zkd | 2020-02-04 | 3000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 || aaa | 2020-01-04 | 4000.00 | 3 | 4 |+---------+------------+---------+--------+------+5 rows in set (0.00 sec)mysql&gt; update emp set sal = 5000 where ename = 'binshow'; //更新语句Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select * from emp;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 5000.00 | 2 | NULL || wb | NULL | 1000.00 | NULL | NULL || zkd | 2020-02-04 | 3000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 || aaa | 2020-01-04 | 4000.00 | 3 | 4 |+---------+------------+---------+--------+------+5 rows in set (0.00 sec)mysql&gt; 123456789101112131415161718192021222324mysql&gt; create table dept(deptno int, deptname varchar(10));Query OK, 0 rows affected (0.01 sec)mysql&gt; desc dept;+----------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+-------------+------+-----+---------+-------+| deptno | int | YES | | NULL | || deptname | varchar(10) | YES | | NULL | |+----------+-------------+------+-----+---------+-------+2 rows in set (0.00 sec)mysql&gt; insert into dept values(1,'tech'),(2,'sale'),(5,'fin');Query OK, 3 rows affected (0.01 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; select * from dept;+--------+----------+| deptno | deptname |+--------+----------+| 1 | tech || 2 | sale || 5 | fin |+--------+----------+3 rows in set (0.00 sec)mysql&gt; 删除数据123456789101112131415mysql&gt; delete from emp where ename = 'wb';Query OK, 1 row affected (0.00 sec)mysql&gt; select * from emp;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 5000.00 | 2 | NULL || zkd | 2020-02-04 | 3000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 || aaa | 2020-01-04 | 4000.00 | 3 | 4 |+---------+------------+---------+--------+------+4 rows in set (0.00 sec)mysql&gt; 查询数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859mysql&gt; select distinct deptno from emp; //查询不重复的数据+--------+ | deptno |+--------+| 2 || 3 |+--------+2 rows in set (0.00 sec)mysql&gt; select * from emp where sal = '5000'; //条件查询+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 5000.00 | 2 | NULL |+---------+------------+---------+--------+------+1 row in set (0.00 sec)mysql&gt; select * from emp order by sal; +---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| zkd | 2020-02-04 | 3000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 || aaa | 2020-01-04 | 4000.00 | 3 | 4 || binshow | 2020-02-02 | 5000.00 | 2 | NULL |+---------+------------+---------+--------+------+4 rows in set (0.00 sec)mysql&gt; select * from emp order by deptno,sal desc;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 5000.00 | 2 | NULL || aaa | 2020-01-04 | 4000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 |+---------+------------+---------+--------+------+4 rows in set (0.00 sec)mysql&gt; select * from emp order by deptno,sal desc limit 2;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 5000.00 | 2 | NULL || aaa | 2020-01-04 | 4000.00 | 3 | 4 |+---------+------------+---------+--------+------+2 rows in set (0.00 sec)mysql&gt; select * from emp order by deptno,sal desc limit 1,2;+-------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+-------+------------+---------+--------+------+| aaa | 2020-01-04 | 4000.00 | 3 | 4 || zkd | 2020-02-04 | 3000.00 | 3 | 4 |+-------+------------+---------+--------+------+2 rows in set (0.00 sec)mysql&gt; 聚合操作的顺序问题1select * from table a where id = 1 group by age having xxx; 一条完整的 SELECT 语句内部的执行顺序是这样的： FROM 子句组装数据（包括通过 ON 进行连接）； WHERE 子句进行条件筛选； GROUP BY 分组 ； 使用聚集函数进行计算； HAVING 筛选分组； 计算所有的表达式； SELECT 的字段； ORDER BY 排序； LIMIT 筛选。 having 和 where的区别：having是对聚合后的结果进行条件的过滤，而where是聚合前就对记录过滤 12345678910111213141516171819202122232425262728293031323334mysql&gt; select count(1) from emp;+----------+| count(1) |+----------+| 4 |+----------+1 row in set (0.01 sec)mysql&gt; select deptno,count(1) from emp group by deptno;+--------+----------+| deptno | count(1) |+--------+----------+| 2 | 2 || 3 | 1 || 1 | 1 |+--------+----------+3 rows in set (0.00 sec)mysql&gt; select deptno,count(1) from emp group by deptno having count(1)&gt;1; //统计人数大于1的部门+--------+----------+| deptno | count(1) |+--------+----------+| 2 | 2 |+--------+----------+1 row in set (0.00 sec)mysql&gt; select sum(sal),max(sal),min(sal) from emp; //统计薪水总和，最大薪水，最小薪水+----------+----------+----------+| sum(sal) | max(sal) | min(sal) |+----------+----------+----------+| 16000.00 | 6000.00 | 1000.00 |+----------+----------+----------+1 row in set (0.00 sec)mysql&gt; 表连接 内连接和外连接的区别：内连接仅仅选出两个表中相互匹配的记录。 左连接和右连接的区别： 左连接指包含所有左表中的记录甚至是右边表中没有和她匹配的记录。 12345678910111213141516171819202122232425262728293031323334mysql&gt; select * from emp;+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 5000.00 | 2 | NULL || aaa | 2020-01-04 | 4000.00 | 3 | 4 || sbb | 2018-02-01 | 6000.00 | 1 | 18 || bbb | 2018-04-11 | 1000.00 | 2 | 36 |+---------+------------+---------+--------+------+4 rows in set (0.00 sec)mysql&gt; select * from dept;+--------+----------+| deptno | deptname |+--------+----------+| 1 | tech || 2 | sale || 5 | fin || 3 | hr |+--------+----------+4 rows in set (0.00 sec)mysql&gt; select ename ,deptname from emp , dept where emp.deptno = dept.deptno;+---------+----------+| ename | deptname |+---------+----------+| sbb | tech || bbb | sale || binshow | sale || aaa | hr |+---------+----------+4 rows in set (0.00 sec)mysql&gt; 12345678910111213141516171819202122232425262728mysql&gt; insert into emp values('ccc','2010-02-03','10000',4,45);Query OK, 1 row affected (0.00 sec) //查询emp中所有用户名和所在部门名称mysql&gt; select ename , deptname from emp left join dept on emp.deptno = dept.deptno;+---------+----------+| ename | deptname |+---------+----------+| binshow | sale || aaa | hr || sbb | tech || bbb | sale || ccc | NULL |+---------+----------+5 rows in set (0.01 sec)mysql&gt; select ename , deptname from dept right join emp on emp.deptno = dept.deptno; //和上面相同+---------+----------+| ename | deptname |+---------+----------+| binshow | sale || aaa | hr || sbb | tech || bbb | sale || ccc | NULL |+---------+----------+5 rows in set (0.00 sec)mysql&gt; 子查询123456789101112mysql&gt; select * from emp where deptno in(select deptno from dept);+---------+------------+---------+--------+------+| ename | hiredate | sal | deptno | age1 |+---------+------------+---------+--------+------+| binshow | 2020-02-02 | 5000.00 | 2 | NULL || aaa | 2020-01-04 | 4000.00 | 3 | 4 || sbb | 2018-02-01 | 6000.00 | 1 | 18 || bbb | 2018-04-11 | 1000.00 | 2 | 36 |+---------+------------+---------+--------+------+4 rows in set (0.00 sec)mysql&gt; Union联合123456789101112131415161718192021222324252627282930313233mysql&gt; select deptno from emp -&gt; union -&gt; select deptno from dept; //去重了+--------+| deptno |+--------+| 2 || 3 || 1 || 4 || 5 |+--------+5 rows in set (0.00 sec)mysql&gt; select deptno from emp -&gt; union all -&gt; select deptno from dept;+--------+| deptno |+--------+| 2 || 3 || 1 || 2 || 4 || 1 || 2 || 5 || 3 |+--------+9 rows in set (0.00 sec)mysql&gt; DCL数据控制语言暂无 数据类型/运算符/常用函数待补充","link":"/2021/05/11/MySQL%EF%BC%88%E4%B8%80%EF%BC%89%E8%AF%AD%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"},{"title":"MySQL（七）explain详解","text":"explain详解~ Explain 详解（上）前文说过：一条查询语句经过MySQL查询优化器的各种基于成本和规则的优化之后会生成一个执行计划。 这个执行计划就是具体执行查询的方式：比如多表连接的顺序、对每个表采用什么访问方案等。这些执行计划可以通过explain来查看： 除了 select开头的sql语句，其余的delete、insert和update等语句前面都可以加上explain这个词。 EXPLAIN语句输出的各个列的作用先大致罗列一下： 列名 描述 id 在一个大的查询语句中每个SELECT关键字都对应一个唯一的id select_type SELECT关键字对应的那个查询的类型 table 表名 partitions 匹配的分区信息 type 针对单表的访问方法 possible_keys 可能用到的索引 key 实际上使用的索引 key_len 实际使用到的索引长度 ref 当使用索引列等值查询时，与索引列进行等值匹配的对象信息 rows 预估的需要读取的记录条数 filtered 某个表经过搜索条件过滤后剩余记录条数的百分比 Extra 一些额外的信息 123456789101112131415CREATE TABLE single_table ( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1), UNIQUE KEY idx_key2 (key2), KEY idx_key3 (key3), KEY idx_key_part(key_part1, key_part2, key_part3)) Engine=InnoDB CHARSET=utf8; 我们仍然假设有两个和single_table表构造一模一样的s1、s2表，而且这两个表里边儿有10000条记录，除id列外其余的列都插入随机值。为了让大家有比较好的阅读体验，我们下边并不准备严格按照EXPLAIN输出列的顺序来介绍这些列分别是干嘛的，大家注意一下就好了。 执行计划输出中各列详解table不论我们的查询语句有多复杂，里边儿包含了多少个表，到最后也是需要对每个表进行单表访问的，所以设计MySQL的大叔规定EXPLAIN语句输出的每条记录都对应着某个单表的访问方法，该条记录的table列代表着该表的表名。所以我们看一条比较简单的查询语句： 1234567mysql&gt; EXPLAIN SELECT * FROM s1;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+1 row in set, 1 warning (0.00 sec) 这个查询语句只涉及对s1表的单表查询，所以EXPLAIN输出中只有一条记录，其中的table列的值是s1，表明这条记录是用来说明对s1表的单表访问方法的。 下边我们看一下一个连接查询的执行计划： 12345678mysql&gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+| 1 | SIMPLE | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | NULL || 1 | SIMPLE | s2 | NULL | ALL | NULL | NULL | NULL | NULL | 9954 | 100.00 | Using join buffer (Block Nested Loop) |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+2 rows in set, 1 warning (0.01 sec) 可以看到这个连接查询的执行计划中有两条记录，这两条记录的table列分别是s1和s2，这两条记录用来分别说明对s1表和s2表的访问方法是什么。 id我们知道我们写的查询语句一般都以SELECT关键字开头，比较简单的查询语句里只有一个SELECT关键字，比如下边这个查询语句： 1SELECT * FROM s1 WHERE key1 = 'a'; 稍微复杂一点的连接查询中也只有一个SELECT关键字，比如： 123SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key1 WHERE s1.common_field = 'a'; 但是下边两种情况下在一条查询语句中会出现多个SELECT关键字： 查询中包含子查询的情况 比如下边这个查询语句中就包含2个SELECT关键字： 12SELECT * FROM s1 WHERE key1 IN (SELECT * FROM s2); 查询中包含UNION语句的情况 比如下边这个查询语句中也包含2个SELECT关键字： 1SELECT * FROM s1 UNION SELECT * FROM s2; 查询语句中每出现一个SELECT关键字，设计MySQL的大叔就会为它分配一个唯一的id值。这个id值就是EXPLAIN语句的第一个列，比如下边这个查询中只有一个SELECT关键字，所以EXPLAIN的结果中也就只有一条id列为1的记录： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 = 'a';+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | ref | idx_key1 | idx_key1 | 303 | const | 8 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+1 row in set, 1 warning (0.03 sec) 对于连接查询来说，一个SELECT关键字后边的FROM子句中可以跟随多个表，所以在连接查询的执行计划中，每个表都会对应一条记录，但是这些记录的id值都是相同的，比如： 1mysql&gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+| 1 | SIMPLE | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | NULL || 1 | SIMPLE | s2 | NULL | ALL | NULL | NULL | NULL | NULL | 9954 | 100.00 | Using join buffer (Block Nested Loop) |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+2 rows in set, 1 warning (0.01 sec) 可以看到，上述连接查询中参与连接的s1和s2表分别对应一条记录，但是这两条记录对应的id值都是1。这里需要大家记住的是，在连接查询的执行计划中，每个表都会对应一条记录，这些记录的id列的值是相同的，出现在前边的表表示驱动表，出现在后边的表表示被驱动表。所以从上边的EXPLAIN输出中我们可以看出，查询优化器准备让s1表作为驱动表，让s2表作为被驱动表来执行查询。 对于包含子查询的查询语句来说，就可能涉及多个SELECT关键字，所以在包含子查询的查询语句的执行计划中，每个SELECT关键字都会对应一个唯一的id值，比如这样： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2) OR key3 = 'a';+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+| 1 | PRIMARY | s1 | NULL | ALL | idx_key3 | NULL | NULL | NULL | 9688 | 100.00 | Using where || 2 | SUBQUERY | s2 | NULL | index | idx_key1 | idx_key1 | 303 | NULL | 9954 | 100.00 | Using index |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+2 rows in set, 1 warning (0.02 sec) 从输出结果中我们可以看到，s1表在外层查询中，外层查询有一个独立的SELECT关键字，所以第一条记录的id值就是1，s2表在子查询中，子查询有一个独立的SELECT关键字，所以第二条记录的id值就是2。 但是这里大家需要特别注意，查询优化器可能对涉及子查询的查询语句进行重写，从而转换为连接查询。所以如果我们想知道查询优化器对某个包含子查询的语句是否进行了重写，直接查看执行计划就好了，比如说： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key3 FROM s2 WHERE common_field = 'a');+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+------------------------------+| 1 | SIMPLE | s2 | NULL | ALL | idx_key3 | NULL | NULL | NULL | 9954 | 10.00 | Using where; Start temporary || 1 | SIMPLE | s1 | NULL | ref | idx_key1 | idx_key1 | 303 | xiaohaizi.s2.key3 | 1 | 100.00 | End temporary |+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+------------------------------+2 rows in set, 1 warning (0.00 sec) 可以看到，虽然我们的查询语句是一个子查询，但是执行计划中s1和s2表对应的记录的id值全部是1，这就表明了查询优化器将子查询转换为了连接查询。 对于包含UNION子句的查询语句来说，每个SELECT关键字对应一个id值也是没错的，不过还是有点儿特别的东西，比方说下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 UNION SELECT * FROM s2;+----+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+| 1 | PRIMARY | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | NULL || 2 | UNION | s2 | NULL | ALL | NULL | NULL | NULL | NULL | 9954 | 100.00 | NULL || NULL | UNION RESULT | &lt;union1,2&gt; | NULL | ALL | NULL | NULL | NULL | NULL | NULL | NULL | Using temporary |+----+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+3 rows in set, 1 warning (0.00 sec) 这个语句的执行计划的第三条记录是个什么鬼？为毛id值是NULL，而且table列长的也怪怪的？大家别忘了UNION子句是干嘛用的，它会把多个查询的结果集合并起来并对结果集中的记录进行去重，怎么去重呢？MySQL使用的是内部的临时表。正如上边的查询计划中所示，UNION子句是为了把id为1的查询和id为2的查询的结果集合并起来并去重，所以在内部创建了一个名为&lt;union1, 2&gt;的临时表（就是执行计划第三条记录的table列的名称），id为NULL表明这个临时表是为了合并两个查询的结果集而创建的。 跟UNION对比起来，UNION ALL就不需要为最终的结果集进行去重，它只是单纯的把多个查询的结果集中的记录合并成一个并返回给用户，所以也就不需要使用临时表。所以在包含UNION ALL子句的查询的执行计划中，就没有那个id为NULL的记录，如下所示： 1mysql&gt; EXPLAIN SELECT * FROM s1 UNION ALL SELECT * FROM s2;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+| 1 | PRIMARY | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | NULL || 2 | UNION | s2 | NULL | ALL | NULL | NULL | NULL | NULL | 9954 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+2 rows in set, 1 warning (0.01 sec) select_type通过上边的内容我们知道，一条大的查询语句里边可以包含若干个SELECT关键字，每个SELECT关键字代表着一个小的查询语句，而每个SELECT关键字的FROM子句中都可以包含若干张表（这些表用来做连接查询），每一张表都对应着执行计划输出中的一条记录，对于在同一个SELECT关键字中的表来说，它们的id值是相同的。 设计MySQL的大叔为每一个SELECT关键字代表的小查询都定义了一个称之为select_type的属性，意思是我们只要知道了某个小查询的select_type属性，就知道了这个小查询在整个大查询中扮演了一个什么角色，口说无凭，我们还是先来见识见识这个select_type都能取哪些值（为了精确起见，我们直接使用文档中的英文做简要描述，随后会进行详细解释的）： 名称 描述 SIMPLE Simple SELECT (not using UNION or subqueries) PRIMARY Outermost SELECT UNION Second or later SELECT statement in a UNION UNION RESULT Result of a UNION SUBQUERY First SELECT in subquery DEPENDENT SUBQUERY First SELECT in subquery, dependent on outer query DEPENDENT UNION Second or later SELECT statement in a UNION, dependent on outer query DERIVED Derived table MATERIALIZED Materialized subquery UNCACHEABLE SUBQUERY A subquery for which the result cannot be cached and must be re-evaluated for each row of the outer query UNCACHEABLE UNION The second or later select in a UNION that belongs to an uncacheable subquery (see UNCACHEABLE SUBQUERY) 英文描述太简单，不知道说了啥？来详细瞅瞅里边儿的每个值都是干啥吃的： SIMPLE 查询语句中不包含UNION或者子查询的查询都算作是SIMPLE类型，比方说下边这个单表查询的select_type的值就是SIMPLE： 1mysql&gt; EXPLAIN SELECT * FROM s1;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+1 row in set, 1 warning (0.00 sec) 当然，连接查询也算是SIMPLE类型，比如： 1mysql&gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+| 1 | SIMPLE | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | NULL || 1 | SIMPLE | s2 | NULL | ALL | NULL | NULL | NULL | NULL | 9954 | 100.00 | Using join buffer (Block Nested Loop) |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+2 rows in set, 1 warning (0.01 sec) PRIMARY 对于包含UNION、UNION ALL或者子查询的大查询来说，它是由几个小查询组成的，其中最左边的那个查询的select_type值就是PRIMARY，比方说： 1mysql&gt; EXPLAIN SELECT * FROM s1 UNION SELECT * FROM s2;+----+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+| 1 | PRIMARY | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | NULL || 2 | UNION | s2 | NULL | ALL | NULL | NULL | NULL | NULL | 9954 | 100.00 | NULL || NULL | UNION RESULT | &lt;union1,2&gt; | NULL | ALL | NULL | NULL | NULL | NULL | NULL | NULL | Using temporary |+----+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+3 rows in set, 1 warning (0.00 sec) 从结果中可以看到，最左边的小查询SELECT * FROM s1对应的是执行计划中的第一条记录，它的select_type值就是PRIMARY。 UNION 对于包含UNION或者UNION ALL的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询以外，其余的小查询的select_type值就是UNION，可以对比上一个例子的效果，这就不多举例子了。 UNION RESULT MySQL选择使用临时表来完成UNION查询的去重工作，针对该临时表的查询的select_type就是UNION RESULT，例子上边有，就不赘述了。 SUBQUERY 如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是不相关子查询，并且查询优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个SELECT关键字代表的那个查询的select_type就是SUBQUERY，比如下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2) OR key3 = 'a';+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+| 1 | PRIMARY | s1 | NULL | ALL | idx_key3 | NULL | NULL | NULL | 9688 | 100.00 | Using where || 2 | SUBQUERY | s2 | NULL | index | idx_key1 | idx_key1 | 303 | NULL | 9954 | 100.00 | Using index |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+2 rows in set, 1 warning (0.00 sec) 可以看到，外层查询的select_type就是PRIMARY，子查询的select_type就是SUBQUERY。需要大家注意的是，由于select_type为SUBQUERY的子查询由于会被物化，所以只需要执行一遍。 DEPENDENT SUBQUERY 如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是相关子查询，则该子查询的第一个SELECT关键字代表的那个查询的select_type就是DEPENDENT SUBQUERY，比如下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2 WHERE s1.key2 = s2.key2) OR key3 = 'a';+----+--------------------+-------+------------+------+-------------------+----------+---------+-------------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------------+-------+------------+------+-------------------+----------+---------+-------------------+------+----------+-------------+| 1 | PRIMARY | s1 | NULL | ALL | idx_key3 | NULL | NULL | NULL | 9688 | 100.00 | Using where || 2 | DEPENDENT SUBQUERY | s2 | NULL | ref | idx_key2,idx_key1 | idx_key2 | 5 | xiaohaizi.s1.key2 | 1 | 10.00 | Using where |+----+--------------------+-------+------------+------+-------------------+----------+---------+-------------------+------+----------+-------------+2 rows in set, 2 warnings (0.00 sec) 需要大家注意的是，select_type为DEPENDENT SUBQUERY的查询可能会被执行多次。 DEPENDENT UNION 在包含UNION或者UNION ALL的大查询中，如果各个小查询都依赖于外层查询的话，那除了最左边的那个小查询之外，其余的小查询的select_type的值就是DEPENDENT UNION。说的有些绕哈，比方说下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2 WHERE key1 = 'a' UNION SELECT key1 FROM s1 WHERE key1 = 'b');+----+--------------------+------------+------------+------+---------------+----------+---------+-------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------------+------------+------------+------+---------------+----------+---------+-------+------+----------+--------------------------+| 1 | PRIMARY | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | Using where || 2 | DEPENDENT SUBQUERY | s2 | NULL | ref | idx_key1 | idx_key1 | 303 | const | 12 | 100.00 | Using where; Using index || 3 | DEPENDENT UNION | s1 | NULL | ref | idx_key1 | idx_key1 | 303 | const | 8 | 100.00 | Using where; Using index || NULL | UNION RESULT | &lt;union2,3&gt; | NULL | ALL | NULL | NULL | NULL | NULL | NULL | NULL | Using temporary |+----+--------------------+------------+------------+------+---------------+----------+---------+-------+------+----------+--------------------------+4 rows in set, 1 warning (0.03 sec) 这个查询比较复杂啊，大查询里包含了一个子查询，子查询里又是由UNION连起来的两个小查询。从执行计划中可以看出来，SELECT key1 FROM s2 WHERE key1 = 'a'这个小查询由于是子查询中第一个查询，所以它的select_type是DEPENDENT SUBQUERY，而SELECT key1 FROM s1 WHERE key1 = 'b'这个查询的select_type就是DEPENDENT UNION。 DERIVED 对于采用物化的方式执行的包含派生表的查询，该派生表对应的子查询的select_type就是DERIVED，比方说下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM (SELECT key1, count(*) as c FROM s1 GROUP BY key1) AS derived_s1 where c &gt; 1;+----+-------------+------------+------------+-------+---------------+----------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+------------+------------+-------+---------------+----------+---------+------+------+----------+-------------+| 1 | PRIMARY | &lt;derived2&gt; | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 33.33 | Using where || 2 | DERIVED | s1 | NULL | index | idx_key1 | idx_key1 | 303 | NULL | 9688 | 100.00 | Using index |+----+-------------+------------+------------+-------+---------------+----------+---------+------+------+----------+-------------+2 rows in set, 1 warning (0.00 sec) 从执行计划中可以看出，id为2的记录就代表子查询的执行方式，它的select_type是DERIVED，说明该子查询是以物化的方式执行的。id为1的记录代表外层查询，大家注意看它的table列显示的是&lt;derived2&gt;，表示该查询是针对将派生表物化之后的表进行查询的。 1小贴士：如果派生表可以通过和外层查询合并的方式执行的话，执行计划又是另一番景象，大家可以试试哈～ MATERIALIZED 当查询优化器在执行包含子查询的语句时，选择将子查询物化之后与外层查询进行连接查询时，该子查询对应的select_type属性就是MATERIALIZED，比如下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2);+----+--------------+-------------+------------+--------+---------------+------------+---------+-------------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------+-------------+------------+--------+---------------+------------+---------+-------------------+------+----------+-------------+| 1 | SIMPLE | s1 | NULL | ALL | idx_key1 | NULL | NULL | NULL | 9688 | 100.00 | Using where || 1 | SIMPLE | &lt;subquery2&gt; | NULL | eq_ref | &lt;auto_key&gt; | &lt;auto_key&gt; | 303 | xiaohaizi.s1.key1 | 1 | 100.00 | NULL || 2 | MATERIALIZED | s2 | NULL | index | idx_key1 | idx_key1 | 303 | NULL | 9954 | 100.00 | Using index |+----+--------------+-------------+------------+--------+---------------+------------+---------+-------------------+------+----------+-------------+3 rows in set, 1 warning (0.01 sec) 执行计划的第三条记录的id值为2，说明该条记录对应的是一个单表查询，从它的select_type值为MATERIALIZED可以看出，查询优化器是要把子查询先转换成物化表。然后看执行计划的前两条记录的id值都为1，说明这两条记录对应的表进行连接查询，需要注意的是第二条记录的table列的值是&lt;subquery2&gt;，说明该表其实就是id为2对应的子查询执行之后产生的物化表，然后将s1和该物化表进行连接查询。 UNCACHEABLE SUBQUERY 不常用，就不多唠叨了。 UNCACHEABLE UNION 不常用，就不多唠叨了。 partitions由于我们压根儿就没唠叨过分区是个啥，所以这个输出列我们也就不说了哈，一般情况下我们的查询语句的执行计划的partitions列的值都是NULL。 type我们前边说过执行计划的一条记录就代表着MySQL对某个表的执行查询时的访问方法，其中的type列就表明了这个访问方法是个啥，比方说下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 = 'a';+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | ref | idx_key1 | idx_key1 | 303 | const | 8 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+1 row in set, 1 warning (0.04 sec) 可以看到type列的值是ref，表明MySQL即将使用ref访问方法来执行对s1表的查询。但是我们之前只唠叨过对使用InnoDB存储引擎的表进行单表访问的一些访问方法，完整的访问方法如下：system，const，eq_ref，ref，fulltext，ref_or_null，index_merge，unique_subquery，index_subquery，range，index，ALL。当然我们还要详细唠叨一下哈： system 当表中只有一条记录并且该表使用的存储引擎的统计数据是精确的，比如MyISAM、Memory，那么对该表的访问方法就是system。比方说我们新建一个MyISAM表，并为其插入一条记录： 1mysql&gt; CREATE TABLE t(i int) Engine=MyISAM;Query OK, 0 rows affected (0.05 sec)mysql&gt; INSERT INTO t VALUES(1);Query OK, 1 row affected (0.01 sec) 然后我们看一下查询这个表的执行计划： 1mysql&gt; EXPLAIN SELECT * FROM t;+----+-------------+-------+------------+--------+---------------+------+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+------+---------+------+------+----------+-------+| 1 | SIMPLE | t | NULL | system | NULL | NULL | NULL | NULL | 1 | 100.00 | NULL |+----+-------------+-------+------------+--------+---------------+------+---------+------+------+----------+-------+1 row in set, 1 warning (0.00 sec) 可以看到type列的值就是system了。 1小贴士：你可以把表改成使用InnoDB存储引擎，试试看执行计划的type列是什么。 const 这个我们前边唠叨过，就是当我们根据主键或者唯一二级索引列与常数进行等值匹配时，对单表的访问方法就是const，比如： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE id = 5;+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | const | PRIMARY | PRIMARY | 4 | const | 1 | 100.00 | NULL |+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------+1 row in set, 1 warning (0.01 sec) eq_ref 在连接查询时，如果被驱动表是通过主键或者唯一二级索引列等值匹配的方式进行访问的（如果该主键或者唯一二级索引是联合索引的话，所有的索引列都必须进行等值比较），则对该被驱动表的访问方法就是eq_ref，比方说： 1mysql&gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.id = s2.id;+----+-------------+-------+------------+--------+---------------+---------+---------+-----------------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+---------+---------+-----------------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | ALL | PRIMARY | NULL | NULL | NULL | 9688 | 100.00 | NULL || 1 | SIMPLE | s2 | NULL | eq_ref | PRIMARY | PRIMARY | 4 | xiaohaizi.s1.id | 1 | 100.00 | NULL |+----+-------------+-------+------------+--------+---------------+---------+---------+-----------------+------+----------+-------+2 rows in set, 1 warning (0.01 sec) 从执行计划的结果中可以看出，MySQL打算将s1作为驱动表，s2作为被驱动表，重点关注s2的访问方法是eq_ref，表明在访问s2表的时候可以通过主键的等值匹配来进行访问。 ref 当通过普通的二级索引列与常量进行等值匹配时来查询某个表，那么对该表的访问方法就可能是ref，最开始举过例子了，就不重复举例了。 fulltext 全文索引，我们没有细讲过，跳过～ ref_or_null 当对普通二级索引进行等值匹配查询，该索引列的值也可以是NULL值时，那么对该表的访问方法就可能是ref_or_null，比如说： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 = 'a' OR key1 IS NULL;+----+-------------+-------+------------+-------------+---------------+----------+---------+-------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------------+---------------+----------+---------+-------+------+----------+-----------------------+| 1 | SIMPLE | s1 | NULL | ref_or_null | idx_key1 | idx_key1 | 303 | const | 9 | 100.00 | Using index condition |+----+-------------+-------+------------+-------------+---------------+----------+---------+-------+------+----------+-----------------------+1 row in set, 1 warning (0.01 sec) index_merge 一般情况下对于某个表的查询只能使用到一个索引，但我们唠叨单表访问方法时特意强调了在某些场景下可以使用Intersection、Union、Sort-Union这三种索引合并的方式来执行查询，忘掉的回去补一下哈，我们看一下执行计划中是怎么体现MySQL使用索引合并的方式来对某个表执行查询的： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 = 'a' OR key3 = 'a';+----+-------------+-------+------------+-------------+-------------------+-------------------+---------+------+------+----------+---------------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------------+-------------------+-------------------+---------+------+------+----------+---------------------------------------------+| 1 | SIMPLE | s1 | NULL | index_merge | idx_key1,idx_key3 | idx_key1,idx_key3 | 303,303 | NULL | 14 | 100.00 | Using union(idx_key1,idx_key3); Using where |+----+-------------+-------+------------+-------------+-------------------+-------------------+---------+------+------+----------+---------------------------------------------+1 row in set, 1 warning (0.01 sec) 从执行计划的type列的值是index_merge就可以看出，MySQL打算使用索引合并的方式来执行对s1表的查询。 unique_subquery 类似于两表连接中被驱动表的eq_ref访问方法，unique_subquery是针对在一些包含IN子查询的查询语句中，如果查询优化器决定将IN子查询转换为EXISTS子查询，而且子查询可以使用到主键进行等值匹配的话，那么该子查询执行计划的type列的值就是unique_subquery，比如下边的这个查询语句： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key2 IN (SELECT id FROM s2 where s1.key1 = s2.key1) OR key3 = 'a';+----+--------------------+-------+------------+-----------------+------------------+---------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------------+-------+------------+-----------------+------------------+---------+---------+------+------+----------+-------------+| 1 | PRIMARY | s1 | NULL | ALL | idx_key3 | NULL | NULL | NULL | 9688 | 100.00 | Using where || 2 | DEPENDENT SUBQUERY | s2 | NULL | unique_subquery | PRIMARY,idx_key1 | PRIMARY | 4 | func | 1 | 10.00 | Using where |+----+--------------------+-------+------------+-----------------+------------------+---------+---------+------+------+----------+-------------+2 rows in set, 2 warnings (0.00 sec) 可以看到执行计划的第二条记录的type值就是unique_subquery，说明在执行子查询时会使用到id列的索引。 index_subquery index_subquery与unique_subquery类似，只不过访问子查询中的表时使用的是普通的索引，比如这样： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE common_field IN (SELECT key3 FROM s2 where s1.key1 = s2.key1) OR key3 = 'a';+----+--------------------+-------+------------+----------------+-------------------+----------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------------+-------+------------+----------------+-------------------+----------+---------+------+------+----------+-------------+| 1 | PRIMARY | s1 | NULL | ALL | idx_key3 | NULL | NULL | NULL | 9688 | 100.00 | Using where || 2 | DEPENDENT SUBQUERY | s2 | NULL | index_subquery | idx_key1,idx_key3 | idx_key3 | 303 | func | 1 | 10.00 | Using where |+----+--------------------+-------+------------+----------------+-------------------+----------+---------+------+------+----------+-------------+2 rows in set, 2 warnings (0.01 sec) range 如果使用索引获取某些范围区间的记录，那么就可能使用到range访问方法，比如下边的这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN ('a', 'b', 'c');+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | s1 | NULL | range | idx_key1 | idx_key1 | 303 | NULL | 27 | 100.00 | Using index condition |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-----------------------+1 row in set, 1 warning (0.01 sec) 或者： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 &gt; 'a' AND key1 &lt; 'b';+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | s1 | NULL | range | idx_key1 | idx_key1 | 303 | NULL | 294 | 100.00 | Using index condition |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-----------------------+1 row in set, 1 warning (0.00 sec) index 当我们可以使用索引覆盖，但需要扫描全部的索引记录时，该表的访问方法就是index，比如这样： 1mysql&gt; EXPLAIN SELECT key_part2 FROM s1 WHERE key_part3 = 'a';+----+-------------+-------+------------+-------+---------------+--------------+---------+------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+--------------+---------+------+------+----------+--------------------------+| 1 | SIMPLE | s1 | NULL | index | NULL | idx_key_part | 909 | NULL | 9688 | 10.00 | Using where; Using index |+----+-------------+-------+------------+-------+---------------+--------------+---------+------+------+----------+--------------------------+1 row in set, 1 warning (0.00 sec) 上述查询中的搜索列表中只有key_part2一个列，而且搜索条件中也只有key_part3一个列，这两个列又恰好包含在idx_key_part这个索引中，可是搜索条件key_part3不能直接使用该索引进行ref或者range方式的访问，只能扫描整个idx_key_part索引的记录，所以查询计划的type列的值就是index。 1小贴士：再一次强调，对于使用InnoDB存储引擎的表来说，二级索引的记录只包含索引列和主键列的值，而聚簇索引中包含用户定义的全部列以及一些隐藏列，所以扫描二级索引的代价比直接全表扫描，也就是扫描聚簇索引的代价更低一些。 ALL 最熟悉的全表扫描，就不多唠叨了，直接看例子： 1mysql&gt; EXPLAIN SELECT * FROM s1;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+1 row in set, 1 warning (0.00 sec) 一般来说，这些访问方法按照我们介绍它们的顺序性能依次变差。其中除了All这个访问方法外，其余的访问方法都能用到索引，除了index_merge访问方法外，其余的访问方法都最多只能用到一个索引。 possible_keys和key在EXPLAIN语句输出的执行计划中，possible_keys列表示在某个查询语句中，对某个表执行单表查询时可能用到的索引有哪些，key列表示实际用到的索引有哪些，比方说下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 &gt; 'z' AND key3 = 'a';+----+-------------+-------+------------+------+-------------------+----------+---------+-------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+-------------------+----------+---------+-------+------+----------+-------------+| 1 | SIMPLE | s1 | NULL | ref | idx_key1,idx_key3 | idx_key3 | 303 | const | 6 | 2.75 | Using where |+----+-------------+-------+------------+------+-------------------+----------+---------+-------+------+----------+-------------+1 row in set, 1 warning (0.01 sec) 上述执行计划的possible_keys列的值是idx_key1,idx_key3，表示该查询可能使用到idx_key1,idx_key3两个索引，然后key列的值是idx_key3，表示经过查询优化器计算使用不同索引的成本后，最后决定使用idx_key3来执行查询比较划算。 不过有一点比较特别，就是在使用index访问方法来查询某个表时，possible_keys列是空的，而key列展示的是实际使用到的索引，比如这样： 1mysql&gt; EXPLAIN SELECT key_part2 FROM s1 WHERE key_part3 = 'a';+----+-------------+-------+------------+-------+---------------+--------------+---------+------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+--------------+---------+------+------+----------+--------------------------+| 1 | SIMPLE | s1 | NULL | index | NULL | idx_key_part | 909 | NULL | 9688 | 10.00 | Using where; Using index |+----+-------------+-------+------------+-------+---------------+--------------+---------+------+------+----------+--------------------------+1 row in set, 1 warning (0.00 sec) 另外需要注意的一点是，possible_keys列中的值并不是越多越好，可能使用的索引越多，查询优化器计算查询成本时就得花费更长时间，所以如果可以的话，尽量删除那些用不到的索引。 key_lenkey_len列表示当优化器决定使用某个索引执行查询时，该索引记录的最大长度，它是由这三个部分构成的： 对于使用固定长度类型的索引列来说，它实际占用的存储空间的最大长度就是该固定值，对于指定字符集的变长类型的索引列来说，比如某个索引列的类型是VARCHAR(100)，使用的字符集是utf8，那么该列实际占用的最大存储空间就是100 × 3 = 300个字节。 如果该索引列可以存储NULL值，则key_len比不可以存储NULL值时多1个字节。 对于变长字段来说，都会有2个字节的空间来存储该变长列的实际长度。 比如下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE id = 5;+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | const | PRIMARY | PRIMARY | 4 | const | 1 | 100.00 | NULL |+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------+1 row in set, 1 warning (0.01 sec) 由于id列的类型是INT，并且不可以存储NULL值，所以在使用该列的索引时key_len大小就是4。当索引列可以存储NULL值时，比如： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key2 = 5;+----+-------------+-------+------------+-------+---------------+----------+---------+-------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+----------+---------+-------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | const | idx_key2 | idx_key2 | 5 | const | 1 | 100.00 | NULL |+----+-------------+-------+------------+-------+---------------+----------+---------+-------+------+----------+-------+1 row in set, 1 warning (0.00 sec) 可以看到key_len列就变成了5，比使用id列的索引时多了1。 对于可变长度的索引列来说，比如下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 = 'a';+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | ref | idx_key1 | idx_key1 | 303 | const | 8 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+1 row in set, 1 warning (0.00 sec) 由于key1列的类型是VARCHAR(100)，所以该列实际最多占用的存储空间就是300字节，又因为该列允许存储NULL值，所以key_len需要加1，又因为该列是可变长度列，所以key_len需要加2，所以最后ken_len的值就是303。 有的同学可能有疑问：你在前边唠叨InnoDB行格式的时候不是说，存储变长字段的实际长度不是可能占用1个字节或者2个字节么？为什么现在不管三七二十一都用了2个字节？这里需要强调的一点是，执行计划的生成是在MySQL server层中的功能，并不是针对具体某个存储引擎的功能，设计MySQL的大叔在执行计划中输出key_len列主要是为了让我们区分某个使用联合索引的查询具体用了几个索引列，而不是为了准确的说明针对某个具体存储引擎存储变长字段的实际长度占用的空间到底是占用1个字节还是2个字节。比方说下边这个使用到联合索引idx_key_part的查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key_part1 = 'a';+----+-------------+-------+------------+------+---------------+--------------+---------+-------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+--------------+---------+-------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | ref | idx_key_part | idx_key_part | 303 | const | 12 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+--------------+---------+-------+------+----------+-------+1 row in set, 1 warning (0.00 sec) 我们可以从执行计划的key_len列中看到值是303，这意味着MySQL在执行上述查询中只能用到idx_key_part索引的一个索引列，而下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key_part1 = 'a' AND key_part2 = 'b';+----+-------------+-------+------------+------+---------------+--------------+---------+-------------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+--------------+---------+-------------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | ref | idx_key_part | idx_key_part | 606 | const,const | 1 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+--------------+---------+-------------+------+----------+-------+1 row in set, 1 warning (0.01 sec) 这个查询的执行计划的ken_len列的值是606，说明执行这个查询的时候可以用到联合索引idx_key_part的两个索引列。 ref当使用索引列等值匹配的条件去执行查询时，也就是在访问方法是const、eq_ref、ref、ref_or_null、unique_subquery、index_subquery其中之一时，ref列展示的就是与索引列作等值匹配的东东是个啥，比如只是一个常数或者是某个列。大家看下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 = 'a';+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | ref | idx_key1 | idx_key1 | 303 | const | 8 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+1 row in set, 1 warning (0.01 sec) 可以看到ref列的值是const，表明在使用idx_key1索引执行查询时，与key1列作等值匹配的对象是一个常数，当然有时候更复杂一点： 1mysql&gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.id = s2.id;+----+-------------+-------+------------+--------+---------------+---------+---------+-----------------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+---------+---------+-----------------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | ALL | PRIMARY | NULL | NULL | NULL | 9688 | 100.00 | NULL || 1 | SIMPLE | s2 | NULL | eq_ref | PRIMARY | PRIMARY | 4 | xiaohaizi.s1.id | 1 | 100.00 | NULL |+----+-------------+-------+------------+--------+---------------+---------+---------+-----------------+------+----------+-------+2 rows in set, 1 warning (0.00 sec) 可以看到对被驱动表s2的访问方法是eq_ref，而对应的ref列的值是xiaohaizi.s1.id，这说明在对被驱动表进行访问时会用到PRIMARY索引，也就是聚簇索引与一个列进行等值匹配的条件，于s2表的id作等值匹配的对象就是xiaohaizi.s1.id列（注意这里把数据库名也写出来了）。 有的时候与索引列进行等值匹配的对象是一个函数，比方说下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s2.key1 = UPPER(s1.key1);+----+-------------+-------+------------+------+---------------+----------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+----------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | NULL || 1 | SIMPLE | s2 | NULL | ref | idx_key1 | idx_key1 | 303 | func | 1 | 100.00 | Using index condition |+----+-------------+-------+------------+------+---------------+----------+---------+------+------+----------+-----------------------+2 rows in set, 1 warning (0.00 sec) 我们看执行计划的第二条记录，可以看到对s2表采用ref访问方法执行查询，然后在查询计划的ref列里输出的是func，说明与s2表的key1列进行等值匹配的对象是一个函数。 rows如果查询优化器决定使用全表扫描的方式对某个表执行查询时，执行计划的rows列就代表预计需要扫描的行数，如果使用索引来执行查询时，执行计划的rows列就代表预计扫描的索引记录行数。比如下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 &gt; 'z';+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | s1 | NULL | range | idx_key1 | idx_key1 | 303 | NULL | 266 | 100.00 | Using index condition |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-----------------------+1 row in set, 1 warning (0.00 sec) 我们看到执行计划的rows列的值是266，这意味着查询优化器在经过分析使用idx_key1进行查询的成本之后，觉得满足key1 &gt; 'z'这个条件的记录只有266条。 filtered之前在分析连接查询的成本时提出过一个condition filtering的概念，就是MySQL在计算驱动表扇出时采用的一个策略： 如果使用的是全表扫描的方式执行的单表查询，那么计算驱动表扇出时需要估计出满足搜索条件的记录到底有多少条。 如果使用的是索引执行的单表扫描，那么计算驱动表扇出的时候需要估计出满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条。 比方说下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 &gt; 'z' AND common_field = 'a';+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+------------------------------------+| 1 | SIMPLE | s1 | NULL | range | idx_key1 | idx_key1 | 303 | NULL | 266 | 10.00 | Using index condition; Using where |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+------------------------------------+1 row in set, 1 warning (0.00 sec) 从执行计划的key列中可以看出来，该查询使用idx_key1索引来执行查询，从rows列可以看出满足key1 &gt; 'z'的记录有266条。执行计划的filtered列就代表查询优化器预测在这266条记录中，有多少条记录满足其余的搜索条件，也就是common_field = 'a'这个条件的百分比。此处filtered列的值是10.00，说明查询优化器预测在266条记录中有10.00%的记录满足common_field = 'a'这个条件。 对于单表查询来说，这个filtered列的值没什么意义，我们更关注在连接查询中驱动表对应的执行计划记录的filtered值，比方说下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key1 WHERE s1.common_field = 'a';+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------+| 1 | SIMPLE | s1 | NULL | ALL | idx_key1 | NULL | NULL | NULL | 9688 | 10.00 | Using where || 1 | SIMPLE | s2 | NULL | ref | idx_key1 | idx_key1 | 303 | xiaohaizi.s1.key1 | 1 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------+2 rows in set, 1 warning (0.00 sec) 从执行计划中可以看出来，查询优化器打算把s1当作驱动表，s2当作被驱动表。我们可以看到驱动表s1表的执行计划的rows列为9688， filtered列为10.00，这意味着驱动表s1的扇出值就是9688 × 10.00% = 968.8，这说明还要对被驱动表执行大约968次查询。 Explain 详解（下）执行计划输出中各列详解Extra顾名思义，Extra列是用来说明一些额外信息的，我们可以通过这些额外信息来更准确的理解MySQL到底将如何执行给定的查询语句。MySQL提供的额外信息有好几十个，我们就不一个一个介绍了（都介绍了感觉我们的文章就跟文档差不多了～），所以我们只挑一些平时常见的或者比较重要的额外信息介绍给大家哈。 No tables used 当查询语句的没有FROM子句时将会提示该额外信息，比如： 1mysql&gt; EXPLAIN SELECT 1;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No tables used |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+1 row in set, 1 warning (0.00 sec) Impossible WHERE 查询语句的WHERE子句永远为FALSE时将会提示该额外信息，比方说： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE 1 != 1;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Impossible WHERE |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------+1 row in set, 1 warning (0.01 sec) No matching min/max row 当查询列表处有MIN或者MAX聚集函数，但是并没有符合WHERE子句中的搜索条件的记录时，将会提示该额外信息，比方说： 1mysql&gt; EXPLAIN SELECT MIN(key1) FROM s1 WHERE key1 = 'abcdefg';+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No matching min/max row |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------------------+1 row in set, 1 warning (0.00 sec) Using index 当我们的查询列表以及搜索条件中只包含属于某个索引的列，也就是在可以使用索引覆盖的情况下，在Extra列将会提示该额外信息。比方说下边这个查询中只需要用到idx_key1而不需要回表操作： 1mysql&gt; EXPLAIN SELECT key1 FROM s1 WHERE key1 = 'a';+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------------+| 1 | SIMPLE | s1 | NULL | ref | idx_key1 | idx_key1 | 303 | const | 8 | 100.00 | Using index |+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------------+1 row in set, 1 warning (0.00 sec) Using index condition 有些搜索条件中虽然出现了索引列，但却不能使用到索引，比如下边这个查询： 1SELECT * FROM s1 WHERE key1 &gt; 'z' AND key1 LIKE '%a'; 其中的key1 &gt; 'z'可以使用到索引，但是key1 LIKE '%a'却无法使用到索引，在以前版本的MySQL中，是按照下边步骤来执行这个查询的： 先根据key1 &gt; 'z'这个条件，从二级索引idx_key1中获取到对应的二级索引记录。 根据上一步骤得到的二级索引记录中的主键值进行回表，找到完整的用户记录再检测该记录是否符合key1 LIKE '%a'这个条件，将符合条件的记录加入到最后的结果集。 但是虽然key1 LIKE '%a'不能组成范围区间参与range访问方法的执行，但这个条件毕竟只涉及到了key1列，所以设计MySQL的大叔把上边的步骤改进了一下： 先根据key1 &gt; 'z'这个条件，定位到二级索引idx_key1中对应的二级索引记录。 对于指定的二级索引记录，先不着急回表，而是先检测一下该记录是否满足key1 LIKE '%a'这个条件，如果这个条件不满足，则该二级索引记录压根儿就没必要回表。 对于满足key1 LIKE '%a'这个条件的二级索引记录执行回表操作。 我们说回表操作其实是一个随机IO，比较耗时，所以上述修改虽然只改进了一点点，但是可以省去好多回表操作的成本。设计MySQL的大叔们把他们的这个改进称之为索引条件下推（英文名：Index Condition Pushdown）。 如果在查询语句的执行过程中将要使用索引条件下推这个特性，在Extra列中将会显示Using index condition，比如这样： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 &gt; 'z' AND key1 LIKE '%b'; +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-----------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-----------------------+ | 1 | SIMPLE | s1 | NULL | range | idx_key1 | idx_key1 | 303 | NULL | 266 | 100.00 | Using index condition | +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-----------------------+ 1 row in set, 1 warning (0.01 sec) Using where 当我们使用全表扫描来执行对某个表的查询，并且该语句的WHERE子句中有针对该表的搜索条件时，在Extra列中会提示上述额外信息。比如下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE common_field = 'a';+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | SIMPLE | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 10.00 | Using where |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+1 row in set, 1 warning (0.01 sec) 当使用索引访问来执行对某个表的查询，并且该语句的WHERE子句中有除了该索引包含的列之外的其他搜索条件时，在Extra列中也会提示上述额外信息。比如下边这个查询虽然使用idx_key1索引执行查询，但是搜索条件中除了包含key1的搜索条件key1 = 'a'，还有包含common_field的搜索条件，所以Extra列会显示Using where的提示： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 = 'a' AND common_field = 'a';+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------------+| 1 | SIMPLE | s1 | NULL | ref | idx_key1 | idx_key1 | 303 | const | 8 | 10.00 | Using where |+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------------+1 row in set, 1 warning (0.00 sec) Using join buffer (Block Nested Loop) 在连接查询执行过程过，当被驱动表不能有效的利用索引加快访问速度，MySQL一般会为其分配一块名叫join buffer的内存块来加快查询速度，也就是我们所讲的基于块的嵌套循环算法，比如下边这个查询语句： 1mysql&gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.common_field = s2.common_field;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+| 1 | SIMPLE | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | NULL || 1 | SIMPLE | s2 | NULL | ALL | NULL | NULL | NULL | NULL | 9954 | 10.00 | Using where; Using join buffer (Block Nested Loop) |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+2 rows in set, 1 warning (0.03 sec) 可以在对s2表的执行计划的Extra列显示了两个提示： Using join buffer (Block Nested Loop)：这是因为对表s2的访问不能有效利用索引，只好退而求其次，使用join buffer来减少对s2表的访问次数，从而提高性能。 Using where：可以看到查询语句中有一个s1.common_field = s2.common_field条件，因为s1是驱动表，s2是被驱动表，所以在访问s2表时，s1.common_field的值已经确定下来了，所以实际上查询s2表的条件就是s2.common_field = 一个常数，所以提示了Using where额外信息。 Not exists 当我们使用左（外）连接时，如果WHERE子句中包含要求被驱动表的某个列等于NULL值的搜索条件，而且那个列又是不允许存储NULL值的，那么在该表的执行计划的Extra列就会提示Not exists额外信息，比如这样： 1mysql&gt; EXPLAIN SELECT * FROM s1 LEFT JOIN s2 ON s1.key1 = s2.key1 WHERE s2.id IS NULL;+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------------------+| 1 | SIMPLE | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | NULL || 1 | SIMPLE | s2 | NULL | ref | idx_key1 | idx_key1 | 303 | xiaohaizi.s1.key1 | 1 | 10.00 | Using where; Not exists |+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------------------+2 rows in set, 1 warning (0.00 sec) 上述查询中s1表是驱动表，s2表是被驱动表，s2.id列是不允许存储NULL值的，而WHERE子句中又包含s2.id IS NULL的搜索条件，这意味着必定是驱动表的记录在被驱动表中找不到匹配ON子句条件的记录才会把该驱动表的记录加入到最终的结果集，所以对于某条驱动表中的记录来说，如果能在被驱动表中找到1条符合ON子句条件的记录，那么该驱动表的记录就不会被加入到最终的结果集，也就是说我们没有必要到被驱动表中找到全部符合ON子句条件的记录，这样可以稍微节省一点性能。 1小贴士：右（外）连接可以被转换为左（外）连接，所以就不提右（外）连接的情况了。 Using intersect(...)、Using union(...)和Using sort_union(...) 如果执行计划的Extra列出现了Using intersect(...)提示，说明准备使用Intersect索引合并的方式执行查询，括号中的...表示需要进行索引合并的索引名称；如果出现了Using union(...)提示，说明准备使用Union索引合并的方式执行查询；出现了Using sort_union(...)提示，说明准备使用Sort-Union索引合并的方式执行查询。比如这个查询的执行计划： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 = 'a' AND key3 = 'a';+----+-------------+-------+------------+-------------+-------------------+-------------------+---------+------+------+----------+-------------------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------------+-------------------+-------------------+---------+------+------+----------+-------------------------------------------------+| 1 | SIMPLE | s1 | NULL | index_merge | idx_key1,idx_key3 | idx_key3,idx_key1 | 303,303 | NULL | 1 | 100.00 | Using intersect(idx_key3,idx_key1); Using where |+----+-------------+-------+------------+-------------+-------------------+-------------------+---------+------+------+----------+-------------------------------------------------+1 row in set, 1 warning (0.01 sec) 其中Extra列就显示了Using intersect(idx_key3,idx_key1)，表明MySQL即将使用idx_key3和idx_key1这两个索引进行Intersect索引合并的方式执行查询。 1小贴士：剩下两种类型的索引合并的Extra列信息就不一一举例子了，自己写个查询瞅瞅呗～ Zero limit 当我们的LIMIT子句的参数为0时，表示压根儿不打算从表中读出任何记录，将会提示该额外信息，比如这样： 1mysql&gt; EXPLAIN SELECT * FROM s1 LIMIT 0;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Zero limit |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------+1 row in set, 1 warning (0.00 sec) Using filesort 有一些情况下对结果集中的记录进行排序是可以使用到索引的，比如下边这个查询： 1mysql&gt; EXPLAIN SELECT * FROM s1 ORDER BY key1 LIMIT 10;+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------+| 1 | SIMPLE | s1 | NULL | index | NULL | idx_key1 | 303 | NULL | 10 | 100.00 | NULL |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------+1 row in set, 1 warning (0.03 sec) 这个查询语句可以利用idx_key1索引直接取出key1列的10条记录，然后再进行回表操作就好了。但是很多情况下排序操作无法使用到索引，只能在内存中（记录较少的时候）或者磁盘中（记录较多的时候）进行排序，设计MySQL的大叔把这种在内存中或者磁盘上进行排序的方式统称为文件排序（英文名：filesort）。如果某个查询需要使用文件排序的方式执行查询，就会在执行计划的Extra列中显示Using filesort提示，比如这样： 1mysql&gt; EXPLAIN SELECT * FROM s1 ORDER BY common_field LIMIT 10;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+| 1 | SIMPLE | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | Using filesort |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+1 row in set, 1 warning (0.00 sec) 需要注意的是，如果查询中需要使用filesort的方式进行排序的记录非常多，那么这个过程是很耗费性能的，我们最好想办法将使用文件排序的执行方式改为使用索引进行排序。 Using temporary 在许多查询的执行过程中，MySQL可能会借助临时表来完成一些功能，比如去重、排序之类的，比如我们在执行许多包含DISTINCT、GROUP BY、UNION等子句的查询过程中，如果不能有效利用索引来完成查询，MySQL很有可能寻求通过建立内部的临时表来执行查询。如果查询中使用到了内部的临时表，在执行计划的Extra列将会显示Using temporary提示，比方说这样： 1mysql&gt; EXPLAIN SELECT DISTINCT common_field FROM s1;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-----------------+| 1 | SIMPLE | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | Using temporary |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-----------------+1 row in set, 1 warning (0.00 sec) 再比如： 1mysql&gt; EXPLAIN SELECT common_field, COUNT(*) AS amount FROM s1 GROUP BY common_field;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------+| 1 | SIMPLE | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | Using temporary; Using filesort |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------+1 row in set, 1 warning (0.00 sec) 不知道大家注意到没有，上述执行计划的Extra列不仅仅包含Using temporary提示，还包含Using filesort提示，可是我们的查询语句中明明没有写ORDER BY子句呀？这是因为MySQL会在包含GROUP BY子句的查询中默认添加上ORDER BY子句，也就是说上述查询其实和下边这个查询等价： 1EXPLAIN SELECT common_field, COUNT(*) AS amount FROM s1 GROUP BY common_field ORDER BY common_field; 如果我们并不想为包含GROUP BY子句的查询进行排序，需要我们显式的写上ORDER BY NULL，就像这样： 1mysql&gt; EXPLAIN SELECT common_field, COUNT(*) AS amount FROM s1 GROUP BY common_field ORDER BY NULL;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-----------------+| 1 | SIMPLE | s1 | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 100.00 | Using temporary |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-----------------+1 row in set, 1 warning (0.00 sec) 这回执行计划中就没有Using filesort的提示了，也就意味着执行查询时可以省去对记录进行文件排序的成本了。 另外，执行计划中出现Using temporary并不是一个好的征兆，因为建立与维护临时表要付出很大成本的，所以我们最好能使用索引来替代掉使用临时表，比方说下边这个包含GROUP BY子句的查询就不需要使用临时表： 1mysql&gt; EXPLAIN SELECT key1, COUNT(*) AS amount FROM s1 GROUP BY key1;+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+| 1 | SIMPLE | s1 | NULL | index | idx_key1 | idx_key1 | 303 | NULL | 9688 | 100.00 | Using index |+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+1 row in set, 1 warning (0.00 sec) 从Extra的Using index的提示里我们可以看出，上述查询只需要扫描idx_key1索引就可以搞定了，不再需要临时表了。 Start temporary, End temporary 我们前边唠叨子查询的时候说过，查询优化器会优先尝试将IN子查询转换成semi-join，而semi-join又有好多种执行策略，当执行策略为DuplicateWeedout时，也就是通过建立临时表来实现为外层查询中的记录进行去重操作时，驱动表查询执行计划的Extra列将显示Start temporary提示，被驱动表查询执行计划的Extra列将显示End temporary提示，就是这样： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key3 FROM s2 WHERE common_field = 'a');+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+------------------------------+| 1 | SIMPLE | s2 | NULL | ALL | idx_key3 | NULL | NULL | NULL | 9954 | 10.00 | Using where; Start temporary || 1 | SIMPLE | s1 | NULL | ref | idx_key1 | idx_key1 | 303 | xiaohaizi.s2.key3 | 1 | 100.00 | End temporary |+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+------------------------------+2 rows in set, 1 warning (0.00 sec) LooseScan 在将In子查询转为semi-join时，如果采用的是LooseScan执行策略，则在驱动表执行计划的Extra列就是显示LooseScan提示，比如这样： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key3 IN (SELECT key1 FROM s2 WHERE key1 &gt; 'z');+----+-------------+-------+------------+-------+---------------+----------+---------+-------------------+------+----------+-------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+----------+---------+-------------------+------+----------+-------------------------------------+| 1 | SIMPLE | s2 | NULL | range | idx_key1 | idx_key1 | 303 | NULL | 270 | 100.00 | Using where; Using index; LooseScan || 1 | SIMPLE | s1 | NULL | ref | idx_key3 | idx_key3 | 303 | xiaohaizi.s2.key1 | 1 | 100.00 | NULL |+----+-------------+-------+------------+-------+---------------+----------+---------+-------------------+------+----------+-------------------------------------+2 rows in set, 1 warning (0.01 sec) FirstMatch(tbl_name) 在将In子查询转为semi-join时，如果采用的是FirstMatch执行策略，则在被驱动表执行计划的Extra列就是显示FirstMatch(tbl_name)提示，比如这样： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE common_field IN (SELECT key1 FROM s2 where s1.key3 = s2.key3);+----+-------------+-------+------------+------+-------------------+----------+---------+-------------------+------+----------+-----------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+-------------------+----------+---------+-------------------+------+----------+-----------------------------+| 1 | SIMPLE | s1 | NULL | ALL | idx_key3 | NULL | NULL | NULL | 9688 | 100.00 | Using where || 1 | SIMPLE | s2 | NULL | ref | idx_key1,idx_key3 | idx_key3 | 303 | xiaohaizi.s1.key3 | 1 | 4.87 | Using where; FirstMatch(s1) |+----+-------------+-------+------------+------+-------------------+----------+---------+-------------------+------+----------+-----------------------------+2 rows in set, 2 warnings (0.00 sec) Json格式的执行计划我们上边介绍的EXPLAIN语句输出中缺少了一个衡量执行计划好坏的重要属性 —— 成本。不过设计MySQL的大叔贴心的为我们提供了一种查看某个执行计划花费的成本的方式： 在EXPLAIN单词和真正的查询语句中间加上FORMAT=JSON。 这样我们就可以得到一个json格式的执行计划，里边儿包含该计划花费的成本，比如这样： 1mysql&gt; EXPLAIN FORMAT=JSON SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key2 WHERE s1.common_field = 'a'\\G*************************** 1. row ***************************EXPLAIN: { &quot;query_block&quot;: { &quot;select_id&quot;: 1, # 整个查询语句只有1个SELECT关键字，该关键字对应的id号为1 &quot;cost_info&quot;: { &quot;query_cost&quot;: &quot;3197.16&quot; # 整个查询的执行成本预计为3197.16 }, &quot;nested_loop&quot;: [ # 几个表之间采用嵌套循环连接算法执行 # 以下是参与嵌套循环连接算法的各个表的信息 { &quot;table&quot;: { &quot;table_name&quot;: &quot;s1&quot;, # s1表是驱动表 &quot;access_type&quot;: &quot;ALL&quot;, # 访问方法为ALL，意味着使用全表扫描访问 &quot;possible_keys&quot;: [ # 可能使用的索引 &quot;idx_key1&quot; ], &quot;rows_examined_per_scan&quot;: 9688, # 查询一次s1表大致需要扫描9688条记录 &quot;rows_produced_per_join&quot;: 968, # 驱动表s1的扇出是968 &quot;filtered&quot;: &quot;10.00&quot;, # condition filtering代表的百分比 &quot;cost_info&quot;: { &quot;read_cost&quot;: &quot;1840.84&quot;, # 稍后解释 &quot;eval_cost&quot;: &quot;193.76&quot;, # 稍后解释 &quot;prefix_cost&quot;: &quot;2034.60&quot;, # 单次查询s1表总共的成本 &quot;data_read_per_join&quot;: &quot;1M&quot; # 读取的数据量 }, &quot;used_columns&quot;: [ # 执行查询中涉及到的列 &quot;id&quot;, &quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;, &quot;key_part1&quot;, &quot;key_part2&quot;, &quot;key_part3&quot;, &quot;common_field&quot; ], # 对s1表访问时针对单表查询的条件 &quot;attached_condition&quot;: &quot;((`xiaohaizi`.`s1`.`common_field` = 'a') and (`xiaohaizi`.`s1`.`key1` is not null))&quot; } }, { &quot;table&quot;: { &quot;table_name&quot;: &quot;s2&quot;, # s2表是被驱动表 &quot;access_type&quot;: &quot;ref&quot;, # 访问方法为ref，意味着使用索引等值匹配的方式访问 &quot;possible_keys&quot;: [ # 可能使用的索引 &quot;idx_key2&quot; ], &quot;key&quot;: &quot;idx_key2&quot;, # 实际使用的索引 &quot;used_key_parts&quot;: [ # 使用到的索引列 &quot;key2&quot; ], &quot;key_length&quot;: &quot;5&quot;, # key_len &quot;ref&quot;: [ # 与key2列进行等值匹配的对象 &quot;xiaohaizi.s1.key1&quot; ], &quot;rows_examined_per_scan&quot;: 1, # 查询一次s2表大致需要扫描1条记录 &quot;rows_produced_per_join&quot;: 968, # 被驱动表s2的扇出是968（由于后边没有多余的表进行连接，所以这个值也没啥用） &quot;filtered&quot;: &quot;100.00&quot;, # condition filtering代表的百分比 # s2表使用索引进行查询的搜索条件 &quot;index_condition&quot;: &quot;(`xiaohaizi`.`s1`.`key1` = `xiaohaizi`.`s2`.`key2`)&quot;, &quot;cost_info&quot;: { &quot;read_cost&quot;: &quot;968.80&quot;, # 稍后解释 &quot;eval_cost&quot;: &quot;193.76&quot;, # 稍后解释 &quot;prefix_cost&quot;: &quot;3197.16&quot;, # 单次查询s1、多次查询s2表总共的成本 &quot;data_read_per_join&quot;: &quot;1M&quot; # 读取的数据量 }, &quot;used_columns&quot;: [ # 执行查询中涉及到的列 &quot;id&quot;, &quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;, &quot;key_part1&quot;, &quot;key_part2&quot;, &quot;key_part3&quot;, &quot;common_field&quot; ] } } ] }}1 row in set, 2 warnings (0.00 sec) 我们使用#后边跟随注释的形式为大家解释了EXPLAIN FORMAT=JSON语句的输出内容，但是大家可能有疑问&quot;cost_info&quot;里边的成本看着怪怪的，它们是怎么计算出来的？先看s1表的&quot;cost_info&quot;部分： 1&quot;cost_info&quot;: { &quot;read_cost&quot;: &quot;1840.84&quot;, &quot;eval_cost&quot;: &quot;193.76&quot;, &quot;prefix_cost&quot;: &quot;2034.60&quot;, &quot;data_read_per_join&quot;: &quot;1M&quot;} read_cost是由下边这两部分组成的： IO成本 检测rows × (1 - filter)条记录的CPU成本 1小贴士：rows和filter都是我们前边介绍执行计划的输出列，在JSON格式的执行计划中，rows相当于rows_examined_per_scan，filtered名称不变。 eval_cost是这样计算的： 检测 rows × filter条记录的成本。 prefix_cost就是单独查询s1表的成本，也就是： read_cost + eval_cost data_read_per_join表示在此次查询中需要读取的数据量，我们就不多唠叨这个了。 1小贴士：大家其实没必要关注MySQL为啥使用这么古怪的方式计算出read_cost和eval_cost，关注prefix_cost是查询s1表的成本就好了。 对于s2表的&quot;cost_info&quot;部分是这样的： 1&quot;cost_info&quot;: { &quot;read_cost&quot;: &quot;968.80&quot;, &quot;eval_cost&quot;: &quot;193.76&quot;, &quot;prefix_cost&quot;: &quot;3197.16&quot;, &quot;data_read_per_join&quot;: &quot;1M&quot;} 由于s2表是被驱动表，所以可能被读取多次，这里的read_cost和eval_cost是访问多次s2表后累加起来的值，大家主要关注里边儿的prefix_cost的值代表的是整个连接查询预计的成本，也就是单次查询s1表和多次查询s2表后的成本的和，也就是： 1968.80 + 193.76 + 2034.60 = 3197.16 Extented EXPLAIN最后，设计MySQL的大叔还为我们留了个彩蛋，在我们使用EXPLAIN语句查看了某个查询的执行计划后，紧接着还可以使用SHOW WARNINGS语句查看与这个查询的执行计划有关的一些扩展信息，比如这样： 1mysql&gt; EXPLAIN SELECT s1.key1, s2.key1 FROM s1 LEFT JOIN s2 ON s1.key1 = s2.key1 WHERE s2.common_field IS NOT NULL;+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------+| 1 | SIMPLE | s2 | NULL | ALL | idx_key1 | NULL | NULL | NULL | 9954 | 90.00 | Using where || 1 | SIMPLE | s1 | NULL | ref | idx_key1 | idx_key1 | 303 | xiaohaizi.s2.key1 | 1 | 100.00 | Using index |+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------+2 rows in set, 1 warning (0.00 sec)mysql&gt; SHOW WARNINGS\\G*************************** 1. row *************************** Level: Note Code: 1003Message: /* select#1 */ select `xiaohaizi`.`s1`.`key1` AS `key1`,`xiaohaizi`.`s2`.`key1` AS `key1` from `xiaohaizi`.`s1` join `xiaohaizi`.`s2` where ((`xiaohaizi`.`s1`.`key1` = `xiaohaizi`.`s2`.`key1`) and (`xiaohaizi`.`s2`.`common_field` is not null))1 row in set (0.00 sec) 大家可以看到SHOW WARNINGS展示出来的信息有三个字段，分别是Level、Code、Message。我们最常见的就是Code为1003的信息，当Code值为1003时，Message字段展示的信息类似于查询优化器将我们的查询语句重写后的语句。比如我们上边的查询本来是一个左（外）连接查询，但是有一个s2.common_field IS NOT NULL的条件，着就会导致查询优化器把左（外）连接查询优化为内连接查询，从SHOW WARNINGS的Message字段也可以看出来，原本的LEFT JOIN已经变成了JOIN。 但是大家一定要注意，我们说Message字段展示的信息类似于查询优化器将我们的查询语句重写后的语句，并不是等价于，也就是说Message字段展示的信息并不是标准的查询语句，在很多情况下并不能直接拿到黑框框中运行，它只能作为帮助我们理解查MySQL将如何执行查询语句的一个参考依据而已。 otpimizer trace 表的神奇功效在MySQL 5.6以及之后的版本中，设计MySQL的大叔贴心的为这部分小伙伴提出了一个optimizer trace的功能，这个功能可以让我们方便的查看优化器生成执行计划的整个过程，这个功能的开启与关闭由系统变量optimizer_trace决定，我们看一下： 1mysql&gt; SHOW VARIABLES LIKE 'optimizer_trace';+-----------------+--------------------------+| Variable_name | Value |+-----------------+--------------------------+| optimizer_trace | enabled=off,one_line=off |+-----------------+--------------------------+1 row in set (0.02 sec) 可以看到enabled值为off，表明这个功能默认是关闭的。 1小贴士：one_line的值是控制输出格式的，如果为on那么所有输出都将在一行中展示，不适合人阅读，所以我们就保持其默认值为off吧。 如果想打开这个功能，必须首先把enabled的值改为on，就像这样： 1mysql&gt; SET optimizer_trace=&quot;enabled=on&quot;;Query OK, 0 rows affected (0.00 sec) 然后我们就可以输入我们想要查看优化过程的查询语句，当该查询语句执行完成后，就可以到information_schema数据库下的OPTIMIZER_TRACE表中查看完整的优化过程。这个OPTIMIZER_TRACE表有4个列，分别是： QUERY：表示我们的查询语句。 TRACE：表示优化过程的JSON格式文本。 MISSING_BYTES_BEYOND_MAX_MEM_SIZE：由于优化过程可能会输出很多，如果超过某个限制时，多余的文本将不会被显示，这个字段展示了被忽略的文本字节数。 INSUFFICIENT_PRIVILEGES：表示是否没有权限查看优化过程，默认值是0，只有某些特殊情况下才会是1，我们暂时不关心这个字段的值。 完整的使用optimizer trace功能的步骤总结如下： 1# 1. 打开optimizer trace功能 (默认情况下它是关闭的):SET optimizer_trace=&quot;enabled=on&quot;;# 2. 这里输入你自己的查询语句SELECT ...; # 3. 从OPTIMIZER_TRACE表中查看上一个查询的优化过程SELECT * FROM information_schema.OPTIMIZER_TRACE;# 4. 可能你还要观察其他语句执行的优化过程，重复上边的第2、3步...# 5. 当你停止查看语句的优化过程时，把optimizer trace功能关闭SET optimizer_trace=&quot;enabled=off&quot;; 现在我们有一个搜索条件比较多的查询语句，它的执行计划如下： 1mysql&gt; EXPLAIN SELECT * FROM s1 WHERE -&gt; key1 &gt; 'z' AND -&gt; key2 &lt; 1000000 AND -&gt; key3 IN ('a', 'b', 'c') AND -&gt; common_field = 'abc';+----+-------------+-------+------------+-------+----------------------------+----------+---------+------+------+----------+------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+----------------------------+----------+---------+------+------+----------+------------------------------------+| 1 | SIMPLE | s1 | NULL | range | idx_key2,idx_key1,idx_key3 | idx_key2 | 5 | NULL | 12 | 0.42 | Using index condition; Using where |+----+-------------+-------+------------+-------+----------------------------+----------+---------+------+------+----------+------------------------------------+1 row in set, 1 warning (0.00 sec) 可以看到该查询可能使用到的索引有3个，那么为什么优化器最终选择了idx_key2而不选择其他的索引或者直接全表扫描呢？这时候就可以通过otpimzer trace功能来查看优化器的具体工作过程： 1SET optimizer_trace=&quot;enabled=on&quot;;SELECT * FROM s1 WHERE key1 &gt; 'z' AND key2 &lt; 1000000 AND key3 IN ('a', 'b', 'c') AND common_field = 'abc'; SELECT * FROM information_schema.OPTIMIZER_TRACE\\G 我们直接看一下通过查询OPTIMIZER_TRACE表得到的输出（我使用#后跟随注释的形式为大家解释了优化过程中的一些比较重要的点，大家重点关注一下）： 1*************************** 1. row ***************************# 分析的查询语句是什么QUERY: SELECT * FROM s1 WHERE key1 &gt; 'z' AND key2 &lt; 1000000 AND key3 IN ('a', 'b', 'c') AND common_field = 'abc'# 优化的具体过程TRACE: { &quot;steps&quot;: [ { &quot;join_preparation&quot;: { # prepare阶段 &quot;select#&quot;: 1, &quot;steps&quot;: [ { &quot;IN_uses_bisection&quot;: true }, { &quot;expanded_query&quot;: &quot;/* select#1 */ select `s1`.`id` AS `id`,`s1`.`key1` AS `key1`,`s1`.`key2` AS `key2`,`s1`.`key3` AS `key3`,`s1`.`key_part1` AS `key_part1`,`s1`.`key_part2` AS `key_part2`,`s1`.`key_part3` AS `key_part3`,`s1`.`common_field` AS `common_field` from `s1` where ((`s1`.`key1` &gt; 'z') and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in ('a','b','c')) and (`s1`.`common_field` = 'abc'))&quot; } ] /* steps */ } /* join_preparation */ }, { &quot;join_optimization&quot;: { # optimize阶段 &quot;select#&quot;: 1, &quot;steps&quot;: [ { &quot;condition_processing&quot;: { # 处理搜索条件 &quot;condition&quot;: &quot;WHERE&quot;, # 原始搜索条件 &quot;original_condition&quot;: &quot;((`s1`.`key1` &gt; 'z') and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in ('a','b','c')) and (`s1`.`common_field` = 'abc'))&quot;, &quot;steps&quot;: [ { # 等值传递转换 &quot;transformation&quot;: &quot;equality_propagation&quot;, &quot;resulting_condition&quot;: &quot;((`s1`.`key1` &gt; 'z') and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in ('a','b','c')) and (`s1`.`common_field` = 'abc'))&quot; }, { # 常量传递转换 &quot;transformation&quot;: &quot;constant_propagation&quot;, &quot;resulting_condition&quot;: &quot;((`s1`.`key1` &gt; 'z') and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in ('a','b','c')) and (`s1`.`common_field` = 'abc'))&quot; }, { # 去除没用的条件 &quot;transformation&quot;: &quot;trivial_condition_removal&quot;, &quot;resulting_condition&quot;: &quot;((`s1`.`key1` &gt; 'z') and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in ('a','b','c')) and (`s1`.`common_field` = 'abc'))&quot; } ] /* steps */ } /* condition_processing */ }, { # 替换虚拟生成列 &quot;substitute_generated_columns&quot;: { } /* substitute_generated_columns */ }, { # 表的依赖信息 &quot;table_dependencies&quot;: [ { &quot;table&quot;: &quot;`s1`&quot;, &quot;row_may_be_null&quot;: false, &quot;map_bit&quot;: 0, &quot;depends_on_map_bits&quot;: [ ] /* depends_on_map_bits */ } ] /* table_dependencies */ }, { &quot;ref_optimizer_key_uses&quot;: [ ] /* ref_optimizer_key_uses */ }, { # 预估不同单表访问方法的访问成本 &quot;rows_estimation&quot;: [ { &quot;table&quot;: &quot;`s1`&quot;, &quot;range_analysis&quot;: { &quot;table_scan&quot;: { # 全表扫描的行数以及成本 &quot;rows&quot;: 9688, &quot;cost&quot;: 2036.7 } /* table_scan */, # 分析可能使用的索引 &quot;potential_range_indexes&quot;: [ { &quot;index&quot;: &quot;PRIMARY&quot;, # 主键不可用 &quot;usable&quot;: false, &quot;cause&quot;: &quot;not_applicable&quot; }, { &quot;index&quot;: &quot;idx_key2&quot;, # idx_key2可能被使用 &quot;usable&quot;: true, &quot;key_parts&quot;: [ &quot;key2&quot; ] /* key_parts */ }, { &quot;index&quot;: &quot;idx_key1&quot;, # idx_key1可能被使用 &quot;usable&quot;: true, &quot;key_parts&quot;: [ &quot;key1&quot;, &quot;id&quot; ] /* key_parts */ }, { &quot;index&quot;: &quot;idx_key3&quot;, # idx_key3可能被使用 &quot;usable&quot;: true, &quot;key_parts&quot;: [ &quot;key3&quot;, &quot;id&quot; ] /* key_parts */ }, { &quot;index&quot;: &quot;idx_key_part&quot;, # idx_keypart不可用 &quot;usable&quot;: false, &quot;cause&quot;: &quot;not_applicable&quot; } ] /* potential_range_indexes */, &quot;setup_range_conditions&quot;: [ ] /* setup_range_conditions */, &quot;group_index_range&quot;: { &quot;chosen&quot;: false, &quot;cause&quot;: &quot;not_group_by_or_distinct&quot; } /* group_index_range */, # 分析各种可能使用的索引的成本 &quot;analyzing_range_alternatives&quot;: { &quot;range_scan_alternatives&quot;: [ { # 使用idx_key2的成本分析 &quot;index&quot;: &quot;idx_key2&quot;, # 使用idx_key2的范围区间 &quot;ranges&quot;: [ &quot;NULL &lt; key2 &lt; 1000000&quot; ] /* ranges */, &quot;index_dives_for_eq_ranges&quot;: true, # 是否使用index dive &quot;rowid_ordered&quot;: false, # 使用该索引获取的记录是否按照主键排序 &quot;using_mrr&quot;: false, # 是否使用mrr &quot;index_only&quot;: false, # 是否是索引覆盖访问 &quot;rows&quot;: 12, # 使用该索引获取的记录条数 &quot;cost&quot;: 15.41, # 使用该索引的成本 &quot;chosen&quot;: true # 是否选择该索引 }, { # 使用idx_key1的成本分析 &quot;index&quot;: &quot;idx_key1&quot;, # 使用idx_key1的范围区间 &quot;ranges&quot;: [ &quot;z &lt; key1&quot; ] /* ranges */, &quot;index_dives_for_eq_ranges&quot;: true, # 同上 &quot;rowid_ordered&quot;: false, # 同上 &quot;using_mrr&quot;: false, # 同上 &quot;index_only&quot;: false, # 同上 &quot;rows&quot;: 266, # 同上 &quot;cost&quot;: 320.21, # 同上 &quot;chosen&quot;: false, # 同上 &quot;cause&quot;: &quot;cost&quot; # 因为成本太大所以不选择该索引 }, { # 使用idx_key3的成本分析 &quot;index&quot;: &quot;idx_key3&quot;, # 使用idx_key3的范围区间 &quot;ranges&quot;: [ &quot;a &lt;= key3 &lt;= a&quot;, &quot;b &lt;= key3 &lt;= b&quot;, &quot;c &lt;= key3 &lt;= c&quot; ] /* ranges */, &quot;index_dives_for_eq_ranges&quot;: true, # 同上 &quot;rowid_ordered&quot;: false, # 同上 &quot;using_mrr&quot;: false, # 同上 &quot;index_only&quot;: false, # 同上 &quot;rows&quot;: 21, # 同上 &quot;cost&quot;: 28.21, # 同上 &quot;chosen&quot;: false, # 同上 &quot;cause&quot;: &quot;cost&quot; # 同上 } ] /* range_scan_alternatives */, # 分析使用索引合并的成本 &quot;analyzing_roworder_intersect&quot;: { &quot;usable&quot;: false, &quot;cause&quot;: &quot;too_few_roworder_scans&quot; } /* analyzing_roworder_intersect */ } /* analyzing_range_alternatives */, # 对于上述单表查询s1最优的访问方法 &quot;chosen_range_access_summary&quot;: { &quot;range_access_plan&quot;: { &quot;type&quot;: &quot;range_scan&quot;, &quot;index&quot;: &quot;idx_key2&quot;, &quot;rows&quot;: 12, &quot;ranges&quot;: [ &quot;NULL &lt; key2 &lt; 1000000&quot; ] /* ranges */ } /* range_access_plan */, &quot;rows_for_plan&quot;: 12, &quot;cost_for_plan&quot;: 15.41, &quot;chosen&quot;: true } /* chosen_range_access_summary */ } /* range_analysis */ } ] /* rows_estimation */ }, { # 分析各种可能的执行计划 #（对多表查询这可能有很多种不同的方案，单表查询的方案上边已经分析过了，直接选取idx_key2就好） &quot;considered_execution_plans&quot;: [ { &quot;plan_prefix&quot;: [ ] /* plan_prefix */, &quot;table&quot;: &quot;`s1`&quot;, &quot;best_access_path&quot;: { &quot;considered_access_paths&quot;: [ { &quot;rows_to_scan&quot;: 12, &quot;access_type&quot;: &quot;range&quot;, &quot;range_details&quot;: { &quot;used_index&quot;: &quot;idx_key2&quot; } /* range_details */, &quot;resulting_rows&quot;: 12, &quot;cost&quot;: 17.81, &quot;chosen&quot;: true } ] /* considered_access_paths */ } /* best_access_path */, &quot;condition_filtering_pct&quot;: 100, &quot;rows_for_plan&quot;: 12, &quot;cost_for_plan&quot;: 17.81, &quot;chosen&quot;: true } ] /* considered_execution_plans */ }, { # 尝试给查询添加一些其他的查询条件 &quot;attaching_conditions_to_tables&quot;: { &quot;original_condition&quot;: &quot;((`s1`.`key1` &gt; 'z') and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in ('a','b','c')) and (`s1`.`common_field` = 'abc'))&quot;, &quot;attached_conditions_computation&quot;: [ ] /* attached_conditions_computation */, &quot;attached_conditions_summary&quot;: [ { &quot;table&quot;: &quot;`s1`&quot;, &quot;attached&quot;: &quot;((`s1`.`key1` &gt; 'z') and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in ('a','b','c')) and (`s1`.`common_field` = 'abc'))&quot; } ] /* attached_conditions_summary */ } /* attaching_conditions_to_tables */ }, { # 再稍稍的改进一下执行计划 &quot;refine_plan&quot;: [ { &quot;table&quot;: &quot;`s1`&quot;, &quot;pushed_index_condition&quot;: &quot;(`s1`.`key2` &lt; 1000000)&quot;, &quot;table_condition_attached&quot;: &quot;((`s1`.`key1` &gt; 'z') and (`s1`.`key3` in ('a','b','c')) and (`s1`.`common_field` = 'abc'))&quot; } ] /* refine_plan */ } ] /* steps */ } /* join_optimization */ }, { &quot;join_execution&quot;: { # execute阶段 &quot;select#&quot;: 1, &quot;steps&quot;: [ ] /* steps */ } /* join_execution */ } ] /* steps */}# 因优化过程文本太多而丢弃的文本字节大小，值为0时表示并没有丢弃MISSING_BYTES_BEYOND_MAX_MEM_SIZE: 0# 权限字段INSUFFICIENT_PRIVILEGES: 01 row in set (0.00 sec) 大家看到这个输出的第一感觉就是这文本也太多了点儿吧，其实这只是优化器执行过程中的一小部分，设计MySQL的大叔可能会在之后的版本中添加更多的优化过程信息。不过杂乱之中其实还是蛮有规律的，优化过程大致分为了三个阶段： prepare阶段 optimize阶段 execute阶段 我们所说的基于成本的优化主要集中在optimize阶段，对于单表查询来说，我们主要关注optimize阶段的&quot;rows_estimation&quot;这个过程，这个过程深入分析了对单表查询的各种执行方案的成本；对于多表连接查询来说，我们更多需要关注&quot;considered_execution_plans&quot;这个过程，这个过程里会写明各种不同的连接方式所对应的成本。反正优化器最终会选择成本最低的那种方案来作为最终的执行计划，也就是我们使用EXPLAIN语句所展现出的那种方案。 如果有小伙伴对使用EXPLAIN语句展示出的对某个查询的执行计划很不理解，大家可以尝试使用optimizer trace功能来详细了解每一种执行方案对应的成本，相信这个功能能让大家更深入的了解MySQL查询优化器。","link":"/2021/10/08/MySQL%EF%BC%88%E4%B8%83%EF%BC%89explain%E8%AF%A6%E8%A7%A3/"},{"title":"MySQL（三）索引","text":"本节主要描述了以下两个方面： B+树索引的由来 B+树索引的使用建议 B+树索引的由来没有索引的查找对某个列精确匹配的情况： 1SELECT [列名列表] FROM 表名 WHERE 列名 = xxx; 在一个页中的查找： 表中记录很少，所有的记录都存放在一个数据页中，可以根据搜索条件的不同分为两种情况 以主键为搜索条件：在页目录中使用二分法定位到对应的槽，然后遍历该槽对应分组中的记录即可找到。 以其他列作为搜索条件：在数据页中并没有对非主键列建立所谓的页目录，所以无法通过二分快速定位，只能从最小记录遍历，速度很慢。 在很多页中查找 大部分情况下我们表中存放的记录都是非常多的，需要好多的数据页来存储这些记录。在很多页中查找记录的话可以分为两个步骤： 定位到记录所在的页。 从所在的页内中查找相应的记录。 在没有索引的情况下，不论是根据主键列或者其他列的值进行查找，由于我们并不能快速的定位到记录所在的页，所以只能从第一个页沿着双向链表一直往下找，在每一个页中根据我们上述的查找方式去查找指定的记录。这样的方式无疑是很慢的 索引的建立先建一个表： 1234567891011121314151617mysql&gt; create table index_demo( -&gt; c1 int, -&gt; c2 int, -&gt; c3 char(1), -&gt; primary key(c1) -&gt; ) row_format = Compact;Query OK, 0 rows affected (0.04 sec)mysql&gt; desc index_demo;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| c1 | int | NO | PRI | NULL | || c2 | int | YES | | NULL | || c3 | char(1) | YES | | NULL | |+-------+---------+------+-----+---------+-------+3 rows in set (0.01 sec) 这个表中c1列是主键，行记录格式如下： 我们只在示意图里展示记录的这几个部分： record_type：记录头信息的一项属性，表示记录的类型，0表示普通记录、2表示最小记录、3表示最大记录、1我们还没用过，等会再说～ next_record：记录头信息的一项属性，表示下一条地址相对于本条记录的地址偏移量。 各个列的值：这里只记录在index_demo表中的三个列，分别是c1、c2和c3。 其他信息：除了上述3种信息以外的所有信息，包括其他隐藏列的值以及记录的额外信息。 将记录格式示意图的其他信息去掉并把它竖起来的效果就是这样： 把一些记录放到页里边的示意图就是： 一个简单的索引方案可以借鉴与上一篇说的如何在一个数据页内快速定位记录的方法，为这些数据页页建立一个目录： 要为数据页建立目录的话需要对数据页中的数据做一些约束：下一个数据页中记录的主键值比如大于上一个页中的主键值。 假如一个页中存放的记录最多只有3条，那么再放入一个记录时就会出现分配一个新的数据页的情形： 这里新分配的数据页编号并不是连续的，他们只是维护者上一个页和下一个页的编号而建立了链表关系。 显然这样分配不满足约束了，需要进行一次记录移动（也称为页分裂）： 给所有的数据页建立一个目录项，每个目录项包含当前页中的主键最小值和页号 做完目录项之后： 这个目录项做完之后，将这几个目录项在物理存储器上连续的存储，就可以实现根据主键值快速查找的功能了，比如现在查找主键为20的记录： 先从目录项中根据二分法快速确定出主键值为20的记录在目录项3中（因为 12 &lt; 20 &lt; 209），它对应的页是页9。 再根据前边说的在页中查找记录的方式去页9中定位具体的记录。 InnoDB中的索引方案上面简单的索引方案存在了两个问题： InnoDB是用数据页来作为管理存储空间的基本单位，也就是说最多只能保证16KB的连续存储空间，如果表中的记录增多，那么要把所有的目录项都存放进去是非常不现实的 记录会进行删改的，如果我们把某一页的记录全部删除了，那么目录项该页后面的目录都要移动。 针对上面的两个问题，InnoDB找到了一种灵活管理所有目录项的方式： 发现这些目录项其实长得跟我们的用户记录差不多，只不过目录项中的两个列是主键和页号而已，所以他们复用了之前存储用户记录的数据页来存储目录项，为了和用户记录做一下区分，我们把这些用来表示目录项的记录称为目录项记录。那InnoDB怎么区分一条记录是普通的用户记录还是目录项记录呢？别忘了记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 1：目录项记录 2：最小记录 3：最大记录 这样的话，可以用数据页来存放所有的目录项了： 我们新分配了一个页号为30的数据页来存放目录项： 目录项记录中的 record_type 是 1 ，而普通的用户记录这个值是0 目录项记录中只有主键和页编号两个列，而普通的用户记录可以有很多。 记录头信息中还有一个 min_rec_mask的属性，只有在存储目录项记录的页的主键最小的目录项记录中这个值是1。 初次之外，目录项记录和普通用户记录就没区别了，他们存储也是用的一样的数据页。 现在以查找主键为20的记录为例，根据某个主键值去查找记录的步骤就可以大致拆分成下边两步： 先到存储目录项记录的页，也就是页30中通过二分法快速定位到对应目录项，因为12 &lt; 20 &lt; 209，所以定位到对应的记录所在的页就是页9。 再到存储用户记录的页9中根据二分法快速定位到主键值为20的用户记录。 当存放目录项的数据页满了时，可能需要新的数据页来存放目录项： 那么如果存放目录项的数据页很多的时候，如何快速的定位到时哪个数据页存放了要查找的主键值呢？ 再加一层目录： 以此类推，索引就变成了一棵B+树了： 从图中可以看出来，我们的实际用户记录其实都存放在B+树的最底层的节点上，这些节点也被称为叶子节点或叶节点，其余用来存放目录项的节点称为非叶子节点或者内节点，其中B+树最上边的那个节点也称为根节点。 这种数据结构查找能力就非常牛逼了，假如一个普通数据页存放记录100条，存放目录项记录1000条，那么： 如果B+树只有1层，也就是只有1个用于存放用户记录的节点，最多能存放100条记录。 如果B+树有2层，最多能存放1000×100=100000条记录。 如果B+树有3层，最多能存放1000×1000×100=100000000条记录。 如果B+树有4层，最多能存放1000×1000×1000×100=100000000000条记录。 所以一般情况下，我们用到的B+树都不会超过4层，那我们通过主键值去查找某条记录最多只需要做4个页面内的查找（查找3个目录项页和一个用户记录页），又因为在每个页面内有所谓的Page Directory（页目录），所以在页面内也可以通过二分法实现快速定位记录，这不是很牛么，哈哈！ 聚集索引上面的B+树索引有两个特点： 使用记录主键值的大小来对记录和数据页进行排序： 页内的记录是按照主键的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个双向链表 B+树的叶子节点存储的是完整的用户记录（也就是所有列的值） 这种完整的用户记录都存放在叶子节点的索引叫做聚集索引，这种索引的好处就是根据主键找到叶子节点就可以拿到所有的数据。 InnoDB存储引擎会自动为主键（如果没有它会自动帮我们添加）建立聚簇索引，聚簇索引的叶子节点包含完整的用户记录。 二级索引上面的索引只有在索引条件为主键时才会发挥作用？那么如果搜索条件是其他列呢？该怎么处理呢？ 其实不需要，我们可以多建立几颗B+树，不同的B+树排序规则不一样，比如说用c2列的大小作为排序规则： 这样的B+树叶子节点存放的是C2列 + 主键这一列。 所以如果我们现在想通过c2列的值查找某些记录的话就可以使用我们刚刚建好的这个B+树了。以查找c2列的值为4的记录为例，查找过程如下： 确定目录项记录页 根据根页面，也就是页44，可以快速定位到目录项记录所在的页为页42（因为2 &lt; 4 &lt; 9）。 通过目录项记录页确定用户记录真实所在的页。 在页42中可以快速定位到实际存储用户记录的页，但是由于c2列并没有唯一性约束，所以c2列值为4的记录可能分布在多个数据页中，又因为2 &lt; 4 ≤ 4，所以确定实际存储用户记录的页在页34和页35中。 在真实存储用户记录的页中定位到具体的记录。 到页34和页35中定位到具体的记录。 但是这个B+树的叶子节点中的记录只存储了c2和c1（也就是主键）两个列，所以我们必须再根据主键值去聚簇索引中再查找一遍完整的用户记录。 **我们根据这个以c2列大小排序的B+树只能确定我们要查找记录的主键值，所以如果我们想根据c2列的值查找到完整的用户记录的话，仍然需要到聚簇索引中再查一遍，这个过程也被称为回表**。也就是根据c2列的值查询一条完整的用户记录需要使用到2棵B+树！！！ 联合索引我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，比方说我们想让B+树按照c2和c3列的大小进行排序，这个包含两层含义： 先把各个记录和页按照c2列进行排序。 在记录的c2列相同的情况下，采用c3列进行排序 如图所示，我们需要注意一下几点： 每条目录项记录都由c2、c3、页号这三个部分组成，各条记录先按照c2列的值进行排序，如果记录的c2列相同，则按照c3列的值进行排序。 B+树叶子节点处的用户记录由c2、c3和主键c1列组成。 千万要注意一点，以c2和c3列的大小为排序规则建立的B+树称为联合索引，它的意思与分别为c2和c3列分别建立索引的表述是不同的： 建立联合索引只会建立如上图一样的1棵B+树。 为c2和c3列分别建立索引会分别以c2和c3列的大小为排序规则建立2棵B+树。 一些注意事项B+树索引的使用索引的代价 空间上的代价：每建立一个索引都为要它建立一棵B+树，每一棵B+树的每一个节点都是一个数据页，一个页默认会占用16KB的存储空间，一棵很大的B+树由许多数据页组成，那可是很大的一片存储空间呢 时间上的代价：每次对表中的数据进行增、删、改操作时，都需要去修改各个B+树索引 索引适用的条件新建一个表： 123456789CREATE TABLE person_info( id INT NOT NULL auto_increment, name VARCHAR(100) NOT NULL, birthday DATE NOT NULL, phone_number CHAR(11) NOT NULL, country varchar(100) NOT NULL, PRIMARY KEY (id), KEY idx_name_birthday_phone_number (name, birthday, phone_number)); 存在两个索引： 主键索引 二级索引：还是个联合索引 idx_name_birthday_phone_number 这个联合索引的B+树如下所示： 可以看出排序的顺序是： 先按照name列的值进行排序。 如果name列的值相同，则按照birthday列的值进行排序。 如果birthday列的值也相同，则按照phone_number的值进行排序。 全值匹配下面这个SQL就可以用到这个联合索引: 1SELECT * FROM person_info WHERE name = 'Ashburn' AND birthday = '1990-09-27' AND phone_number = '15123983239'; 因为B+树的数据页和记录先是按照name列的值进行排序的，所以先可以很快定位name列的值是Ashburn的记录位置。 在name列相同的记录里又是按照birthday列的值进行排序的，所以在name列的值是Ashburn的记录里又可以快速定位birthday列的值是'1990-09-27'的记录。 如果很不幸，name和birthday列的值都是相同的，那记录是按照phone_number列的值排序的，所以联合索引中的三个列都可能被用到。 下面这个呢？顺序变换了 1SELECT * FROM person_info WHERE birthday = '1990-09-27' AND phone_number = '15123983239' AND name = 'Ashburn'; 其实没有影响，因为MySQL有一个叫查询优化器的东西，会分析这些搜索条件并且按照可以使用的索引中列的顺序来决定先使用哪个搜索条件，后使用哪个搜索条件 匹配左边的列下面的两个SQL是可以用到这个联合索引的： 12SELECT * FROM person_info WHERE name = 'Ashburn';SELECT * FROM person_info WHERE name = 'Ashburn' AND birthday = '1990-09-27'; 下面这个SQL则用不到联合索引了： 因为B+树的数据页和记录先是按照name列的值排序的，在name列的值相同的情况下才使用birthday列进行排序，也就是说name列的值不同的记录中birthday的值可能是无序的。而现在你跳过name列直接根据birthday的值去查找明显是不行的。 1SELECT * FROM person_info WHERE birthday = '1990-09-27'; 但是需要特别注意的一点是，如果我们想使用联合索引中尽可能多的列，搜索条件中的各个列必须是联合索引中从最左边连续的列。比方说联合索引idx_name_birthday_phone_number中列的定义顺序是name、birthday、phone_number，如果我们的搜索条件中只有name和phone_number，而没有中间的birthday，比方说这样： 1SELECT * FROM person_info WHERE name = 'Ashburn' AND phone_number = '15123983239'; 这样只能用到name列的索引，birthday和phone_number的索引就用不上了，因为name值相同的记录先按照birthday的值进行排序，birthday值相同的记录才按照phone_number值进行排序。 匹配列的前缀字符串排序的规则一般是： 先按照字符串的第一个字符进行排序。 如果第一个字符相同再按照第二个字符进行排序。 如果第二个字符相同再按照第三个字符进行排序，依此类推。 所以如果下面这个模糊查找是可以用到联合索引的： 1SELECT * FROM person_info WHERE name LIKE 'As%'; 但是如果是给出后缀或重点的某个字符串就不行了： 1SELECT * FROM person_info WHERE name LIKE '%As%'; 这里有一个小技巧：比如存url这种前面都是www开头的字符串，可以考虑倒过来存在数据库中，这样就可以利用前缀匹配了。 匹配范围值所有记录都是按照索引列的值从小到大的顺序排好序的，所以这极大的方便我们查找索引列的值在某个范围内的记录。 1SELECT * FROM person_info WHERE name &gt; 'Asa' AND name &lt; 'Barlow'; 由于B+树中的数据页和记录是先按name列排序的，所以我们上边的查询过程其实是这样的： 找到name值为Asa的记录。 找到name值为Barlow的记录。 哦啦，由于所有记录都是由链表连起来的（记录之间用单链表，数据页之间用双链表），所以他们之间的记录都可以很容易的取出来喽～ 找到这些记录的主键值，再到聚簇索引中回表查找完整的记录。 不过在使用联合进行范围查找的时候需要注意，如果对多个列同时进行范围查找的话，只有对索引最左边的那个列进行范围查找的时候才能用到B+树索引，比方说这样： 1SELECT * FROM person_info WHERE name &gt; 'Asa' AND name &lt; 'Barlow' AND birthday &gt; '1980-01-01'; 上边这个查询可以分成两个部分： 通过条件name &gt; 'Asa' AND name &lt; 'Barlow' 来对name进行范围，查找的结果可能有多条name值不同的记录， 对这些name值不同的记录继续通过birthday &gt; '1980-01-01'条件继续过滤。 这样子对于联合索引idx_name_birthday_phone_number来说，只能用到name列的部分，而用不到birthday列的部分，因为只有name值相同的情况下才能用birthday列的值进行排序，而这个查询中通过name进行范围查找的记录中可能并不是按照birthday列进行排序的，所以在搜索条件中继续以birthday列进行查找时是用不到这个B+树索引的 精确匹配某一列并范围匹配另外一列如果左边的列是精确查找，则右边的列可以进行范围查找，比方说这样： 1SELECT * FROM person_info WHERE name = 'Ashburn' AND birthday &gt; '1980-01-01' AND birthday &lt; '2000-12-31' AND phone_number &gt; '15100000000'; 这个查询的条件可以分为3个部分： name = 'Ashburn'，对name列进行精确查找，当然可以使用B+树索引了。 birthday &gt; '1980-01-01' AND birthday &lt; '2000-12-31'，由于name列是精确查找，所以通过name = 'Ashburn'条件查找后得到的结果的name值都是相同的，它们会再按照birthday的值进行排序。所以此时对birthday列进行范围查找是可以用到B+树索引的。 phone_number &gt; '15100000000'，通过birthday的范围查找的记录的birthday的值可能不同，所以这个条件无法再利用B+树索引了，只能遍历上一步查询得到的记录。 同理，下边的查询也是可能用到这个idx_name_birthday_phone_number联合索引的： 1SELECT * FROM person_info WHERE name = 'Ashburn' AND birthday = '1980-01-01' AND AND phone_number &gt; '15100000000'; 用于排序的时候-OrderBy有时候的查询语句需要对查出来的记录通过Orderby进行排序操作，一般情况是这样操作的： 一般情况下，我们只能把记录都加载到内存中，再用一些排序算法，比如快速排序、归并排序、吧啦吧啦排序等等在内存中对这些记录进行排序，有的时候可能查询的结果集太大以至于不能在内存中进行排序的话，还可能暂时借助磁盘的空间来存放中间结果，排序操作完成后再把排好序的结果集返回到客户端。在MySQL中，把这种在内存中或者磁盘上进行排序的方式统称为文件排序。 1SELECT * FROM person_info ORDER BY name, birthday, phone_number LIMIT 10; 这个查询的结果集需要先按照name值排序，如果记录的name值相同，则需要按照birthday来排序，如果birthday的值相同，则需要按照phone_number排序。大家可以回过头去看我们建立的idx_name_birthday_phone_number索引的示意图，因为这个B+树索引本身就是按照上述规则排好序的，所以直接从索引中提取数据，然后进行回表操作取出该索引中不包含的列就好了。 但是如果顺序不对的话就不能了。 orderBy后面的列的顺序必须和联合索引的顺序一致才行，如果是下面这个sql 就只能用到部分了 1SELECT * FROM person_info WHERE name = 'A' ORDER BY birthday, phone_number LIMIT 10; 不可以用于排序的情况ASC、DESC混用对于使用联合索引进行排序的场景，我们要求各个排序列的排序顺序是一致的，也就是要么各个列都是ASC规则排序，要么都是DESC规则排序。 1小贴士：ORDER BY子句后的列如果不加ASC或者DESC默认是按照ASC排序规则排序的，也就是升序排序的。 为啥会有这种奇葩规定呢？这个还得回头想想这个idx_name_birthday_phone_number联合索引中记录的结构： 先按照记录的name列的值进行升序排列。 如果记录的name列的值相同，再按照birthday列的值进行升序排列。 如果记录的birthday列的值相同，再按照phone_number列的值进行升序排列。 如果查询中的各个排序列的排序顺序是一致的，比方说下边这两种情况： ORDER BY name, birthday LIMIT 10 这种情况直接从索引的最左边开始往右读10行记录就可以了。 ORDER BY name DESC, birthday DESC LIMIT 10， 这种情况直接从索引的最右边开始往左读10行记录就可以了。 但是如果我们查询的需求是先按照name列进行升序排列，再按照birthday列进行降序排列的话，比如说这样的查询语句： 1SELECT * FROM person_info ORDER BY name, birthday DESC LIMIT 10; 这样如果使用索引排序的话过程就是这样的： 先从索引的最左边确定name列最小的值，然后找到name列等于该值的所有记录，然后从name列等于该值的最右边的那条记录开始往左找10条记录。 如果name列等于最小的值的记录不足10条，再继续往右找name值第二小的记录，重复上边那个过程，直到找到10条记录为止。 累不累？累！重点是这样不能高效使用索引，而要采取更复杂的算法去从索引中取数据，设计MySQL的大叔觉得这样还不如直接文件排序来的快，所以就规定使用联合索引的各个排序列的排序顺序必须是一致的。 WHERE子句中出现非排序使用到的索引列如果WHERE子句中出现了非排序使用到的索引列，那么排序依然是使用不到索引的，比方说这样： 1SELECT * FROM person_info WHERE country = 'China' ORDER BY name LIMIT 10; 这个查询只能先把符合搜索条件country = 'China'的记录提取出来后再进行排序，是使用不到索引。注意和下边这个查询作区别： 1SELECT * FROM person_info WHERE name = 'A' ORDER BY birthday, phone_number LIMIT 10; 虽然这个查询也有搜索条件，但是name = 'A'可以使用到索引idx_name_birthday_phone_number，而且过滤剩下的记录还是按照birthday、phone_number列排序的，所以还是可以使用索引进行排序的。 排序列包含非同一个索引的列有时候用来排序的多个列不是一个索引里的，这种情况也不能使用索引进行排序，比方说： 1SELECT * FROM person_info ORDER BY name, country LIMIT 10; name和country并不属于一个联合索引中的列，所以无法使用索引进行排序，至于为啥我就不想再唠叨了，自己用前边的理论自己捋一捋把～ 排序列使用了复杂的表达式要想使用索引进行排序操作，必须保证索引列是以单独列的形式出现，而不是修饰过的形式，比方说这样： 1SELECT * FROM person_info ORDER BY UPPER(name) LIMIT 10; 使用了UPPER函数修饰过的列就不是单独的列啦，这样就无法使用索引进行排序啦。 用于分组的时候-group by有时候我们为了方便统计表中的一些信息，会把表中的记录按照某些列进行分组。比如下边这个分组查询： 1SELECT name, birthday, phone_number, COUNT(*) FROM person_info GROUP BY name, birthday, phone_number 这个查询语句相当于做了3次分组操作： 先把记录按照name值进行分组，所有name值相同的记录划分为一组。 将每个name值相同的分组里的记录再按照birthday的值进行分组，将birthday值相同的记录放到一个小分组里，所以看起来就像在一个大分组里又化分了好多小分组。 再将上一步中产生的小分组按照phone_number的值分成更小的分组，所以整体上看起来就像是先把记录分成一个大分组，然后把大分组分成若干个小分组，然后把若干个小分组再细分成更多的小小分组。 然后针对那些小小分组进行统计，比如在我们这个查询语句中就是统计每个小小分组包含的记录条数。如果没有索引的话，这个分组过程全部需要在内存里实现，而如果有了索引的话，恰巧这个分组顺序又和我们的B+树中的索引列的顺序是一致的，而我们的B+树索引又是按照索引列排好序的，这不正好么，所以可以直接使用B+树索引进行分组。 和使用B+树索引进行排序是一个道理，分组列的顺序也需要和索引列的顺序一致，也可以只使用索引列中左边的列进行分组，吧啦吧啦的～ 尽量用覆盖索引上边的讨论对回表这个词儿多是一带而过，可能大家没啥深刻的体会，下边我们详细唠叨下。还是用idx_name_birthday_phone_number索引为例，看下边这个查询： 1SELECT * FROM person_info WHERE name &gt; 'Asa' AND name &lt; 'Barlow'; 在使用idx_name_birthday_phone_number索引进行查询时大致可以分为这两个步骤： 从索引idx_name_birthday_phone_number对应的B+树中取出name值在Asa～Barlow之间的用户记录。 由于索引idx_name_birthday_phone_number对应的B+树用户记录中只包含name、age、birthday、id这4个字段，而查询列表是*，意味着要查询表中所有字段，也就是还要包括country字段。这时需要把从上一步中获取到的每一条记录的id字段都到聚簇索引对应的B+树中找到完整的用户记录，也就是我们通常所说的回表，然后把完整的用户记录返回给查询用户。 由于索引idx_name_birthday_phone_number对应的B+树中的记录首先会按照name列的值进行排序，所以值在Asa～Barlow之间的记录在磁盘中的存储是相连的，集中分布在一个或几个数据页中，我们可以很快的把这些连着的记录从磁盘中读出来，这种读取方式我们也可以称为顺序I/O。根据第1步中获取到的记录的id字段的值可能并不相连，而在聚簇索引中记录是根据id（也就是主键）的顺序排列的，所以根据这些并不连续的id值到聚簇索引中访问完整的用户记录可能分布在不同的数据页中，这样读取完整的用户记录可能要访问更多的数据页，这种读取方式我们也可以称为随机I/O。一般情况下，顺序I/O比随机I/O的性能高很多，所以步骤1的执行可能很快，而步骤2就慢一些。所以这个使用索引idx_name_birthday_phone_number的查询有这么两个特点： 会使用到两个B+树索引，一个二级索引，一个聚簇索引。 访问二级索引使用顺序I/O，访问聚簇索引使用随机I/O。 需要回表的记录越多，使用二级索引的性能就越低，甚至让某些查询宁愿使用全表扫描也不使用二级索引。比方说name值在Asa～Barlow之间的用户记录数量占全部记录数量90%以上，那么如果使用idx_name_birthday_phone_number索引的话，有90%多的id值需要回表，这不是吃力不讨好么，还不如直接去扫描聚簇索引（也就是全表扫描）。 那什么时候采用全表扫描的方式，什么使用采用二级索引 + 回表的方式去执行查询呢？这个就是传说中的查询优化器做的工作，查询优化器会事先对表中的记录计算一些统计数据，然后再利用这些统计数据根据查询的条件来计算一下需要回表的记录数，需要回表的记录数越多，就越倾向于使用全表扫描，反之倾向于使用二级索引 + 回表的方式。当然优化器做的分析工作不仅仅是这么简单，但是大致上是个这个过程。一般情况下，限制查询获取较少的记录数会让优化器更倾向于选择使用二级索引 + 回表的方式进行查询，因为回表的记录越少，性能提升就越高，比方说上边的查询可以改写成这样： 1SELECT * FROM person_info WHERE name &gt; 'Asa' AND name &lt; 'Barlow' LIMIT 10; 添加了LIMIT 10的查询更容易让优化器采用二级索引 + 回表的方式进行查询。 对于有排序需求的查询，上边讨论的采用全表扫描还是二级索引 + 回表的方式进行查询的条件也是成立的，比方说下边这个查询： 1SELECT * FROM person_info ORDER BY name, birthday, phone_number; 由于查询列表是*，所以如果使用二级索引进行排序的话，需要把排序完的二级索引记录全部进行回表操作，这样操作的成本还不如直接遍历聚簇索引然后再进行文件排序（filesort）低，所以优化器会倾向于使用全表扫描的方式执行查询。如果我们加了LIMIT子句，比如这样： 1SELECT * FROM person_info ORDER BY name, birthday, phone_number LIMIT 10; 这样需要回表的记录特别少，优化器就会倾向于使用二级索引 + 回表的方式执行查询。 为了彻底告别回表操作带来的性能损耗，我们建议：最好在查询列表里只包含索引列，比如这样： 1SELECT name, birthday, phone_number FROM person_info WHERE name &gt; 'Asa' AND name &lt; 'Barlow' 因为我们只查询name, birthday, phone_number这三个索引列的值，所以在通过idx_name_birthday_phone_number索引得到结果后就不必到聚簇索引中再查找记录的剩余列，也就是country列的值了，这样就省去了回表操作带来的性能损耗。我们把这种只需要用到索引的查询方式称为索引覆盖。排序操作也优先使用覆盖索引的方式进行查询，比方说这个查询： 1SELECT name, birthday, phone_number FROM person_info ORDER BY name, birthday, phone_number; 虽然这个查询中没有LIMIT子句，但是采用了覆盖索引，所以查询优化器就会直接使用idx_name_birthday_phone_number索引进行排序而不需要回表操作了。 当然，如果业务需要查询出索引以外的列，那还是以保证业务需求为重。但是我们很不鼓励用*号作为查询列表，最好把我们需要查询的列依次标明。 建立合适的索引建议只为用于搜索、排序或分组的列创建索引也就是说，只为出现在WHERE子句中的列、连接子句中的连接列，或者出现在ORDER BY或GROUP BY子句中的列创建索引。而出现在查询列表中的列就没必要建立索引了： 1SELECT birthday, country FROM person_name WHERE name = 'Ashburn'; 像查询列表中的birthday、country这两个列就不需要建立索引，我们只需要为出现在WHERE子句中的name列创建索引就可以了。 考虑列的基数列的基数指的是某一列中不重复数据的个数，比方说某个列包含值2, 5, 8, 2, 5, 8, 2, 5, 8，虽然有9条记录，但该列的基数却是3。也就是说，在记录行数一定的情况下，列的基数越大，该列中的值越分散，列的基数越小，该列中的值越集中。这个列的基数指标非常重要，直接影响我们是否能有效的利用索引。假设某个列的基数为1，也就是所有记录在该列中的值都一样，那为该列建立索引是没有用的，因为所有值都一样就无法排序，无法进行快速查找了～ 而且如果某个建立了二级索引的列的重复值特别多，那么使用这个二级索引查出的记录还可能要做回表操作，这样性能损耗就更大了。所以结论就是：最好为那些列的基数大的列建立索引，为基数太小列的建立索引效果可能不好。 索引列的类型尽量小我们在定义表结构的时候要显式的指定列的类型，以整数类型为例，有TINYINT、MEDIUMINT、INT、BIGINT这么几种，它们占用的存储空间依次递增，我们这里所说的类型大小指的就是该类型表示的数据范围的大小。能表示的整数范围当然也是依次递增，如果我们想要对某个整数列建立索引的话，在表示的整数范围允许的情况下，尽量让索引列使用较小的类型，比如我们能使用INT就不要使用BIGINT，能使用MEDIUMINT就不要使用INT～ 这是因为： 数据类型越小，在查询时进行的比较操作越快（这是CPU层次的东东） 数据类型越小，索引占用的存储空间就越少，在一个数据页内就可以放下更多的记录，从而减少磁盘I/O带来的性能损耗，也就意味着可以把更多的数据页缓存在内存中，从而加快读写效率。 这个建议对于表的主键来说更加适用，因为不仅是聚簇索引中会存储主键值，其他所有的二级索引的节点处都会存储一份记录的主键值，如果主键适用更小的数据类型，也就意味着节省更多的存储空间和更高效的I/O。 索引字符串值的前缀我们知道一个字符串其实是由若干个字符组成，如果我们在MySQL中使用utf8字符集去存储字符串的话，编码一个字符需要占用1~3个字节。假设我们的字符串很长，那存储一个字符串就需要占用很大的存储空间。在我们需要为这个字符串列建立索引时，那就意味着在对应的B+树中有这么两个问题： B+树索引中的记录需要把该列的完整字符串存储起来，而且字符串越长，在索引中占用的存储空间越大。 如果B+树索引中索引列存储的字符串很长，那在做字符串比较时会占用更多的时间。 我们前边儿说过索引列的字符串前缀其实也是排好序的，所以索引的设计者提出了个方案 — 只对字符串的前几个字符进行索引也就是说在二级索引的记录中只保留字符串前几个字符。这样在查找记录时虽然不能精确的定位到记录的位置，但是能定位到相应前缀所在的位置，然后根据前缀相同的记录的主键值回表查询完整的字符串值，再对比就好了。这样只在B+树中存储字符串的前几个字符的编码，既节约空间，又减少了字符串的比较时间，还大概能解决排序的问题，何乐而不为，比方说我们在建表语句中只对name列的前10个字符进行索引可以这么写： 1CREATE TABLE person_info( name VARCHAR(100) NOT NULL, birthday DATE NOT NULL, phone_number CHAR(11) NOT NULL, country varchar(100) NOT NULL, KEY idx_name_birthday_phone_number (name(10), birthday, phone_number)); name(10)就表示在建立的B+树索引中只保留记录的前10个字符的编码，这种只索引字符串值的前缀的策略是我们非常鼓励的，尤其是在字符串类型能存储的字符比较多的时候。 索引列前缀对排序的影响如果使用了索引列前缀，比方说前边只把name列的前10个字符放到了二级索引中，下边这个查询可能就有点儿尴尬了： 1SELECT * FROM person_info ORDER BY name LIMIT 10; 因为二级索引中不包含完整的name列信息，所以无法对前十个字符相同，后边的字符不同的记录进行排序，也就是使用索引列前缀的方式无法支持使用索引排序，只好乖乖的用文件排序喽。 让索引列在比较表达式中单独出现假设表中有一个整数列my_col，我们为这个列建立了索引。下边的两个WHERE子句虽然语义是一致的，但是在效率上却有差别： WHERE my_col * 2 &lt; 4 WHERE my_col &lt; 4/2 第1个WHERE子句中my_col列并不是以单独列的形式出现的，而是以my_col * 2这样的表达式的形式出现的，存储引擎会依次遍历所有的记录，计算这个表达式的值是不是小于4，所以这种情况下是使用不到为my_col列建立的B+树索引的。而第2个WHERE子句中my_col列并是以单独列的形式出现的，这样的情况可以直接使用B+树索引。 所以结论就是：如果索引列在比较表达式中不是以单独列的形式出现，而是以某个表达式，或者函数调用形式出现的话，是用不到索引的。 主键插入顺序我们知道，对于一个使用InnoDB存储引擎的表来说，在我们没有显式的创建索引时，表中的数据实际上都是存储在聚簇索引的叶子节点的。而记录又是存储在数据页中的，数据页和记录又是按照记录主键值从小到大的顺序进行排序，所以如果我们插入的记录的主键值是依次增大的话，那我们每插满一个数据页就换到下一个数据页继续插，而如果我们插入的主键值忽大忽小的话，这就比较麻烦了，假设某个数据页存储的记录已经满了，它存储的主键值在1~100之间： 如果此时再插入一条主键值为9的记录，那它插入的位置就如下图： 可这个数据页已经满了啊，再插进来咋办呢？我们需要把当前页面分裂成两个页面，把本页中的一些记录移动到新创建的这个页中。页面分裂和记录移位意味着什么？意味着：性能损耗！所以如果我们想尽量避免这样无谓的性能损耗，最好让插入的记录的主键值依次递增，这样就不会发生这样的性能损耗了。所以我们建议：让主键具有AUTO_INCREMENT，让存储引擎自己为表生成主键，而不是我们手动插入 ，比方说我们可以这样定义person_info表： 1CREATE TABLE person_info( id INT UNSIGNED NOT NULL AUTO_INCREMENT, name VARCHAR(100) NOT NULL, birthday DATE NOT NULL, phone_number CHAR(11) NOT NULL, country varchar(100) NOT NULL, PRIMARY KEY (id), KEY idx_name_birthday_phone_number (name(10), birthday, phone_number)); 我们自定义的主键列id拥有AUTO_INCREMENT属性，在插入记录时存储引擎会自动为我们填入自增的主键值。 冗余和重复索引有时候有的同学有意或者无意的就对同一个列创建了多个索引，比方说这样写建表语句： 1CREATE TABLE person_info( id INT UNSIGNED NOT NULL AUTO_INCREMENT, name VARCHAR(100) NOT NULL, birthday DATE NOT NULL, phone_number CHAR(11) NOT NULL, country varchar(100) NOT NULL, PRIMARY KEY (id), KEY idx_name_birthday_phone_number (name(10), birthday, phone_number), KEY idx_name (name(10))); 我们知道，通过idx_name_birthday_phone_number索引就可以对name列进行快速搜索，再创建一个专门针对name列的索引就算是一个冗余索引，维护这个索引只会增加维护的成本，并不会对搜索有什么好处。 另一种情况，我们可能会对某个列重复建立索引，比方说这样： 1CREATE TABLE repeat_index_demo ( c1 INT PRIMARY KEY, c2 INT, UNIQUE uidx_c1 (c1), INDEX idx_c1 (c1)); 我们看到，c1既是主键、又给它定义为一个唯一索引，还给它定义了一个普通索引，可是主键本身就会生成聚簇索引，所以定义的唯一索引和普通索引是重复的，这种情况要避免。 总结","link":"/2021/09/06/MySQL%EF%BC%88%E4%B8%89%EF%BC%89%E7%B4%A2%E5%BC%95/"},{"title":"MySQL（二）存储引擎","text":"本篇讲述了存储引擎InnoDB： 行记录存储格式 数据页存储格式 InnoDB中的缓冲池 LRU链表是如何优化的 InnoDB和MyISAM的区别 一条SQL语句的执行流程 客户端请求 连接器（验证用户身份，给予权限） 查询缓存（存在缓存则直接返回，不存在则执行后续操作，mysql8.0之后取消了缓存） 分析器（对SQL进行词法分析和语法分析操作 优化器（主要对执行的sql优化选择最优的执行方案方法） 执行器（执行时会先看用户是否有执行权限，有才去使用这个引擎提供的接口） 去引擎层获取数据返回（如果开启查询缓存则会缓存查询结果） InnoDB存储引擎InnoDB 现在是 MySQL 默认的存储引擎，支持事务、行锁设计、MVCC、外键等特点。 行记录存储结构InnoDB将表中的数据存储在磁盘中，即使关机重启了数据还是存在的。 但是处理数据的过程是发生在内存中的，所以需要把磁盘中的数据加载到内存中，如果是处理写入或修改请求的话，还需要把内存中的内容刷新到磁盘上。 而读写磁盘和读写内存的速度相差好几个数量级，如果一行一行的读速度非常慢。所以InnoDB采取的方式是：将数据划分为若干个页，以页作为磁盘和内存之间交互的基本单位，InnoDB中页的大小一般为 16 KB。也就是在一般情况下，一次最少从磁盘中读取16KB的内容到内存中，一次最少把内存中的16KB内容刷新到磁盘中。也就是说数据页才是InnoDB内存和磁盘交换数据的基本单元。 行格式有4种：分别为 Compact 、 Redunant、Dynamic 和 Compressed。 可以在创建表的时候就指定行格式,也可以修改行格式,如下： 123CREATE TABLE 表名 (列的信息) ROW_FORMAT=行格式名称 ALTER TABLE 表名 ROW_FORMAT=行格式名称 创建一个例子 12345678910111213141516171819202122mysql&gt; CREATE TABLE record_format_demo ( -&gt; c1 VARCHAR(10), -&gt; c2 VARCHAR(10) NOT NULL, -&gt; c3 CHAR(10), -&gt; c4 VARCHAR(10) -&gt; ) CHARSET=ascii ROW_FORMAT=COMPACT; // 创建一个数据库指定行格式为 compact Query OK, 0 rows affected (0.04 sec)mysql&gt; INSERT INTO record_format_demo(c1, c2, c3, c4) VALUES('aaaa', 'bbb', 'cc', 'd'), ('eeee', 'fff', NULL, NULL);Query OK, 2 rows affected (0.01 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; SELECT * FROM record_format_demo -&gt; ;+------+-----+------+------+| c1 | c2 | c3 | c4 |+------+-----+------+------+| aaaa | bbb | cc | d || eeee | fff | NULL | NULL |+------+-----+------+------+2 rows in set (0.00 sec) ascii字符集只包括空格、标点符号、数字、大小写字母和一些不可见字符 Compact行格式分为记录的额外信息 + 记录的真实数据。 额外信息是为了描述这个记录而必须要加入的一些信息： 变长字段长度列表MySQL中支持一些变长字段比如 varchar、text 等。这些字段存储多少个字节是不确定的，所以存储的时候需要把这些数据占用的真实字节数页存起来 在Compact行格式中，把所有变长字段的真实数据占用的字节长度都存放在记录的开头部位，从而形成一个变长字段长度列表，各变长字段数据占用的字节数按照列的顺序逆序存放，注意是逆序存放！ 比如record_format_demo这个表其中的C1、C2 和 C4都是varchar类型的，所以这三列需要存储长度： 列名 存储内容 内容长度（十进制表示） 内容长度（十六进制表示） c1 'aaaa' 4 0x04 c2 'bbb' 3 0x03 c4 'd' 1 0x01 又因为这些长度值需要按照列的逆序存放，所以最后变长字段长度列表的字节串用十六进制表示的效果就是（各个字节之间实际上没有空格，用空格隔开只是方便理解）： 123456789mysql&gt; SELECT * FROM record_format_demo -&gt; ;+------+-----+------+------+| c1 | c2 | c3 | c4 |+------+-----+------+------+| aaaa | bbb | cc | d || eeee | fff | NULL | NULL |+------+-----+------+------+2 rows in set (0.00 sec) 如果该可变字段允许存储的最大字节数（M×W）超过255字节并且真实存储的字节数（L）超过127字节，则使用2个字节，否则使用1个字节 另外需要注意的一点是，变长字段长度列表中只存储值为 非NULL 的列内容占用的长度，值为 NULL 的列的长度是不储存的 。也就是说对于第二条记录来说，因为c4列的值为NULL，所以第二条记录的变长字段长度列表只需要存储c1和c2列的长度即可。其中c1列存储的值为'eeee'，占用的字节数为4，c2列存储的值为'fff'，占用的字节数为3，所以变长字段长度列表需2个字节。填充完变长字段长度列表的两条记录的对比图如下： Null值列表记录头信息记录的真实数据Redundant已经过时了，就不讲了 Dynamic和CompressedMySQL版本为5.7，默认的行格式就是Dynamic，这俩行格式和Compact行格式挺像，只不过在处理行溢出数据时有点儿分歧，它们不会在记录的真实数据处存储字段真实数据的前768个字节，而是把所有的字节都存储到其他页面中，只在记录的真实数据处存储其他页面的地址，就像这样： 数据页存储结构 从图中可以看出，一个InnoDB数据页的存储空间大致被划分成了7个部分，有的部分占用的字节数是确定的，有的部分占用的字节数是不确定的。下边我们用表格的方式来大致描述一下这7个部分都存储一些啥内容（快速的瞅一眼就行了，后边会详细唠叨的）： 名称 中文名 占用空间大小 简单描述 File Header 文件头部 38字节 页的一些通用信息 Page Header 页面头部 56字节 数据页专有的一些信息 Infimum + Supremum 最小记录和最大记录 26字节 两个虚拟的行记录 User Records 用户记录 不确定 实际存储的行记录内容 Free Space 空闲空间 不确定 页中尚未使用的空间 Page Directory 页面目录 不确定 页中的某些记录的相对位置 File Trailer 文件尾部 8字节 校验页是否完整 行记录在页中的存储存储的记录会按照指定的行格式存储到User Records部分。 但是在一开始生成页的时候，其实并没有User Records这个部分，每当我们插入一条记录，都会从Free Space部分，也就是尚未使用的存储空间中申请一个记录大小的空间划分到User Records部分，当Free Space部分的空间全部被User Records部分替代掉之后，也就意味着这个页使用完了，如果还有新的记录插入的话，就需要去申请新的页了，这个过程的图示如下： 再说记录头先建个表： 1234567mysql&gt; CREATE TABLE page_demo( -&gt; c1 INT, -&gt; c2 INT, -&gt; c3 VARCHAR(10000), -&gt; PRIMARY KEY (c1) -&gt; ) CHARSET=ascii ROW_FORMAT=Compact;Query OK, 0 rows affected (0.03 sec) 行格式如下： 记录头信息的相关数据： 名称 大小（单位：bit） 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 表示当前记录拥有的记录数 heap_no 13 表示当前记录在记录堆的位置信息 record_type 3 表示当前记录的类型，0表示普通记录，1表示B+树非叶节点记录，2表示最小记录，3表示最大记录 next_record 16 表示下一条记录的相对位置 插入数据后： 123mysql&gt; INSERT INTO page_demo VALUES(1, 100, 'aaaa'), (2, 200, 'bbbb'), (3, 300, 'cccc'), (4, 400, 'dddd');Query OK, 4 rows affected (0.00 sec)Records: 4 Duplicates: 0 Warnings: 0 delete_mask :标记着当前记录是否被删除，占用1个二进制位。0表示没有删除，1表示记录被删除了。 真实的删除之后还要把其他记录重新排列需要消耗性能，所以只是打一个删除的标记。所有被删除的记录会组成一个垃圾链表。这个链表中的所有记录占据的空间为可重用空间。之后如果有新记录插入到表中的话，可能把这些被删除的记录占用的存储空间覆盖掉。 Min_rec_mask: B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned: Heap_no: 表示当前记录在页面中的位置。如上图所示，分别为2、3、4、5 , 0 和 1为InnoDB自动插入的两个记录，分别为最小记录和最大记录。这里的最小和最大比较的是主键的值，无论插入多少行，这两条都是最小和最大记录且不存放在页的 UserRecord中 record_type：共有4种类型的记录，0表示普通记录，1表示B+树非叶节点记录，2表示最小记录，3表示最大记录。从图中我们也可以看出来，我们自己插入的记录就是普通记录 next_record：它表示从当前记录的真实数据到下一条记录的真实数据的地址偏移量，其实就是一个链表。 如果删除了第二条记录： 从图中可以看出来，删除第2条记录前后主要发生了这些变化： 第2条记录并没有从存储空间中移除，而是把该条记录的delete_mask值设置为1。 第2条记录的next_record值变为了0，意味着该记录没有下一条记录了。 第1条记录的next_record指向了第3条记录。 还有一点你可能忽略了，就是最大记录的n_owned值从5变成了4，关于这一点的变化我们稍后会详细说明的。 所以，不论我们怎么对页中的记录做增删改操作，InnoDB始终会维护一条记录的单链表，链表中的各个节点是按照主键值由小到大的顺序连接起来的。 如果我们再次把这条记录插入到表中，会发生什么事呢？ 会直接复用之前的被删除记录的存储空间。 Page Directory（页目录）方便在页内查找记录的设计： 将所有正常的记录（包括最大和最小记录，不包括标记为已删除的记录）划分为几个组。 每个组的最后一条记录（也就是组内最大的那条记录）的头信息中的n_owned属性表示该记录拥有多少条记录，也就是该组内共有几条记录。 将每个组的最后一条记录的地址偏移量单独提取出来按顺序存储到靠近页的尾部的地方，这个地方就是所谓的Page Directory，也就是页目录（此时应该返回头看看页面各个部分的图）。页面目录中的这些地址偏移量被称为槽（英文名：Slot），所以这个页面目录就是由槽组成的。 现在页目录有2个槽，说明我们的记录被分成了两个组： 槽0中的值是112，代表最大记录的地址偏移量，（就是从页面的0字节开始数，数112个字节） 槽1中的值是99，代表最小记录的地址偏移量。 最小记录的n_owned值为1，这就代表着以最小记录结尾的这个分组中只有1条记录，也就是最小记录本身。 最大记录的n_owned值为5，这就代表着以最大记录结尾的这个分组中只有5条记录，包括最大记录本身还有我们自己插入的4条记录。 那么为什么最小记录的n_owned值为1，而最大记录为5呢？是如何划分的呢？ 其实是有规定的： 对于最小记录所在的分组只能有 1 条记录，最大记录所在的分组拥有的记录条数只能在 1~8 条之间，剩下的分组中记录的条数范围只能在是 4~8 条之间。所以分组是按照下边的步骤进行的： 初始情况下一个数据页里只有最小记录和最大记录两条记录，它们分属于两个分组。 之后每插入一条记录，都会从页目录中找到主键值比本记录的主键值大并且差值最小的槽，然后把该槽对应的记录的n_owned值加1，表示本组内又添加了一条记录，直到该组中的记录数等于8个。 在一个组中的记录数等于8个后再插入一条记录时，会将组中的记录拆分成两个组，一个组中4条记录，另一个5条记录。这个过程会在页目录中新增一个槽来记录这个新增分组中最大的那条记录的偏移量。 12// 继续插入数据mysql&gt; INSERT INTO page_demo VALUES(5, 500, 'eeee'), (6, 600, 'ffff'), (7, 700, 'gggg'), (8, 800, 'hhhh'), (9, 900, 'iiii'), (10, 1000, 'jjjj'), (11, 1100, 'kkkk'), (12, 1200, 'llll'), (13, 1300, 'mmmm'), (14, 1400, 'nnnn'), (15, 1500, 'oooo'), (16, 1600, 'pppp');Query OK, 12 rows affected (0.00 sec)Records: 12 Duplicates: 0 Warnings: 0 二分查找了：比如说查找主键为5的记录： 计算中间槽的位置，槽2对应的记录为8，大于5，所以high = 2 ， low = 0继续二叉中安 找到槽1，槽1对应的主键为4，小于5，所以low = 1， high不变 确定主键为5的记录在槽2里面，遍历就行了。 有了页目录之后： 所以在一个数据页中查找指定主键值的记录的过程分为两步： 通过二分法确定该记录所在的槽。 通过记录的next_record属性遍历该槽所在的组中的各个记录。 Page Header（页面头部）为了能得到一个数据页中存储的记录的状态信息，比如本页中已经存储了多少条记录，第一条记录的地址是什么，页目录中存储了多少个槽等等，特意在页中定义了一个叫Page Header的部分，它是页结构的第二部分，这个部分占用固定的56个字节，专门存储各种状态信息，具体各个字节都是干嘛的看下表： 名称 占用空间大小 描述 PAGE_N_DIR_SLOTS 2字节 在页目录中的槽数量 PAGE_HEAP_TOP 2字节 还未使用的空间最小地址，也就是说从该地址之后就是Free Space PAGE_N_HEAP 2字节 本页中的记录的数量（包括最小和最大记录以及标记为删除的记录） PAGE_FREE 2字节 第一个已经标记为删除的记录地址（各个已删除的记录通过next_record也会组成一个单链表，这个单链表中的记录可以被重新利用） PAGE_GARBAGE 2字节 已删除记录占用的字节数 PAGE_LAST_INSERT 2字节 最后插入记录的位置 PAGE_DIRECTION 2字节 记录插入的方向 PAGE_N_DIRECTION 2字节 一个方向连续插入的记录数量 PAGE_N_RECS 2字节 该页中记录的数量（不包括最小和最大记录以及被标记为删除的记录） PAGE_MAX_TRX_ID 8字节 修改当前页的最大事务ID，该值仅在二级索引中定义 PAGE_LEVEL 2字节 当前页在B+树中所处的层级 PAGE_INDEX_ID 8字节 索引ID，表示当前页属于哪个索引 PAGE_BTR_SEG_LEAF 10字节 B+树叶子段的头部信息，仅在B+树的Root页定义 PAGE_BTR_SEG_TOP 10字节 B+树非叶子段的头部信息，仅在B+树的Root页定义 如果大家认真看过前边的文章，从PAGE_N_DIR_SLOTS到PAGE_LAST_INSERT以及PAGE_N_RECS的意思大家一定是清楚的，如果不清楚，对不起，你应该回头再看一遍前边的文章。剩下的状态信息看不明白不要着急，饭要一口一口吃，东西要一点一点学（一定要稍安勿躁哦，不要被这些名词吓到）。在这里我们先唠叨一下PAGE_DIRECTION和PAGE_N_DIRECTION的意思： PAGE_DIRECTION 假如新插入的一条记录的主键值比上一条记录的主键值比上一条记录大，我们说这条记录的插入方向是右边，反之则是左边。用来表示最后一条记录插入方向的状态就是PAGE_DIRECTION。 PAGE_N_DIRECTION 假设连续几次插入新记录的方向都是一致的，InnoDB会把沿着同一个方向插入记录的条数记下来，这个条数就用PAGE_N_DIRECTION这个状态表示。当然，如果最后一条记录的插入方向改变了的话，这个状态的值会被清零重新统计。 File Header（文件头部）File Trailer （文件尾部）InnoDB中的缓冲池缓冲池的目的缓冲池简单来说就是一块内存区域，通过内存的速度来弥补CPU和磁盘之间速度的差异。Buffer Pool默认有128M大小。 在数据库中读取页的操作，首先将从磁盘读到的页存放在缓冲池中，下次再读相同的页时，首先判断该页是否在缓冲池中，如果在则命中，否则从磁盘中进行读取。 在数据库中修改页的操作，首先修改在缓冲池中的页，然后再以一定的频率刷新到磁盘上（通过一种checkpoint到机制触发，并不是每次修改都会刷新）。 1234567891011121314mysql&gt; select version();+-----------+| version() |+-----------+| 8.0.23 |+-----------+1 row in set (0.00 sec)mysql&gt; show variables like 'innodb_buffer_pool_size';+-------------------------+-----------+| Variable_name | Value |+-------------------------+-----------+| innodb_buffer_pool_size | 134217728 |+-------------------------+-----------+1 row in set (0.01 sec) 缓冲池内部组成Buffer Pool中的缓存页和数据页大小是一样的，都是16KB。 每个缓存页都有一些控制信息，比如⻚所属的表空间编号、⻚号、缓存⻚在Buffer Pool中的地址、链表节点信息、⼀些锁信息以及LSN信息等。这些都放在控制块中，控制块和缓存页是一一对应的。 每个控制块⼤约占⽤缓存⻚⼤⼩的5%，在MySQL5.7.21这个版本 中，每个控制块占⽤的⼤⼩是808字节。⽽我们设置的 innodb_buffer_pool_size并不包含这部分控制块占⽤的内存空间⼤⼩，也就是说InnoDB在为Buffer Pool向操作系统申请连续 的内存空间时，这⽚连续的内存空间⼀般会⽐ innodb_buffer_pool_size的值⼤5%左右。 Free链表所有空闲的 缓存页对应的控制卡作为一个节点放入到一个链表中去，这个链表就称为空闲链表。 为了管理这个free链表，特定新建了一个基节点来保存链表的头节点、尾节点以及节点数量等信息。需要注意的是，链表的基节点占用的内存空间并不包含在Buffer Pool申请的连续内存空间之内 有了free链表之后，每次从磁盘中加载一个页到Buffer Pool中时，都会从free链表中去一个空闲的缓存页，并将该缓存页对应的控制块的信息填上（该页所在的表空间、页号等信息），然后将这个控制块从链表中移除。 如何知道访问的页在不在缓冲池中回头想想，我们是通过表空间号 + 页号来定位一个页的，所以是不是可以： 将 表空间号 + 页号 看做是一个 key ， 而缓存页就是对应的value。这样组成一个哈希表来解决问题！ 访问的页如果在这个哈希表中，则直接使用缓存页就好，如果不在就从free链表中选一个空闲的缓存页，然后将磁盘中对应的页加载到该缓存页的位置。 Flush链表如果我们修改了Buffer Pool中某个缓存⻚的数据，那它就和磁盘 上的⻚不⼀致了，这样的缓存⻚也被称为脏⻚（英⽂名：dirty page） 如果发生一次修改就同步到磁盘的话，就会严重影响程序的性能。所以其实是用一个flush链表来管理，未来的某个时间统一同步到磁盘中去。 凡是修改过的缓存页对应的控制卡都要作为一个节点加入到链表中去，这个链表就是存储脏页的链表Flush链表。 LRU链表LRU链表是为了解决：free链表中已经没有多余的空闲缓存页了，需要把旧的缓存页从Buffer Pool中删除。 如何判断删除哪些页呢？ 简单的LRU链表按照最近最少使用的原则去淘汰缓存页的，所以这个链表可以被称为LRU链表（LRU的英文全称：Least Recently Used）。 当我们需要访问某个页时，可以这样处理LRU链表： 如果该页不在Buffer Pool中，在把该页从磁盘加载到Buffer Pool中的缓存页时，就把该缓存页对应的控制块作为节点塞到链表的头部。 如果该页已经缓存在Buffer Pool中，则直接把该页对应的控制块移动到LRU链表的头部。 也就是说：只要我们使用到某个缓存页，就把该缓存页调整到LRU链表的头部，这样LRU链表尾部就是最近最少使用的缓存页喽～ 所以当Buffer Pool中的空闲缓存页使用完时，到LRU链表的尾部找些缓存页淘汰就OK啦 划分区域的LRU链表简易的LRU链表存在的问题： 预读：InnoDB认为执行当前的请求可能之后会读取某些页面，就预先把它们加载到Buffer Pool中。根据触发方式的不同，预读又可以细分为下边两种： 线性预读 设计InnoDB的大叔提供了一个系统变量innodb_read_ahead_threshold，如果顺序访问了某个区（extent）的页面超过这个系统变量的值，就会触发一次异步读取下一个区中全部的页面到Buffer Pool的请求，注意异步读取意味着从磁盘中加载这些被预读的页面并不会影响到当前工作线程的正常执行。这个innodb_read_ahead_threshold系统变量的值默认是56，我们可以在服务器启动时通过启动参数或者服务器运行过程中直接调整该系统变量的值，不过它是一个全局变量，注意使用SET GLOBAL命令来修改哦。 随机预读 如果Buffer Pool中已经缓存了某个区的13个连续的页面，不论这些页面是不是顺序读取的，都会触发一次异步读取本区中所有其的页面到Buffer Pool的请求。设计InnoDB的大叔同时提供了innodb_random_read_ahead系统变量，它的默认值为OFF，也就意味着InnoDB并不会默认开启随机预读的功能，如果我们想开启该功能，可以通过修改启动参数或者直接使用SET GLOBAL命令把该变量的值设置为ON。 预读本来是个好事儿，如果预读到Buffer Pool中的页成功的被使用到，那就可以极大的提高语句执行的效率。可是如果用不到呢？这些预读的页都会放到LRU链表的头部，但是如果此时Buffer Pool的容量不太大而且很多预读的页面都没有用到的话，这就会导致处在LRU链表尾部的一些缓存页会很快的被淘汰掉，也就是所谓的劣币驱逐良币，会大大降低缓存命中率。 情况二：扫描全表的查询语句（比如没有建立合适的索引或者压根儿没有WHERE子句的查询）。 扫描全表意味着什么？意味着将访问到该表所在的所有页加载到Buffer Pool中，这也就意味着Buffer Pool中的所有页都被替换了，其他查询语句在执行时又得执行一次从磁盘加载到Buffer Pool的操作。而这种全表扫描的语句执行的频率也不高，每次执行都要把Buffer Pool中的缓存页换一次血，这严重的影响到其他查询对 Buffer Pool的使用，从而大大降低了缓存命中率。 总结一下上边说的可能降低Buffer Pool的两种情况： 加载到Buffer Pool中的页不一定被用到。 如果非常多的使用频率偏低的页被同时加载到Buffer Pool时，可能会把那些使用频率非常高的页从Buffer Pool中淘汰掉。 因为有这两种情况的存在，把这个LRU链表按照一定比例分成两截，分别是： 一部分存储使用频率非常高的缓存页，所以这一部分链表也叫做热数据，或者称young区域。 另一部分存储使用频率不是很高的缓存页，所以这一部分链表也叫做冷数据，或者称old区域。 为了方便大家理解，我们把示意图做了简化，各位领会精神就好： 大家要特别注意一个事儿：我们是按照某个比例将LRU链表分成两半的，不是某些节点固定是young区域的，某些节点固定是old区域的，随着程序的运行，某个节点所属的区域也可能发生变化。那这个划分成两截的比例怎么确定呢？对于InnoDB存储引擎来说，我们可以通过查看系统变量innodb_old_blocks_pct的值来确定old区域在LRU链表中所占的比例，比方说这样： 1234567mysql&gt; SHOW VARIABLES LIKE 'innodb_old_blocks_pct';+-----------------------+-------+| Variable_name | Value |+-----------------------+-------+| innodb_old_blocks_pct | 37 |+-----------------------+-------+1 row in set (0.01 sec) 从结果可以看出来，默认情况下，old区域在LRU链表中所占的比例是37%，也就是说old区域大约占LRU链表的3/8。 有了这个被划分成young和old区域的LRU链表之后，设计InnoDB的大叔就可以针对我们上边提到的两种可能降低缓存命中率的情况进行优化了： 针对预读的页面可能不进行后续访情况的优化： 当磁盘上的某个页面在初次加载到Buffer Pool中的某个缓存页时，该缓存页对应的控制块会被放到old区域的头部。这样针对预读到Buffer Pool却不进行后续访问的页面就会被逐渐从old区域逐出，而不会影响young区域中被使用比较频繁的缓存页。 针对全表扫描时，短时间内访问大量使用频率非常低的页面情况的优化 在进行全表扫描时，虽然首次被加载到Buffer Pool的页被放到了old区域的头部，但是后续会被马上访问到，每次进行访问的时候又会把该页放到young区域的头部，这样仍然会把那些使用频率比较高的页面给顶下去。有同学会想：可不可以在第一次访问该页面时不将其从old区域移动到young区域的头部，后续访问时再将其移动到young区域的头部。回答是：行不通！因为设计InnoDB的大叔规定每次去页面中读取一条记录时，都算是访问一次页面，而一个页面中可能会包含很多条记录，也就是说读取完某个页面的记录就相当于访问了这个页面好多次。 咋办？全表扫描有一个特点，那就是它的执行频率非常低，谁也不会没事儿老在那写全表扫描的语句玩，而且在执行全表扫描的过程中，即使某个页面中有很多条记录，也就是去多次访问这个页面所花费的时间也是非常少的。所以我们只需要规定，在对某个处在old区域的缓存页进行第一次访问时就在它对应的控制块中记录下来这个访问时间，如果后续的访问时间与第一次访问的时间在某个时间间隔内，那么该页面就不会被从old区域移动到young区域的头部，否则将它移动到young区域的头部。上述的这个间隔时间是由系统变量innodb_old_blocks_time控制的，你看： 1234567mysql&gt; SHOW VARIABLES LIKE 'innodb_old_blocks_time';+------------------------+-------+| Variable_name | Value |+------------------------+-------+| innodb_old_blocks_time | 1000 |+------------------------+-------+1 row in set (0.01 sec) 这个innodb_old_blocks_time的默认值是1000，它的单位是毫秒，也就意味着对于从磁盘上被加载到LRU链表的old区域的某个页来说，如果第一次和最后一次访问该页面的时间间隔小于1s（很明显在一次全表扫描的过程中，多次访问一个页面中的时间不会超过1s），那么该页是不会被加入到young区域的～ 当然，像innodb_old_blocks_pct一样，我们也可以在服务器启动或运行时设置innodb_old_blocks_time的值，这里就不赘述了，你自己试试吧～ 这里需要注意的是，如果我们把innodb_old_blocks_time的值设置为0，那么每次我们访问一个页面时就会把该页面放到young区域的头部。 综上所述，正是因为将LRU链表划分为young和old区域这两个部分，又添加了innodb_old_blocks_time这个系统变量，才使得预读机制和全表扫描造成的缓存命中率降低的问题得到了遏制，因为用不到的预读页面以及全表扫描的页面都只会被放到old区域，而不影响young区域中的缓存页。 更进一步优化LRU链表LRU链表这就说完了么？没有，早着呢～ 对于young区域的缓存页来说，我们每次访问一个缓存页就要把它移动到LRU链表的头部，这样开销是不是太大啦，毕竟在young区域的缓存页都是热点数据，也就是可能被经常访问的，这样频繁的对LRU链表进行节点移动操作是不是不太好啊？是的，为了解决这个问题其实我们还可以提出一些优化策略，比如只有被访问的缓存页位于young区域的1/4的后边，才会被移动到LRU链表头部，这样就可以降低调整LRU链表的频率，从而提升性能（也就是说如果某个缓存页对应的节点在young区域的1/4中，再次访问该缓存页时也不会将其移动到LRU链表头部）。 还有没有什么别的针对LRU链表的优化措施呢？ 但是不论怎么优化，千万别忘了我们的初心：尽量高效的提高 Buffer Pool 的缓存命中率。 其他链表为了更好的管理Buffer Pool中的缓存页，除了我们上边提到的一些措施，设计InnoDB的大叔们还引进了其他的一些链表，比如unzip LRU链表用于管理解压页，zip clean链表用于管理没有被解压的压缩页，zip free数组中每一个元素都代表一个链表，它们组成所谓的伙伴系统来为压缩页提供内存空间等等，反正是为了更好的管理这个Buffer Pool引入了各种链表或其他数据结构，具体的使用方式就不啰嗦了，大家有兴趣深究的再去找些更深的书或者直接看源代码吧，也可以直接来找我哈～ 刷新脏数据到磁盘后台有专门的线程每隔一段时间负责把脏页刷新到磁盘，这样可以不影响用户线程处理正常的请求。主要有两种刷新路径： 从LRU链表的冷数据中刷新一部分页面到磁盘。 后台线程会定时从LRU链表尾部开始扫描一些页面，扫描的页面数量可以通过系统变量innodb_lru_scan_depth来指定，如果从里边儿发现脏页，会把它们刷新到磁盘。这种刷新页面的方式被称之为BUF_FLUSH_LRU。 从flush链表中刷新一部分页面到磁盘。 后台线程也会定时从flush链表中刷新一部分页面到磁盘，刷新的速率取决于当时系统是不是很繁忙。这种刷新页面的方式被称之为BUF_FLUSH_LIST。 有时候后台线程刷新脏页的进度比较慢，导致用户线程在准备加载一个磁盘页到Buffer Pool时没有可用的缓存页，这时就会尝试看看LRU链表尾部有没有可以直接释放掉的未修改页面，如果没有的话会不得不将LRU链表尾部的一个脏页同步刷新到磁盘（和磁盘交互是很慢的，这会降低处理用户请求的速度）。这种刷新单个页面到磁盘中的刷新方式被称之为BUF_FLUSH_SINGLE_PAGE。 当然，有时候系统特别繁忙时，也可能出现用户线程批量的从flush链表中刷新脏页的情况，很显然在处理用户请求过程中去刷新脏页是一种严重降低处理速度的行为（毕竟磁盘的速度满的要死），这属于一种迫不得已的情况，不过这得放在后边唠叨redo日志的checkpoint时说了。 多个Buffer Pool实例可以允许多个缓冲池实例，每个页根据哈希值平均分配到不同的缓冲池实例中，这样的话可以减少数据库内部的资源竞争，增加并发处理能力。 12345mysql&gt; show variables like 'innodb_buffer_pool_instances'\\G*************************** 1. row ***************************Variable_name: innodb_buffer_pool_instances Value: 11 row in set (0.01 sec) 不同存储引擎文件存储结构每个数据表都有一个对应的.frm文件，保存每个数据表的元数据信息，包括表结构的定义信息。 MyISAM 物理文件结构为： .frm文件：与表相关的元数据信息都存放在frm文件，包括表结构的定义信息等 .MYD (MYData) 文件：MyISAM 存储引擎专用，用于存储MyISAM 表的数据 .MYI (MYIndex)文件：MyISAM 存储引擎专用，用于存储MyISAM 表的索引相关信息 InnoDB 物理文件结构为： .frm 文件：与表相关的元数据信息都存放在frm文件，包括表结构的定义信息等 .ibd 文件或 .ibdata 文件： 这两种文件都是存放 InnoDB 数据的文件，之所以有两种文件形式存放 InnoDB 的数据，是因为 InnoDB 的数据存储方式能够通过配置来决定是使用共享表空间存放存储数据，还是用独享表空间存放存储数据。 独享表空间存储方式使用.ibd文件，并且每个表一个.ibd文件 共享表空间存储方式使用.ibdata文件，所有表共同使用一个.ibdata文件（或多个，可自己配置） InnoDB和MyISAM的区别（5点） InnoDB 支持事务，MyISAM 不支持事务。这是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一； InnoDB 支持外键，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转为 MYISAM 会失败； InnoDB 是聚簇索引，MyISAM 是非聚簇索引。聚簇索引的文件存放在主键索引的叶子节点上，因此 InnoDB 必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。而 MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 InnoDB 不保存表的具体行数，执行select count(*) from table 时需要全表扫描。而 MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快； InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁。一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。这也是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一； 一张表，里面有ID自增主键，当insert了17条记录之后，删除了第15,16,17条记录，再把Mysql重启，再insert一条记录，这条记录的ID是18还是15 ? 如果表的类型是MyISAM，那么是18。因为MyISAM表会把自增主键的最大ID 记录到数据文件中，重启MySQL自增主键的最大ID也不会丢失； 如果表的类型是InnoDB，那么是15。因为InnoDB 表只是把自增主键的最大ID记录到内存中，所以重启数据库或对表进行OPTION操作，都会导致最大ID丢失 自测总结 InnoDB是如何存储记录的 记录行格式是什么？Compact格式是怎么样的 数据页是什么东西？ 行记录在数据页中是怎么查找的 InnoDB中的缓冲池是干嘛的 缓冲池中有哪些链表结构，分别存什么？ 缓冲池中数据满了怎么办？ 普通的LRU会有什么问题？如何改进的 缓冲池是如何刷脏数据到磁盘中的","link":"/2021/05/11/MySQL%EF%BC%88%E4%BA%8C%EF%BC%89%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/"},{"title":"MySQL（六）内部优化","text":"","link":"/2021/10/08/MySQL%EF%BC%88%E5%85%AD%EF%BC%89%E5%86%85%E9%83%A8%E4%BC%98%E5%8C%96/"},{"title":"MySQL（五）单表和多表查询","text":"SQL查询的时候会有表连接，那么单表和多表之间存在不同访问方法。 单表访问方法MySQL Server中有一个称为查询优化器的模块，一条查询语句进行语法解析之后就会被交给查询优化器来进行优化，优化的结果就是生成一个所谓的执行计划，这个执行计划表明了应该使用哪些索引进行查询，表之间的连接顺序是啥样的，最后会按照执行计划中的步骤调用存储引擎提供的方法来真正的执行查询，并将查询结果返回给用户。 123456789101112131415CREATE TABLE single_table ( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1), // 二级索引 UNIQUE KEY idx_key2 (key2), // 唯一二级索引 KEY idx_key3 (key3), // 二级索引 KEY idx_key_part(key_part1, key_part2, key_part3) // 联合索引) Engine=InnoDB CHARSET=utf8; 我们为这个single_table表建立了1个聚簇索引和4个二级索引，分别是： 为id列建立的聚簇索引。 为key1列建立的idx_key1二级索引。 为key2列建立的idx_key2二级索引，而且该索引是唯一二级索引。 为key3列建立的idx_key3二级索引。 为key_part1、key_part2、key_part3列建立的idx_key_part二级索引，这也是一个联合索引。 访问方法（access method）的概念对于单个表的查询来说，查询的执行方式大致分为下边两种： 使用全表扫描进行查询:把表的每一行记录都扫一遍嘛，把符合搜索条件的记录加入到结果集。 使用索引进行查询 因为直接使用全表扫描的方式执行查询要遍历好多记录，所以代价可能太大了。如果查询语句中的搜索条件可以使用到某个索引，那直接使用索引来执行查询可能会加快查询执行的时间。使用索引来执行查询的方式五花八门，又可以细分为许多种类： 针对主键或唯一二级索引的等值查询 针对普通二级索引的等值查询 针对索引列的范围查询 直接扫描整个索引 MySQL执行查询语句的方式称之为访问方法。同一个查询语句可能可以使用多种不同的访问方法来执行，虽然最后的查询结果都是一样的，但是执行的时间可能差非常多。 const常数：主键索引、唯一二级索引有的时候我们可以通过主键列来定位一条记录，比方说这个查询： 1SELECT * FROM single_table WHERE id = 1438; MySQL会直接利用主键值在聚簇索引中定位对应的用户记录，就像这样： 对于single_table表的聚簇索引来说，展示的就是id列。我们想突出的重点就是：B+树叶子节点中的记录是按照索引列排序的，对于的聚簇索引来说，它对应的B+树叶子节点中的记录就是按照id列排序的。B+树本来就是一个矮矮的大胖子，所以这样根据主键值定位一条记录的速度贼快。类似的，我们根据唯一二级索引列来定位一条记录的速度也是贼快的，比如下边这个查询： 1SELECT * FROM single_table WHERE key2 = 3841; 这个查询的执行过程的示意图就是这样： 可以看到这个查询的执行分两步： 第一步先从idx_key2对应的B+树索引中根据key2列与常数的等值比较条件定位到一条二级索引记录 然后再根据该记录的id值到聚簇索引中获取到完整的用户记录。 认为通过主键或者唯一二级索引列与常数的等值比较来定位一条记录是像坐火箭一样快的，所以他们把这种通过主键或者唯一二级索引列来定位一条记录的访问方法定义为：const，意思是常数级别的，代价是可以忽略不计的。 不过这种const访问方法只能在主键列或者唯一二级索引列和一个常数进行等值比较时才有效，如果主键或者唯一二级索引是由多个列构成的话，索引中的每一个列都需要与常数进行等值比较，这个const访问方法才有效（这是因为只有该索引中全部列都采用等值比较才可以定位唯一的一条记录）。 对于唯一二级索引来说，查询该列为NULL值的情况比较特殊，比如这样： 1SELECT * FROM single_table WHERE key2 IS NULL; 因为唯一二级索引列并不限制NULL值的数量，所以上述语句可能访问到多条记录，也就是说上边这个语句不可以使用const访问方法来执行。 ref：普通二级索引有时候我们对某个普通的二级索引列与常数进行等值比较，比如这样： 1SELECT * FROM single_table WHERE key1 = 'abc'; 对于这个查询，我们当然可以选择全表扫描来逐一对比搜索条件是否满足要求，我们也可以先使用二级索引找到对应记录的id值，然后再回表到聚簇索引中查找完整的用户记录。由于普通二级索引并不限制索引列值的唯一性，所以可能找到多条对应的记录，也就是说使用二级索引来执行查询的代价取决于等值匹配到的二级索引记录条数。如果匹配的记录较少，则回表的代价还是比较低的，所以MySQL可能选择使用索引而不是全表扫描的方式来执行查询。 这种搜索条件为二级索引列与常数等值比较，采用二级索引来执行查询的访问方法称为：ref 从图示中可以看出，对于普通的二级索引来说，通过索引列进行等值比较后可能匹配到多条连续的记录，而不是像主键或者唯一二级索引那样最多只能匹配1条记录，所以这种访问方法比差了那么一丢丢，但是在二级索引等值比较时匹配的记录数较少时的效率还是很高的（如果匹配的二级索引记录太多那么回表的成本就太大了），跟坐高铁差不多。不过需要注意下边两种情况： 二级索引列值为NULL的情况 不论是普通的二级索引，还是唯一二级索引，它们的索引列对包含NULL值的数量并不限制，所以我们采用key IS NULL这种形式的搜索条件最多只能使用ref的访问方法，而不是const的访问方法。 对于某个包含多个索引列的二级索引来说，只要是最左边的连续索引列是与常数的等值比较就可能采用ref的访问方法，比方说下边这几个查询： 12345SELECT * FROM single_table WHERE key_part1 = 'god like';SELECT * FROM single_table WHERE key_part1 = 'god like' AND key_part2 = 'legendary';SELECT * FROM single_table WHERE key_part1 = 'god like' AND key_part2 = 'legendary' AND key_part3 = 'penta kill'; 但是如果最左边的连续索引列并不全部是等值比较的话，它的访问方法就不能称为ref了，比方说这样： 1SELECT * FROM single_table WHERE key_part1 = 'god like' AND key_part2 &gt; 'legendary'; ref_or_null有时候我们不仅想找出某个二级索引列的值等于某个常数的记录，还想把该列的值为NULL的记录也找出来，就像下边这个查询： 1SELECT * FROM single_demo WHERE key1 = 'abc' OR key1 IS NULL; 当使用二级索引而不是全表扫描的方式执行该查询时，这种类型的查询使用的访问方法就称为ref_or_null，这个ref_or_null访问方法的执行过程如下： 可以看到，上边的查询相当于先分别从idx_key1索引对应的B+树中找出key1 IS NULL和key1 = 'abc'的两个连续的记录范围，然后根据这些二级索引记录中的id值再回表查找完整的用户记录。 range：利用索引范围匹配我们之前介绍的几种访问方法都是在对索引列与某一个常数进行等值比较的时候才可能使用到（ref_or_null比较奇特，还计算了值为NULL的情况），但是有时候我们面对的搜索条件更复杂，比如下边这个查询： 1SELECT * FROM single_table WHERE key2 IN (1438, 6328) OR (key2 &gt;= 38 AND key2 &lt;= 79); 我们当然还可以使用全表扫描的方式来执行这个查询，不过也可以使用二级索引 + 回表的方式执行，如果采用二级索引 + 回表的方式来执行的话，那么此时的搜索条件就不只是要求索引列与常数的等值匹配了，而是索引列需要匹配某个或某些范围的值，在本查询中key2列的值只要匹配下列3个范围中的任何一个就算是匹配成功了： key2的值是1438 key2的值是6328 key2的值在38和79之间。 设计MySQL的大叔把这种利用索引进行范围匹配的访问方法称之为：range。 1此处所说的使用索引进行范围匹配中的 `索引` 可以是聚簇索引，也可以是二级索引。 如果把这几个所谓的key2列的值需要满足的范围在数轴上体现出来的话，那应该是这个样子： 也就是从数学的角度看，每一个所谓的范围都是数轴上的一个区间，3个范围也就对应着3个区间： 范围1：key2 = 1438 范围2：key2 = 6328 范围3：key2 ∈ [38, 79]，注意这里是闭区间。 我们可以把那种索引列等值匹配的情况称之为单点区间，上边所说的范围1和范围2都可以被称为单点区间，像范围3这种的我们可以称为连续范围区间。 index：遍历二级索引看下边这个查询： 1SELECT key_part1, key_part2, key_part3 FROM single_table WHERE key_part2 = 'abc'; 由于key_part2并不是联合索引idx_key_part最左索引列，所以我们无法使用ref或者range访问方法来执行这个语句。但是这个查询符合下边这两个条件： 它的查询列表只有3个列：key_part1, key_part2, key_part3，而索引idx_key_part又包含这三个列。 搜索条件中只有key_part2列。这个列也包含在索引idx_key_part中。 也就是说我们可以直接通过遍历idx_key_part索引的叶子节点的记录来比较key_part2 = 'abc'这个条件是否成立，把匹配成功的二级索引记录的key_part1, key_part2, key_part3列的值直接加到结果集中就行了。由于二级索引记录比聚簇索记录小的多（聚簇索引记录要存储所有用户定义的列以及所谓的隐藏列，而二级索引记录只需要存放索引列和主键），而且这个过程也不用进行回表操作，所以直接遍历二级索引比直接遍历聚簇索引的成本要小很多，设计MySQL的大叔就把这种采用**遍历二级索引记录的执行方式称之为：index**。 all：全表扫描最直接的查询执行方式就是我们已经提了无数遍的全表扫描，对于InnoDB表来说也就是直接扫描聚簇索引，设计MySQL的大叔把这种使用全表扫描执行查询的方式称之为：all。 注意事项重温 二级索引 + 回表一般情况下只能利用单个二级索引执行查询，比方说下边的这个查询： 1SELECT * FROM single_table WHERE key1 = 'abc' AND key2 &gt; 1000; 查询优化器会识别到这个查询中的两个搜索条件： key1 = 'abc' key2 &gt; 1000 优化器一般会根据single_table表的统计数据来判断到底使用哪个条件到对应的二级索引中查询扫描的行数会更少，选择那个扫描行数较少的条件到对应的二级索引中查询（关于如何比较的细节我们后边的章节中会唠叨）。然后将从该二级索引中查询到的结果经过回表得到完整的用户记录后再根据其余的WHERE条件过滤记录。一般来说，等值查找比范围查找需要扫描的行数更少（也就是ref的访问方法一般比range好，但这也不总是一定的，也可能采用ref访问方法的那个索引列的值为特定值的行数特别多），所以这里假设优化器决定使用idx_key1索引进行查询，那么整个查询过程可以分为两个步骤： 步骤1：使用二级索引定位记录的阶段，也就是根据条件key1 = 'abc'从idx_key1索引代表的B+树中找到对应的二级索引记录。 步骤2：回表阶段，也就是根据上一步骤中找到的记录的主键值进行回表操作，也就是到聚簇索引中找到对应的完整的用户记录，再根据条件key2 &gt; 1000到完整的用户记录继续过滤。将最终符合过滤条件的记录返回给用户。 这里需要特别提醒大家的一点是，因为二级索引的节点中的记录只包含索引列和主键，所以在步骤1中使用idx_key1索引进行查询时只会用到与key1列有关的搜索条件，其余条件，比如key2 &gt; 1000这个条件在步骤1中是用不到的，只有在步骤2完成回表操作后才能继续针对完整的用户记录中继续过滤。 1小贴士：需要注意的是，我们说一般情况下执行一个查询只会用到二级索引，不过还是有特殊情况的，我们后边会详细唠叨的。 明确range访问方法使用的范围区间其实对于B+树索引来说，只要索引列和常数使用=、&lt;=&gt;、IN、NOT IN、IS NULL、IS NOT NULL、&gt;、&lt;、&gt;=、&lt;=、BETWEEN、!=（不等于也可以写成&lt;&gt;）或者LIKE操作符连接起来，就可以产生一个所谓的区间。 1小贴士：LIKE操作符比较特殊，只有在匹配完整字符串或者匹配字符串前缀时才可以利用索引，具体原因我们在前边的章节中唠叨过了，这里就不赘述了。IN操作符的效果和若干个等值匹配操作符`=`之间用`OR`连接起来是一样的，也就是说会产生多个单点区间，比如下边这两个语句的效果是一样的：SELECT * FROM single_table WHERE key2 IN (1438, 6328); SELECT * FROM single_table WHERE key2 = 1438 OR key2 = 6328; 不过在日常的工作中，一个查询的WHERE子句可能有很多个小的搜索条件，这些搜索条件需要使用AND或者OR操作符连接起来 当我们想使用range访问方法来执行一个查询语句时，重点就是找出该查询可用的索引以及这些索引对应的范围区间。下边分两种情况看一下怎么从由AND或OR组成的复杂搜索条件中提取出正确的范围区间。 所有搜索条件都可以使用某个索引的情况有时候每个搜索条件都可以使用到某个索引，比如下边这个查询语句： 1SELECT * FROM single_table WHERE key2 &gt; 100 AND key2 &gt; 200; 这个查询中的搜索条件都可以使用到key2，也就是说每个搜索条件都对应着一个idx_key2的范围区间。这两个小的搜索条件使用AND连接起来，也就是要取两个范围区间的交集，在我们使用range访问方法执行查询时，使用的idx_key2索引的范围区间的确定过程就如下图所示： key2 &gt; 100和key2 &gt; 200交集当然就是key2 &gt; 200了，也就是说上边这个查询使用idx_key2的范围区间就是(200, +∞)。这东西小学都学过吧，再不济初中肯定都学过。我们再看一下使用OR将多个搜索条件连接在一起的情况： 1SELECT * FROM single_table WHERE key2 &gt; 100 OR key2 &gt; 200; OR意味着需要取各个范围区间的并集，所以上边这个查询在我们使用range访问方法执行查询时： 也就是说上边这个查询使用idx_key2的范围区间就是(100， +∞)。 有的搜索条件无法使用索引的情况比如下边这个查询： 1SELECT * FROM single_table WHERE key2 &gt; 100 AND common_field = 'abc'; 请注意，这个查询语句中能利用的索引只有idx_key2一个，而idx_key2这个二级索引的记录中又不包含common_field这个字段，所以在使用二级索引idx_key2定位定位记录的阶段用不到common_field = 'abc'这个条件，这个条件是在回表获取了完整的用户记录后才使用的，而范围区间是为了到索引中取记录中提出的概念，所以在确定范围区间的时候不需要考虑common_field = 'abc'这个条件，我们在为某个索引确定范围区间的时候只需要把用不到相关索引的搜索条件替换为TRUE就好了。 1小贴士：之所以把用不到索引的搜索条件替换为TRUE，是因为我们不打算使用这些条件进行在该索引上进行过滤，所以不管索引的记录满不满足这些条件，我们都把它们选取出来，待到之后回表的时候再使用它们过滤。 我们把上边的查询中用不到idx_key2的搜索条件替换后就是这样： 1SELECT * FROM single_table WHERE key2 &gt; 100 AND TRUE; 化简之后就是这样： 1SELECT * FROM single_table WHERE key2 &gt; 100; 也就是说最上边那个查询使用idx_key2的范围区间就是：(100, +∞)。 再来看一下使用OR的情况： 1SELECT * FROM single_table WHERE key2 &gt; 100 OR common_field = 'abc'; 同理，我们把使用不到idx_key2索引的搜索条件替换为TRUE： 1SELECT * FROM single_table WHERE key2 &gt; 100 OR TRUE; 接着化简： 1SELECT * FROM single_table WHERE TRUE; 额，这也就说说明如果我们强制使用idx_key2执行查询的话，对应的范围区间就是(-∞, +∞)，也就是需要将全部二级索引的记录进行回表，这个代价肯定比直接全表扫描都大了。也就是说一个使用到索引的搜索条件和没有使用该索引的搜索条件使用OR连接起来后是无法使用该索引的。 复杂搜索条件下找出范围匹配的区间有的查询的搜索条件可能特别复杂，光是找出范围匹配的各个区间就挺烦的，比方说下边这个： 1SELECT * FROM single_table WHERE (key1 &gt; 'xyz' AND key2 = 748 ) OR (key1 &lt; 'abc' AND key1 &gt; 'lmn') OR (key1 LIKE '%suf' AND key1 &gt; 'zzz' AND (key2 &lt; 8000 OR common_field = 'abc')) ; 我滴个神，这个搜索条件真是绝了，不过大家不要被复杂的表象迷住了双眼，按着下边这个套路分析一下： 首先查看WHERE子句中的搜索条件都涉及到了哪些列，哪些列可能使用到索引。 这个查询的搜索条件涉及到了key1、key2、common_field这3个列，然后key1列有普通的二级索引idx_key1，key2列有唯一二级索引idx_key2。 对于那些可能用到的索引，分析它们的范围区间。 假设我们使用idx_key1执行查询 我们需要把那些用不到该索引的搜索条件暂时移除掉，移除方法也简单，直接把它们替换为TRUE就好了。上边的查询中除了有关key2和common_field列不能使用到idx_key1索引外，key1 LIKE '%suf'也使用不到索引，所以把这些搜索条件替换为TRUE之后的样子就是这样： 1(key1 &gt; 'xyz' AND TRUE ) OR(key1 &lt; 'abc' AND key1 &gt; 'lmn') OR(TRUE AND key1 &gt; 'zzz' AND (TRUE OR TRUE)) 化简一下上边的搜索条件就是下边这样： 1(key1 &gt; 'xyz') OR(key1 &lt; 'abc' AND key1 &gt; 'lmn') OR(key1 &gt; 'zzz') 替换掉永远为TRUE或FALSE的条件 因为符合key1 &lt; 'abc' AND key1 &gt; 'lmn'永远为FALSE，所以上边的搜索条件可以被写成这样： 1(key1 &gt; 'xyz') OR (key1 &gt; 'zzz') 继续化简区间 key1 &gt; 'xyz'和key1 &gt; 'zzz'之间使用OR操作符连接起来的，意味着要取并集，所以最终的结果化简的到的区间就是：key1 &gt; xyz。也就是说：上边那个有一坨搜索条件的查询语句如果使用 idx_key1 索引执行查询的话，需要把满足key1 &gt; xyz的二级索引记录都取出来，然后拿着这些记录的id再进行回表，得到完整的用户记录之后再使用其他的搜索条件进行过滤。 假设我们使用idx_key2执行查询 我们需要把那些用不到该索引的搜索条件暂时使用TRUE条件替换掉，其中有关key1和common_field的搜索条件都需要被替换掉，替换结果就是： 1(TRUE AND key2 = 748 ) OR(TRUE AND TRUE) OR(TRUE AND TRUE AND (key2 &lt; 8000 OR TRUE)) 哎呀呀，key2 &lt; 8000 OR TRUE的结果肯定是TRUE呀，也就是说化简之后的搜索条件成这样了： 1key2 = 748 OR TRUE 这个化简之后的结果就更简单了： 1TRUE 这个结果也就意味着如果我们要使用idx_key2索引执行查询语句的话，需要扫描idx_key2二级索引的所有记录，然后再回表，这不是得不偿失么，所以这种情况下不会使用idx_key2索引的。 索引合并MySQL在一般情况下执行一个查询时最多只会用到单个二级索引， 但还是存在有的时候使用了多个二级索引，这种执行方法称为：索引合并。 Intersection合并Intersection翻译过来的意思是交集。这里是说某个查询可以使用多个二级索引，将从多个二级索引中查询到的结果取交集，比方说下边这个查询： 1SELECT * FROM single_table WHERE key1 = 'a' AND key3 = 'b'; 假设这个查询使用Intersection合并的方式执行的话，那这个过程就是这样的： 从idx_key1二级索引对应的B+树中取出key1 = 'a'的相关记录。 从idx_key3二级索引对应的B+树中取出key3 = 'b'的相关记录。 二级索引的记录都是由索引列 + 主键构成的，所以我们可以计算出这两个结果集中id值的交集。 按照上一步生成的id值列表进行回表操作，也就是从聚簇索引中把指定id值的完整用户记录取出来，返回给用户。 这里有同学会思考：为啥不直接使用idx_key1或者idx_key2只根据某个搜索条件去读取一个二级索引，然后回表后再过滤另外一个搜索条件呢？这里要分析一下两种查询执行方式之间需要的成本代价。 只读取一个二级索引的成本： 按照某个搜索条件读取一个二级索引 根据从该二级索引得到的主键值进行回表操作，然后再过滤其他的搜索条件 读取多个二级索引之后取交集成本： 按照不同的搜索条件分别读取不同的二级索引 将从多个二级索引得到的主键值取交集，然后进行回表操作 虽然读取多个二级索引比读取一个二级索引消耗性能，但是**读取二级索引的操作是顺序I/O，而回表操作是随机I/O**，所以如果只读取一个二级索引时需要回表的记录数特别多，而读取多个二级索引之后取交集的记录数非常少，当节省的因为回表而造成的性能损耗比访问多个二级索引带来的性能损耗更高时，读取多个二级索引后取交集比只读取一个二级索引的成本更低。 MySQL在某些特定的情况下才可能会使用到Intersection索引合并： 情况一：二级索引列是等值匹配的情况，对于联合索引来说，在联合索引中的每个列都必须等值匹配，不能出现只出现匹配部分列的情况。 比方说下边这个查询可能用到idx_key1和idx_key_part这两个二级索引进行Intersection索引合并的操作： 1SELECT * FROM single_table WHERE key1 = 'a' AND key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c'; 而下边这两个查询就不能进行Intersection索引合并： 1SELECT * FROM single_table WHERE key1 &gt; 'a' AND key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c';SELECT * FROM single_table WHERE key1 = 'a' AND key_part1 = 'a'; 第一个查询是因为对key1进行了范围匹配，第二个查询是因为联合索引idx_key_part中的key_part2列并没有出现在搜索条件中，所以这两个查询不能进行Intersection索引合并。 情况二：主键列可以是范围匹配 比方说下边这个查询可能用到主键和idx_key_part进行Intersection索引合并的操作： 1SELECT * FROM single_table WHERE id &gt; 100 AND key1 = 'a'; 为啥呢？凭啥呀？突然冒出这么两个规定让大家一脸懵逼，下边我们慢慢品一品这里头的玄机。这话还得从InnoDB的索引结构说起，你要是记不清麻烦再回头看看。对于InnoDB的二级索引来说，记录先是按照索引列进行排序，如果该二级索引是一个联合索引，那么会按照联合索引中的各个列依次排序。而二级索引的用户记录是由索引列 + 主键构成的，二级索引列的值相同的记录可能会有好多条，这些索引列的值相同的记录又是按照主键的值进行排序的。所以重点来了，之所以在二级索引列都是等值匹配的情况下才可能使用Intersection索引合并，是因为只有在这种情况下根据二级索引查询出的结果集是按照主键值排序的。 so？还是没看懂根据二级索引查询出的结果集是按照主键值排序的对使用Intersection索引合并有啥好处？小伙子，别忘了Intersection索引合并会把从多个二级索引中查询出的主键值求交集，如果从各个二级索引中查询的到的结果集本身就是已经按照主键排好序的，那么求交集的过程就很easy啦。假设某个查询使用Intersection索引合并的方式从idx_key1和idx_key2这两个二级索引中获取到的主键值分别是： 从idx_key1中获取到已经排好序的主键值：1、3、5 从idx_key2中获取到已经排好序的主键值：2、3、4 那么求交集的过程就是这样：逐个取出这两个结果集中最小的主键值，如果两个值相等，则加入最后的交集结果中，否则丢弃当前较小的主键值，再取该丢弃的主键值所在结果集的后一个主键值来比较，直到某个结果集中的主键值用完了，如果还是觉得不太明白那继续往下看： 先取出这两个结果集中较小的主键值做比较，因为1 &lt; 2，所以把idx_key1的结果集的主键值1丢弃，取出后边的3来比较。 因为3 &gt; 2，所以把idx_key2的结果集的主键值2丢弃，取出后边的3来比较。 因为3 = 3，所以把3加入到最后的交集结果中，继续两个结果集后边的主键值来比较。 后边的主键值也不相等，所以最后的交集结果中只包含主键值3。 别看我们写的啰嗦，这个过程其实可快了，时间复杂度是O(n)，但是如果从各个二级索引中查询出的结果集并不是按照主键排序的话，那就要先把结果集中的主键值排序完再来做上边的那个过程，就比较耗时了。 1小贴士：按照有序的主键值去回表取记录有个专有名词儿，叫：Rowid Ordered Retrieval，简称ROR，以后大家在某些地方见到这个名词儿就眼熟了。 另外，不仅是多个二级索引之间可以采用Intersection索引合并，索引合并也可以有聚簇索引参加，也就是我们上边写的情况二：在搜索条件中有主键的范围匹配的情况下也可以使用Intersection索引合并索引合并。为啥主键这就可以范围匹配了？还是得回到应用场景里，比如看下边这个查询： 1SELECT * FROM single_table WHERE key1 = 'a' AND id &gt; 100; 假设这个查询可以采用Intersection索引合并，我们理所当然的以为这个查询会分别按照id &gt; 100这个条件从聚簇索引中获取一些记录，在通过key1 = 'a'这个条件从idx_key1二级索引中获取一些记录，然后再求交集，其实这样就把问题复杂化了，没必要从聚簇索引中获取一次记录。别忘了二级索引的记录中都带有主键值的，所以可以在从idx_key1中获取到的主键值上直接运用条件id &gt; 100过滤就行了，这样多简单。所以涉及主键的搜索条件只不过是为了从别的二级索引得到的结果集中过滤记录罢了，是不是等值匹配不重要。 当然，上边说的情况一和情况二只是发生Intersection索引合并的必要条件，不是充分条件。也就是说即使情况一、情况二成立，也不一定发生Intersection索引合并，这得看优化器的心情。优化器在下边两个条件满足的情况下才趋向于使用Intersection索引合并： 单独根据搜索条件从某个二级索引中获取的记录数太多，导致回表开销太大 通过Intersection索引合并后需要回表的记录数大大减少 Union合并我们在写查询语句时经常想把既符合某个搜索条件的记录取出来，也把符合另外的某个搜索条件的记录取出来，我们说这些不同的搜索条件之间是OR关系。有时候OR关系的不同搜索条件会使用到同一个索引，比方说这样： 1SELECT * FROM single_table WHERE key1 = 'a' OR key3 = 'b' Intersection是交集的意思，这适用于使用不同索引的搜索条件之间使用AND连接起来的情况；Union是并集的意思，适用于使用不同索引的搜索条件之间使用OR连接起来的情况。与Intersection索引合并类似，MySQL在某些特定的情况下才可能会使用到Union索引合并： 情况一：二级索引列是等值匹配的情况，对于联合索引来说，在联合索引中的每个列都必须等值匹配，不能出现只出现匹配部分列的情况。 比方说下边这个查询可能用到idx_key1和idx_key_part这两个二级索引进行Union索引合并的操作： 1SELECT * FROM single_table WHERE key1 = 'a' OR ( key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c'); 而下边这两个查询就不能进行Union索引合并： 1SELECT * FROM single_table WHERE key1 &gt; 'a' OR (key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c');SELECT * FROM single_table WHERE key1 = 'a' OR key_part1 = 'a'; 第一个查询是因为对key1进行了范围匹配，第二个查询是因为联合索引idx_key_part中的key_part2列并没有出现在搜索条件中，所以这两个查询不能进行Union索引合并。 情况二：主键列可以是范围匹配 情况三：使用Intersection索引合并的搜索条件 这种情况其实也挺好理解，就是搜索条件的某些部分使用Intersection索引合并的方式得到的主键集合和其他方式得到的主键集合取交集，比方说这个查询： 1SELECT * FROM single_table WHERE key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c' OR (key1 = 'a' AND key3 = 'b'); 优化器可能采用这样的方式来执行这个查询： 先按照搜索条件key1 = 'a' AND key3 = 'b'从索引idx_key1和idx_key3中使用Intersection索引合并的方式得到一个主键集合。 再按照搜索条件key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c'从联合索引idx_key_part中得到另一个主键集合。 采用Union索引合并的方式把上述两个主键集合取并集，然后进行回表操作，将结果返回给用户。 当然，查询条件符合了这些情况也不一定就会采用Union索引合并，也得看优化器的心情。优化器在下边两个条件满足的情况下才趋向于使用Union索引合并： 单独根据搜索条件从某个二级索引中获取的记录数比较少 通过Intersection索引合并后需要回表的记录数大大减少 Sort-Union合并Union索引合并的使用条件太苛刻，必须保证各个二级索引列在进行等值匹配的条件下才可能被用到，比方说下边这个查询就无法使用到Union索引合并： 1SELECT * FROM single_table WHERE key1 &lt; 'a' OR key3 &gt; 'z' 这是因为根据key1 &lt; 'a'从idx_key1索引中获取的二级索引记录的主键值不是排好序的，根据key3 &gt; 'z'从idx_key3索引中获取的二级索引记录的主键值也不是排好序的，但是key1 &lt; 'a'和key3 &gt; 'z'这两个条件又特别让我们动心，所以我们可以这样： 先根据key1 &lt; 'a'条件从idx_key1二级索引总获取记录，并按照记录的主键值进行排序 再根据key3 &gt; 'z'条件从idx_key3二级索引总获取记录，并按照记录的主键值进行排序 因为上述的两个二级索引主键值都是排好序的，剩下的操作和Union索引合并方式就一样了。 我们把上述这种先按照二级索引记录的主键值进行排序，之后按照Union索引合并方式执行的方式称之为Sort-Union索引合并，很显然，这种Sort-Union索引合并比单纯的Union索引合并多了一步对二级索引记录的主键值排序的过程。 1小贴士：为啥有Sort-Union索引合并，就没有Sort-Intersection索引合并么？是的，的确没有Sort-Intersection索引合并这么一说，Sort-Union的适用场景是单独根据搜索条件从某个二级索引中获取的记录数比较少，这样即使对这些二级索引记录按照主键值进行排序的成本也不会太高而Intersection索引合并的适用场景是单独根据搜索条件从某个二级索引中获取的记录数太多，导致回表开销太大，合并后可以明显降低回表开销，但是如果加入Sort-Intersection后，就需要为大量的二级索引记录按照主键值进行排序，这个成本可能比回表查询都高了，所以也就没有引入Sort-Intersection这个玩意儿。 索引合并注意事项联合索引替代Intersection索引合并1SELECT * FROM single_table WHERE key1 = 'a' AND key3 = 'b'; 这个查询之所以可能使用Intersection索引合并的方式执行，还不是因为idx_key1和idx_key2是两个单独的B+树索引，你要是把这两个列搞一个联合索引，那直接使用这个联合索引就把事情搞定了，何必用啥索引合并呢，就像这样： 1ALTER TABLE single_table drop index idx_key1, idx_key3, add index idx_key1_key3(key1, key3); 这样我们把没用的idx_key1、idx_key3都干掉，再添加一个联合索引idx_key1_key3，使用这个联合索引进行查询简直是又快又好，既不用多读一棵B+树，也不用合并结果，何乐而不为？ 1小贴士：不过小心有单独对key3列进行查询的业务场景，这样子不得不再把key3列的单独索引给加上。 连接的原理连接简介连接的本质为了故事的顺利发展，我们先建立两个简单的表并给它们填充一点数据： 1mysql&gt; CREATE TABLE t1 (m1 int, n1 char(1));Query OK, 0 rows affected (0.02 sec)mysql&gt; CREATE TABLE t2 (m2 int, n2 char(1));Query OK, 0 rows affected (0.02 sec)mysql&gt; INSERT INTO t1 VALUES(1, 'a'), (2, 'b'), (3, 'c');Query OK, 3 rows affected (0.00 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; INSERT INTO t2 VALUES(2, 'b'), (3, 'c'), (4, 'd');Query OK, 3 rows affected (0.00 sec)Records: 3 Duplicates: 0 Warnings: 0 我们成功建立了t1、t2两个表，这两个表都有两个列，一个是INT类型的，一个是CHAR(1)类型的，填充好数据的两个表长这样： 12mysql&gt; SELECT * FROM t1;+------+------+| m1 | n1 |+------+------+| 1 | a || 2 | b || 3 | c |+------+------+3 rows in set (0.00 sec)mysql&gt; SELECT * FROM t2;+------+------+| m2 | n2 |+------+------+| 2 | b || 3 | c || 4 | d |+------+------+3 rows in set (0.00 sec) 连接的本质就是把各个连接表中的记录都取出来依次匹配的组合加入结果集并返回给用户。所以我们把t1和t2两个表连接起来的过程如下图所示： 这个过程看起来就是把`t1`表的记录和`t2`的记录连起来组成新的更大的记录，所以这个查询过程称之为连接查询。连接查询的结果集中包含一个表中的每一条记录与另一个表中的每一条记录相互匹配的组合，像这样的结果集就可以称之为`笛卡尔积`。因为表`t1`中有3条记录，表`t2`中也有3条记录，所以这两个表连接之后的笛卡尔积就有`3×3=9`行记录。在`MySQL`中，连接查询的语法也很随意，只要在`FROM`语句后边跟多个表名就好了，比如我们把`t1`表和`t2`表连接起来的查询语句可以写成这样： 1mysql&gt; SELECT * FROM t1, t2;+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 1 | a | 2 | b || 2 | b | 2 | b || 3 | c | 2 | b || 1 | a | 3 | c || 2 | b | 3 | c || 3 | c | 3 | c || 1 | a | 4 | d || 2 | b | 4 | d || 3 | c | 4 | d |+------+------+------+------+9 rows in set (0.00 sec) 连接过程简介如果我们乐意，我们可以连接任意数量张表，但是如果没有任何限制条件的话，这些表连接起来产生的笛卡尔积可能是非常巨大的。比方说3个100行记录的表连接起来产生的笛卡尔积就有100×100×100=1000000行数据！所以在连接的时候过滤掉特定记录组合是有必要的，在连接查询中的过滤条件可以分成两种： 涉及单表的条件 这种只设计单表的过滤条件我们之前都提到过一万遍了，我们之前也一直称为搜索条件，比如t1.m1 &gt; 1是只针对t1表的过滤条件，t2.n2 &lt; 'd'是只针对t2表的过滤条件。 涉及两表的条件 这种过滤条件我们之前没见过，比如t1.m1 = t2.m2、t1.n1 &gt; t2.n2等，这些条件中涉及到了两个表，我们稍后会仔细分析这种过滤条件是如何使用的哈。 下边我们就要看一下携带过滤条件的连接查询的大致执行过程了，比方说下边这个查询语句： 1SELECT * FROM t1, t2 WHERE t1.m1 &gt; 1 AND t1.m1 = t2.m2 AND t2.n2 &lt; 'd'; 在这个查询中我们指明了这三个过滤条件： t1.m1 &gt; 1 t1.m1 = t2.m2 t2.n2 &lt; 'd' 那么这个连接查询的大致执行过程如下： 首先确定第一个需要查询的表，这个表称之为驱动表。怎样在单表中执行查询语句我们在前一章都唠叨过了，只需要选取代价最小的那种访问方法去执行单表查询语句就好了（就是说从const、ref、ref_or_null、range、index、all这些执行方法中选取代价最小的去执行查询）。此处假设使用t1作为驱动表，那么就需要到t1表中找满足t1.m1 &gt; 1的记录，因为表中的数据太少，我们也没在表上建立二级索引，所以此处查询t1表的访问方法就设定为all吧，也就是采用全表扫描的方式执行单表查询。关于如何提升连接查询的性能我们之后再说，现在先把基本概念捋清楚哈。所以查询过程就如下图所示： 我们可以看到，t1表中符合t1.m1 &gt; 1的记录有两条。 针对上一步骤中从驱动表产生的结果集中的每一条记录，分别需要到t2表中查找匹配的记录，所谓匹配的记录，指的是符合过滤条件的记录。因为是根据t1表中的记录去找t2表中的记录，所以t2表也可以被称之为被驱动表。上一步骤从驱动表中得到了2条记录，所以需要查询2次t2表。此时涉及两个表的列的过滤条件t1.m1 = t2.m2就派上用场了： 当t1.m1 = 2时，过滤条件t1.m1 = t2.m2就相当于t2.m2 = 2，所以此时t2表相当于有了t1.m1 = 2、t2.n2 &lt; 'd'这两个过滤条件，然后到t2表中执行单表查询。 当t1.m1 = 3时，过滤条件t1.m1 = t2.m2就相当于t2.m2 = 3，所以此时t2表相当于有了t1.m1 = 3、t2.n2 &lt; 'd'这两个过滤条件，然后到t2表中执行单表查询。 所以整个连接查询的执行过程就如下图所示： 也就是说整个连接查询最后的结果只有两条符合过滤条件的记录： 1+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b || 3 | c | 3 | c |+------+------+------+------+ 从上边两个步骤可以看出来，我们上边唠叨的这个两表连接查询共需要查询1次t1表，2次t2表。当然这是在特定的过滤条件下的结果，如果我们把t1.m1 &gt; 1这个条件去掉，那么从t1表中查出的记录就有3条，就需要查询3次t3表了。也就是说在两表连接查询中，驱动表只需要访问一次，被驱动表可能被访问多次。 内连接和外连接为了大家更好理解后边内容，我们先创建两个有现实意义的表， 1CREATE TABLE student ( number INT NOT NULL AUTO_INCREMENT COMMENT '学号', name VARCHAR(5) COMMENT '姓名', major VARCHAR(30) COMMENT '专业', PRIMARY KEY (number)) Engine=InnoDB CHARSET=utf8 COMMENT '学生信息表';CREATE TABLE score ( number INT COMMENT '学号', subject VARCHAR(30) COMMENT '科目', score TINYINT COMMENT '成绩', PRIMARY KEY (number, score)) Engine=InnoDB CHARSET=utf8 COMMENT '学生成绩表'; 我们新建了一个学生信息表，一个学生成绩表，然后我们向上述两个表中插入一些数据，为节省篇幅，具体插入过程就不唠叨了，插入后两表中的数据如下： 1mysql&gt; SELECT * FROM student;+----------+-----------+--------------------------+| number | name | major |+----------+-----------+--------------------------+| 20180101 | 杜子腾 | 软件学院 || 20180102 | 范统 | 计算机科学与工程 || 20180103 | 史珍香 | 计算机科学与工程 |+----------+-----------+--------------------------+3 rows in set (0.00 sec)mysql&gt; SELECT * FROM score;+----------+-----------------------------+-------+| number | subject | score |+----------+-----------------------------+-------+| 20180101 | 母猪的产后护理 | 78 || 20180101 | 论萨达姆的战争准备 | 88 || 20180102 | 论萨达姆的战争准备 | 98 || 20180102 | 母猪的产后护理 | 100 |+----------+-----------------------------+-------+4 rows in set (0.00 sec) 现在我们想把每个学生的考试成绩都查询出来就需要进行两表连接了（因为score中没有姓名信息，所以不能单纯只查询score表）。连接过程就是从student表中取出记录，在score表中查找number相同的成绩记录，所以过滤条件就是student.number = socre.number，整个查询语句就是这样： 1mysql&gt; SELECT * FROM student, score WHERE student.number = score.number;+----------+-----------+--------------------------+----------+-----------------------------+-------+| number | name | major | number | subject | score |+----------+-----------+--------------------------+----------+-----------------------------+-------+| 20180101 | 杜子腾 | 软件学院 | 20180101 | 母猪的产后护理 | 78 || 20180101 | 杜子腾 | 软件学院 | 20180101 | 论萨达姆的战争准备 | 88 || 20180102 | 范统 | 计算机科学与工程 | 20180102 | 论萨达姆的战争准备 | 98 || 20180102 | 范统 | 计算机科学与工程 | 20180102 | 母猪的产后护理 | 100 |+----------+-----------+--------------------------+----------+-----------------------------+-------+4 rows in set (0.00 sec) 字段有点多哦，我们少查询几个字段： 1mysql&gt; SELECT s1.number, s1.name, s2.subject, s2.score FROM student AS s1, score AS s2 WHERE s1.number = s2.number;+----------+-----------+-----------------------------+-------+| number | name | subject | score |+----------+-----------+-----------------------------+-------+| 20180101 | 杜子腾 | 母猪的产后护理 | 78 || 20180101 | 杜子腾 | 论萨达姆的战争准备 | 88 || 20180102 | 范统 | 论萨达姆的战争准备 | 98 || 20180102 | 范统 | 母猪的产后护理 | 100 |+----------+-----------+-----------------------------+-------+4 rows in set (0.00 sec) 从上述查询结果中我们可以看到，各个同学对应的各科成绩就都被查出来了，可是有个问题，史珍香同学，也就是学号为20180103的同学因为某些原因没有参加考试，所以在score表中没有对应的成绩记录。那如果老师想查看所有同学的考试成绩，即使是缺考的同学也应该展示出来，但是到目前为止我们介绍的连接查询是无法完成这样的需求的。我们稍微思考一下这个需求，其本质是想：驱动表中的记录即使在被驱动表中没有匹配的记录，也仍然需要加入到结果集。为了解决这个问题，就有了内连接和外连接的概念： 对于内连接的两个表，驱动表中的记录在被驱动表中找不到匹配的记录，该记录不会加入到最后的结果集，我们上边提到的连接都是所谓的内连接。 对于外连接的两个表，驱动表中的记录即使在被驱动表中没有匹配的记录，也仍然需要加入到结果集。 在MySQL中，根据选取驱动表的不同，外连接仍然可以细分为2种： 左外连接:选取左侧的表为驱动表。 右外连接:选取右侧的表为驱动表。 可是这样仍然存在问题，即使对于外连接来说，有时候我们也并不想把驱动表的全部记录都加入到最后的结果集。这就犯难了，有时候匹配失败要加入结果集，有时候又不要加入结果集，这咋办，有点儿愁啊。。。噫，把过滤条件分为两种不就解决了这个问题了么，所以放在不同地方的过滤条件是有不同语义的： WHERE子句中的过滤条件 WHERE子句中的过滤条件就是我们平时见的那种，不论是内连接还是外连接，凡是不符合WHERE子句中的过滤条件的记录都不会被加入最后的结果集。 ON子句中的过滤条件 对于外连接的驱动表的记录来说，如果无法在被驱动表中找到匹配ON子句中的过滤条件的记录，那么该记录仍然会被加入到结果集中，对应的被驱动表记录的各个字段使用NULL值填充。 需要注意的是，这个ON子句是专门为外连接驱动表中的记录在被驱动表找不到匹配记录时应不应该把该记录加入结果集这个场景下提出的，所以如果把ON子句放到内连接中，MySQL会把它和WHERE子句一样对待，也就是说：内连接中的WHERE子句和ON子句是等价的。 一般情况下，我们都把只涉及单表的过滤条件放到WHERE子句中，把涉及两表的过滤条件都放到ON子句中，我们也一般把放到ON子句中的过滤条件也称之为连接条件。 1小贴士：左外连接和右外连接简称左连接和右连接，所以下边提到的左外连接和右外连接中的`外`字都用括号扩起来，以表示这个字儿可有可无。 左（外）连接的语法左（外）连接的语法还是挺简单的，比如我们要把t1表和t2表进行左外连接查询可以这么写： 1SELECT * FROM t1 LEFT [OUTER] JOIN t2 ON 连接条件 [WHERE 普通过滤条件]; 其中中括号里的OUTER单词是可以省略的。对于LEFT JOIN类型的连接来说，我们把放在左边的表称之为外表或者驱动表，右边的表称之为内表或者被驱动表。所以上述例子中t1就是外表或者驱动表，t2就是内表或者被驱动表。需要注意的是，对于左（外）连接和右（外）连接来说，必须使用ON子句来指出连接条件。了解了左（外）连接的基本语法之后，再次回到我们上边那个现实问题中来，看看怎样写查询语句才能把所有的学生的成绩信息都查询出来，即使是缺考的考生也应该被放到结果集中： 1mysql&gt; SELECT s1.number, s1.name, s2.subject, s2.score FROM student AS s1 LEFT JOIN score AS s2 ON s1.number = s2.number;+----------+-----------+-----------------------------+-------+| number | name | subject | score |+----------+-----------+-----------------------------+-------+| 20180101 | 杜子腾 | 母猪的产后护理 | 78 || 20180101 | 杜子腾 | 论萨达姆的战争准备 | 88 || 20180102 | 范统 | 论萨达姆的战争准备 | 98 || 20180102 | 范统 | 母猪的产后护理 | 100 || 20180103 | 史珍香 | NULL | NULL |+----------+-----------+-----------------------------+-------+5 rows in set (0.04 sec) 从结果集中可以看出来，虽然史珍香并没有对应的成绩记录，但是由于采用的是连接类型为左（外）连接，所以仍然把她放到了结果集中，只不过在对应的成绩记录的各列使用NULL值填充而已。 右（外）连接的语法右（外）连接和左（外）连接的原理是一样一样的，语法也只是把LEFT换成RIGHT而已： 1SELECT * FROM t1 RIGHT [OUTER] JOIN t2 ON 连接条件 [WHERE 普通过滤条件]; 只不过驱动表是右边的表，被驱动表是左边的表，具体就不唠叨了。 内连接的语法内连接和外连接的根本区别就是在驱动表中的记录不符合ON子句中的连接条件时不会把该记录加入到最后的结果集，我们最开始唠叨的那些连接查询的类型都是内连接。不过之前仅仅提到了一种最简单的内连接语法，就是直接把需要连接的多个表都放到FROM子句后边。其实针对内连接，MySQL提供了好多不同的语法，我们以t1和t2表为例瞅瞅： 1SELECT * FROM t1 [INNER | CROSS] JOIN t2 [ON 连接条件] [WHERE 普通过滤条件]; 也就是说在MySQL中，下边这几种内连接的写法都是等价的： SELECT * FROM t1 JOIN t2; SELECT * FROM t1 INNER JOIN t2; SELECT * FROM t1 CROSS JOIN t2; 上边的这些写法和直接把需要连接的表名放到FROM语句之后，用逗号,分隔开的写法是等价的： 1SELECT * FROM t1, t2; 现在我们虽然介绍了很多种内连接的书写方式，不过熟悉一种就好了，这里我们推荐INNER JOIN的形式书写内连接（因为INNER JOIN语义很明确嘛，可以和LEFT JOIN 和RIGHT JOIN很轻松的区分开）。这里需要注意的是，由于在内连接中ON子句和WHERE子句是等价的，所以内连接中不要求强制写明ON子句。 我们前边说过，连接的本质就是把各个连接表中的记录都取出来依次匹配的组合加入结果集并返回给用户。不论哪个表作为驱动表，两表连接产生的笛卡尔积肯定是一样的。而对于内连接来说，由于凡是不符合ON子句或WHERE子句中的条件的记录都会被过滤掉，其实也就相当于从两表连接的笛卡尔积中把不符合过滤条件的记录给踢出去，所以对于内连接来说，驱动表和被驱动表是可以互换的，并不会影响最后的查询结果。但是对于外连接来说，由于驱动表中的记录即使在被驱动表中找不到符合ON子句连接条件的记录，所以此时驱动表和被驱动表的关系就很重要了，也就是说左外连接和右外连接的驱动表和被驱动表不能轻易互换。 小结上边说了很多，给大家的感觉不是很直观，我们直接把表t1和t2的三种连接方式写在一起，这样大家理解起来就很easy了： 1mysql&gt; SELECT * FROM t1 INNER JOIN t2 ON t1.m1 = t2.m2;+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b || 3 | c | 3 | c |+------+------+------+------+2 rows in set (0.00 sec)mysql&gt; SELECT * FROM t1 LEFT JOIN t2 ON t1.m1 = t2.m2;+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b || 3 | c | 3 | c || 1 | a | NULL | NULL |+------+------+------+------+3 rows in set (0.00 sec)mysql&gt; SELECT * FROM t1 RIGHT JOIN t2 ON t1.m1 = t2.m2;+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b || 3 | c | 3 | c || NULL | NULL | 4 | d |+------+------+------+------+3 rows in set (0.00 sec) 连接的原理上边贼啰嗦的介绍都只是为了唤醒大家对连接、内连接、外连接这些概念的记忆，这些基本概念是为了真正进入本章主题做的铺垫。真正的重点是MySQL采用了什么样的算法来进行表与表之间的连接，了解了这个之后，大家才能明白为啥有的连接查询运行的快如闪电，有的却慢如蜗牛。 嵌套循环连接（Nested-Loop Join）我们前边说过，对于两表连接来说，驱动表只会被访问一遍，但被驱动表却要被访问到好多遍，具体访问几遍取决于对驱动表执行单表查询后的结果集中的记录条数。对于内连接来说，选取哪个表为驱动表都没关系，而外连接的驱动表是固定的，也就是说左（外）连接的驱动表就是左边的那个表，右（外）连接的驱动表就是右边的那个表。我们上边已经大致介绍过t1表和t2表执行内连接查询的大致过程，我们温习一下： 步骤1：选取驱动表，使用与驱动表相关的过滤条件，选取代价最低的单表访问方法来执行对驱动表的单表查询。 步骤2：对上一步骤中查询驱动表得到的结果集中每一条记录，都分别到被驱动表中查找匹配的记录。 通用的两表连接过程如下图所示： 如果有3个表进行连接的话，那么步骤2中得到的结果集就像是新的驱动表，然后第三个表就成为了被驱动表，重复上边过程，也就是步骤2中得到的结果集中的每一条记录都需要到t3表中找一找有没有匹配的记录，用伪代码表示一下这个过程就是这样： 1for each row in t1 { #此处表示遍历满足对t1单表查询结果集中的每一条记录 for each row in t2 { #此处表示对于某条t1表的记录来说，遍历满足对t2单表查询结果集中的每一条记录 for each row in t3 { #此处表示对于某条t1和t2表的记录组合来说，对t3表进行单表查询 if row satisfies join conditions, send to client } }} 这个过程就像是一个嵌套的循环，所以这种驱动表只访问一次，但被驱动表却可能被多次访问，访问次数取决于对驱动表执行单表查询后的结果集中的记录条数的连接执行方式称之为嵌套循环连接（Nested-Loop Join），这是最简单，也是最笨拙的一种连接查询算法。 使用索引加快连接速度我们知道在嵌套循环连接的步骤2中可能需要访问多次被驱动表，如果访问被驱动表的方式都是全表扫描的话，妈呀，那得要扫描好多次呀～～～ 但是别忘了，查询t2表其实就相当于一次单表扫描，我们可以利用索引来加快查询速度哦。回顾一下最开始介绍的t1表和t2表进行内连接的例子： 1SELECT * FROM t1, t2 WHERE t1.m1 &gt; 1 AND t1.m1 = t2.m2 AND t2.n2 &lt; 'd'; 我们使用的其实是嵌套循环连接算法执行的连接查询，再把上边那个查询执行过程表拉下来给大家看一下： 查询驱动表t1后的结果集中有两条记录，嵌套循环连接算法需要对被驱动表查询2次： 当t1.m1 = 2时，去查询一遍t2表，对t2表的查询语句相当于： 1SELECT * FROM t2 WHERE t2.m2 = 2 AND t2.n2 &lt; 'd'; 当t1.m1 = 3时，再去查询一遍t2表，此时对t2表的查询语句相当于： 1SELECT * FROM t2 WHERE t2.m2 = 3 AND t2.n2 &lt; 'd'; 可以看到，原来的t1.m1 = t2.m2这个涉及两个表的过滤条件在针对t2表做查询时关于t1表的条件就已经确定了，所以我们只需要单单优化对t2表的查询了，上述两个对t2表的查询语句中利用到的列是m2和n2列，我们可以： 在m2列上建立索引，因为对m2列的条件是等值查找，比如t2.m2 = 2、t2.m2 = 3等，所以可能使用到ref的访问方法，假设使用ref的访问方法去执行对t2表的查询的话，需要回表之后再判断t2.n2 &lt; d这个条件是否成立。 这里有一个比较特殊的情况，就是假设m2列是t2表的主键或者唯一二级索引列，那么使用t2.m2 = 常数值这样的条件从t2表中查找记录的过程的代价就是常数级别的。我们知道在单表中使用主键值或者唯一二级索引列的值进行等值查找的方式称之为const，而设计MySQL的大叔把在连接查询中对被驱动表使用主键值或者唯一二级索引列的值进行等值查找的查询执行方式称之为：eq_ref。 在n2列上建立索引，涉及到的条件是t2.n2 &lt; 'd'，可能用到range的访问方法，假设使用range的访问方法对t2表的查询的话，需要回表之后再判断在m2列上的条件是否成立。 假设m2和n2列上都存在索引的话，那么就需要从这两个里边儿挑一个代价更低的去执行对t2表的查询。当然，建立了索引不一定使用索引，只有在二级索引 + 回表的代价比全表扫描的代价更低时才会使用索引。 另外，有时候连接查询的查询列表和过滤条件中可能只涉及被驱动表的部分列，而这些列都是某个索引的一部分，这种情况下即使不能使用eq_ref、ref、ref_or_null或者range这些访问方法执行对被驱动表的查询的话，也可以使用索引扫描，也就是index的访问方法来查询被驱动表。所以我们建议在真实工作中最好不要使用*作为查询列表，最好把真实用到的列作为查询列表。 基于块的嵌套循环连接（Block Nested-Loop Join）扫描一个表的过程其实是先把这个表从磁盘上加载到内存中，然后从内存中比较匹配条件是否满足。现实生活中的表可不像t1、t2这种只有3条记录，成千上万条记录都是少的，几百万、几千万甚至几亿条记录的表到处都是。内存里可能并不能完全存放的下表中所有的记录，所以在扫描表前边记录的时候后边的记录可能还在磁盘上，等扫描到后边记录的时候可能内存不足，所以需要把前边的记录从内存中释放掉。我们前边又说过，采用嵌套循环连接算法的两表连接过程中，被驱动表可是要被访问好多次的，如果这个被驱动表中的数据特别多而且不能使用索引进行访问，那就相当于要从磁盘上读好几次这个表，这个I/O代价就非常大了，所以我们得想办法：尽量减少访问被驱动表的次数。 当被驱动表中的数据非常多时，每次访问被驱动表，被驱动表的记录会被加载到内存中，在内存中的每一条记录只会和驱动表结果集的一条记录做匹配，之后就会被从内存中清除掉。然后再从驱动表结果集中拿出另一条记录，再一次把被驱动表的记录加载到内存中一遍，周而复始，驱动表结果集中有多少条记录，就得把被驱动表从磁盘上加载到内存中多少次。所以我们可不可以在把被驱动表的记录加载到内存的时候，一次性和多条驱动表中的记录做匹配，这样就可以大大减少重复从磁盘上加载被驱动表的代价了。所以设计MySQL的大叔提出了一个join buffer的概念，join buffer就是执行连接查询前申请的一块固定大小的内存，先把若干条驱动表结果集中的记录装在这个join buffer中，然后开始扫描被驱动表，每一条被驱动表的记录一次性和join buffer中的多条驱动表记录做匹配，因为匹配的过程都是在内存中完成的，所以这样可以显著减少被驱动表的I/O代价。使用join buffer的过程如下图所示： 最好的情况是join buffer足够大，能容纳驱动表结果集中的所有记录，这样只需要访问一次被驱动表就可以完成连接操作了。设计MySQL的大叔把这种加入了join buffer的嵌套循环连接算法称之为基于块的嵌套连接（Block Nested-Loop Join）算法。 这个join buffer的大小是可以通过启动参数或者系统变量join_buffer_size进行配置，默认大小为262144字节（也就是256KB），最小可以设置为128字节。当然，对于优化被驱动表的查询来说，最好是为被驱动表加上效率高的索引，如果实在不能使用索引，并且自己的机器的内存也比较大可以尝试调大join_buffer_size的值来对连接查询进行优化。 另外需要注意的是，驱动表的记录并不是所有列都会被放到join buffer中，只有查询列表中的列和过滤条件中的列才会被放到join buffer中，所以再次提醒我们，最好不要把*作为查询列表，只需要把我们关心的列放到查询列表就好了，这样还可以在join buffer中放置更多的记录呢哈。 自测总结 单表访问时的访问方法有哪几种，分别代表什么意思 范围查找时的范围区间怎么判断 索引合并是什么？什么时候会发生 表连接的原理是什么 内连接和外连接分别是啥意思 左外连接和右外连接是啥意思 连接的原理是什么？","link":"/2021/10/08/MySQL%EF%BC%88%E4%BA%94%EF%BC%89%E5%8D%95%E8%A1%A8%E5%92%8C%E5%A4%9A%E8%A1%A8%E6%9F%A5%E8%AF%A2/"},{"title":"MySQL（十一）锁","text":"主要讲述了MySQL中的锁 解决并发事务并发事务访问相同的记录有下面三种情况： 读-读 情况：即并发事务相继读取相同的记录 写-写 情况：即并发事务相继对相同的记录做出改动。 读-写 或 写-读 情况：也就是一个事务进行读取操作，另一个进行改动操作。会产生脏读 、 不可重复读 、 幻读的问题 写-写这种情况会发生 脏写 的问题，任何一种隔离级别都不允许这种问题的发生 所以在多个未提交事务相继对一条记录做改动时，需要让它们排队执行，这个排队的过程其实是通过 锁 来实现的。 这个所谓的 锁 其实是一个内存中的结构，在事务执行前本来是没有锁的，也就是说一开始是没有 锁结构 和记录进行关联的，如图所示： 当一个事务想对这条记录做改动时，首先会看看内存中有没有与这条记录关联的 锁结构 ，当没有的时候就会在内存中生成一个 锁结构 与之关联。比方说事务 T1 要对这条记录做改动，就需要生成一个 锁结构 与之关联： 其实在 锁结构 里有很多信息，不过为了简化理解，我们现在只把两个比较重要的属性拿了出来： trx信息 ：代表这个锁结构是哪个事务生成的。 is_waiting ：代表当前事务是否在等待。 如图所示，当事务 T1 改动了这条记录后，就生成了一个 锁结构 与该记录关联，因为之前没有别的事务为这条记录加锁，所以 is_waiting 属性就是 false ，我们把这个场景就称之为获取锁成功，或者加锁成功，然后就可以继续执行操作了。 在事务 T1 提交之前，另一个事务 T2 也想对该记录做改动，那么先去看看有没有 锁结构 与这条记录关联，发现有一个 锁结构 与之关联后，然后也生成了一个 锁结构 与这条记录关联，不过 锁结构 的is_waiting 属性值为 true ，表示当前事务需要等待，我们把这个场景就称之为获取锁失败，或者加锁失败，或者没有成功的获取到锁，画个图表示就是这样： 在事务 T1 提交之后，就会把该事务生成的 锁结构 释放掉，然后看看还有没有别的事务在等待获取锁，发现了事务 T2 还在等待获取锁，所以把事务 T2 对应的锁结构的 is_waiting 属性设置为 false ，然后把该事务对应的线程唤醒，让它继续执行，此时事务 T2 就算获取到锁了。效果图就是这样 事务的不同隔离级别会产生的问题SQL标准 规定不同隔离级别下可能发生的问题不一样： 在 READ UNCOMMITTED 隔离级别下， 脏读 、 不可重复读 、 幻读 都可能发生。 在 READ COMMITTED 隔离级别下， 不可重复读 、 幻读 可能发生， 脏读 不可以发生。 在 REPEATABLE READ 隔离级别下， 幻读 可能发生， 脏读 和 不可重复读 不可以发生。 在 SERIALIZABLE 隔离级别下，上述问题都不可以发生。 幻读问题的产生是因为某个事务读了一个范围的记录，之后别的事务在该范围内插入了新记录，该事务再次读取该范围的记录时，可以读到新插入的记录，所以幻读问题准确的说并不是因为读取和写入一条相同记录而产生的， 如何解决幻读问题MySQL 在REPEATABLE READ 隔离级别实际上就已经解决了 幻读 问题： 读操作利用多版本并发控制（ MVCC ），写操作进行 加锁 所谓的 MVCC 我们在前一章有过详细的描述，就是通过生成一个 ReadView ，然后通过 ReadView 找到符合条件的记录版本（历史版本是由 undo日志 构建的），其实就像是在生成 ReadView 的那个时刻做了一次时间静止（就像用相机拍了一个快照），查询语句只能读到在生成 ReadView 之前已提交事务所做的更改，在生成 ReadView 之前未提交的事务或者之后才开启的事务所做的更改是看不到的。 而写操作肯定针对的是最新版本的记录，读记录的历史版本和改动记录的最新版本本身并不冲突，也就是采用MVCC 时， 读-写 操作并不冲突。 我们说过普通的SELECT语句在READ COMMITTED和REPEATABLE READ隔离级别下会使用到MVCC读取记录。在READ COMMITTED隔离级别下，一个事务在执行过程中每次执行SELECT操作时都会生成一个ReadView，ReadView的存在本身就保证了事务不可以读取到未提交的事务所做的更改，也就是避免了脏读现象；REPEATABLE READ隔离级别下，一个事务在执行过程中只有第一次执行SELECT操作才会生成一个ReadView，之后的SELECT操作都复用这个ReadView，这样也就避免了不可重复读和幻读的问题。 读、写操作都采用 加锁 的方式。 如果我们的一些业务场景不允许读取记录的旧版本，而是每次都必须去读取记录的最新版本，比方在银行存款的事务中，你需要先把账户的余额读出来，然后将其加上本次存款的数额，最后再写到数据库中。在将账户余额读取出来后，就不想让别的事务再访问该余额，直到本次存款事务执行完成，其他事务才可以访问账户的余额。这样在读取记录的时候也就需要对其进行 加锁 操作，这样也就意味着 读 操作和 写 操作也像 写-写 操作那样排队执行。 我们说脏读的产生是因为当前事务读取了另一个未提交事务写的一条记录，如果另一个事务在写记录的时候就给这条记录加锁，那么当前事务就无法继续读取该记录了，所以也就不会有脏读问题的产生了。不可重复读的产生是因为当前事务先读取一条记录，另外一个事务对该记录做了改动之后并提交之后，当前事务再次读取时会获得不同的值，如果在当前事务读取记录时就给该记录加锁，那么另一个事务就无法修改该记录，自然也不会发生不可重复读了。我们说幻读问题的产生是因为当前事务读取了一个范围的记录，然后另外的事务向该范围内插入了新记录，当前事务再次读取该范围的记录时发现了新插入的新记录，我们把新插入的那些记录称之为幻影记录。采用加锁的方式解决幻读问题就有那么一丢丢麻烦了，因为当前事务在第一次读取记录时那些幻影记录并不存在，所以读取的时候加锁就有点尴尬 一致性读事务利用 MVCC 进行的读取操作称之为 一致性读 ，或者 一致性无锁读 ，有的地方也称之为 快照读 。所有普通的 SELECT 语句（ plain SELECT ）在 READ COMMITTED 、 REPEATABLE READ 隔离级别下都算是 一致性读 ，比方说： 123SELECT * FROM t;SELECT * FROM t1 INNER JOIN t2 ON t1.col1 = t2.col2 一致性读 并不会对表中的任何记录做 加锁 操作，其他事务可以自由的对表中的记录做改动。 锁定读共享锁和独占锁我们前边说过，并发事务的 读-读 情况并不会引起什么问题，不过对于 写-写 、 读-写 或 写-读 这些情况可能会引起一些问题，需要使用 MVCC 或者 加锁 的方式来解决它们。在使用 加锁 的方式解决问题时，由于既要允许 读-读 情况不受影响，又要使 写-写 、 读-写 或 写-读 情况中的操作相互阻塞，所以设计 MySQL 的大叔给锁分了个类： 共享锁 ，英文名： Shared Locks ，简称 S锁 。在事务要读取一条记录时，需要先获取该记录的 S锁 。 独占锁 ，也常称 排他锁 ，英文名： Exclusive Locks ，简称 X锁 。在事务要改动一条记录时，需要先获取该记录的 X锁 。 假如事务 T1 首先获取了一条记录的 S锁 之后，事务 T2 接着也要访问这条记录： 如果事务 T2 想要再获取一个记录的 S锁 ，那么事务 T2 也会获得该锁，也就意味着事务 T1 和 T2 在该记录上同时持有 S锁 。 如果事务 T2 想要再获取一个记录的 X锁 ，那么此操作会被阻塞，直到事务 T1 提交之后将 S锁 释放掉。 如果事务 T1 首先获取了一条记录的 X锁 之后，那么不管事务 T2 接着想获取该记录的 S锁 还是 X锁 都会被阻塞，直到事务 T1 提交。 所以我们说 S锁 和 S锁 是兼容的， S锁 和 X锁 是不兼容的， X锁 和 X锁 也是不兼容的，画个表表示一下就是这样： 锁定读的语句我们前边说在采用 加锁 方式解决 脏读 、 不可重复读 、 幻读 这些问题时，读取一条记录时需要获取一下该记录的 S锁 ，其实这是不严谨的，有时候想在读取记录时就获取记录的 X锁 ，来禁止别的事务读写该记录，为此设计 MySQL 的大叔提出了两种比较特殊的 SELECT 语句格式： 对读取的记录加 S锁 ： 1SELECT ... LOCK IN SHARE MODE; 也就是在普通的 SELECT 语句后边加 LOCK IN SHARE MODE ，如果当前事务执行了该语句，那么它会为读取到的记录加 S锁 ，这样允许别的事务继续获取这些记录的 S锁 （比方说别的事务也使用 SELECT … LOCK IN SHARE MODE 语句来读取这些记录），但是不能获取这些记录的 X锁 （比方说使用 SELECT … FOR UPDATE语句来读取这些记录，或者直接修改这些记录）。如果别的事务想要获取这些记录的 X锁 ，那么它们会阻塞，直到当前事务提交之后将这些记录上的 S锁 释放掉。 对读取的记录加 X锁 ： 1SELECT ... FOR UPDATE; 也就是在普通的 SELECT 语句后边加 FOR UPDATE ，如果当前事务执行了该语句，那么它会为读取到的记录加 X锁 ，这样既不允许别的事务获取这些记录的 S锁 （比方说别的事务使用 SELECT … LOCK IN SHARE MODE 语句来读取这些记录），也不允许获取这些记录的 X锁 （比方也说使用 SELECT … FOR UPDATE 语句来读取这些记录，或者直接修改这些记录）。如果别的事务想要获取这些记录的 S锁 或者 X锁 ，那么它们会阻塞，直到当前事务提交之后将这些记录上的 X锁 释放掉。关于更多 锁定读 的加锁细节我们稍后会详细唠叨，稍安勿躁。 写操作平常所用到的 写操作 无非是 DELETE 、 UPDATE 、 INSERT 这三种： DELETE ： 对一条记录做 DELETE 操作的过程其实是先在 B+ 树中定位到这条记录的位置，然后获取一下这条记录的 X 锁 ，然后再执行 delete mark 操作。我们也可以把这个定位待删除记录在 B+ 树中位置的过程看成是一个获取 X锁 的 锁定读 。 UPDATE ： 在对一条记录做 UPDATE 操作时分为三种情况： 如果未修改该记录的键值并且被更新的列占用的存储空间在修改前后未发生变化，则先在 B+ 树中定位到这条记录的位置，然后再获取一下记录的 X锁 ，最后在原记录的位置进行修改操作。其实我们也可以把这个定位待修改记录在 B+ 树中位置的过程看成是一个获取 X锁 的 锁定读 。 如果未修改该记录的键值并且至少有一个被更新的列占用的存储空间在修改前后发生变化，则先在B+ 树中定位到这条记录的位置，然后获取一下记录的 X锁 ，将该记录彻底删除掉（就是把记录彻底移入垃圾链表），最后再插入一条新记录。这个定位待修改记录在 B+ 树中位置的过程看成是一个获取 X 锁 的 锁定读 ，新插入的记录由 INSERT 操作提供的 隐式锁 进行保护。 如果修改了该记录的键值，则相当于在原记录上做 DELETE 操作之后再来一次 INSERT 操作，加锁操作就需要按照 DELETE 和 INSERT 的规则进行了。 INSERT ： 一般情况下，新插入一条记录的操作并不加锁，设计 InnoDB 的大叔通过一种称之为 隐式锁 的东东来保护这条新插入的记录在本事务提交前不被别的事务访问， 当然，在一些特殊情况下INSERT操作也是会获取锁的，具体情况我们后边唠叨。 多粒度锁我们前边提到的 锁 都是针对记录的，也可以被称之为 行级锁 或者 行锁 ，对一条记录加锁影响的也只是这条记录而已，我们就说这个锁的粒度比较细；其实一个事务也可以在 表 级别进行加锁，自然就被称之为 表级锁 或 者 表锁 ，对一个表加锁影响整个表中的记录，我们就说这个锁的粒度比较粗。 给表加的锁也可以分为 共享锁（ S锁 ）和 独占锁 （ X锁 ）： 给表加 S锁 ：如果一个事务给表加了 S锁 那么别的事务可以继续获得该表的 S锁 别的事务可以继续获得该表中的某些记录的 S锁 别的事务不可以继续获得该表的 X锁 别的事务不可以继续获得该表中的某些记录的 X锁 给表加 X锁 ：如果一个事务给表加了 X锁 （意味着该事务要独占这个表），那么： 别的事务不可以继续获得该表的 S锁 别的事务不可以继续获得该表中的某些记录的 S锁 别的事务不可以继续获得该表的 X锁 别的事务不可以继续获得该表中的某些记录的 X锁 我们以大学教学楼中的教室为例来分析一下加锁的情况： 教室一般都是公用的，我们可以随便选教室进去上自习。当然，教室不是自家的，一间教室可以容纳很多同学同时上自习，每当一个人进去上自习，就相当于在教室门口挂了一把 S锁 ，如果很多同学都进去上自习，相当于教室门口挂了很多把 S锁 （类似行级别的 S锁 ）。有的时候教室会进行检修，比方说换地板，换天花板，换灯管啥的，这些维修项目并不能同时开展。如果教室针对某个项目进行检修，就不允许别的同学来上自习，也不允许其他维修项目进行，此时相当于教室门口会挂一把 X锁 （类似行级别的 X锁 ）。 上边提到的这两种锁都是针对 教室 而言的，不过有时候我们会有一些特殊的需求： 有领导要来参观教学楼的环境。 校领导考虑并不想影响同学们上自习，但是此时不能有教室处于维修状态，所以可以在教学楼门口放置一把S锁 （类似表级别的 S锁 ）。此时： 来上自习的学生们看到教学楼门口有 S锁 ，可以继续进入教学楼上自习。 修理工看到教学楼门口有 S锁 ，则先在教学楼门口等着，啥时候领导走了，把教学楼的 S锁 撤掉再进入教学楼维修。 学校要占用教学楼进行考试。 此时不允许教学楼中有正在上自习的教室，也不允许对教室进行维修。所以可以在教学楼门口放置一把 X锁（类似表级别的 X锁 ）。此时： 来上自习的学生们看到教学楼门口有 X锁 ，则需要在教学楼门口等着，啥时候考试结束，把教学楼的 X 锁 撤掉再进入教学楼上自习。 修理工看到教学楼门口有 X锁 ，则先在教学楼门口等着，啥时候考试结束，把教学楼的 X锁 撤掉再进入教学楼维修。 但是这里头有两个问题： 如果我们想对教学楼整体上 S锁 ，首先需要确保教学楼中的没有正在维修的教室，如果有正在维修的教室，需要等到维修结束才可以对教学楼整体上 S锁 。 如果我们想对教学楼整体上 X锁 ，首先需要确保教学楼中的没有上自习的教室以及正在维修的教室，如果有上自习的教室或者正在维修的教室，需要等到全部上自习的同学都上完自习离开，以及维修工维修完教室离开后才可以对教学楼整体上 X锁 。我们在对教学楼整体上锁（ 表锁 ）时，怎么知道教学楼中有没有教室已经被上锁（ 行锁 ）了呢？依次检查每一间教室门口有没有上锁？那这效率也太慢了吧！ 于是乎设计一种称之为 意向锁 （英文名： Intention Locks ）： 意向共享锁，英文名： Intention Shared Lock ，简称 IS锁 。当事务准备在某条记录上加 S锁 时，需要先在表级别加一个 IS锁 。 意向独占锁，英文名： Intention Exclusive Lock ，简称 IX锁 。当事务准备在某条记录上加 X锁 时，需要先在表级别加一个 IX锁 。 视角回到教学楼和教室上来： 如果有学生到教室中上自习，那么他先在整栋教学楼门口放一把 IS锁 （表级锁），然后再到教室门口放一把 S锁 （行锁）。 如果有维修工到教室中维修，那么它先在整栋教学楼门口放一把 IX锁 （表级锁），然后再到教室门口放一把 X锁 （行锁）。 之后： 如果有领导要参观教学楼，也就是想在教学楼门口前放 S锁 （表锁）时，首先要看一下教学楼门口有没有IX锁 ，如果有，意味着有教室在维修，需要等到维修结束把 IX锁 撤掉后才可以在整栋教学楼上加 S锁 。如果有考试要占用教学楼，也就是想在教学楼门口前放 X锁 （表锁）时，首先要看一下教学楼门口有没有IS锁 或 IX锁 ，如果有，意味着有教室在上自习或者维修，需要等到学生们上完自习以及维修结束把 IS锁和 IX锁 撤掉后才可以在整栋教学楼上加 X锁 。 学生在教学楼门口加IS锁时，是不关心教学楼门口是否有IX锁的，维修工在教学楼门口加IX锁时，是不关心教学楼门口是否有IS锁或者其他IX锁的。IS和IX锁只是为了判断当前时间教学楼里有没有被占用的教室用的，也就是在对教学楼加S锁或者X锁时才会用到。 总结一下：IS、IX锁是表级锁，它们的提出仅仅为了在之后加表级别的S锁和X锁时可以快速判断表中的记录是否被上锁，以避免用遍历的方式来查看表中有没有上锁的记录，也就是说其实IS锁和IX锁是兼容的，IX锁和IX锁是兼容的。我们画个表来看一下表级别的各种锁的兼容性： 行锁和表锁InnoDB存储引擎中的锁表锁行锁创建一个表，插入记录： 1234567891011121314151617181920212223242526CREATE TABLE hero ( number INT, name VARCHAR(100), country varchar(100), PRIMARY KEY (number), KEY idx_name (name)) Engine=InnoDB CHARSET=utf8;INSERT INTO hero VALUES (1, 'l刘备', '蜀'), (3, 'z诸葛亮', '蜀'), (8, 'c曹操', '魏'), (15, 'x荀彧', '魏'), (20, 's孙权', '吴'); mysql&gt; SELECT * FROM hero;+--------+------------+---------+| number | name | country |+--------+------------+---------+| 1 | l刘备 | 蜀 || 3 | z诸葛亮 | 蜀 || 8 | c曹操 | 魏 || 15 | x荀彧 | 魏 || 20 | s孙权 | 吴 |+--------+------------+---------+5 rows in set (0.01 sec) 为啥要在’刘备’、’曹操’、’孙权’前边加上’l’、’c’、’s’这几个字母呀？ 这个主要是因为我们采用utf8字符集，该字符集并没有对应的按照汉语拼音进行排序的比较规则，也就是说’刘备’、’曹操’、’孙权’这几个字符串的排序并不是按照它们汉语拼音进行排序的，我怕大家懵逼，所以在汉字前边加上了汉字对应的拼音的第一个字母，这样在排序时就是按照汉语拼音进行排序 当然，我们把 B+树 的索引结构做了一个超级简化，只把索引中的记录给拿了出来，我们这里只是想强调聚簇索引中的记录是按照主键大小排序的，并且省略掉了聚簇索引中的隐藏列， Record Locks记录锁：官方名字叫：LOCK_REC_NOT_GAP 是有 S锁 和 X锁 之分的，让我们分别称之为 S型正经记录锁 和 X型正经记录锁 吧（听起来有点怪怪的），当一个事务获取了一条记录的 S型正经记录锁 后，其他事务也可以继续获取该记录的 S型正经记录锁 ，但不可以继续获取 X型正经记录锁 ；当一个事务获取了一条记录的 X型正经记录锁 后，其他事务既不可以继续获取该记录的 S型正经记录锁 ，也不可以继续获取 X型正经记录锁 Gap Locks间隙锁， MySQL 在 REPEATABLE READ 隔离级别下是可以解决幻读问题的，解决方案有两种，可以使用 MVCC 方案解决，也可以采用 加锁 方案解决。 但是在使用 加锁 方案解决时有个大问题，就是事务在第一次执行读取操作时，那些幻影记录尚不存在，我们无法给这些幻影记录加上 正经记录锁 。不过这难不倒设计 InnoDB 的大叔，他们提出了一种称之为 Gap Locks 的锁，官方的类型名称为： LOCK_GAP ，我们也可以简称为 gap锁 。 比方说我们把 number 值为 8 的那条记录加一个 gap锁 的示意图如下： 如图中为 number 值为 8 的记录加了 gap锁 ，意味着不允许别的事务在 number 值为 8 的记录前边的 间隙插入新记录，其实就是 number 列的值 (3, 8) 这个区间的新记录是不允许立即插入的。比方说有另外一个事务再想插入一条 number 值为 4 的新记录，它定位到该条新记录的下一条记录的 number 值为8，而这条记录上又有一个 gap锁 ，所以就会阻塞插入操作，直到拥有这个 gap锁 的事务提交了之后， number 列的值在区间 (3, 8) 中的新记录才可以被插入。 这个 gap锁 的提出仅仅是为了防止插入幻影记录而提出的，虽然有 共享gap锁 和 独占gap锁 这样的说法，但是它们起到的作用都是相同的。而且如果你对一条记录加了 gap锁 （不论是 共享gap锁 还是 独占gap锁 ），并不会限制其他事务对这条记录加 正经记录锁 或者继续加 gap锁 ，再强调一遍， gap锁 的作用仅仅是为了防止插入幻影记录的而已。 不知道大家发现了一个问题没，给一条记录加了 gap锁 只是不允许其他事务往这条记录前边的间隙插入新记录，那对于最后一条记录之后的间隙，也就是 hero 表中 number 值为 20 的记录之后的间隙该咋办呢？也就是说给哪条记录加 gap锁 才能阻止其他事务插入 number 值在 (20, +∞) 这个区间的新记录呢？这时候应该 想起我们在前边唠叨 数据页 时介绍的两条伪记录了： Infimum 记录，表示该页面中最小的记录。 Supremum 记录，表示该页面中最大的记录。 为了实现阻止其他事务插入 number 值在 (20, +∞) 这个区间的新记录，我们可以给索引中的最后一条记录，也就是 number 值为 20 的那条记录所在页面的 Supremum 记录加上一个 gap锁 ，画个图就是这样： Next-Key Locks有时候我们既想锁住某条记录，又想阻止其他事务在该记录前边的 间隙 插入新记录，所以设计 InnoDB 的大叔们就提出了一种称之为 Next-Key Locks 的锁，官方的类型名称为： LOCK_ORDINARY ，我们也可以简称为next-key锁 。 Insert Intention Locks我们说一个事务在插入一条记录时需要判断一下插入位置是不是被别的事务加了所谓的 gap锁 （ next-key锁 也包含 gap锁 ，后边就不强调了），如果有的话，插入操作需要等待，直到拥有 gap锁 的那个事务提交。但是设计 InnoDB 的大叔规定事务在等待的时候也需要在内存中生成一个 锁结构 ，表明有事务想在某个 间隙 中插入新记录，但是现在在等待。设计 InnoDB 的大叔就把这种类型的锁命名为 Insert IntentionLocks ，官方的类型名称为： LOCK_INSERT_INTENTION ，我们也可以称为 插入意向锁 。 比方说我们把 number 值为 8 的那条记录加一个 插入意向锁 的示意图如下： 为了让大家彻底理解这个 插入意向锁 的功能，我们还是举个例子然后画个图表示一下。比方说现在 T1 number 值为 8 的记录加了一个 gap锁 ，然后 T2 和 T3 分别想向 hero 表中插入 number 值分别为 4 、 5 的两条记录，所以现在为 number 值为 8 的记录加的锁的示意图就如下所示： 从图中可以看到，由于 T1 持有 gap锁 ，所以 T2 和 T3 需要生成一个 插入意向锁 的 锁结构 并且处于等待状态。当 T1 提交后会把它获取到的锁都释放掉，这样 T2 和 T3 就能获取到对应的 插入意向锁 了（本质上就是把插入意向锁对应锁结构的 is_waiting 属性改为 false ）， T2 和 T3 之间也并不会相互阻塞，它们可以同时获取到 number 值为8的 插入意向锁 ，然后执行插入操作。事实上插入意向锁并不会阻止别的事务继续获取该记录上任何类型的锁（ 插入意向锁 就是这么鸡肋）。 隐式锁我们前边说一个事务在执行 INSERT 操作时，如果即将插入的 间隙 已经被其他事务加了 gap锁 ，那么本次INSERT 操作会阻塞，并且当前事务会在该间隙上加一个 插入意向锁 ，否则一般情况下 INSERT 操作是不加锁的。那如果一个事务首先插入了一条记录（此时并没有与该记录关联的锁结构），然后另一个事务：立即使用 SELECT … LOCK IN SHARE MODE 语句读取这条事务，也就是在要获取这条记录的 S锁 ，或者使用 SELECT … FOR UPDATE 语句读取这条事务或者直接修改这条记录，也就是要获取这条记录的 X 锁 ，该咋办？如果允许这种情况的发生，那么可能产生 脏读 问题。立即修改这条记录，也就是要获取这条记录的 X锁 ，该咋办？如果允许这种情况的发生，那么可能产生 脏写 问题。 这时候我们前边唠叨了很多遍的 事务id 又要起作用了。我们把聚簇索引和二级索引中的记录分开看一下： 情景一：对于聚簇索引记录来说，有一个 trx_id 隐藏列，该隐藏列记录着最后改动该记录的 事务id 。那么如果在当前事务中新插入一条聚簇索引记录后，该记录的 trx_id 隐藏列代表的的就是当前事务的事务id ，如果其他事务此时想对该记录添加 S锁 或者 X锁 时，首先会看一下该记录的 trx_id 隐藏列代表的事务是否是当前的活跃事务，如果是的话，那么就帮助当前事务创建一个 X锁 （也就是为当前事务创建一个锁结构， is_waiting 属性是 false ），然后自己进入等待状态（也就是为自己也创建一个锁结构， is_waiting 属性是 true ）。 情景二：对于二级索引记录来说，本身并没有 trx_id 隐藏列，但是在二级索引页面的 Page Header 部分有一个 PAGE_MAX_TRX_ID 属性，该属性代表对该页面做改动的最大的 事务id ，如果PAGE_MAX_TRX_ID 属性值小于当前最小的活跃 事务id ，那么说明对该页面做修改的事务都已经提交了，否则就需要在页面中定位到对应的二级索引记录，然后回表找到它对应的聚簇索引记录，然后再重复 情景一 的做法。 通过上边的叙述我们知道，一个事务对新插入的记录可以不显式的加锁（生成一个锁结构），但是由于事务id 这个牛逼的东东的存在，相当于加了一个 隐式锁 。别的事务在对这条记录加 S锁 或者 X锁时，由于 隐式锁 的存在，会先帮助当前事务生成一个锁结构，然后自己再生成一个锁结构后进入等待 除了插入意向锁，在一些特殊情况下INSERT还会获取一些锁，我们稍后唠叨哈。 锁的内存结构Todo","link":"/2021/10/08/MySQL%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E9%94%81/"},{"title":"MySQL（四）文件系统和表空间","text":"","link":"/2021/10/08/MySQL%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%92%8C%E8%A1%A8%E7%A9%BA%E9%97%B4/"},{"title":"MySQL（十）回滚日志","text":"介绍了回滚日志undolog \u0010 事务回滚的需求我们说过 事务 需要保证 原子性 ，也就是事务中的操作要么全部完成，要么什么也不做。但是偏偏有时候事务执行到一半会出现一些情况，比如： 情况一：事务执行过程中可能遇到各种错误，比如服务器本身的错误，操作系统错误，甚至是突然断电导致的错误。 情况二：程序员可以在事务执行过程中手动输入 ROLLBACK 语句结束当前的事务的执行。 这两种情况都会导致事务执行到一半就结束，但是事务执行过程中可能已经修改了很多东西，为了保证事务的原子性，我们需要把东西改回原先的样子，这个过程就称之为 回滚 （英文名： rollback ），这样就可以造成一个假象：这个事务看起来什么都没做，所以符合 原子性 要求。 数据库中的回滚跟悔棋差不多，你插入了一条记录， 回滚 操作对应的就是把这条记录删除掉；你更新了一条记录， 回滚 操作对应的就是把该记录更新为旧值；你删除了一条记录， 回滚 操作对应的自然就是把该记录再插进去。 从上边的描述中我们已经能隐约感觉到，每当我们要对一条记录做改动时（这里的 改动 可以指 INSERT 、DELETE 、 UPDATE ），都需要留一手 —— 把回滚时所需的东西都给记下来。 比方说： 你插入一条记录时，至少要把这条记录的主键值记下来，之后回滚的时候只需要把这个主键值对应的记录删掉就好了。 你删除了一条记录，至少要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了。 你修改了一条记录，至少要把修改这条记录前的旧值都记录下来，这样之后回滚时再把这条记录更新为旧值就好了。 MySQL把这些为了回滚而记录的这些日志为撤销日志，英文名为 undo log 。 由于查询操作（ SELECT ）并不会修改任何用户记录，所以在查询操作执行时，并不需要记录相应的 undo日志 不同类型的操作产生的 undo日志 的格式也是不同的! 事务ID给事务分配id的时机我们前边在唠叨 事务简介 时说过，一个事务可以是一个只读事务，或者是一个读写事务：我们可以通过 START TRANSACTION READ ONLY 语句开启一个只读事务。在只读事务中不可以对普通的表（其他事务也能访问到的表）进行增、删、改操作，但可以对临时表做增、删、改操作。 我们可以通过 START TRANSACTION READ WRITE 语句开启一个读写事务，或者使用 BEGIN 、 START TRANSACTION 语句开启的事务默认也算是读写事务。 在读写事务中可以对表执行增删改查操作。 如果某个事务执行过程中对某个表执行了增、删、改操作，那么 InnoDB 存储引擎就会给它分配一个独一无二的事务id ，分配方式如下： 对于只读事务来说，只有在它第一次对某个用户创建的临时表执行增、删、改操作时才会为这个事务分配一个事务id ，否则的话是不分配 事务id 的。 对于读写事务来说，只有在它第一次对某个表（包括用户创建的临时表）执行增、删、改操作时才会为这个事务分配一个 事务id ，否则的话也是不分配事务id 我们前边说过对某个查询语句执行EXPLAIN分析它的查询计划时，有时候在Extra列会看到Using temporary的提示，这个表明在执行该查询语句时会用到内部临时表。这个所谓的内部临时表和我们手动用CREATE TEMPORARY TABLE创建的用户临时表并不一样，在事务回滚时并不需要把执行SELECT语句过程中用到的内部临时表也回滚，在执行SELECT语句用到内部临时表时并不会为它分配事务id。 总而言之，只有在事务中对表的记录做改动的时候才会对这个事务分配一个唯一的事务id。 事务id是怎么生成的事务id 本质上就是一个数字，它的分配策略和我们前边提到的对隐藏列 row_id （当用户没有为表创建主键和 UNIQUE 键时 InnoDB 自动创建的列）的分配策略大抵相同，具体策略如下： 服务器会在内存中维护一个全局变量，每当需要为某个事务分配一个 事务id 时，就会把该变量的值当作事务id 分配给该事务，并且把该变量自增1。 每当这个变量的值为 256 的倍数时，就会将该变量的值刷新到系统表空间的页号为 5 的页面中一个称之为Max Trx ID 的属性处，这个属性占用 8 个字节的存储空间。 当系统下一次重新启动时，会将上边提到的 Max Trx ID 属性加载到内存中，将该值加上256之后赋值给我们前边提到的全局变量（因为在上次关机时该全局变量的值可能大于 Max Trx ID 属性值） 这样就可以保证整个系统中分配的 事务id 值是一个递增的数字。先被分配 id 的事务得到的是较小的 事务id ，后被分配 id 的事务得到的是较大的 事务id 。 trx_id隐藏列聚簇索引的记录除了会保存完整的用户数据以外，而且还会自动添加名为trx_id、roll_pointer的隐藏列，如果用户没有在表中定义主键以及UNIQUE键，还会自动添加一个名为row_id的隐藏列。所以一条记录在页面中的真实结构看起来就是这样的： 其中的 trx_id 列其实还蛮好理解的，就是某个对这个聚簇索引记录做改动的语句所在的事务对应的事务id 而已（此处的改动可以是 INSERT 、 DELETE 、 UPDATE 操作） undo日志的格式","link":"/2021/10/08/MySQL%EF%BC%88%E5%8D%81%EF%BC%89%E5%9B%9E%E6%BB%9A%E6%97%A5%E5%BF%97/"},{"title":"Netty（一）前世今生","text":"传统的HTTP服务器原理 创建一个ServerSocket，监听并绑定一个端口 一系列客户端来请求这个端口 服务器使用Accept，获得一个来自客户端的Socket连接对象 启动一个新线程处理连接 4.1 读Socket，得到字节流 4.2. 解码协议，得到Http请求对象 4.3 处理Http请求，得到一个结果，封装成一个HttpResponse对象 4.4 编码协议，将结果序列化字节流 写Socket，将字节流发给客户端 继续循环步骤3 HTTP服务器之所以称为HTTP服务器，是因为编码解码协议是HTTP协议，如果协议是Redis协议，那它就成了Redis服务器，如果协议是WebSocket，那它就成了WebSocket服务器，等等。 使用Netty你就可以定制编解码协议，实现自己的特定协议的服务器。 NIO上面是一个传统处理http的服务器，但是在高并发的环境下，线程数量会比较多，System load也会比较高，于是就有了NIO。 他并不是Java独有的概念，NIO代表的一个词汇叫着IO多路复用。它是由操作系统提供的系统调用，早期这个操作系统调用的名字是select，但是性能低下，后来渐渐演化成了Linux下的epoll和Mac里的kqueue。我们一般就说是epoll，因为没有人拿苹果电脑作为服务器使用对外提供服务。而Netty就是基于Java NIO技术封装的一套框架。为什么要封装，因为原生的Java NIO使用起来没那么方便，而且还有臭名昭著的bug，Netty把它封装之后，提供了一个易于操作的使用模式和接口，用户使用起来也就便捷多了。 说NIO之前先说一下BIO（Blocking IO）,如何理解这个Blocking呢？ 客户端监听（Listen）时，Accept是阻塞的，只有新连接来了，Accept才会返回，主线程才能继 读写socket时，Read是阻塞的，只有请求消息来了，Read才能返回，子线程才能继续处理 读写socket时，Write是阻塞的，只有客户端把消息收了，Write才能返回，子线程才能继续读取下一个请求","link":"/2021/05/24/Netty%EF%BC%88%E4%B8%80%EF%BC%89%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/"},{"title":"Netty（七）案例编写和调优参数","text":"","link":"/2021/05/27/Netty%EF%BC%88%E4%B8%83%EF%BC%89%E6%A1%88%E4%BE%8B%E7%BC%96%E5%86%99%E5%92%8C%E8%B0%83%E4%BC%98%E5%8F%82%E6%95%B0/"},{"title":"Netty（二）Reactor模型","text":"本篇主要讲述了三种Reactor模型： Reactor单线程模型 Reactor多线程模型 Reactor主从模型 三种Reactor模型生活场景的类比生活场景：饭店规模的扩大 一个人包揽所有活：迎宾、点菜、做饭、上菜、送客等。 多找几个伙计：大家一起做上面的事。 进一步分工：搞一个或多个人专门做迎宾。 类比： 饭店伙计：线程 迎宾：接入连接 点菜：请求 做菜：业务处理 上菜：响应 送客：断连 Reactor单线程模型最简单的单Reactor单线程模型。Reactor线程是个多面手，负责多路分离套接字，Accept新连接，并分派请求到处理器链中。该模型适用于处理器链中业务处理组件能快速完成的场景。不过,这种单线程模型不能充分利用多核资源，所以实际使用的不多。、 Reactor多线程模型该模型在处理器链部分采用了多线程（线程池）。 Reactor主从模型是将Reactor分成两部分，mainReactor负责监听server socket, accept新连接，并将建立的socket分派给subReactor。subReactor负责 多路分离已连接的socket,读写网络数据，对业务处理功能，其扔给worker线程池完成。 通常，subReactor个数 上可与CPU个数等同。 主从Reactor源码分析简单说就是两种SocketChannel绑定到两种Reactor模型中去完成主从模型的支持。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class EchoServer { private int port; public EchoServer(int port) { this.port = port; } public static void main(String[] args) throws Exception { new EchoServer(8833).start(); } public void start() throws Exception { //1.Reactor模型的主、从多线程 EventLoopGroup mainGroup = new NioEventLoopGroup(); EventLoopGroup childGroup = new NioEventLoopGroup(); try { //2.构造引导器实例ServerBootstrap ServerBootstrap b = new ServerBootstrap(); b.group(mainGroup, childGroup) .channel(NioServerSocketChannel.class) //2.1 设置NIO的channel .localAddress(new InetSocketAddress(port)) //2.2 配置本地监听端口 .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { //2.3 初始化channel的时候，配置Handler @Override protected void initChannel(final SocketChannel socketChannel) { socketChannel.pipeline() .addLast(&quot;codec&quot;, new HttpServerCodec()) .addLast(&quot;compressor&quot;, new HttpContentCompressor()) .addLast(&quot;aggregator&quot;, new HttpObjectAggregator(65536)) .addLast(&quot;handler&quot;, new EchoServerHandler()); //2.4 加入自定义业务逻辑ChannelHandler } }); ChannelFuture f = b.bind().sync(); //3.启动监听 System.out.println(&quot;Http Server started， Listening on &quot; + port); f.channel().closeFuture().sync(); } finally { mainGroup.shutdownGracefully().sync(); childGroup.shutdownGracefully().sync(); } }}public class EchoServerHandler extends SimpleChannelInboundHandler&lt;FullHttpRequest&gt; { @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, FullHttpRequest fullHttpRequest) throws Exception { String content = String.format(&quot;Receive http request, url: %s , method: %s , content: %s%n&quot; ,fullHttpRequest.uri() ,fullHttpRequest.method() ,fullHttpRequest.content().toString().getBytes(StandardCharsets.UTF_8)); DefaultFullHttpResponse response = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.OK, Unpooled.wrappedBuffer(content.getBytes())); channelHandlerContext.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); }}shengbinbin@chengbinbindeMacBook-Pro netty_example % curl http://localhost:8833/abcReceive http request, url: /abc , method: GET , content: [B@3e40b233 只需要创建两个EventLoopGroup，然后绑定到引导器ServerBootstrap上就好了. mainGroup 是主 Reactor，childGroup 是从 Reactor。它们分别使用不同的 NioEventLoopGroup，主 Reactor 负责处理 Accept，然后把 Channel 注册到从 Reactor 上，从 Reactor 主要负责 Channel 生命周期内的所有 I/O 事件。 1）什么是Channel Channel 的字面意思是“通道”，它是网络通信的载体，提供了基本的 API 用于网络 I/O 操作，如 register、bind、connect、read、write、flush 等。 Netty 实现的 Channel 是以 JDK NIO Channel 为基础的，提供了更高层次的抽象，屏蔽了底层 Socket。 2）什么是ChannleHandler和ChannelPipeline ChannelHandler实现对客户端发送过来的数据进行处理，可能包括编解码、自定义业务逻辑处理等等。 ChannelPipeline 负责组装各种 ChannelHandler，当 I/O 读写事件触发时，ChannelPipeline 会依次调用 ChannelHandler 列表对 Channel 的数据进行拦截和处理。 3）什么是EventLoopGroup？ EventLoopGroup 本质是一个线程池， 是 Netty Reactor 线程模型的具体实现方式，主要负责接收 I/O 请求，并分配线程执行处理请求。我们在demo中使用了它的实现类 NioEventLoopGroup，也是 Netty 中最被推荐使用的线程模型。 我们还通过构建main EventLoopGroup 和 child EventLoopGroup 实现了 “主从Reactor模式”。 4）EventLoopGroup、EventLoop、Channel有什么关系？ 一个 EventLoopGroup 往往包含一个或者多个 EventLoop。 EventLoop 用于处理 Channel 生命周期内的所有 I/O 事件，如 accept、connect、read、write 等 I/O 事件。 EventLoop 同一时间会与一个线程绑定，每个 EventLoop 负责处理多个 Channel。 1234567891011// Configure the server. //主从模型 EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); //自定义业务逻辑 ChannelHandler final EchoServerHandler serverHandler = new EchoServerHandler(); try { //构造引导器实例ServerBootstrap ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) //加入主从group .channel(NioServerSocketChannel.class) //设置NIO的channel 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) { super.group(parentGroup); //加入主reactor模型 ObjectUtil.checkNotNull(childGroup, &quot;childGroup&quot;); if (this.childGroup != null) { throw new IllegalStateException(&quot;childGroup set already&quot;); } this.childGroup = childGroup; //加入从reactor模型 return this; }//====super.group(parentGroup)====public B group(EventLoopGroup group) { ObjectUtil.checkNotNull(group, &quot;group&quot;); if (this.group != null) { throw new IllegalStateException(&quot;group set already&quot;); } this.group = group; return self(); }//this.group = group;public abstract class AbstractBootstrap&lt;B extends AbstractBootstrap&lt;B, C&gt;, C extends Channel&gt; implements Cloneable { volatile EventLoopGroup group; //这个成员变量 final ChannelFuture initAndRegister() { Channel channel = null; try { channel = channelFactory.newChannel(); init(channel); } catch (Throwable t) { if (channel != null) { // channel can be null if newChannel crashed (eg SocketException(&quot;too many open files&quot;)) channel.unsafe().closeForcibly(); // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(channel, GlobalEventExecutor.INSTANCE).setFailure(t); } // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(new FailedChannel(), GlobalEventExecutor.INSTANCE).setFailure(t); } //开始register，将这个channel 注册到主Reactor模型中去，完成绑定关系，在这是ServerSocketChannel ChannelFuture regFuture = config().group().register(channel); if (regFuture.cause() != null) { if (channel.isRegistered()) { channel.close(); } else { channel.unsafe().closeForcibly(); } } // If we are here and the promise is not failed, it's one of the following cases: // 1) If we attempted registration from the event loop, the registration has been completed at this point. // i.e. It's safe to attempt bind() or connect() now because the channel has been registered. // 2) If we attempted registration from the other thread, the registration request has been successfully // added to the event loop's task queue for later execution. // i.e. It's safe to attempt bind() or connect() now: // because bind() or connect() will be executed *after* the scheduled registration task is executed // because register(), bind(), and connect() are all bound to the same thread. return regFuture; } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) { super.group(parentGroup); ObjectUtil.checkNotNull(childGroup, &quot;childGroup&quot;); if (this.childGroup != null) { throw new IllegalStateException(&quot;childGroup set already&quot;); } this.childGroup = childGroup; return this; }private volatile EventLoopGroup childGroup; //从reactor模型就是这个成员变量@Override void init(Channel channel) { setChannelOptions(channel, options0().entrySet().toArray(newOptionArray(0)), logger); setAttributes(channel, attrs0().entrySet().toArray(newAttrArray(0))); ChannelPipeline p = channel.pipeline(); final EventLoopGroup currentChildGroup = childGroup; //childGroup就是currentChildGroup final ChannelHandler currentChildHandler = childHandler; final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] currentChildOptions = childOptions.entrySet().toArray(newOptionArray(0)); final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] currentChildAttrs = childAttrs.entrySet().toArray(newAttrArray(0)); //ChannelInitializer一次性、初始化handler: //负责添加一个ServerBootstrapAcceptor handler，添加完后，自己就移除了: //ServerBootstrapAcceptor handler： 负责接收客户端连接创建连接后，对连接的初始化工作。 p.addLast(new ChannelInitializer&lt;Channel&gt;() { @Override public void initChannel(final Channel ch) { final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); if (handler != null) { pipeline.addLast(handler); } ch.eventLoop().execute(new Runnable() { @Override public void run() { pipeline.addLast(new ServerBootstrapAcceptor( //currentChildGroup传入进去 ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); } }); } }); }public void channelRead(ChannelHandlerContext ctx, Object msg) { final Channel child = (Channel) msg; //SocketChannel child.pipeline().addLast(childHandler); setChannelOptions(child, childOptions, logger); setAttributes(child, childAttrs); try { //这里的SocketChannel 绑定到从Reactor模型中去 childGroup.register(child).addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture future) throws Exception { if (!future.isSuccess()) { forceClose(child, future.cause()); } } }); } catch (Throwable t) { forceClose(child, t); } } 逻辑架构 服务端利用ServerBootstrap进行启动引导，绑定监听端口 启动初始化时有 main EventLoopGroup 和 child EventLoopGroup 两个组件，其中 main EventLoopGroup负责监听网络连接事件。当有新的网络连接时，就将 Channel 注册到 child EventLoopGroup。 child EventLoopGroup 会被分配一个 EventLoop 负责处理该 Channel 的读写事件。 当客户端发起 I/O 读写事件时，服务端 EventLoop 会进行数据的读取，然后通过 ChannelPipeline 依次有序触发各种ChannelHandler进行数据处理。 客户端数据会被依次传递到 ChannelPipeline 的 ChannelInboundHandler 中，在一个handler中处理完后就会传入下一个handler。 当数据写回客户端时，会将处理结果依次传递到 ChannelPipeline 的 ChannelOutboundHandler 中，在一个handler中处理完后就会传入下一个handler，最后返回客户端。","link":"/2021/05/24/Netty%EF%BC%88%E4%BA%8C%EF%BC%89Reactor%E6%A8%A1%E5%9E%8B/"},{"title":"Netty（三）TCP粘包和半包","text":"本篇讲述了什么是TCP的粘包和半包以及Netty是如何解决的 什么是粘包和半包代码演示123456789101112131415161718192021222324252627282930313233343536373839//服务端代码public class TimeServer { public void bind(int port) throws Exception { //初始化两个 loopGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { //服务端初始化的 bootstrap ServerBootstrap s = new ServerBootstrap(); //加载两个 loopGroup s.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 1024)//设置 NioServerSocketChannel 的 TCP 参数，设置 backlog 为 1024 .childHandler(new ChildChannelHandler()); //绑定 IO 事件处理类 ChildChannelHandler。 //异步监听端口，同步等待关闭 ChannelFuture f = s.bind(port).sync(); f.channel().closeFuture().sync(); }finally { //关闭两个 loopGroup bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { int port = 8080; new TimeServer().bind(8080); }}public class ChildChannelHandler extends ChannelInitializer&lt;SocketChannel&gt; { @Override protected void initChannel(SocketChannel ch) throws Exception { //获取 channel 的 pipeline，这里仅仅加进尾端 ch.pipeline().addLast(new TimeServerHandler()); }} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253//客户端代码public class TimeClient { public void connect(int port, String host) { // 创建客户端处理 IO 读写的 NioEventLoopGroup 线程组 EventLoopGroup group = new NioEventLoopGroup(); try { // 创建客户端辅助启动类 Bootstrap b = new Bootstrap(); b.group(group).channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new FirstClientHandler()); } }); //调用connect发起异步请求，调用同步方法等待成功 ChannelFuture f = b.connect(host, port).sync(); f.channel().closeFuture().sync(); }catch (Exception e ) { }finally { group.shutdownGracefully(); } } public static void main(String[] args) { int port = 8080; new TimeClient().connect(port, &quot;127.0.0.1&quot;); }}public class FirstClientHandler extends ChannelInboundHandlerAdapter { @Override public void channelActive(ChannelHandlerContext ctx) { for (int i = 0; i &lt; 1000; i++) { ByteBuf buffer = getByteBuf(ctx); ctx.channel().writeAndFlush(buffer); } } private ByteBuf getByteBuf(ChannelHandlerContext ctx) { byte[] bytes = &quot;你好，我的名字是binshow!&quot;.getBytes(Charset.forName(&quot;utf-8&quot;)); ByteBuf buffer = ctx.alloc().buffer(); buffer.writeBytes(bytes); return buffer; } } 从服务端的控制台输出可以看出，存在三种类型的输出 一种是正常的字符串输出。 一种是多个字符串“粘”在了一起，我们定义这种 ByteBuf 为粘包。 一种是一个字符串被“拆”开，形成一个破碎的包，我们定义这种 ByteBuf 为半包。 原因分析 由图可以看出：发送端的字节流都会先传入缓冲区，再通过网络传入到接收端的缓冲区中，最终由接收端获取。 对于操作系统来说，只认TCP协议，尽管我们的应用层是按照 ByteBuf 为 单位来发送数据，server按照Bytebuf读取，但是到了底层操作系统仍然是按照字节流发送数据，因此，数据到了服务端，也是按照字节流的方式读入，然后到了 Netty 应用层面，重新拼装成ByteBuf，而这里的 ByteBuf 与客户端按顺序发送的 ByteBuf 可能是不对等的。因此，我们需要在客户端根据自定义协议来组装我们应用层的数据包，然后在服务端根据我们的应用层的协议来组装数据包，这个过程通常在服务端称为拆包，而在客户端称为粘包。 粘包的主要原因： 发送方每次写入数据 &lt; 套接字缓冲区大小 接收方读取套接字缓冲区数据不够及时 半包的主要原因： 发送方写入数据 &gt; 套接字缓冲区大小 发送的数据大于协议的MTU（最大传输单元），必须拆包 根本原因：TCP是流式协议，消息是无边界的。而UDP虽然一次运输多个包裹，但是每个包裹是有边界的，所以没有粘包等现象。 Netty对三种常用封帧方式的支持在没有 Netty 的情况下，用户如果自己需要拆包，基本原理就是不断从 TCP 缓冲区中读取数据，每次读取完都需要判断是否是一个完整的数据包 如果当前读取的数据不足以拼接成一个完整的业务数据包，那就保留该数据，继续从 TCP 缓冲区中读取，直到得到一个完整的数据包。 如果当前读到的数据加上已经读取的数据足够拼接成一个数据包，那就将已经读取的数据拼接上本次读取的数据，构成一个完整的业务数据包传递到业务逻辑，多余的数据仍然保留，以便和下次读到的数据尝试拼接。 而在Netty中，已经造好了许多类型的拆包器，我们直接用就好： FixedLengthFrameDecoder固定长度的拆包器 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class TimeClient { public void connect(int port, String host) { // 创建客户端处理 IO 读写的 NioEventLoopGroup 线程组 EventLoopGroup group = new NioEventLoopGroup(); try { // 创建客户端辅助启动类 Bootstrap b = new Bootstrap(); b.group(group).channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new FirstClientHandler()); ch.pipeline().addLast(new FixedLengthFrameDecoder(32)); // } }); //调用connect发起异步请求，调用同步方法等待成功 ChannelFuture f = b.connect(host, port).sync(); f.channel().closeFuture().sync(); }catch (Exception e ) { }finally { group.shutdownGracefully(); } } public static void main(String[] args) { int port = 8080; new TimeClient().connect(port, &quot;127.0.0.1&quot;); }}public class TimeServer { public void bind(int port) throws Exception { //初始化两个 loopGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { //服务端初始化的 bookstrap ServerBootstrap s = new ServerBootstrap(); //加载两个 loopGroup s.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 1024)//设置 NioServerSocketChannel 的 TCP 参数，设置 backlog 为 1024 //.childHandler(new ChildChannelHandler()); //绑定 IO 事件处理类 ChildChannelHandler。 .childHandler(new ChannelInitializer() { @Override protected void initChannel(Channel channel) throws Exception { ChannelPipeline pipeline = channel.pipeline(); pipeline.addLast(new FixedLengthFrameDecoder(32)); pipeline.addLast(new ChildChannelHandler()); } }); //异步监听端口，同步等待关闭 ChannelFuture f = s.bind(port).sync(); f.channel().closeFuture().sync(); }finally { //关闭两个 loopGroup bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { int port = 8080; new TimeServer().bind(8080); }} LineBasedFrameDecoder每个数据包之间以换行符作为分隔符。 1234567891011121314151617181920212223242526272829303132public class TimeServer { public void bind(int port) throws Exception { //初始化两个 loopGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { //服务端初始化的 bootstrap ServerBootstrap s = new ServerBootstrap(); //加载两个 loopGroup s.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 1024) .childHandler(new ChildChannelHandler()); //异步监听端口，同步等待关闭 ChannelFuture f = s.bind(port).sync(); f.channel().closeFuture().sync(); }finally { //关闭两个 loopGroup bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { int port = 8080; new TimeServer().bind(8080); }} 12345678910111213public class ChildChannelHandler extends ChannelInitializer&lt;SocketChannel&gt; { @Override protected void initChannel(SocketChannel ch) throws Exception { // 以行为编解码基础的 LineBasedFrameDecoder，并设置最大行字节为 1024 ch.pipeline().addLast(new LineBasedFrameDecoder(1024)); //2使用 StringDecoder 将链条中的前结点编解码结果解码为字符串 ch.pipeline().addLast(new StringDecoder()); //3.根据链条中的前结点的编解码结果进行业务逻辑处理 ch.pipeline().addLast(new TimeServerHandler()); }} 12345678910111213141516171819202122232425262728293031323334public class TimeClient { public void connect(int port, String host) { // 创建客户端处理 IO 读写的 NioEventLoopGroup 线程组 EventLoopGroup group = new NioEventLoopGroup(); try { // 创建客户端辅助启动类 Bootstrap b = new Bootstrap(); b.group(group).channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new LineBasedFrameDecoder(1024)); ch.pipeline().addLast(new StringDecoder()); ch.pipeline().addLast(new TimeClientHandler()); } }); //调用connect发起异步请求，调用同步方法等待成功 ChannelFuture f = b.connect(host, port).sync(); f.channel().closeFuture().sync(); }catch (Exception e ) { }finally { group.shutdownGracefully(); } } public static void main(String[] args) { int port = 8080; new TimeClient().connect(port, &quot;127.0.0.1&quot;); }} 123456789101112131415161718192021222324252627282930313233343536373839public class TimeClientHandler extends ChannelInboundHandlerAdapter { //日志记录 private static final Logger logger = Logger.getLogger(TimeClientHandler.class.getName()); private int counter; private byte[] req; public TimeClientHandler() { req = (&quot;QUERY TIME ORDER&quot; + System.getProperty(&quot;line.separator&quot;)).getBytes(); } @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { ByteBuf message = null; for (int i=0;i&lt;100;i++) { message= Unpooled.buffer(req.length); message.writeBytes(req); ctx.writeAndFlush(message); } } //当客户端和服务端tcp链路建立成功之后，netty的nio线程会调用channelActive方法 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { String body = (String) msg; System.out.println(&quot;now is : &quot; + body + &quot; ; the counter is : &quot; + ++counter); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { logger.warning(&quot;unexpected exception from downstream: &quot; + cause.getMessage()); ctx.close(); }} DelimiterBasedFrameDecoder1234567891011121314151617181920212223242526272829303132333435363738394041public class EchoServer { public void bind(int port) throws Exception { EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { ServerBootstrap s = new ServerBootstrap(); s.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 1024) //添加日志 .handler(new LoggingHandler()) //处理 .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { protected void initChannel(SocketChannel ch) throws Exception { //根据 &quot;$_&quot; 作为分隔符，然后进行分割 ByteBuf delimitoer = Unpooled.copiedBuffer(&quot;$_&quot;.getBytes()); //根据指定分隔符来切割信息流的开始和结束 ch.pipeline().addLast(new DelimiterBasedFrameDecoder(1024, delimitoer)); //字符串解码 ch.pipeline().addLast(new StringDecoder()); //将字符传递给服务端处理器 ch.pipeline().addLast(new EchoServerHandler()); } }); ChannelFuture f = s.bind(port).sync(); f.channel().closeFuture().sync(); }catch (Exception e) { }finally { bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { new EchoServer().bind(8080); }} 12345678910111213141516171819public class EchoServerHandler extends ChannelInboundHandlerAdapter { int count = 0; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { String body = (String) msg; System.out.println(&quot;this is &quot;+ ++count + &quot; times receive client:[&quot; + body + &quot;]&quot; ); body += &quot;$_&quot;; ByteBuf echo = Unpooled.copiedBuffer(body.getBytes()); ctx.writeAndFlush(echo); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); ctx.close(); }} 123456789101112131415161718192021222324252627282930313233public class EchoClient { public void connect(int port, String host) { // 创建客户端处理 IO 读写的 NioEventLoopGroup 线程组 EventLoopGroup group = new NioEventLoopGroup(); try { // 创建客户端辅助启动类 Bootstrap b = new Bootstrap(); b.group(group).channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { protected void initChannel(SocketChannel ch) throws Exception { ByteBuf delimitoer = Unpooled.copiedBuffer(&quot;$_&quot;.getBytes()); ch.pipeline().addLast(new DelimiterBasedFrameDecoder(1024, delimitoer)); ch.pipeline().addLast(new StringDecoder()); ch.pipeline().addLast(new EchoClientHandler()); } }); //调用connect发起异步请求，调用同步方法等待成功 ChannelFuture f = b.connect(host, port).sync(); f.channel().closeFuture().sync(); }catch (Exception e ) { }finally { group.shutdownGracefully(); } } public static void main(String[] args) { new EchoClient().connect(8080, &quot;127.0.0.1&quot;); }} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class EchoClient { public void connect(int port, String host) { // 创建客户端处理 IO 读写的 NioEventLoopGroup 线程组 EventLoopGroup group = new NioEventLoopGroup(); try { // 创建客户端辅助启动类 Bootstrap b = new Bootstrap(); b.group(group).channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { protected void initChannel(SocketChannel ch) throws Exception { ByteBuf delimitoer = Unpooled.copiedBuffer(&quot;$_&quot;.getBytes()); ch.pipeline().addLast(new DelimiterBasedFrameDecoder(1024, delimitoer)); ch.pipeline().addLast(new StringDecoder()); ch.pipeline().addLast(new EchoClientHandler()); } }); //调用connect发起异步请求，调用同步方法等待成功 ChannelFuture f = b.connect(host, port).sync(); f.channel().closeFuture().sync(); }catch (Exception e ) { }finally { group.shutdownGracefully(); } } public static void main(String[] args) { new EchoClient().connect(8080, &quot;127.0.0.1&quot;); }}//而客户端处理器需要输出结果，查看是不是有分隔符没分割成功的问题public class EchoClientHandler extends ChannelInboundHandlerAdapter { private int counter; static final String ECHO_REQ = &quot;Hi, Lilinfeng. welcome to Netty.$_&quot;; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { for (int i=0;i &lt; 100;i++) { ctx.writeAndFlush(Unpooled.copiedBuffer(ECHO_REQ.getBytes())); } } @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { System.out.println(&quot;this is &quot;+ ++counter + &quot; times receive server :[&quot; + msg + &quot;]&quot;); } @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { ctx.flush(); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); ctx.close(); }} 源码解读","link":"/2021/05/24/Netty%EF%BC%88%E4%B8%89%EF%BC%89TCP%E7%B2%98%E5%8C%85%E5%92%8C%E5%8D%8A%E5%8C%85/"},{"title":"MySQL（八）事务","text":"事务是由一组SQL语句组成的逻辑处理单元，而锁可以保证事务的隔离性。 事务简介事务的起源对于大部分程序员来说，他们的任务就是把现实世界的业务场景映射到数据库世界。比如银行为了存储人们的账户信息会建立一个account表： 123456CREATE TABLE account ( id INT NOT NULL AUTO_INCREMENT COMMENT '自增id', name VARCHAR(100) COMMENT '客户名称', balance INT COMMENT '余额', PRIMARY KEY (id)) Engine=InnoDB CHARSET=utf8; 狗哥和猫爷是一对好基友，他们都到银行开一个账户，他们在现实世界中拥有的资产就会体现在数据库世界的account表中。比如现在狗哥有11元，猫爷只有2元，那么现实中的这个情况映射到数据库的account表就是这样： 123456+----+--------+---------+| id | name | balance |+----+--------+---------+| 1 | 狗哥 | 11 || 2 | 猫爷 | 2 |+----+--------+---------+ 在某个特定的时刻，狗哥猫爷这些家伙在银行所拥有的资产是一个特定的值，这些特定的值也可以被描述为账户在这个特定的时刻现实世界的一个状态。随着时间的流逝，狗哥和猫爷可能陆续进行向账户中存钱、取钱或者向别人转账等操作，这样他们账户中的余额就可能发生变动，每一个操作都相当于现实世界中账户的一次状态转换。数据库世界作为现实世界的一个映射，自然也要进行相应的变动。不变不知道，一变吓一跳，现实世界中一些看似很简单的状态转换，映射到数据库世界却不是那么容易的。比方说有一次猫爷在赌场赌博输了钱，急忙打电话给狗哥要借10块钱，不然那些看场子的就会把自己剁了。现实世界中的狗哥走向了ATM机，输入了猫爷的账号以及10元的转账金额，然后按下确认，狗哥就拔卡走人了。对于数据库世界来说，相当于执行了下边这两条语句： 12UPDATE account SET balance = balance - 10 WHERE id = 1;UPDATE account SET balance = balance + 10 WHERE id = 2; 但是这里头有个问题，上述两条语句只执行了一条时忽然服务器断电了咋办？把狗哥的钱扣了，但是没给猫爷转过去，那猫爷还是逃脱不了被砍死的噩运～ 即使对于单独的一条语句，我们前边唠叨Buffer Pool时也说过，在对某个页面进行读写访问时，都会先把这个页面加载到Buffer Pool中，之后如果修改了某个页面，也不会立即把修改同步到磁盘，而只是把这个修改了的页面加到Buffer Pool的flush链表中，在之后的某个时间点才会刷新到磁盘。如果在将修改过的页刷新到磁盘之前系统崩溃了那岂不是猫爷还是要被砍死？或者在刷新磁盘的过程中（只刷新部分数据到磁盘上）系统奔溃了猫爷也会被砍死？ 怎么才能保证让可怜的猫爷不被砍死呢？其实再仔细想想，我们只是想让某些数据库操作符合现实世界中状态转换的规则而已，设计数据库的大叔们仔细盘算了盘算，现实世界中状态转换的规则有好几条，待我们慢慢道来。 事务的四个基本属性事务是由一组SQL语句组成的逻辑处理单元，具有4个属性，通常简称为事务的ACID属性。 A (Atomicity) 原子性：整个事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样 C (Consistency) 一致性：一致性指的就是数据库在进行事务操作后，会由原来的一致状态，变成另一种一致的状态。也就是说当事务提交后，或者当事务发生回滚后，数据库的完整性约束不能被破坏。 I (Isolation)隔离性：一个事务的执行不能其它事务干扰。即一个事务内部的操作及使用的数据对其它并发事务是隔离的，并发执行的各个事务之间不能互相干扰 D (Durability) 持久性：在事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚 并发事务会带来的问题更新丢失（Lost Update)： 事务A和事务B选择同一行，然后基于最初选定的值更新该行时，由于两个事务都不知道彼此的存在，就会发生丢失更新问题 **脏读(Dirty Reads)**：读到了其他事务还没有提交的数据。 **不可重复读（Non-Repeatable Reads)**：事务 A 多次读取同一数据，事务B在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果不一致。 幻读（Phantom Reads)：幻读与不可重复读类似。它发生在一个事务A读取了几行数据，接着另一个并发事务B插入了一些数据时。在随后的查询中，事务A就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 幻读和不可重复读的区别： 不可重复读的重点是修改：在同一事务中，同样的条件，第一次读的数据和第二次读的数据不一样。（因为中间有其他事务提交了修改） 幻读的重点在于新增或者删除：在同一事务中，同样的条件,，第一次和第二次读出来的记录数不一样。（因为中间有其他事务提交了插入/删除） 如何解决这些问题“更新丢失”通常是应该完全避免的。但防止更新丢失，并不能单靠数据库事务控制器来解决，需要应用程序对要更新的数据加必要的锁来解决，因此，防止更新丢失应该是应用的责任。 “脏读” 、 “不可重复读”和“幻读” ，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决： 一种是加锁：在读取数据前，对其加锁，阻止其他事务对数据进行修改。 另一种是数据多版本并发控制（MultiVersion Concurrency Control，简称 MVCC 或 MCC），也称为多版本数据库：不用加任何锁， 通过一定机制生成一个数据请求时间点的一致性数据快照 （Snapshot)， 并用这个快照来提供一定级别 （语句级或事务级） 的一致性读取。从用户的角度来看，好象是数据库可以提供同一数据的多个版本。 事务的四种隔离级别READ-UNCOMMITTED(读未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读 InnoDB 存储引擎在 REPEATABLE-READ（可重读）事务隔离级别下使用的是Next-Key Lock 算法，因此可以避免幻读的产生. 介绍一下MVCCMultiversion Concurrency Control，中文翻译过来就是多版本并发控制技术，MVCC 的实现是通过保存数据在某个时间点的快照来实现的。也就是说不管需要执行多长时间，每个事务看到的数据都是一致的。 MVCC在innodb中的实现InnoDB 的 MVCC，是通过在每行记录后面保存两个隐藏的列来实现： db_row_id：隐藏的行 ID，用来生成默认聚集索引。如果我们创建数据表的时候没有指定聚集索引，这时 InnoDB 就会用这个隐藏 ID 来创建聚集索引。采用聚集索引的方式可以提升数据的查找效率。 db_trx_id：操作这个数据的事务 ID，也就是最后一个对该数据进行插入或更新的事务 ID。 db_roll_ptr：回滚指针，也就是指向这个记录的 Undo Log 信息。 InnoDB 将行记录快照保存在了 Undo Log 里，我们可以在回滚段中找到它们： 从图中你能看到回滚指针将数据行的所有快照记录都通过链表的结构串联了起来，每个快照的记录都保存了当时的 db_trx_id，也是那个时间点操作这个数据的事务 ID。这样如果我们想要找历史快照，就可以通过遍历回滚指针的方式进行查找 如果一个事务想要查询这个行记录，需要读取哪个版本的行记录呢？这时就需要用到 Read View 了，它帮我们解决了行的可见性问题。Read View 保存了当前事务开启时所有活跃（还没有提交）的事务列表 在 Read VIew 中有几个重要的属性： trx_ids，系统当前正在活跃的事务 ID 集合。 low_limit_id，活跃的事务中最大的事务 ID。 up_limit_id，活跃的事务中最小的事务 ID。 creator_trx_id，创建这个 Read View 的事务 ID。 假设当前有事务 creator_trx_id 想要读取某个行记录，这个行记录的事务 ID 为 trx_id，那么会出现以下几种情况： 如果 trx_id &lt; 活跃的最小事务 ID（up_limit_id），也就是说这个行记录在这些活跃的事务创建之前就已经提交了，那么这个行记录对该事务是可见的。 如果 trx_id &gt; 活跃的最大事务 ID（low_limit_id），这说明该行记录在这些活跃的事务创建之后才创建，那么这个行记录对当前事务不可见。 如果 up_limit_id &lt; trx_id &lt; low_limit_id，说明该行记录所在的事务 trx_id 在目前 creator_trx_id 这个事务创建的时候，可能还处于活跃的状态，因此我们需要在 trx_ids 集合中进行遍历，如果 trx_id 存在于 trx_ids 集合中，证明这个事务 trx_id 还处于活跃状态，不可见。否则，如果 trx_id 不存在于 trx_ids 集合中，证明事务 trx_id 已经提交了，该行记录可见 总而言之，当我们查询一条记录的时候，系统如何通过多版本并发控制技术找到它： 首先获取事务自己的版本号，也就是事务 ID； 获取 Read View； 查询得到的数据，然后与 Read View 中的事务版本号进行比较； 如果不符合 ReadView 规则，就需要从 Undo Log 中获取历史快照； 最后返回符合规则的数据。 InnoDB 中，MVCC 是通过 Undo Log + Read View 进行数据读取，Undo Log 保存了历史快照，而 Read View 规则帮我们判断当前版本的数据是否可见。 在隔离级别为读已提交（Read Commit）时，一个事务中的每一次 SELECT 查询都会获取一次 Read View 当隔离级别为可重复读的时候，就避免了不可重复读，这是因为一个事务只在第一次 SELECT 的时候会获取一次 Read View，而后面所有的 SELECT 都会复用这个 Read View innodb如何解决幻读的问题在可重复读的情况下，InnoDB 可以通过 Next-Key 锁 +MVCC 来解决幻读问题。 事务日志InnoDB 使用日志来减少提交事务时的开销。因为日志中已经记录了事务，就无须在每个事务提交时把缓冲池的脏块刷新(flush)到磁盘中。 事务修改的数据和索引通常会映射到表空间的随机位置，所以刷新这些变更到磁盘需要很多随机 IO。 InnoDB 假设使用常规磁盘，随机IO比顺序IO昂贵得多，因为一个IO请求需要时间把磁头移到正确的位置，然后等待磁盘上读出需要的部分，再转到开始位置。 InnoDB 用日志把随机IO变成顺序IO。一旦日志安全写到磁盘，事务就持久化了，即使断电了，InnoDB可以重放日志并且恢复已经提交的事务。 InnoDB 使用一个后台线程智能地刷新这些变更到数据文件。这个线程可以批量组合写入，使得数据写入更顺序，以提高效率。 事务日志可以帮助提高事务效率： 使用事务日志，存储引擎在修改表的数据时只需要修改其内存拷贝，再把该修改行为记录到持久在硬盘上的事务日志中，而不用每次都将修改的数据本身持久到磁盘。 事务日志采用的是追加的方式，因此写日志的操作是磁盘上一小块区域内的顺序I/O，而不像随机I/O需要在磁盘的多个地方移动磁头，所以采用事务日志的方式相对来说要快得多。 事务日志持久以后，内存中被修改的数据在后台可以慢慢刷回到磁盘。 如果数据的修改已经记录到事务日志并持久化，但数据本身没有写回到磁盘，此时系统崩溃，存储引擎在重启时能够自动恢复这一部分修改的数据。 目前来说，大多数存储引擎都是这样实现的，我们通常称之为预写式日志（Write-Ahead Logging），修改数据需要写两次磁盘。 事务的ACID是如何保证实现的事务的隔离性是通过锁实现，而事务的原子性、一致性和持久性则是通过事务日志实现 。 事务日志包括：重做日志redo和回滚日志undo redo log（重做日志） 实现持久化和原子性 在innoDB的存储引擎中，事务日志通过重做(redo)日志和innoDB存储引擎的日志缓冲(InnoDB Log Buffer)实现。事务开启时，事务中的操作，都会先写入存储引擎的日志缓冲中，在事务提交之前，这些缓冲的日志都需要提前刷新到磁盘上持久化，这就是DBA们口中常说的“日志先行”(Write-Ahead Logging)。当事务提交之后，在Buffer Pool中映射的数据文件才会慢慢刷新到磁盘。此时如果数据库崩溃或者宕机，那么当系统重启进行恢复时，就可以根据redo log中记录的日志，把数据库恢复到崩溃前的一个状态。未完成的事务，可以继续提交，也可以选择回滚，这基于恢复的策略而定。 在系统启动的时候，就已经为redo log分配了一块连续的存储空间，以顺序追加的方式记录Redo Log，通过顺序IO来改善性能。所有的事务共享redo log的存储空间，它们的Redo Log按语句的执行顺序，依次交替的记录在一起。 undo log（回滚日志） 实现一致性 undo log 主要为事务的回滚服务。在事务执行的过程中，除了记录redo log，还会记录一定量的undo log。undo log记录了数据在每个操作前的状态，如果事务执行过程中需要回滚，就可以根据undo log进行回滚操作。单个事务的回滚，只会回滚当前事务做的操作，并不会影响到其他的事务做的操作。 Undo记录的是已部分完成并且写入硬盘的未完成的事务，默认情况下回滚日志是记录下表空间中的（共享表空间或者独享表空间） 二种日志均可以视为一种恢复操作，redo_log是恢复提交事务修改的页操作，而undo_log是回滚行记录到特定版本。二者记录的内容也不同，redo_log是物理日志，记录页的物理修改操作，而undo_log是逻辑日志，根据每行记录进行记录。 又引出个问题：你知道MySQL 有多少种日志吗？ 错误日志：记录出错信息，也记录一些警告信息或者正确的信息。 查询日志：记录所有对数据库请求的信息，不论这些请求是否得到了正确的执行。 慢查询日志：设置一个阈值，将运行时间超过该值的所有SQL语句都记录到慢查询的日志文件中。 二进制日志：记录对数据库执行更改的所有操作。 中继日志：中继日志也是二进制日志，用来给slave 库恢复 事务日志：重做日志redo和回滚日志undo MySQL对分布式事务的支持分布式事务的实现方式有很多，既可以采用 InnoDB 提供的原生的事务支持，也可以采用消息队列来实现分布式事务的最终一致性。这里我们主要聊一下 InnoDB 对分布式事务的支持。 MySQL 从 5.0.3 InnoDB 存储引擎开始支持XA协议的分布式事务。一个分布式事务会涉及多个行动，这些行动本身是事务性的。所有行动都必须一起成功完成，或者一起被回滚。 在MySQL中，使用分布式事务涉及一个或多个资源管理器和一个事务管理器。 如图，MySQL 的分布式事务模型。模型中分三块：应用程序（AP）、资源管理器（RM）、事务管理器（TM）: 应用程序：定义了事务的边界，指定需要做哪些事务； 资源管理器：提供了访问事务的方法，通常一个数据库就是一个资源管理器； 事务管理器：协调参与了全局事务中的各个事务。 分布式事务采用两段式提交（two-phase commit）的方式： 第一阶段所有的事务节点开始准备，告诉事务管理器ready。 第二阶段事务管理器告诉每个节点是commit还是rollback。如果有一个节点失败，就需要全局的节点全部rollback，以此保障事务的原子性。 MySQL中事务的语法我们说事务的本质其实只是一系列数据库操作，只不过这些数据库操作符合ACID特性而已，那么MySQL中如何将某些操作放到一个事务里去执行的呢？我们下边就来重点唠叨唠叨。 开启事务我们可以使用下边两种语句之一来开启一个事务： BEGIN [WORK]; BEGIN语句代表开启一个事务，后边的单词WORK可有可无。开启事务后，就可以继续写若干条语句，这些语句都属于刚刚开启的这个事务。 1234mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; 加入事务的语句... START TRANSACTION; START TRANSACTION语句和BEGIN语句有着相同的功效，都标志着开启一个事务，比如这样： 1234mysql&gt; START TRANSACTION;Query OK, 0 rows affected (0.00 sec)mysql&gt; 加入事务的语句... 不过比BEGIN语句牛逼一点儿的是，可以在START TRANSACTION语句后边跟随几个修饰符，就是它们几个： READ ONLY：标识当前事务是一个只读事务，也就是属于该事务的数据库操作只能读取数据，而不能修改数据。 READ WRITE：标识当前事务是一个读写事务，也就是属于该事务的数据库操作既可以读取数据，也可以修改数据。 WITH CONSISTENT SNAPSHOT：启动一致性读（先不用关心啥是个一致性读，后边的章节才会唠叨）。 比如我们想开启一个只读事务的话，直接把READ ONLY这个修饰符加在START TRANSACTION语句后边就好，比如这样： 1START TRANSACTION READ ONLY; 如果我们想在START TRANSACTION后边跟随多个修饰符的话，可以使用逗号将修饰符分开，比如开启一个只读事务和一致性读，就可以这样写： 1START TRANSACTION READ ONLY, WITH CONSISTENT SNAPSHOT; 或者开启一个读写事务和一致性读，就可以这样写： 1START TRANSACTION READ WRITE, WITH CONSISTENT SNAPSHOT 不过这里需要大家注意的一点是，READ ONLY和READ WRITE是用来设置所谓的事务访问模式的，就是以只读还是读写的方式来访问数据库中的数据，一个事务的访问模式不能同时既设置为只读的也设置为读写的，所以我们不能同时把READ ONLY和READ WRITE放到START TRANSACTION语句后边。另外，如果我们不显式指定事务的访问模式，那么该事务的访问模式就是读写模式。 提交事务开启事务之后就可以继续写需要放到该事务中的语句了，当最后一条语句写完了之后，我们就可以提交该事务了，提交的语句也很简单： 1COMMIT [WORK] COMMIT语句就代表提交一个事务，后边的WORK可有可无。比如我们上边说狗哥给猫爷转10元钱其实对应MySQL中的两条语句，我们就可以把这两条语句放到一个事务中，完整的过程就是这样： 12345678910111213mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; UPDATE account SET balance = balance - 10 WHERE id = 1;Query OK, 1 row affected (0.02 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; UPDATE account SET balance = balance + 10 WHERE id = 2;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; COMMIT;Query OK, 0 rows affected (0.00 sec) 手动中止事务如果我们写了几条语句之后发现上边的某条语句写错了，我们可以手动的使用下边这个语句来将数据库恢复到事务执行之前的样子： 1ROLLBACK [WORK] ROLLBACK语句就代表中止并回滚一个事务，后边的WORK可有可无类似的。比如我们在写狗哥给猫爷转账10元钱对应的MySQL语句时，先给狗哥扣了10元，然后一时大意只给猫爷账户上增加了1元，此时就可以使用ROLLBACK语句进行回滚，完整的过程就是这样： 12345678910111213mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; UPDATE account SET balance = balance - 10 WHERE id = 1;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; UPDATE account SET balance = balance + 1 WHERE id = 2;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; ROLLBACK;Query OK, 0 rows affected (0.00 sec) 这里需要强调一下，ROLLBACK语句是我们程序员手动的去回滚事务时才去使用的，如果事务在执行过程中遇到了某些错误而无法继续执行的话，事务自身会自动的回滚。 123小贴士：我们这里所说的开启、提交、中止事务的语法只是针对使用黑框框时通过mysql客户端程序与服务器进行交互时控制事务的语法，如果大家使用的是别的客户端程序，比如JDBC之类的，那需要参考相应的文档来看看如何控制事务。 支持事务的存储引擎MySQL中并不是所有存储引擎都支持事务的功能，目前只有InnoDB和NDB存储引擎支持（NDB存储引擎不是我们的重点），如果某个事务中包含了修改使用不支持事务的存储引擎的表，那么对该使用不支持事务的存储引擎的表所做的修改将无法进行回滚。比方说我们有两个表，tbl1使用支持事务的存储引擎InnoDB，tbl2使用不支持事务的存储引擎MyISAM，它们的建表语句如下所示： 1234567CREATE TABLE tbl1 ( i int) engine=InnoDB;CREATE TABLE tbl2 ( i int) ENGINE=MyISAM; 我们看看先开启一个事务，写一条插入语句后再回滚该事务，tbl1和tbl2的表现有什么不同： 1234567891011121314mysql&gt; SELECT * FROM tbl1;Empty set (0.00 sec)mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; INSERT INTO tbl1 VALUES(1);Query OK, 1 row affected (0.00 sec)mysql&gt; ROLLBACK;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM tbl1;Empty set (0.00 sec) 可以看到，对于使用支持事务的存储引擎的tbl1表来说，我们在插入一条记录再回滚后，tbl1就恢复到没有插入记录时的状态了。再看看tbl2表的表现： 12345678910111213141516171819mysql&gt; SELECT * FROM tbl2;Empty set (0.00 sec)mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; INSERT INTO tbl2 VALUES(1);Query OK, 1 row affected (0.00 sec)mysql&gt; ROLLBACK;Query OK, 0 rows affected, 1 warning (0.01 sec)mysql&gt; SELECT * FROM tbl2;+------+| i |+------+| 1 |+------+1 row in set (0.00 sec) 可以看到，虽然我们使用了ROLLBACK语句来回滚事务，但是插入的那条记录还是留在了tbl2表中。 自动提交MySQL中有一个系统变量autocommit： 1234567mysql&gt; SHOW VARIABLES LIKE 'autocommit';+---------------+-------+| Variable_name | Value |+---------------+-------+| autocommit | ON |+---------------+-------+1 row in set (0.01 sec) 可以看到它的默认值为ON，也就是说默认情况下，如果我们不显式的使用START TRANSACTION或者BEGIN语句开启一个事务，那么每一条语句都算是一个独立的事务，这种特性称之为事务的自动提交。假如我们在狗哥向猫爷转账10元时不以START TRANSACTION或者BEGIN语句显式的开启一个事务，那么下边这两条语句就相当于放到两个独立的事务中去执行： 12UPDATE account SET balance = balance - 10 WHERE id = 1;UPDATE account SET balance = balance + 10 WHERE id = 2; 当然，如果我们想关闭这种自动提交的功能，可以使用下边两种方法之一： 显式的的使用START TRANSACTION或者BEGIN语句开启一个事务。 这样在本次事务提交或者回滚前会暂时关闭掉自动提交的功能。 把系统变量autocommit的值设置为OFF，就像这样： 1SET autocommit = OFF; 这样的话，我们写入的多条语句就算是属于同一个事务了，直到我们显式的写出COMMIT语句来把这个事务提交掉，或者显式的写出ROLLBACK语句来把这个事务回滚掉。 隐式提交当我们使用START TRANSACTION或者BEGIN语句开启了一个事务，或者把系统变量autocommit的值设置为OFF时，事务就不会进行自动提交，但是如果我们输入了某些语句之后就会悄悄的提交掉，就像我们输入了COMMIT语句了一样，这种因为某些特殊的语句而导致事务提交的情况称为隐式提交，这些会导致事务隐式提交的语句包括： 定义或修改数据库对象的数据定义语言（Data definition language，缩写为：DDL）。 所谓的数据库对象，指的就是数据库、表、视图、存储过程等等这些东西。当我们使用CREATE、ALTER、DELETE等语句去修改这些所谓的数据库对象时，就会隐式的提交前边语句所属于的事务，就像这样： 1234567BEGIN;SELECT ... # 事务中的一条语句UPDATE ... # 事务中的一条语句... # 事务中的其它语句CREATE TABLE ... # 此语句会隐式的提交前边语句所属于的事务 隐式使用或修改mysql数据库中的表 当我们使用ALTER USER、CREATE USER、DROP USER、GRANT、RENAME USER、REVOKE、SET PASSWORD等语句时也会隐式的提交前边语句所属于的事务。 事务控制或关于锁定的语句 当我们在一个事务还没提交或者回滚时就又使用START TRANSACTION或者BEGIN语句开启了另一个事务时，会隐式的提交上一个事务，比如这样： 1234567BEGIN;SELECT ... # 事务中的一条语句UPDATE ... # 事务中的一条语句... # 事务中的其它语句BEGIN; # 此语句会隐式的提交前边语句所属于的事务 或者当前的autocommit系统变量的值为OFF，我们手动把它调为ON时，也会隐式的提交前边语句所属的事务。 或者使用LOCK TABLES、UNLOCK TABLES等关于锁定的语句也会隐式的提交前边语句所属的事务。 加载数据的语句 比如我们使用LOAD DATA语句来批量往数据库中导入数据时，也会隐式的提交前边语句所属的事务。 关于MySQL复制的一些语句 使用START SLAVE、STOP SLAVE、RESET SLAVE、CHANGE MASTER TO等语句时也会隐式的提交前边语句所属的事务。 其它的一些语句 使用ANALYZE TABLE、CACHE INDEX、CHECK TABLE、FLUSH、 LOAD INDEX INTO CACHE、OPTIMIZE TABLE、REPAIR TABLE、RESET等语句也会隐式的提交前边语句所属的事务。 123小贴士：上边提到的一些语句，如果你都认识并且知道是干嘛用的那再好不过了，不认识也不要气馁，这里写出来只是为了内容的完整性，把可能会导致事务隐式提交的情况都列举一下，具体每个语句都是干嘛用的等我们遇到了再说哈。 保存点如果你开启了一个事务，并且已经敲了很多语句，忽然发现上一条语句有点问题，你只好使用ROLLBACK语句来让数据库状态恢复到事务执行之前的样子，然后一切从头再来，总有一种一夜回到解放前的感觉。所以设计数据库的大叔们提出了一个保存点（英文：savepoint）的概念，就是在事务对应的数据库语句中打几个点，我们在调用ROLLBACK语句时可以指定会滚到哪个点，而不是回到最初的原点。定义保存点的语法如下： 1SAVEPOINT 保存点名称; 当我们想回滚到某个保存点时，可以使用下边这个语句（下边语句中的单词WORK和SAVEPOINT是可有可无的）： 1ROLLBACK [WORK] TO [SAVEPOINT] 保存点名称; 不过如果ROLLBACK语句后边不跟随保存点名称的话，会直接回滚到事务执行之前的状态。 如果我们想删除某个保存点，可以使用这个语句： 1RELEASE SAVEPOINT 保存点名称; 下边还是以狗哥向猫爷转账10元的例子展示一下保存点的用法，在执行完扣除狗哥账户的钱10元的语句之后打一个保存点： 12345678910111213141516171819202122232425262728293031323334353637383940414243mysql&gt; SELECT * FROM account;+----+--------+---------+| id | name | balance |+----+--------+---------+| 1 | 狗哥 | 11 || 2 | 猫爷 | 2 |+----+--------+---------+2 rows in set (0.00 sec)mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; UPDATE account SET balance = balance - 10 WHERE id = 1;Query OK, 1 row affected (0.01 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; SAVEPOINT s1; # 一个保存点Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM account;+----+--------+---------+| id | name | balance |+----+--------+---------+| 1 | 狗哥 | 1 || 2 | 猫爷 | 2 |+----+--------+---------+2 rows in set (0.00 sec)mysql&gt; UPDATE account SET balance = balance + 1 WHERE id = 2; # 更新错了Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; ROLLBACK TO s1; # 回滚到保存点s1处Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM account;+----+--------+---------+| id | name | balance |+----+--------+---------+| 1 | 狗哥 | 1 || 2 | 猫爷 | 2 |+----+--------+---------+2 rows in set (0.00 sec) 事务的隔离级别MVCC原理版本链对于使用 InnoDB 存储引擎的表来说，它的聚簇索引记录中都包含两个必要的隐藏列 row_id 并不是必要的，我们创建的表中有主键或者非NULL的UNIQUE键时都不会包含 row_id 列 trx_id ：每次一个事务对某条聚簇索引记录进行改动时，都会把该事务的 事务id 赋值给 trx_id 隐藏列。 roll_pointer ：每次对某条聚簇索引记录进行改动时，都会把旧的版本写入到 undo日志 中，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息。 实际上insert undo只在事务回滚时起作用，当事务提交后，该类型的undo日志就没用了，它占用的Undo Log Segment也会被系统回收（也就是该undo日志占用的Undo页面链表要么被重用，要么被释放）。 虽然真正的insert undo日志占用的存储空间被释放了，但是roll_pointer的值并不会被清除，roll_pointer属性占用7个字节，第一个比特位就标记着它指向的undo日志的类型，如果该比特位的值为1时，就代表着它zhi向的undo日志类型为insert undo。所以我们之后在画图时都会把insert undo给去掉，大家留意一下就好了 假设现在有两个事务id 分别为 100 、 200 的事务对这条记录进行 UPDATE 操作，操作流程如下： 小贴士： 能不能在两个事务中交叉更新同一条记录呢？哈哈，这不就是一个事务修改了另一个未提交事务修改过 的数据，沦为了脏写了么？InnoDB使用锁来保证不会有脏写情况的发生，也就是在第一个事务更新了某 条记录后，就会给这条记录加锁，另一个事务再次更新时就需要等待第一个事务提交了，把锁释放之后 才可以继续更新。关于锁的更多细节我们后续的文章中再唠叨哈 每次对记录进行改动，都会记录一条 undo日志 ，每条 undo日志也都有一个 roll_pointer 属性（ INSERT 操作对应的 undo日志 没有该属性，因为该记录并没有更早的版本），可以将这些 undo日志 都连起来，串成一个链表，所以现在的情况就像下图一样： 对该记录每次更新后，都会将旧值放到一条 undo日志 中，就算是该记录的一个旧版本，随着更新次数的增多，所有的版本都会被 roll_pointer 属性连接成一个链表，我们把这个链表称之为 版本链 ，版本链的头节点就是当前记录最新的值。另外，每个版本中还包含生成该版本时对应的 事务id ，这个信息很重要，我们稍后就会用到。 ReadView对于使用 READ COMMITTED 和 REPEATABLE READ 隔离级别的事务来说，都必须保证读到已经提交了的事务修改过的记录，也就是说假如另一个事务已经修改了记录但是尚未提交，是不能直接读取最新版本的记录的， 核心问题就是：需要判断一下版本链中的哪个版本是当前事务可见的。为此，提出了一个 ReadView 的概念，这个 ReadView 中主要包含4个比较重要的内容: m_ids ：表示在生成 ReadView 时当前系统中活跃的读写事务的 事务id 列表。 min_trx_id ：表示在生成 ReadView 时当前系统中活跃的读写事务中最小的 事务id ，也就是 m_ids 中的最小值。 max_trx_id ：表示生成 ReadView 时系统中应该分配给下一个事务的 id 值。 creator_trx_id ：表示生成该 ReadView 的事务的 事务id 。 注意max_trx_id并不是m_ids中的最大值，事务id是递增分配的。 比方说现在有id为1，2，3这三个事务，之后id为3的事务提交了。那么一个新的读事务在生成ReadView时， m_ids就包括1和2，min_trx_id的值就是1，max_trx_id的值就是4。 有了这个 ReadView ，这样在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见： 如果被访问版本的 trx_id 属性值与 ReadView 中的 creator_trx_id 值相同，意味着当前事务在访问它自己修改过的记录，所以该版本可以被当前事务访问。 如果被访问版本的 trx_id 属性值小于 ReadView 中的 min_trx_id 值，表明生成该版本的事务在当前事务生成 ReadView 前已经提交，所以该版本可以被当前事务访问。 如果被访问版本的 trx_id 属性值大于 ReadView 中的 max_trx_id 值，表明生成该版本的事务在当前事务生成 ReadView 后才开启，所以该版本不可以被当前事务访问。 如果被访问版本的 trx_id 属性值在 ReadView 的 min_trx_id 和 max_trx_id 之间，那就需要判断一下trx_id 属性值是不是在 m_ids 列表中，如果在，说明创建 ReadView 时生成该版本的事务还是活跃的，该版本不可以被访问；如果不在，说明创建 ReadView 时生成该版本的事务已经被提交，该版本可以被访问 如果某个版本的数据对当前事务不可见的话，那就顺着版本链找到下一个版本的数据，继续按照上边的步骤判断可见性，依此类推，直到版本链中的最后一个版本。如果最后一个版本也不可见的话，那么就意味着该条记录对该事务完全不可见，查询结果就不包含该记录 读已提交和可重复读最大的区别就是ReadView生成的时机不同： 读已提交：每次读取数据前都生成一个ReadView 可重复读：在第一次读取数据时生成一个ReadView 读已提交比方说现在系统里有两个 事务id 分别为 100 、 200 的事务在执行： 123456789# Transaction 100BEGIN;UPDATE hero SET name = '关羽' WHERE number = 1;UPDATE hero SET name = '张飞' WHERE number = 1;# Transaction 200BEGIN;# 更新了一些别的表的记录... 再次强调一遍，事务执行过程中，只有在第一次真正修改记录时（比如使用INSERT、DELETE、UPDATE语句），才会被分配一个单独的事务id，这个事务id是递增的。所以我们才在Transaction 200中更新一些别的表的记录，目的是让它分配事务id。 此时版本链如下： 假设现在有一个使用 READ COMMITTED 隔离级别的事务开始执行： 12345# 使用READ COMMITTED隔离级别的事务BEGIN;# SELECT1：Transaction 100、200未提交SELECT * FROM hero WHERE number = 1; # 得到的列name的值为'刘备' 这个 SELECT1 的执行过程如下： 在执行 SELECT 语句时会先生成一个 ReadView ， ReadView 的 m_ids 列表的内容就是 [100, 200] ，min_trx_id 为 100 ， max_trx_id 为 201 ， creator_trx_id 为 0 。 然后从版本链中挑选可见的记录，从图中可以看出，最新版本的列 name 的内容是 ‘张飞’ ，该版本的trx_id 值为 100 ，在 m_ids 列表内，所以不符合可见性要求，根据 roll_pointer 跳到下一个版本。 下一个版本的列 name 的内容是 ‘关羽’ ，该版本的 trx_id 值也为 100 ，也在 m_ids 列表内，所以也不符合要求，继续跳到下一个版本。 下一个版本的列 name 的内容是 ‘刘备’ ，该版本的 trx_id 值为 80 ，小于 ReadView 中的 min_trx_id 值100 ，所以这个版本是符合要求的，最后返回给用户的版本就是这条列 name 为 ‘刘备’ 的记录。 之后，我们把 事务id 为 100 的事务提交一下，事务200中修改了一些记录，就像这样： 123456789101112# Transaction 100BEGIN;UPDATE hero SET name = '关羽' WHERE number = 1;UPDATE hero SET name = '张飞' WHERE number = 1;COMMIT;# Transaction 200BEGIN;# 更新了一些别的表的记录...UPDATE hero SET name = '赵云' WHERE number = 1;UPDATE hero SET name = '诸葛亮' WHERE number = 1; 然后再到刚才使用 READ COMMITTED 隔离级别的事务中继续查找这个 number 为 1 的记录，如下： 1234567891011\\# 使用READ COMMITTED隔离级别的事务BEGIN;\\# SELECT1：Transaction 100、200均未提交SELECT * FROM hero WHERE number = 1; # 得到的列name的值为'刘备'\\# SELECT2：Transaction 100提交，Transaction 200未提交SELECT * FROM hero WHERE number = 1; # 得到的列name的值为'张飞' 这个 SELECT2 的执行过程如下： 在执行 SELECT 语句时会又会单独生成一个 ReadView ，该 ReadView 的 m_ids 列表的内容就是 [200] （ 事务id 为 100 的那个事务已经提交了，所以再次生成快照时就没有它了）， min_trx_id 为 200 ，max_trx_id 为 201 ， creator_trx_id 为 0 。 然后从版本链中挑选可见的记录，从图中可以看出，最新版本的列 name 的内容是 ‘诸葛亮’ ，该版本的trx_id 值为 200 ，在 m_ids 列表内，所以不符合可见性要求，根据 roll_pointer 跳到下一个版本。 下一个版本的列 name 的内容是 ‘赵云’ ，该版本的 trx_id 值为 200 ，也在 m_ids 列表内，所以也不符合要求，继续跳到下一个版本。 下一个版本的列 name 的内容是 ‘张飞’ ，该版本的 trx_id 值为 100 ，小于 ReadView 中的 min_trx_id 值200 ，所以这个版本是符合要求的，最后返回给用户的版本就是这条列 name 为 ‘张飞’ 的记录。 以此类推，如果之后 事务id 为 200 的记录也提交了，再此在使用 READ COMMITTED 隔离级别的事务中查询表hero 中 number 值为 1 的记录时，得到的结果就是 ‘诸葛亮’ 了，具体流程我们就不分析了。 总结一下就是用READ COMMITTED隔离级别的事务在每次查询开始时都会生成一个独立的ReadView。 可重复读比方说现在系统里有两个 事务id 分别为 100 、 200 的事务在执行： 1234567# Transaction 100BEGIN;UPDATE hero SET name = '关羽' WHERE number = 1;UPDATE hero SET name = '张飞' WHERE number = 1;# Transaction 200BEGIN;# 更新了一些别的表的记录 此刻，表 hero 中 number 为 1 的记录得到的版本链表如下所示： 假设现在有一个使用 REPEATABLE READ 隔离级别的事务开始执行： 1234# 使用REPEATABLE READ隔离级别的事务BEGIN;# SELECT1：Transaction 100、200未提交SELECT * FROM hero WHERE number = 1; # 得到的列name的值为'刘备' 这个 SELECT1 的执行过程如下： 在执行 SELECT 语句时会先生成一个 ReadView ， ReadView 的 m_ids 列表的内容就是 [100, 200] ，min_trx_id 为 100 ， max_trx_id 为 201 ， creator_trx_id 为 0 。 然后从版本链中挑选可见的记录，从图中可以看出，最新版本的列 name 的内容是 ‘张飞’ ，该版本的trx_id 值为 100 ，在 m_ids 列表内，所以不符合可见性要求，根据 roll_pointer 跳到下一个版本。 下一个版本的列 name 的内容是 ‘关羽’ ，该版本的 trx_id 值也为 100 ，也在 m_ids 列表内，所以也不符合要求，继续跳到下一个版本。 下一个版本的列 name 的内容是 ‘刘备’ ，该版本的 trx_id 值为 80 ，小于 ReadView 中的 min_trx_id 值100 ，所以这个版本是符合要求的，最后返回给用户的版本就是这条列 name 为 ‘刘备’ 的记录。 之后，我们把 事务id 为 100 的事务提交一下，事务200修改一下记录，就像这样： 123456789101112# Transaction 100BEGIN;UPDATE hero SET name = '关羽' WHERE number = 1;UPDATE hero SET name = '张飞' WHERE number = 1;COMMIT;# Transaction 200BEGIN;# 更新了一些别的表的记录...UPDATE hero SET name = '赵云' WHERE number = 1;UPDATE hero SET name = '诸葛亮' WHERE number = 1; 此刻，表 hero 中 number 为 1 的记录的版本链就长这样： 然后再到刚才使用 REPEATABLE READ 隔离级别的事务中继续查找这个 number 为 1 的记录，如下： 1234567891011# 使用REPEATABLE READ隔离级别的事务BEGIN;# SELECT1：Transaction 100、200均未提交SELECT * FROM hero WHERE number = 1; # 得到的列name的值为'刘备'# SELECT2：Transaction 100提交，Transaction 200未提交SELECT * FROM hero WHERE number = 1; # 得到的列name的值仍为'刘备' 这个 SELECT2 的执行过程如下： 因为当前事务的隔离级别为 REPEATABLE READ ，而之前在执行 SELECT1 时已经生成过 ReadView 了，所以此时直接复用之前的 ReadView ，之前的 ReadView 的 m_ids 列表的内容就是 [100, 200] ， min_trx_id 为100 ， max_trx_id 为 201 ， creator_trx_id 为 0 。 然后从版本链中挑选可见的记录，从图中可以看出，最新版本的列 name 的内容是 ‘诸葛亮’ ，该版本的trx_id 值为 200 ，在 m_ids 列表内，所以不符合可见性要求，根据 roll_pointer 跳到下一个版本。 下一个版本的列 name 的内容是 ‘赵云’ ，该版本的 trx_id 值为 200 ，也在 m_ids 列表内，所以也不符合要求，继续跳到下一个版本。 下一个版本的列 name 的内容是 ‘张飞’ ，该版本的 trx_id 值为 100 ，而 m_ids 列表中是包含值为 100 的事务id 的，所以该版本也不符合要求，同理下一个列 name 的内容是 ‘关羽’ 的版本也不符合要求。继续跳到下一个版本。 下一个版本的列 name 的内容是 ‘刘备’ ，该版本的 trx_id 值为 80 ，小于 ReadView 中的 min_trx_id 值100 ，所以这个版本是符合要求的，最后返回给用户的版本就是这条列 c 为 ‘刘备’ 的记录。也就是说两次 SELECT 查询得到的结果是重复的，记录的列 c 值都是 ‘刘备’ ，这就是 可重复读 的含义。 如果我们之后再把 事务id 为 200 的记录提交了，然后再到刚才使用 REPEATABLE READ 隔离级别的事务中继续查找这个 number 为 1 的记录，得到的结果还是 ‘刘备’ ，具体执行过程大家可以自己分析一下。 MVCC小结 MVCC （Multi-Version Concurrency Control ，多版本并发控制）指的就是在使用 READ COMMITTD 、 REPEATABLE READ 这两种隔离级别的事务在执行普通的 SEELCT 操作时访问记录的版本链的过程，这样子可以使不同事务的 读-写 、 写-读 操作并发执行，从而提升系统性能。 READ COMMITTD 、REPEATABLE READ 这两个隔离级别的一个很大不同就是：生成ReadView的时机不同，READ COMMITTD在每一次进行普通SELECT操作前都会生成一个ReadView，而REPEATABLE READ只在第一次进行普通SELECT操作前生成一个ReadView，之后的查询操作都重复使用这个ReadView就好了 MySQL中的锁锁的分类从对数据操作的粒度分类：为了尽可能提高数据库的并发度，每次锁定的数据范围越小越好，理论上每次只锁定当前操作的数据的方案会得到最大的并发度，但是管理锁是很耗资源的事情（涉及获取，检查，释放锁等动作），因此数据库系统需要在高并发响应和系统性能两方面进行平衡，这样就产生了“锁粒度（Lock granularity）”的概念。 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低（MyISAM 和 MEMORY 存储引擎采用的是表级锁）； 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高（InnoDB 存储引擎既支持行级锁也支持表级锁，但默认情况下是采用行级锁）； 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 适用：从锁的角度来说，表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如Web应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用，如一些在线事务处理（OLTP）系统。 从对数据操作的类型分类： 读锁（共享锁）：针对同一份数据，多个读操作可以同时进行，不会互相影响 写锁（排他锁）：当前写操作没有完成前，它会阻断其他写锁和读锁 1234567891011121314151617181920mysql&gt; lock table dept read; //加上读锁Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from dept;+--------+----------+| deptno | deptname |+--------+----------+| 1 | tech || 2 | sale || 5 | fin || 3 | hr |+--------+----------+4 rows in set (0.00 sec)mysql&gt; update dept set deptno = 6 where deptname = 'tech'; //不能修改了ERROR 1099 (HY000): Table 'dept' was locked with a READ lock and can't be updatedmysql&gt; unlock table; //解锁Query OK, 0 rows affected (0.00 sec) MyISAM 的表锁有两种模式： 表共享读锁 （Table Read Lock）：不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求； 表独占写锁 （Table Write Lock）：会阻塞其他用户对同一表的读和写操作； MyISAM 表的读操作与写操作之间，以及写操作之间是串行的。当一个线程获得对一个表的写锁后， 只有持有锁的线程可以对表进行更新操作。 其他线程的读、 写操作都会等待，直到锁被释放为止。 默认情况下，写锁比读锁具有更高的优先级：当一个锁释放时，这个锁会优先给写锁队列中等候的获取锁请求，然后再给读锁队列中等候的获取锁请求。 InnoDB 实现了以下两种类型的行锁： 共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。 排他锁（X）：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。 为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB 还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁： 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的 IS 锁。 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的 IX 锁。 索引失效会导致行锁变表锁。比如 vchar 查询不写单引号的情况。 解释一下悲观锁和乐观锁乐观锁与悲观锁是两种并发控制的思想，可用于解决丢失更新问题 乐观锁会“乐观地”假定大概率不会发生并发更新冲突，访问、处理数据过程中不加锁，只在更新数据时再根据版本号或时间戳判断是否有冲突，有则处理，无则提交事务。用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式 乐观锁的版本号机制在表中设计一个版本字段 version，第一次读的时候，会获取 version 字段的取值。然后对数据进行更新或删除操作时，会执行UPDATE ... SET version=version+1 WHERE version=version。此时如果已经有事务对这条数据进行了更改，修改就不会成功。 这种方式类似我们熟悉的 SVN、CVS 版本管理系统，当我们修改了代码进行提交时，首先会检查当前版本号与服务器上的版本号是否一致，如果一致就可以直接提交，如果不一致就需要更新服务器上的最新代码，然后再进行提交。 悲观锁会“悲观地”假定大概率会发生并发更新冲突，访问、处理数据前就加排他锁，在整个数据处理过程中锁定数据，事务提交或回滚后才释放锁。另外与乐观锁相对应的，悲观锁是由数据库自己实现了的，要用的时候，我们直接调用数据库的相关语句就可以了。 从这两种锁的设计思想中，你能看出乐观锁和悲观锁的适用场景： 乐观锁适合读操作多的场景，相对来说写的操作比较少。它的优点在于程序实现，不存在死锁问题，不过适用场景也会相对乐观，因为它阻止不了除了程序以外的数据库操作。 悲观锁适合写操作多的场景，因为写的操作具有排它性。采用悲观锁的方式，可以在数据库层面阻止其他事务对该数据的操作权限，防止读 - 写和写 - 写的冲突。 InnoDB锁的模式 InnoDB 三种行锁的方式 **记录锁(Record Locks)**： 单个行记录上的锁。对索引项加锁，锁定符合条件的行。其他事务不能修改和删除加锁项； 1SELECT * FROM table WHERE id = 1 FOR UPDATE; 它会在 id=1 的记录上加上记录锁，以阻止其他事务插入，更新，删除 id=1 这一行 在通过 主键索引 与 唯一索引 对数据行进行 UPDATE 操作时，也会对该行数据加记录锁： 1UPDATE SET age = 50 WHERE id = 1; 间隙锁（Gap Locks）： 当我们使用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁。对于键值在条件范围内但并不存在的记录，叫做“间隙”。 InnoDB 也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁。 对索引项之间的“间隙”加锁，锁定记录的范围（对第一条记录前的间隙或最后一条将记录后的间隙加锁），不包含索引项本身。其他事务不能在锁范围内插入数据，这样就防止了别的事务新增幻影行。 间隙锁基于非唯一索引，它锁定一段范围内的索引记录。间隙锁基于下面将会提到的Next-Key Locking 算法，请务必牢记：使用间隙锁锁住的是一个区间，而不仅仅是这个区间中的每一条数据。 1SELECT * FROM table WHERE id BETWEN 1 AND 10 FOR UPDATE; 即所有在（1，10）区间内的记录行都会被锁住，所有id 为 2、3、4、5、6、7、8、9 的数据行的插入会被阻塞，但是 1 和 10 两条记录行并不会被锁住。 GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况 临键锁(Next-key Locks)： 临键锁，是记录锁与间隙锁的组合，它的封锁范围，既包含索引记录，又包含索引区间。(临键锁的主要目的，也是为了避免幻读(Phantom Read)。如果把事务的隔离级别降级为RC，临键锁则也会失效。) Next-Key 可以理解为一种特殊的间隙锁，也可以理解为一种特殊的算法。通过临建锁可以解决幻读的问题。 每个数据行上的非唯一索引列上都会存在一把临键锁，当某个事务持有该数据行的临键锁时，会锁住一段左开右闭区间的数据。需要强调的一点是，InnoDB 中行级锁是基于索引实现的，临键锁只与非唯一索引列有关，在唯一索引列（包括主键列）上不存在临键锁。 对于行的查询，都是采用该方法，主要目的是解决幻读的问题 死锁该如何解决死锁产生： 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环 当事务试图以不同的顺序锁定资源时，就可能产生死锁。多个事务同时锁定同一个资源时也可能会产生死锁 锁的行为和顺序和存储引擎相关。以同样的顺序执行语句，有些存储引擎会产生死锁有些不会——死锁有双重原因：真正的数据冲突；存储引擎的实现方式。 检测死锁：数据库系统实现了各种死锁检测和死锁超时的机制。InnoDB存储引擎能检测到死锁的循环依赖并立即返回一个错误。 死锁恢复：死锁发生以后，只有部分或完全回滚其中一个事务，才能打破死锁，InnoDB目前处理死锁的方法是，将持有最少行级排他锁的事务进行回滚。所以事务型应用程序在设计时必须考虑如何处理死锁，多数情况下只需要重新执行因死锁回滚的事务即可。 外部锁的死锁检测：发生死锁后，InnoDB 一般都能自动检测到，并使一个事务释放锁并回退，另一个事务获得锁，继续完成事务。但在涉及外部锁，或涉及表锁的情况下，InnoDB 并不能完全自动检测到死锁， 这需要通过设置锁等待超时参数 innodb_lock_wait_timeout 来解决 死锁影响性能：死锁会影响性能而不是会产生严重错误，因为InnoDB会自动检测死锁状况并回滚其中一个受影响的事务。在高并发系统上，当许多线程等待同一个锁时，死锁检测可能导致速度变慢。 有时当发生死锁时，禁用死锁检测（使用innodb_deadlock_detect配置选项）可能会更有效，这时可以依赖innodb_lock_wait_timeout设置进行事务回滚。 MyISAM避免死锁： 在自动加锁的情况下，MyISAM 总是一次获得 SQL 语句所需要的全部锁，所以 MyISAM 表不会出现死锁。 InnoDB避免死锁： 为了在单个InnoDB表上执行多个并发写入操作时避免死锁，可以在事务开始时通过为预期要修改的每个元祖（行）使用SELECT ... FOR UPDATE语句来获取必要的锁，即使这些行的更改语句是在之后才执行的。 在事务中，如果要更新记录，应该直接申请足够级别的锁，即排他锁，而不应先申请共享锁、更新时再申请排他锁，因为这时候当用户再申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁 如果事务需要修改或锁定多个表，则应在每个事务中以相同的顺序使用加锁语句。 在应用中，如果不同的程序会并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会 通过SELECT ... LOCK IN SHARE MODE获取行的读锁后，如果当前事务再需要对该记录进行更新操作，则很有可能造成死锁。 改变事务隔离级别 如果出现死锁，可以用 show engine innodb status;命令来确定最后一个死锁产生的原因。返回结果中包括死锁相关事务的详细信息，如引发死锁的 SQL 语句，事务已经获得的锁，正在等待什么锁，以及被回滚的事务等。据此可以分析死锁产生的原因和改进措施。 我们都不希望出现死锁的情况，可以采取一些方法避免死锁的发生： 如果事务涉及多个表，操作比较复杂，那么可以尽量一次锁定所有的资源，而不是逐步来获取，这样可以减少死锁发生的概率； 如果事务需要更新数据表中的大部分数据，数据表又比较大，这时可以采用锁升级的方式，比如将行级锁升级为表级锁，从而减少死锁产生的概率； 不同事务并发读写多张数据表，可以约定访问表的顺序，采用相同的顺序降低死锁发生的概率。 总结","link":"/2021/10/08/MySQL%EF%BC%88%E5%85%AB%EF%BC%89%E4%BA%8B%E5%8A%A1/"},{"title":"Netty（八）优化和安全","text":"","link":"/2021/05/27/Netty%EF%BC%88%E5%85%AB%EF%BC%89%E4%BC%98%E5%8C%96%E5%92%8C%E5%AE%89%E5%85%A8/"},{"title":"MySQL（九）日志详解","text":"主要讲述了redo日志和undo日志 redo日志redo日志的来源前提： InnoDB存储引擎是以数据页为单位来管理存储空间的，我们进行的增删改查操作其实本质上都是在访问页面（包括读页面、写页面、创建新页面等操作）。 在真正访问页面之前，需要把在磁盘上的页缓存到内存中的Buffer Pool之后才可以访问 事务的持久性：对于一个已经提交的事务，在事务提交后即使系统发生了崩溃，这个事务对数据库中所做的更改也不能丢失。 基于上述三点：我们只是在内存中的Buffer Pool中修改了页面，这个时候机器故障了，内存中的数据都失效了，那么这个事务的提交就丢失了，如何解决这种呢？ 一个很简单的做法就是在事务提交完成之前把该事务所修改的所有页面都刷新到磁盘，但是这个简单粗暴的做法有些问题： 刷新一个完整的数据页太浪费了：可能只是改了数据页中的某一行记录而已。 随机IO刷起来比较慢：一个事务可能包含很多语句，即使是一条语句也可能修改许多页面，倒霉催的是该事务修改的这些页面可能并不相邻，这就意味着在将某个事务修改的Buffer Pool中的页面刷新到磁盘时，需要进行很多的随机IO，随机IO比顺序IO要慢，尤其对于传统的机械硬盘来说。 有没有更好的办法呢? 我们只是想让已经提交了的事务对数据库中数据所做的修改永久生效，即使后来系统崩溃，在重启后也能把这种修改恢复出来。所以我们其实没有必要在每次事务提交时就把该事务在内存中修改过的全部页面刷新到磁盘，只需要把修改了哪些东西记录一下就好，比方说某个事务将系统表空间中的第100号页面中偏移量为1000处的那个字节的值1改成2我们只需要记录一下： 将第0号表空间的100号页面的偏移量为1000处的值更新为2。 这样我们在事务提交时，把上述内容也就是日志刷新到磁盘中，即使之后系统崩溃了，重启之后只要按照上述内容所记录的步骤重新更新一下数据页，那么该事务对数据库中所做的修改又可以被恢复出来，也就意味着满足持久性的要求。因为在系统奔溃重启时需要按照上述内容所记录的步骤重新更新数据页，所以上述内容也被称之为重做日志，英文名为redo log， 这种事务提交时将尝试的redo日志刷新到磁盘的好处有： redo日志占用的空间非常小 redo日志是顺序写入磁盘的：在执行事务的过程中，每执行一条语句，就可能产生若干条redo日志，这些日志是按照产生的顺序写入磁盘的，也就是使用顺序IO。 redo日志格式通过上边的内容我们知道，redo日志本质上只是记录了一下事务对数据库做了哪些修改。 设计InnoDB的大叔们针对事务对数据库的不同修改场景定义了多种类型的redo日志，但是绝大部分类型的redo日志都有下边这种通用的结构： 各个部分的详细释义如下： type：该条redo日志的类型。在MySQL 5.7.21这个版本中，设计InnoDB的大叔一共为redo日志设计了53种不同的类型 space ID：表空间ID。 page number：页号。 data：该条redo日志的具体内容。 简单的redo日志类型我们前边介绍InnoDB的记录行格式的时候说过，如果我们没有为某个表显式的定义主键，并且表中也没有定义Unique键，那么InnoDB会自动的为表添加一个称之为row_id的隐藏列作为主键。为这个row_id隐藏列赋值的方式如下： 服务器会在内存中维护一个全局变量，每当向某个包含隐藏的row_id列的表中插入一条记录时，就会把该变量的值当作新记录的row_id列的值，并且把该变量自增1。 每当这个变量的值为256的倍数时，就会将该变量的值刷新到系统表空间的页号为7的页面中一个称之为Max Row ID的属性处（我们前边介绍表空间结构时详细说过）。 当系统启动时，会将上边提到的Max Row ID属性加载到内存中，将该值加上256之后赋值给我们前边提到的全局变量（因为在上次关机时该全局变量的值可能大于Max Row ID属性值）。 这个Max Row ID属性占用的存储空间是8个字节，当某个事务向某个包含row_id隐藏列的表插入一条记录，并且为该记录分配的row_id值为256的倍数时，就会向系统表空间页号为7的页面的相应偏移量处写入8个字节的值。但是我们要知道，这个写入实际上是在Buffer Pool中完成的，我们需要为这个页面的修改记录一条redo日志，以便在系统奔溃后能将已经提交的该事务对该页面所做的修改恢复出来。这种情况下对页面的修改是极其简单的，redo日志中只需要记录一下在某个页面的某个偏移量处修改了几个字节的值，具体被修改的内容是啥就好了，设计InnoDB的大叔把这种极其简单的redo日志称之为物理日志，并且根据在页面中写入数据的多少划分了几种不同的redo日志类型： MLOG_1BYTE（type字段对应的十进制数字为1）：表示在页面的某个偏移量处写入1个字节的redo日志类型。 MLOG_2BYTE（type字段对应的十进制数字为2）：表示在页面的某个偏移量处写入2个字节的redo日志类型。 MLOG_4BYTE（type字段对应的十进制数字为4）：表示在页面的某个偏移量处写入4个字节的redo日志类型。 MLOG_8BYTE（type字段对应的十进制数字为8）：表示在页面的某个偏移量处写入8个字节的redo日志类型。 MLOG_WRITE_STRING（type字段对应的十进制数字为30）：表示在页面的某个偏移量处写入一串数据。 我们上边提到的Max Row ID属性实际占用8个字节的存储空间，所以在修改页面中的该属性时，会记录一条类型为MLOG_8BYTE的redo日志，MLOG_8BYTE的redo日志结构如下所示： 其余MLOG_1BYTE、MLOG_2BYTE、MLOG_4BYTE类型的redo日志结构和MLOG_8BYTE的类似，只不过具体数据中包含对应个字节的数据罢了。MLOG_WRITE_STRING类型的redo日志表示写入一串数据，但是因为不能确定写入的具体数据占用多少字节，所以需要在日志结构中添加一个len字段： 123小贴士：只要将MLOG_WRITE_STRING类型的redo日志的len字段填充上1、2、4、8这些数字，就可以分别替代MLOG_1BYTE、MLOG_2BYTE、MLOG_4BYTE、MLOG_8BYTE这些类型的redo日志，为啥还要多此一举设计这么多类型呢？还不是因为省空间啊，能不写len字段就不写len字段，省一个字节算一个字节。 复杂一些的redo日志类型有时候执行一条语句会修改非常多的页面，包括系统数据页面和用户数据页面（用户数据指的就是聚簇索引和二级索引对应的B+树）。以一条INSERT语句为例，它除了要向B+树的页面中插入数据，也可能更新系统数据Max Row ID的值，不过对于我们用户来说，平时更关心的是语句对B+树所做更新： 表中包含多少个索引，一条INSERT语句就可能更新多少棵B+树。 针对某一棵B+树来说，既可能更新叶子节点页面，也可能更新内节点页面，也可能创建新的页面（在该记录插入的叶子节点的剩余空间比较少，不足以存放该记录时，会进行页面的分裂，在内节点页面中添加目录项记录）。 在语句执行过程中，INSERT语句对所有页面的修改都得保存到redo日志中去。这句话说的比较轻巧，做起来可就比较麻烦了，比方说将记录插入到聚簇索引中时，如果定位到的叶子节点的剩余空间足够存储该记录时，那么只更新该叶子节点页面就好，那么只记录一条MLOG_WRITE_STRING类型的redo日志，表明在页面的某个偏移量处增加了哪些数据就好了么？那就too young too naive了～ 别忘了一个数据页中除了存储实际的记录之后，还有什么File Header、Page Header、Page Directory等等部分（在唠叨数据页的章节有详细讲解），所以每往叶子节点代表的数据页里插入一条记录时，还有其他很多地方会跟着更新，比如说： 可能更新Page Directory中的槽信息。 Page Header中的各种页面统计信息，比如PAGE_N_DIR_SLOTS表示的槽数量可能会更改，PAGE_HEAP_TOP代表的还未使用的空间最小地址可能会更改，PAGE_N_HEAP代表的本页面中的记录数量可能会更改，吧啦吧啦，各种信息都可能会被修改。 我们知道在数据页里的记录是按照索引列从小到大的顺序组成一个单向链表的，每插入一条记录，还需要更新上一条记录的记录头信息中的next_record属性来维护这个单向链表。 还有别的吧啦吧啦的更新的地方，就不一一唠叨了… 画一个简易的示意图就像是这样： ![image_1d3gv4i7vtsirf81ikl1q2140n2g.png-67.2kB][4] 说了这么多，就是想表达：把一条记录插入到一个页面时需要更改的地方非常多。这时我们如果使用上边介绍的简单的物理redo日志来记录这些修改时，可以有两种解决方案： 方案一：在每个修改的地方都记录一条redo日志。 也就是如上图所示，有多少个加粗的块，就写多少条物理redo日志。这样子记录redo日志的缺点是显而易见的，因为被修改的地方是在太多了，可能记录的redo日志占用的空间都比整个页面占用的空间都多了～ 方案二：将整个页面的第一个被修改的字节到最后一个修改的字节之间所有的数据当成是一条物理redo日志中的具体数据。 从图中也可以看出来，第一个被修改的字节到最后一个修改的字节之间仍然有许多没有修改过的数据，我们把这些没有修改的数据也加入到redo日志中去岂不是太浪费了～ 正因为上述两种使用物理redo日志的方式来记录某个页面中做了哪些修改比较浪费，设计InnoDB的大叔本着勤俭节约的初心，提出了一些新的redo日志类型，比如： MLOG_REC_INSERT（对应的十进制数字为9）：表示插入一条使用非紧凑行格式的记录时的redo日志类型。 MLOG_COMP_REC_INSERT（对应的十进制数字为38）：表示插入一条使用紧凑行格式的记录时的redo日志类型。 123小贴士：Redundant是一种比较原始的行格式，它就是非紧凑的。而Compact、Dynamic以及Compressed行格式是较新的行格式，它们是紧凑的（占用更小的存储空间）。 MLOG_COMP_PAGE_CREATE（type字段对应的十进制数字为58）：表示创建一个存储紧凑行格式记录的页面的redo日志类型。 MLOG_COMP_REC_DELETE（type字段对应的十进制数字为42）：表示删除一条使用紧凑行格式记录的redo日志类型。 MLOG_COMP_LIST_START_DELETE（type字段对应的十进制数字为44）：表示从某条给定记录开始删除页面中的一系列使用紧凑行格式记录的redo日志类型。 MLOG_COMP_LIST_END_DELETE（type字段对应的十进制数字为43）：与MLOG_COMP_LIST_START_DELETE类型的redo日志呼应，表示删除一系列记录直到MLOG_COMP_LIST_END_DELETE类型的redo日志对应的记录为止。 123小贴士：我们前边唠叨InnoDB数据页格式的时候重点强调过，数据页中的记录是按照索引列大小的顺序组成单向链表的。有时候我们会有删除索引列的值在某个区间范围内的所有记录的需求，这时候如果我们每删除一条记录就写一条redo日志的话，效率可能有点低，所以提出MLOG_COMP_LIST_START_DELETE和MLOG_COMP_LIST_END_DELETE类型的redo日志，可以很大程度上减少redo日志的条数。 MLOG_ZIP_PAGE_COMPRESS（type字段对应的十进制数字为51）：表示压缩一个数据页的redo日志类型。 ······还有很多很多种类型，这就不列举了，等用到再说哈～ 这些类型的redo日志既包含物理层面的意思，也包含逻辑层面的意思，具体指： 物理层面看，这些日志都指明了对哪个表空间的哪个页进行了修改。 逻辑层面看，在系统奔溃重启时，并不能直接根据这些日志里的记载，将页面内的某个偏移量处恢复成某个数据，而是需要调用一些事先准备好的函数，执行完这些函数后才可以将页面恢复成系统奔溃前的样子。 大家看到这可能有些懵逼，我们还是以类型为MLOG_COMP_REC_INSERT这个代表插入一条使用紧凑行格式的记录时的redo日志为例来理解一下我们上边所说的物理层面和逻辑层面到底是个啥意思。废话少说，直接看一下这个类型为MLOG_COMP_REC_INSERT的redo日志的结构（由于字段太多了，我们把它们竖着看效果好些）： ![image_1d3bn8tsq1ssp1nmdks8kdr17e31t.png-85.7kB][5] 这个类型为MLOG_COMP_REC_INSERT的redo日志结构有几个地方需要大家注意： 我们前边在唠叨索引的时候说过，在一个数据页里，不论是叶子节点还是非叶子节点，记录都是按照索引列从小到大的顺序排序的。对于二级索引来说，当索引列的值相同时，记录还需要按照主键值进行排序。图中n_uniques的值的含义是在一条记录中，需要几个字段的值才能确保记录的唯一性，这样当插入一条记录时就可以按照记录的前n_uniques个字段进行排序。对于聚簇索引来说，n_uniques的值为主键的列数，对于其他二级索引来说，该值为索引列数+主键列数。这里需要注意的是，唯一二级索引的值可能为NULL，所以该值仍然为索引列数+主键列数。 field1_len ~ fieldn_len代表着该记录若干个字段占用存储空间的大小，需要注意的是，这里不管该字段的类型是固定长度大小的（比如INT），还是可变长度大小（比如VARCHAR(M)）的，该字段占用的大小始终要写入redo日志中。 offset代表的是该记录的前一条记录在页面中的地址。为啥要记录前一条记录的地址呢？这是因为每向数据页插入一条记录，都需要修改该页面中维护的记录链表，每条记录的记录头信息中都包含一个称为next_record的属性，所以在插入新记录时，需要修改前一条记录的next_record属性。 我们知道一条记录其实由额外信息和真实数据这两部分组成，这两个部分的总大小就是一条记录占用存储空间的总大小。通过end_seg_len的值可以间接的计算出一条记录占用存储空间的总大小，为啥不直接存储一条记录占用存储空间的总大小呢？这是因为写redo日志是一个非常频繁的操作，设计InnoDB的大叔想方设法想减小redo日志本身占用的存储空间大小，所以想了一些弯弯绕的算法来实现这个目标，end_seg_len这个字段就是为了节省redo日志存储空间而提出来的。至于具体设计InnoDB的大叔到底是用了什么神奇魔法减小redo日志大小的，我们这就不多唠叨了，因为的确有那么一丢丢小复杂，说清楚还是有一点点麻烦的，而且说明白了也没啥用。 mismatch_index的值也是为了节省redo日志的大小而设立的，大家可以忽略。 很显然这个类型为MLOG_COMP_REC_INSERT的redo日志并没有记录PAGE_N_DIR_SLOTS的值修改为了啥，PAGE_HEAP_TOP的值修改为了啥，PAGE_N_HEAP的值修改为了啥等等这些信息，而只是把在本页面中插入一条记录所有必备的要素记了下来，之后系统奔溃重启时，服务器会调用相关向某个页面插入一条记录的那个函数，而redo日志中的那些数据就可以被当成是调用这个函数所需的参数，在调用完该函数后，页面中的PAGE_N_DIR_SLOTS、PAGE_HEAP_TOP、PAGE_N_HEAP等等的值也就都被恢复到系统奔溃前的样子了。这就是所谓的逻辑日志的意思。 redo日志格式小结虽然上边说了一大堆关于redo日志格式的内容，但是如果你不是为了写一个解析redo日志的工具或者自己开发一套redo日志系统的话，那就没必要把InnoDB中的各种类型的redo日志格式都研究的透透的，没那个必要。上边我只是象征性的介绍了几种类型的redo日志格式，目的还是想让大家明白：redo日志会把事务在执行过程中对数据库所做的所有修改都记录下来，在之后系统奔溃重启后可以把事务所做的任何修改都恢复出来。 123小贴士：为了节省redo日志占用的存储空间大小，设计InnoDB的大叔对redo日志中的某些数据还可能进行压缩处理，比方说spacd ID和page number一般占用4个字节来存储，但是经过压缩后，可能使用更小的空间来存储。具体压缩算法就不唠叨了。 Mini-Transaction以组的形式写入redo日志语句在执行过程中可能修改若干个页面。比如我们前边说的一条INSERT语句可能修改系统表空间页号为7的页面的Max Row ID属性（当然也可能更新别的系统页面，只不过我们没有都列举出来而已），还会更新聚簇索引和二级索引对应B+树中的页面。由于对这些页面的更改都发生在Buffer Pool中，所以在修改完页面之后，需要记录一下相应的redo日志。在执行语句的过程中产生的redo日志被设计InnoDB的大叔人为的划分成了若干个不可分割的组，比如： 更新Max Row ID属性时产生的redo日志是不可分割的。 向聚簇索引对应B+树的页面中插入一条记录时产生的redo日志是不可分割的。 向某个二级索引对应B+树的页面中插入一条记录时产生的redo日志是不可分割的。 还有其他的一些对页面的访问操作时产生的redo日志是不可分割的。。。 怎么理解这个不可分割的意思呢？我们以向某个索引对应的B+树插入一条记录为例，在向B+树中插入这条记录之前，需要先定位到这条记录应该被插入到哪个叶子节点代表的数据页中，定位到具体的数据页之后，有两种可能的情况： 情况一：该数据页的剩余的空闲空间充足，足够容纳这一条待插入记录，那么事情很简单，直接把记录插入到这个数据页中，记录一条类型为MLOG_COMP_REC_INSERT的redo日志就好了，我们把这种情况称之为乐观插入。假如某个索引对应的B+树长这样： ![image_1d4fc7b6b1ftt16ji11as4a63h23.png-30.8kB][6] 现在我们要插入一条键值为10的记录，很显然需要被插入到页b中，由于页b现在有足够的空间容纳一条记录，所以直接将该记录插入到页b中就好了，就像这样： ![image_1d4fcbg9e1m1b1qtj1emgphorrl2g.png-43.3kB][7] 情况二：该数据页剩余的空闲空间不足，那么事情就悲剧了，我们前边说过，遇到这种情况要进行所谓的页分裂操作，也就是新建一个叶子节点，然后把原先数据页中的一部分记录复制到这个新的数据页中，然后再把记录插入进去，把这个叶子节点插入到叶子节点链表中，最后还要在内节点中添加一条目录项记录指向这个新创建的页面。很显然，这个过程要对多个页面进行修改，也就意味着会产生多条redo日志，我们把这种情况称之为悲观插入。假如某个索引对应的B+树长这样： ![image_1d4fcomne1lpsp691hg2o416hh2t.png-44.5kB][8] 现在我们要插入一条键值为10的记录，很显然需要被插入到页b中，但是从图中也可以看出来，此时页b已经塞满了记录，没有更多的空闲空间来容纳这条新记录了，所以我们需要进行页面的分裂操作，就像这样： ![image_1d4fkn8gv1n7enuq23kt1n1uvk3n.png-96.9kB][9] 如果作为内节点的页a的剩余空闲空间也不足以容纳增加一条目录项记录，那需要继续做内节点页a的分裂操作，也就意味着会修改更多的页面，从而产生更多的redo日志。另外，对于悲观插入来说，由于需要新申请数据页，还需要改动一些系统页面，比方说要修改各种段、区的统计信息信息，各种链表的统计信息（比如什么FREE链表、FSP_FREE_FRAG链表吧啦吧啦我们在唠叨表空间那一章中介绍过的各种东东）等等等等，反正总共需要记录的redo日志有二、三十条。 123小贴士：其实不光是悲观插入一条记录会生成许多条redo日志，设计InnoDB的大叔为了其他的一些功能，在乐观插入时也可能产生多条redo日志（具体是为了什么功能我们就不多说了，要不篇幅就受不了了～）。 设计InnoDB的大叔们认为向某个索引对应的B+树中插入一条记录的这个过程必须是原子的，不能说插了一半之后就停止了。比方说在悲观插入过程中，新的页面已经分配好了，数据也复制过去了，新的记录也插入到页面中了，可是没有向内节点中插入一条目录项记录，这个插入过程就是不完整的，这样会形成一棵不正确的B+树。我们知道redo日志是为了在系统奔溃重启时恢复崩溃前的状态，如果在悲观插入的过程中只记录了一部分redo日志，那么在系统奔溃重启时会将索引对应的B+树恢复成一种不正确的状态，这是设计InnoDB的大叔们所不能忍受的。所以他们规定在执行这些需要保证原子性的操作时必须以组的形式来记录的redo日志，在进行系统奔溃重启恢复时，针对某个组中的redo日志，要么把全部的日志都恢复掉，要么一条也不恢复。怎么做到的呢？这得分情况讨论： 有的需要保证原子性的操作会生成多条redo日志，比如向某个索引对应的B+树中进行一次悲观插入就需要生成许多条redo日志。 如何把这些redo日志划分到一个组里边儿呢？设计InnoDB的大叔做了一个很简单的小把戏，就是在该组中的最后一条redo日志后边加上一条特殊类型的redo日志，该类型名称为MLOG_MULTI_REC_END，type字段对应的十进制数字为31，该类型的redo日志结构很简单，只有一个type字段： ![image_1d4fna6k51fok1mpd1tikkmihg144.png-15kB][10] 所以某个需要保证原子性的操作产生的一系列redo日志必须要以一个类型为MLOG_MULTI_REC_END结尾，就像这样： ![image_1d4fol2v71fjalphluu1kuf1d8t4h.png-41.4kB][11] 这样在系统奔溃重启进行恢复时，只有当解析到类型为MLOG_MULTI_REC_END的redo日志，才认为解析到了一组完整的redo日志，才会进行恢复。否则的话直接放弃前边解析到的redo日志。 有的需要保证原子性的操作只生成一条redo日志，比如更新Max Row ID属性的操作就只会生成一条redo日志。 其实在一条日志后边跟一个类型为MLOG_MULTI_REC_END的redo日志也是可以的，不过设计InnoDB的大叔比较勤俭节约，它们不想浪费一个比特位。别忘了虽然redo日志的类型比较多，但撑死了也就是几十种，是小于127这个数字的，也就是说我们用7个比特位就足以包括所有的redo日志类型，而type字段其实是占用1个字节的，也就是说我们可以省出来一个比特位用来表示该需要保证原子性的操作只产生单一的一条redo日志，示意图如下： ![image_1d4fqlji7md35pdmvvhvibqb4u.png-27.4kB][12] 如果type字段的第一个比特为为1，代表该需要保证原子性的操作只产生了单一的一条redo日志，否则表示该需要保证原子性的操作产生了一系列的redo日志。 Mini-Transaction的概念设计MySQL的大叔把对底层页面中的一次原子访问的过程称之为一个Mini-Transaction，简称mtr，比如上边所说的修改一次Max Row ID的值算是一个Mini-Transaction，向某个索引对应的B+树中插入一条记录的过程也算是一个Mini-Transaction。通过上边的叙述我们也知道，一个所谓的mtr可以包含一组redo日志，在进行奔溃恢复时这一组redo日志作为一个不可分割的整体。 一个事务可以包含若干条语句，每一条语句其实是由若干个mtr组成，每一个mtr又可以包含若干条redo日志，画个图表示它们的关系就是这样： ![image_1d4hgjr7t4es1v2mf2b1bt51rf95b.png-27.6kB][13] redo日志的写入过程redo log block设计InnoDB的大叔为了更好的进行系统奔溃恢复，他们把通过mtr生成的redo日志都放在了大小为512字节的页中。为了和我们前边提到的表空间中的页做区别，我们这里把用来存储redo日志的页称为block（你心里清楚页和block的意思其实差不多就行了）。一个redo log block的示意图如下： ![image_1d4hor6e7nq1mkm1sa41he71rif75.png-57.2kB][14] 真正的redo日志都是存储到占用496字节大小的log block body中，图中的log block header和log block trailer存储的是一些管理信息。我们来看看这些所谓的管理信息都是啥： ![image_1d4hp4u8g13e317mkngoag21clv7i.png-113.9kB][15] 其中log block header的几个属性的意思分别如下： LOG_BLOCK_HDR_NO：每一个block都有一个大于0的唯一标号，本属性就表示该标号值。 LOG_BLOCK_HDR_DATA_LEN：表示block中已经使用了多少字节，初始值为12（因为log block body从第12个字节处开始）。随着往block中写入的redo日志越来也多，本属性值也跟着增长。如果log block body已经被全部写满，那么本属性的值被设置为512。 LOG_BLOCK_FIRST_REC_GROUP：一条redo日志也可以称之为一条redo日志记录（redo log record），一个mtr会生产多条redo日志记录，这些redo日志记录被称之为一个redo日志记录组（redo log record group）。LOG_BLOCK_FIRST_REC_GROUP就代表该block中第一个mtr生成的redo日志记录组的偏移量（其实也就是这个block里第一个mtr生成的第一条redo日志的偏移量）。 LOG_BLOCK_CHECKPOINT_NO：表示所谓的checkpoint的序号，checkpoint是我们后续内容的重点，现在先不用清楚它的意思，稍安勿躁。 log block trailer中属性的意思如下： LOG_BLOCK_CHECKSUM：表示block的校验值，用于正确性校验，我们暂时不关心它。 redo日志缓冲区我们前边说过，设计InnoDB的大叔为了解决磁盘速度过慢的问题而引入了Buffer Pool。同理，写入redo日志时也不能直接直接写到磁盘上，实际上在服务器启动时就向操作系统申请了一大片称之为redo log buffer的连续内存空间，翻译成中文就是redo日志缓冲区，我们也可以简称为log buffer。这片内存空间被划分成若干个连续的redo log block，就像这样： ![image_1d4i4orkr17vl1m5l3hl1l341pad1j.png-76.5kB][16] 我们可以通过启动参数innodb_log_buffer_size来指定log buffer的大小，在MySQL 5.7.21这个版本中，该启动参数的默认值为16MB。 redo日志写入log buffer向log buffer中写入redo日志的过程是顺序的，也就是先往前边的block中写，当该block的空闲空间用完之后再往下一个block中写。当我们想往log buffer中写入redo日志时，第一个遇到的问题就是应该写在哪个block的哪个偏移量处，所以设计InnoDB的大叔特意提供了一个称之为buf_free的全局变量，该变量指明后续写入的redo日志应该写入到log buffer中的哪个位置，如图所示： ![image_1d4jsb3pac9t1pl76drruf1b0574.png-98.4kB][17] 我们前边说过一个mtr执行过程中可能产生若干条redo日志，这些redo日志是一个不可分割的组，所以其实并不是每生成一条redo日志，就将其插入到log buffer中，而是每个mtr运行过程中产生的日志先暂时存到一个地方，当该mtr结束的时候，将过程中产生的一组redo日志再全部复制到log buffer中。我们现在假设有两个名为T1、T2的事务，每个事务都包含2个mtr，我们给这几个mtr命名一下： 事务T1的两个mtr分别称为mtr_T1_1和mtr_T1_2。 事务T2的两个mtr分别称为mtr_T2_1和mtr_T2_2。 每个mtr都会产生一组redo日志，用示意图来描述一下这些mtr产生的日志情况： ![image_1d4ie92r31t57c94e661n861skv2t.png-95.1kB][18] 不同的事务可能是并发执行的，所以T1、T2之间的mtr可能是交替执行的。每当一个mtr执行完成时，伴随该mtr生成的一组redo日志就需要被复制到log buffer中，也就是说不同事务的mtr可能是交替写入log buffer的，我们画个示意图（为了美观，我们把一个mtr中产生的所有的redo日志当作一个整体来画）： ![image_1d4jsd7861q6dn9n17gs1cdd1kek7h.png-102.6kB][19] 从示意图中我们可以看出来，不同的mtr产生的一组redo日志占用的存储空间可能不一样，有的mtr产生的redo日志量很少，比如mtr_t1_1、mtr_t2_1就被放到同一个block中存储，有的mtr产生的redo日志量非常大，比如mtr_t1_2产生的redo日志甚至占用了3个block来存储。 123小贴士：对照着上图，自己分析一下每个block的LOG_BLOCK_HDR_DATA_LEN、LOG_BLOCK_FIRST_REC_GROUP属性值都是什么哈～ redo 日志（下）标签： MySQL 是怎样运行的 redo日志文件redo日志刷盘时机我们前边说mtr运行过程中产生的一组redo日志在mtr结束时会被复制到log buffer中，可是这些日志总在内存里呆着也不是个办法，在一些情况下它们会被刷新到磁盘里，比如： log buffer空间不足时 log buffer的大小是有限的（通过系统变量innodb_log_buffer_size指定），如果不停的往这个有限大小的log buffer里塞入日志，很快它就会被填满。设计InnoDB的大叔认为如果当前写入log buffer的redo日志量已经占满了log buffer总容量的大约一半左右，就需要把这些日志刷新到磁盘上。 事务提交时 我们前边说过之所以使用redo日志主要是因为它占用的空间少，还是顺序写，在事务提交时可以不把修改过的Buffer Pool页面刷新到磁盘，但是为了保证持久性，必须要把修改这些页面对应的redo日志刷新到磁盘。 Force Log at Commit 后台线程不停的刷刷刷 后台有一个线程，大约每秒都会刷新一次log buffer中的redo日志到磁盘。 正常关闭服务器时 做所谓的checkpoint时（我们现在没介绍过checkpoint的概念，稍后会仔细唠叨，稍安勿躁） 其他的一些情况… redo日志文件组MySQL的数据目录（使用SHOW VARIABLES LIKE 'datadir'查看）下默认有两个名为ib_logfile0和ib_logfile1的文件，log buffer中的日志默认情况下就是刷新到这两个磁盘文件中。如果我们对默认的redo日志文件不满意，可以通过下边几个启动参数来调节： innodb_log_group_home_dir 该参数指定了redo日志文件所在的目录，默认值就是当前的数据目录。 innodb_log_file_size 该参数指定了每个redo日志文件的大小，在MySQL 5.7.21这个版本中的默认值为48MB， innodb_log_files_in_group 该参数指定redo日志文件的个数，默认值为2，最大值为100。 从上边的描述中可以看到，磁盘上的redo日志文件不只一个，而是以一个日志文件组的形式出现的。这些文件以ib_logfile[数字]（数字可以是0、1、2…）的形式进行命名。在将redo日志写入日志文件组时，是从ib_logfile0开始写，如果ib_logfile0写满了，就接着ib_logfile1写，同理，ib_logfile1写满了就去写ib_logfile2，依此类推。如果写到最后一个文件该咋办？那就重新转到ib_logfile0继续写，所以整个过程如下图所示： ![image_1d4mu4s6f7491l7l1jcc6pc1rbk16.png-49.7kB][1] 总共的redo日志文件大小其实就是：innodb_log_file_size × innodb_log_files_in_group。 1小贴士：如果采用循环使用的方式向redo日志文件组里写数据的话，那岂不是要追尾，也就是后写入的redo日志覆盖掉前边写的redo日志？当然可能了！所以设计InnoDB的大叔提出了checkpoint的概念，稍后我们重点唠叨～ redo日志文件格式我们前边说过log buffer本质上是一片连续的内存空间，被划分成了若干个512字节大小的block。将log buffer中的redo日志刷新到磁盘的本质就是把block的镜像写入日志文件中，所以redo日志文件其实也是由若干个512字节大小的block组成。 redo日志文件组中的每个文件大小都一样，格式也一样，都是由两部分组成： 前2048个字节，也就是前4个block是用来存储一些管理信息的。 从第2048字节往后是用来存储log buffer中的block镜像的。 所以我们前边所说的循环使用redo日志文件，其实是从每个日志文件的第2048个字节开始算，画个示意图就是这样： ![image_1d4njgt351je21kitk7u1gbioa46j.png-64.9kB][2] 普通block的格式我们在唠叨log buffer的时候都说过了，就是log block header、log block body、log block trialer这三个部分，就不重复介绍了。这里需要介绍一下每个redo日志文件前2048个字节，也就是前4个特殊block的格式都是干嘛的，废话少说，先看图： ![image_1d4n63euu1t3u1ten1tgicecsar4c.png-51.1kB][3]从图中可以看出来，这4个block分别是： log file header：描述该redo日志文件的一些整体属性，看一下它的结构： ![image_1d4nfhoa914vbne4kao7cstr95m.png-65.5kB][4] 各个属性的具体释义如下： 属性名 长度（单位：字节） 描述 LOG_HEADER_FORMAT 4 redo日志的版本，在MySQL 5.7.21中该值永远为1 LOG_HEADER_PAD1 4 做字节填充用的，没什么实际意义，忽略～ LOG_HEADER_START_LSN 8 标记本redo日志文件开始的LSN值，也就是文件偏移量为2048字节初对应的LSN值（关于什么是LSN我们稍后再看哈，看不懂的先忽略）。 LOG_HEADER_CREATOR 32 一个字符串，标记本redo日志文件的创建者是谁。正常运行时该值为MySQL的版本号，比如：&quot;MySQL 5.7.21&quot;，使用mysqlbackup命令创建的redo日志文件的该值为&quot;ibbackup&quot;和创建时间。 LOG_BLOCK_CHECKSUM 4 本block的校验值，所有block都有，我们不关心 123小贴士：设计InnoDB的大叔对redo日志的block格式做了很多次修改，如果你阅读的其他书籍中发现上述的属性和你阅读书籍中的属性有些出入，不要慌，正常现象，忘记以前的版本吧。另外，LSN值我们后边才会介绍，现在千万别纠结LSN是个啥。 checkpoint1：记录关于checkpoint的一些属性，看一下它的结构： ![image_1d4njq08pd2a5j9pc01qcn2ps7g.png-60.1kB][5] 各个属性的具体释义如下： 属性名 长度（单位：字节） 描述 LOG_CHECKPOINT_NO 8 服务器做checkpoint的编号，每做一次checkpoint，该值就加1。 LOG_CHECKPOINT_LSN 8 服务器做checkpoint结束时对应的LSN值，系统奔溃恢复时将从该值开始。 LOG_CHECKPOINT_OFFSET 8 上个属性中的LSN值在redo日志文件组中的偏移量 LOG_CHECKPOINT_LOG_BUF_SIZE 8 服务器在做checkpoint操作时对应的log buffer的大小 LOG_BLOCK_CHECKSUM 4 本block的校验值，所有block都有，我们不关心 123小贴士：现在看不懂上边这些关于checkpoint和LSN的属性的释义是很正常的，我就是想让大家对上边这些属性混个脸熟，后边我们后详细唠叨的。 第三个block未使用，忽略～ checkpoint2：结构和checkpoint1一样。 Log Sequeue Number自系统开始运行，就不断的在修改页面，也就意味着会不断的生成redo日志。redo日志的量在不断的递增，就像人的年龄一样，自打出生起就不断递增，永远不可能缩减了。设计InnoDB的大叔为记录已经写入的redo日志量，设计了一个称之为Log Sequeue Number的全局变量，翻译过来就是：日志序列号，简称lsn。不过不像人一出生的年龄是0岁，设计InnoDB的大叔规定初始的lsn值为8704（也就是一条redo日志也没写入时，lsn的值为8704）。 我们知道在向log buffer中写入redo日志时不是一条一条写入的，而是以一个mtr生成的一组redo日志为单位进行写入的。而且实际上是把日志内容写在了log blcok body处。但是在统计lsn的增长量时，是按照实际写入的日志量加上占用的log block header和log block trailer来计算的。我们来看一个例子： 系统第一次启动后初始化log buffer时，buf_free（就是标记下一条redo日志应该写入到log buffer的位置的变量）就会指向第一个block的偏移量为12字节（log block header的大小）的地方，那么lsn值也会跟着增加12： ![image_1d4v2r59mr10jdl1vs4fk61huv79.png-50.9kB][6] 如果某个mtr产生的一组redo日志占用的存储空间比较小，也就是待插入的block剩余空闲空间能容纳这个mtr提交的日志时，lsn增长的量就是该mtr生成的redo日志占用的字节数，就像这样： ![image_1d4v57vgl1obr1kfcfuunp44bo2t.png-54kB][7] 我们假设上图中mtr_1产生的redo日志量为200字节，那么lsn就要在8716的基础上增加200，变为8916。 如果某个mtr产生的一组redo日志占用的存储空间比较大，也就是待插入的block剩余空闲空间不足以容纳这个mtr提交的日志时，lsn增长的量就是该mtr生成的redo日志占用的字节数加上额外占用的log block header和log block trailer的字节数，就像这样： ![image_1d4v37u011jhc1rpa1fpi5a82ca9.png-99.3kB][8] 我们假设上图中mtr_2产生的redo日志量为1000字节，为了将mtr_2产生的redo日志写入log buffer，我们不得不额外多分配两个block，所以lsn的值需要在8916的基础上增加1000 + 12×2 + 4 × 2 = 1032。 123小贴士：为什么初始的lsn值为8704呢？我也不太清楚，人家就这么规定的。其实你也可以规定你一生下来算1岁，只要保证随着时间的流逝，你的年龄不断增长就好了。 从上边的描述中可以看出来，每一组由mtr生成的redo日志都有一个唯一的LSN值与其对应，LSN值越小，说明redo日志产生的越早。 flushed_to_disk_lsnredo日志是首先写到log buffer中，之后才会被刷新到磁盘上的redo日志文件。所以设计InnoDB的大叔提出了一个称之为buf_next_to_write的全局变量，标记当前log buffer中已经有哪些日志被刷新到磁盘中了。画个图表示就是这样： ![image_1d4q3upvq17n8cargmibugve29.png-84.3kB][9] 我们前边说lsn是表示当前系统中写入的redo日志量，这包括了写到log buffer而没有刷新到磁盘的日志，相应的，设计InnoDB的大叔提出了一个表示刷新到磁盘中的redo日志量的全局变量，称之为flushed_to_disk_lsn。系统第一次启动时，该变量的值和初始的lsn值是相同的，都是8704。随着系统的运行，redo日志被不断写入log buffer，但是并不会立即刷新到磁盘，lsn的值就和flushed_to_disk_lsn的值拉开了差距。我们演示一下： 系统第一次启动后，向log buffer中写入了mtr_1、mtr_2、mtr_3这三个mtr产生的redo日志，假设这三个mtr开始和结束时对应的lsn值分别是： mtr_1：8716 ～ 8916 mtr_2：8916 ～ 9948 mtr_3：9948 ～ 10000 此时的lsn已经增长到了10000，但是由于没有刷新操作，所以此时flushed_to_disk_lsn的值仍为8704，如图： ![image_1d4v3ubbacgm13171s481trb6kj1m.png-88.5kB][10] 随后进行将log buffer中的block刷新到redo日志文件的操作，假设将mtr_1和mtr_2的日志刷新到磁盘，那么flushed_to_disk_lsn就应该增长mtr_1和mtr_2写入的日志量，所以flushed_to_disk_lsn的值增长到了9948，如图： ![image_1d4v40upc1tnt1dpe1l14u2ar4n23.png-100.2kB][11] 综上所述，当有新的redo日志写入到log buffer时，首先lsn的值会增长，但flushed_to_disk_lsn不变，随后随着不断有log buffer中的日志被刷新到磁盘上，flushed_to_disk_lsn的值也跟着增长。如果两者的值相同时，说明log buffer中的所有redo日志都已经刷新到磁盘中了。 123小贴士：应用程序向磁盘写入文件时其实是先写到操作系统的缓冲区中去，如果某个写入操作要等到操作系统确认已经写到磁盘时才返回，那需要调用一下操作系统提供的fsync函数。其实只有当系统执行了fsync函数后，flushed_to_disk_lsn的值才会跟着增长，当仅仅把log buffer中的日志写入到操作系统缓冲区却没有显式的刷新到磁盘时，另外的一个称之为write_lsn的值跟着增长。不过为了大家理解上的方便，我们在讲述时把flushed_to_disk_lsn和write_lsn的概念混淆了起来。 lsn值和redo日志文件偏移量的对应关系因为lsn的值是代表系统写入的redo日志量的一个总和，一个mtr中产生多少日志，lsn的值就增加多少（当然有时候要加上log block header和log block trailer的大小），这样mtr产生的日志写到磁盘中时，很容易计算某一个lsn值在redo日志文件组中的偏移量，如图： ![image_1d4v5sdrj1p1jrhmnfrq4pa073n.png-49.3kB][12] 初始时的LSN值是8704，对应文件偏移量2048，之后每个mtr向磁盘中写入多少字节日志，lsn的值就增长多少。 flush链表中的LSN我们知道一个mtr代表一次对底层页面的原子访问，在访问过程中可能会产生一组不可分割的redo日志，在mtr结束时，会把这一组redo日志写入到log buffer中。除此之外，在mtr结束时还有一件非常重要的事情要做，就是把在mtr执行过程中可能修改过的页面加入到Buffer Pool的flush链表。为了防止大家早已忘记flush链表是个啥，我们再看一下图： ![image_1d4uln1ejrt4cerr6h1tc41uok3k.png-227kB][13] 当第一次修改某个缓存在Buffer Pool中的页面时，就会把这个页面对应的控制块插入到flush链表的头部，之后再修改该页面时由于它已经在flush链表中了，就不再次插入了。也就是说flush链表中的脏页是按照页面的第一次修改时间从大到小进行排序的。在这个过程中会在缓存页对应的控制块中记录两个关于页面何时修改的属性： oldest_modification：如果某个页面被加载到Buffer Pool后进行第一次修改，那么就将修改该页面的mtr开始时对应的lsn值写入这个属性。 newest_modification：每修改一次页面，都会将修改该页面的mtr结束时对应的lsn值写入这个属性。也就是说该属性表示页面最近一次修改后对应的系统lsn值。 我们接着上边唠叨flushed_to_disk_lsn的例子看一下： 假设mtr_1执行过程中修改了页a，那么在mtr_1执行结束时，就会将页a对应的控制块加入到flush链表的头部。并且将mtr_1开始时对应的lsn，也就是8716写入页a对应的控制块的oldest_modification属性中，把mtr_1结束时对应的lsn，也就是8404写入页a对应的控制块的newest_modification属性中。画个图表示一下（为了让图片美观一些，我们把oldest_modification缩写成了o_m，把newest_modification缩写成了n_m）： ![image_1d4v63pct1v9o14l3812gnj11de44.png-31.8kB][14] 接着假设mtr_2执行过程中又修改了页b和页c两个页面，那么在mtr_2执行结束时，就会将页b和页c对应的控制块都加入到flush链表的头部。并且将mtr_2开始时对应的lsn，也就是8404写入页b和页c对应的控制块的oldest_modification属性中，把mtr_2结束时对应的lsn，也就是9436写入页b和页c对应的控制块的newest_modification属性中。画个图表示一下： ![image_1d4v64vte14tq1oc911s1v8gnn51.png-59.4kB][15] 从图中可以看出来，每次新插入到flush链表中的节点都是被放在了头部，也就是说flush链表中前边的脏页修改的时间比较晚，后边的脏页修改时间比较早。 接着假设mtr_3执行过程中修改了页b和页d，不过页b之前已经被修改过了，所以它对应的控制块已经被插入到了flush链表，所以在mtr_2执行结束时，只需要将页d对应的控制块都加入到flush链表的头部即可。所以需要将mtr_3开始时对应的lsn，也就是9436写入页c对应的控制块的oldest_modification属性中，把mtr_3结束时对应的lsn，也就是10000写入页c对应的控制块的newest_modification属性中。另外，由于页b在mtr_3执行过程中又发生了一次修改，所以需要更新页b对应的控制块中newest_modification的值为10000。画个图表示一下： ![image_1d4v68bhl1jb9r8m6vn1b157cn5e.png-110.8kB][16] 总结一下上边说的，就是：flush链表中的脏页按照修改发生的时间顺序进行排序，也就是按照oldest_modification代表的LSN值进行排序，被多次更新的页面不会重复插入到flush链表中，但是会更新newest_modification属性的值。 checkpoint有一个很不幸的事实就是我们的redo日志文件组容量是有限的，我们不得不选择循环使用redo日志文件组中的文件，但是这会造成最后写的redo日志与最开始写的redo日志追尾，这时应该想到：redo日志只是为了系统奔溃后恢复脏页用的，如果对应的脏页已经刷新到了磁盘，也就是说即使现在系统奔溃，那么在重启后也用不着使用redo日志恢复该页面了，所以该redo日志也就没有存在的必要了，那么它占用的磁盘空间就可以被后续的redo日志所重用。也就是说：判断某些redo日志占用的磁盘空间是否可以覆盖的依据就是它对应的脏页是否已经刷新到磁盘里。我们看一下前边一直唠叨的那个例子： ![image_1d4v6epcasjm11u4l131nj41vgs68.png-112.1kB][17] 如图，虽然mtr_1和mtr_2生成的redo日志都已经被写到了磁盘上，但是它们修改的脏页仍然留在Buffer Pool中，所以它们生成的redo日志在磁盘上的空间是不可以被覆盖的。之后随着系统的运行，如果页a被刷新到了磁盘，那么它对应的控制块就会从flush链表中移除，就像这样子： ![image_1d4v6h6kp7311ni21mkn1ejkm397i.png-99.3kB][18] 这样mtr_1生成的redo日志就没有用了，它们占用的磁盘空间就可以被覆盖掉了。设计InnoDB的大叔提出了一个全局变量checkpoint_lsn来代表当前系统中可以被覆盖的redo日志总量是多少，这个变量初始值也是8704。 比方说现在页a被刷新到了磁盘，mtr_1生成的redo日志就可以被覆盖了，所以我们需要进行一个增加checkpoint_lsn的操作，我们把这个过程称之为做一次checkpoint。做一次checkpoint其实可以分为两个步骤： 步骤一：计算一下当前系统中可以被覆盖的redo日志对应的lsn值最大是多少。 redo日志可以被覆盖，意味着它对应的脏页被刷到了磁盘，只要我们计算出当前系统中被最早修改的脏页对应的oldest_modification值，那凡是在系统lsn值小于该节点的oldest_modification值时产生的redo日志都是可以被覆盖掉的，我们就把该脏页的oldest_modification赋值给checkpoint_lsn。 比方说当前系统中页a已经被刷新到磁盘，那么flush链表的尾节点就是页c，该节点就是当前系统中最早修改的脏页了，它的oldest_modification值为8404，我们就把8404赋值给checkpoint_lsn（也就是说在redo日志对应的lsn值小于8404时就可以被覆盖掉）。 步骤二：将checkpoint_lsn和对应的redo日志文件组偏移量以及此次checkpint的编号写到日志文件的管理信息（就是checkpoint1或者checkpoint2）中。 设计InnoDB的大叔维护了一个目前系统做了多少次checkpoint的变量checkpoint_no，每做一次checkpoint，该变量的值就加1。我们前边说过计算一个lsn值对应的redo日志文件组偏移量是很容易的，所以可以计算得到该checkpoint_lsn在redo日志文件组中对应的偏移量checkpoint_offset，然后把这三个值都写到redo日志文件组的管理信息中。 我们说过，每一个redo日志文件都有2048个字节的管理信息，但是上述关于checkpoint的信息只会被写到日志文件组的第一个日志文件的管理信息中。不过我们是存储到checkpoint1中还是checkpoint2中呢？设计InnoDB的大叔规定，当checkpoint_no的值是偶数时，就写到checkpoint1中，是奇数时，就写到checkpoint2中。 记录完checkpoint的信息之后，redo日志文件组中各个lsn值的关系就像这样： ![image_1d4v9cgu21mmcafb1hsp1qtj1di0p.png-79.5kB][19] 批量从flush链表中刷出脏页我们在介绍Buffer Pool的时候说过，一般情况下都是后台的线程在对LRU链表和flush链表进行刷脏操作，这主要因为刷脏操作比较慢，不想影响用户线程处理请求。但是如果当前系统修改页面的操作十分频繁，这样就导致写日志操作十分频繁，系统lsn值增长过快。如果后台的刷脏操作不能将脏页刷出，那么系统无法及时做checkpoint，可能就需要用户线程同步的从flush链表中把那些最早修改的脏页（oldest_modification最小的脏页）刷新到磁盘，这样这些脏页对应的redo日志就没用了，然后就可以去做checkpoint了。 查看系统中的各种LSN值我们可以使用SHOW ENGINE INNODB STATUS命令查看当前InnoDB存储引擎中的各种LSN值的情况，比如： 12345678910111213mysql&gt; SHOW ENGINE INNODB STATUS\\G(...省略前边的许多状态)LOG---Log sequence number 124476971Log flushed up to 124099769Pages flushed up to 124052503Last checkpoint at 1240524940 pending log flushes, 0 pending chkp writes24 log i/o's done, 2.00 log i/o's/second----------------------(...省略后边的许多状态) 其中： Log sequence number：代表系统中的lsn值，也就是当前系统已经写入的redo日志量，包括写入log buffer中的日志。 Log flushed up to：代表flushed_to_disk_lsn的值，也就是当前系统已经写入磁盘的redo日志量。 Pages flushed up to：代表flush链表中被最早修改的那个页面对应的oldest_modification属性值。 Last checkpoint at：当前系统的checkpoint_lsn值。 innodb_flush_log_at_trx_commit的用法我们前边说为了保证事务的持久性，用户线程在事务提交时需要将该事务执行过程中产生的所有redo日志都刷新到磁盘上。这一条要求太狠了，会很明显的降低数据库性能。如果有的同学对事务的持久性要求不是那么强烈的话，可以选择修改一个称为innodb_flush_log_at_trx_commit的系统变量的值，该变量有3个可选的值： 0：当该系统变量值为0时，表示在事务提交时不立即向磁盘中同步redo日志，这个任务是交给后台线程做的。 这样很明显会加快请求处理速度，但是如果事务提交后服务器挂了，后台线程没有及时将redo日志刷新到磁盘，那么该事务对页面的修改会丢失。 1：当该系统变量值为0时，表示在事务提交时需要将redo日志同步到磁盘，可以保证事务的持久性。1也是innodb_flush_log_at_trx_commit的默认值。 2：当该系统变量值为0时，表示在事务提交时需要将redo日志写到操作系统的缓冲区中，但并不需要保证将日志真正的刷新到磁盘。 这种情况下如果数据库挂了，操作系统没挂的话，事务的持久性还是可以保证的，但是操作系统也挂了的话，那就不能保证持久性了。 崩溃恢复在服务器不挂的情况下，redo日志简直就是个大累赘，不仅没用，反而让性能变得更差。但是万一，我说万一啊，万一数据库挂了，那redo日志可是个宝了，我们就可以在重启时根据redo日志中的记录就可以将页面恢复到系统奔溃前的状态。我们接下来大致看一下恢复过程是个啥样。 确定恢复的起点我们前边说过，checkpoint_lsn之前的redo日志都可以被覆盖，也就是说这些redo日志对应的脏页都已经被刷新到磁盘中了，既然它们已经被刷盘，我们就没必要恢复它们了。对于checkpoint_lsn之后的redo日志，它们对应的脏页可能没被刷盘，也可能被刷盘了，我们不能确定，所以需要从checkpoint_lsn开始读取redo日志来恢复页面。 当然，redo日志文件组的第一个文件的管理信息中有两个block都存储了checkpoint_lsn的信息，我们当然是要选取最近发生的那次checkpoint的信息。衡量checkpoint发生时间早晚的信息就是所谓的checkpoint_no，我们只要把checkpoint1和checkpoint2这两个block中的checkpoint_no值读出来比一下大小，哪个的checkpoint_no值更大，说明哪个block存储的就是最近的一次checkpoint信息。这样我们就能拿到最近发生的checkpoint对应的checkpoint_lsn值以及它在redo日志文件组中的偏移量checkpoint_offset。 确定恢复的终点redo日志恢复的起点确定了，那终点是哪个呢？这个还得从block的结构说起。我们说在写redo日志的时候都是顺序写的，写满了一个block之后会再往下一个block中写： ![image_1d4viej35t9nvld8o3141s8pp.png-69.5kB][20] 普通block的log block header部分有一个称之为LOG_BLOCK_HDR_DATA_LEN的属性，该属性值记录了当前block里使用了多少字节的空间。对于被填满的block来说，该值永远为512。如果该属性的值不为512，那么就是它了，它就是此次奔溃恢复中需要扫描的最后一个block。 怎么恢复确定了需要扫描哪些redo日志进行奔溃恢复之后，接下来就是怎么进行恢复了。假设现在的redo日志文件中有5条redo日志，如图： ![image_1d4vjuf9l17og1papl3e16is1m9f16.png-59.9kB][21] 由于redo 0在checkpoint_lsn后边，恢复时可以不管它。我们现在可以按照redo日志的顺序依次扫描checkpoint_lsn之后的各条redo日志，按照日志中记载的内容将对应的页面恢复出来。这样没什么问题，不过设计InnoDB的大叔还是想了一些办法加快这个恢复的过程： 使用哈希表 根据redo日志的space ID和page number属性计算出散列值，把space ID和page number相同的redo日志放到哈希表的同一个槽里，如果有多个space ID和page number都相同的redo日志，那么它们之间使用链表连接起来，按照生成的先后顺序链接起来的，如图所示： ![image_1d50lj9da176rojd12ja1lodognc.png-156.4kB][22] 之后就可以遍历哈希表，因为对同一个页面进行修改的redo日志都放在了一个槽里，所以可以一次性将一个页面修复好（避免了很多读取页面的随机IO），这样可以加快恢复速度。另外需要注意一点的是，同一个页面的redo日志是按照生成时间顺序进行排序的，所以恢复的时候也是按照这个顺序进行恢复，如果不按照生成时间顺序进行排序的话，那么可能出现错误。比如原先的修改操作是先插入一条记录，再删除该条记录，如果恢复时不按照这个顺序来，就可能变成先删除一条记录，再插入一条记录，这显然是错误的。 跳过已经刷新到磁盘的页面 我们前边说过，checkpoint_lsn之前的redo日志对应的脏页确定都已经刷到磁盘了，但是checkpoint_lsn之后的redo日志我们不能确定是否已经刷到磁盘，主要是因为在最近做的一次checkpoint后，可能后台线程又不断的从LRU链表和flush链表中将一些脏页刷出Buffer Pool。这些在checkpoint_lsn之后的redo日志，如果它们对应的脏页在奔溃发生时已经刷新到磁盘，那在恢复时也就没有必要根据redo日志的内容修改该页面了。 那在恢复时怎么知道某个redo日志对应的脏页是否在奔溃发生时已经刷新到磁盘了呢？这还得从页面的结构说起，我们前边说过每个页面都有一个称之为File Header的部分，在File Header里有一个称之为FIL_PAGE_LSN的属性，该属性记载了最近一次修改页面时对应的lsn值（其实就是页面控制块中的newest_modification值）。如果在做了某次checkpoint之后有脏页被刷新到磁盘中，那么该页对应的FIL_PAGE_LSN代表的lsn值肯定大于checkpoint_lsn的值，凡是符合这种情况的页面就不需要做恢复操作了，所以更进一步提升了奔溃恢复的速度。 遗漏的问题：LOG_BLOCK_HDR_NO是如何计算的我们前边说过，对于实际存储redo日志的普通的log block来说，在log block header处有一个称之为LOG_BLOCK_HDR_NO的属性（忘记了的话回头再看看哈），我们说这个属性代表一个唯一的标号。这个属性是初次使用该block时分配的，跟当时的系统lsn值有关。使用下边的公式计算该block的LOG_BLOCK_HDR_NO值： 1((lsn / 512) &amp; 0x3FFFFFFFUL) + 1 这个公式里的0x3FFFFFFFUL可能让大家有点困惑，其实它的二进制表示可能更亲切一点： ![image_1d4rt3sm81pbe1tij3pm147op9c30.png-36.9kB][23] 从图中可以看出，0x3FFFFFFFUL对应的二进制数的前2位为0，后30位的值都为1。我们刚开始学计算机的时候就学过，一个二进制位与0做与运算（&amp;）的结果肯定是0，一个二进制位与1做与运算（&amp;）的结果就是原值。让一个数和0x3FFFFFFFUL做与运算的意思就是要将该值的前2个比特位的值置为0，这样该值就肯定小于或等于0x3FFFFFFFUL了。这也就说明了，不论lsn多大，((lsn / 512) &amp; 0x3FFFFFFFUL)的值肯定在00x3FFFFFFFUL之间，再加1的话肯定在10x40000000UL之间。而0x40000000UL这个值大家应该很熟悉，这个值就代表着1GB。也就是说系统最多能产生不重复的LOG_BLOCK_HDR_NO值只有1GB个。设计InnoDB的大叔规定redo日志文件组中包含的所有文件大小总和不得超过512GB，一个block大小是512字节，也就是说redo日志文件组中包含的block块最多为1GB个，所以有1GB个不重复的编号值也就够用了。 另外，LOG_BLOCK_HDR_NO值的第一个比特位比较特殊，称之为flush bit，如果该值为1，代表着本block是在某次将log buffer中的block刷新到磁盘的操作中的第一个被刷入的block。 redo日志文件redo日志刷盘时机我们前边说mtr运行过程中产生的一组redo日志在mtr结束时会被复制到log buffer中，可是这些日志总在内存里呆着也不是个办法，在一些情况下它们会被刷新到磁盘里，比如： log buffer空间不足时 log buffer的大小是有限的（通过系统变量innodb_log_buffer_size指定），如果不停的往这个有限大小的log buffer里塞入日志，很快它就会被填满。设计InnoDB的大叔认为如果当前写入log buffer的redo日志量已经占满了log buffer总容量的大约一半左右，就需要把这些日志刷新到磁盘上。 事务提交时 我们前边说过之所以使用redo日志主要是因为它占用的空间少，还是顺序写，在事务提交时可以不把修改过的Buffer Pool页面刷新到磁盘，但是为了保证持久性，必须要把修改这些页面对应的redo日志刷新到磁盘。 Force Log at Commit 后台线程不停的刷刷刷 后台有一个线程，大约每秒都会刷新一次log buffer中的redo日志到磁盘。 正常关闭服务器时 做所谓的checkpoint时（我们现在没介绍过checkpoint的概念，稍后会仔细唠叨，稍安勿躁） 其他的一些情况… redo日志文件组MySQL的数据目录（使用SHOW VARIABLES LIKE 'datadir'查看）下默认有两个名为ib_logfile0和ib_logfile1的文件，log buffer中的日志默认情况下就是刷新到这两个磁盘文件中。如果我们对默认的redo日志文件不满意，可以通过下边几个启动参数来调节： innodb_log_group_home_dir 该参数指定了redo日志文件所在的目录，默认值就是当前的数据目录。 innodb_log_file_size 该参数指定了每个redo日志文件的大小，在MySQL 5.7.21这个版本中的默认值为48MB， innodb_log_files_in_group 该参数指定redo日志文件的个数，默认值为2，最大值为100。 从上边的描述中可以看到，磁盘上的redo日志文件不只一个，而是以一个日志文件组的形式出现的。这些文件以ib_logfile[数字]（数字可以是0、1、2…）的形式进行命名。在将redo日志写入日志文件组时，是从ib_logfile0开始写，如果ib_logfile0写满了，就接着ib_logfile1写，同理，ib_logfile1写满了就去写ib_logfile2，依此类推。如果写到最后一个文件该咋办？那就重新转到ib_logfile0继续写，所以整个过程如下图所示： ![image_1d4mu4s6f7491l7l1jcc6pc1rbk16.png-49.7kB][1] 总共的redo日志文件大小其实就是：innodb_log_file_size × innodb_log_files_in_group。 1小贴士：如果采用循环使用的方式向redo日志文件组里写数据的话，那岂不是要追尾，也就是后写入的redo日志覆盖掉前边写的redo日志？当然可能了！所以设计InnoDB的大叔提出了checkpoint的概念，稍后我们重点唠叨～ redo日志文件格式我们前边说过log buffer本质上是一片连续的内存空间，被划分成了若干个512字节大小的block。将log buffer中的redo日志刷新到磁盘的本质就是把block的镜像写入日志文件中，所以redo日志文件其实也是由若干个512字节大小的block组成。 redo日志文件组中的每个文件大小都一样，格式也一样，都是由两部分组成： 前2048个字节，也就是前4个block是用来存储一些管理信息的。 从第2048字节往后是用来存储log buffer中的block镜像的。 所以我们前边所说的循环使用redo日志文件，其实是从每个日志文件的第2048个字节开始算，画个示意图就是这样： ![image_1d4njgt351je21kitk7u1gbioa46j.png-64.9kB][2] 普通block的格式我们在唠叨log buffer的时候都说过了，就是log block header、log block body、log block trialer这三个部分，就不重复介绍了。这里需要介绍一下每个redo日志文件前2048个字节，也就是前4个特殊block的格式都是干嘛的，废话少说，先看图： ![image_1d4n63euu1t3u1ten1tgicecsar4c.png-51.1kB][3]从图中可以看出来，这4个block分别是： log file header：描述该redo日志文件的一些整体属性，看一下它的结构： ![image_1d4nfhoa914vbne4kao7cstr95m.png-65.5kB][4] 各个属性的具体释义如下： 属性名 长度（单位：字节） 描述 LOG_HEADER_FORMAT 4 redo日志的版本，在MySQL 5.7.21中该值永远为1 LOG_HEADER_PAD1 4 做字节填充用的，没什么实际意义，忽略～ LOG_HEADER_START_LSN 8 标记本redo日志文件开始的LSN值，也就是文件偏移量为2048字节初对应的LSN值（关于什么是LSN我们稍后再看哈，看不懂的先忽略）。 LOG_HEADER_CREATOR 32 一个字符串，标记本redo日志文件的创建者是谁。正常运行时该值为MySQL的版本号，比如：&quot;MySQL 5.7.21&quot;，使用mysqlbackup命令创建的redo日志文件的该值为&quot;ibbackup&quot;和创建时间。 LOG_BLOCK_CHECKSUM 4 本block的校验值，所有block都有，我们不关心 1小贴士：设计InnoDB的大叔对redo日志的block格式做了很多次修改，如果你阅读的其他书籍中发现上述的属性和你阅读书籍中的属性有些出入，不要慌，正常现象，忘记以前的版本吧。另外，LSN值我们后边才会介绍，现在千万别纠结LSN是个啥。 checkpoint1：记录关于checkpoint的一些属性，看一下它的结构： ![image_1d4njq08pd2a5j9pc01qcn2ps7g.png-60.1kB][5] 各个属性的具体释义如下： 属性名 长度（单位：字节） 描述 LOG_CHECKPOINT_NO 8 服务器做checkpoint的编号，每做一次checkpoint，该值就加1。 LOG_CHECKPOINT_LSN 8 服务器做checkpoint结束时对应的LSN值，系统奔溃恢复时将从该值开始。 LOG_CHECKPOINT_OFFSET 8 上个属性中的LSN值在redo日志文件组中的偏移量 LOG_CHECKPOINT_LOG_BUF_SIZE 8 服务器在做checkpoint操作时对应的log buffer的大小 LOG_BLOCK_CHECKSUM 4 本block的校验值，所有block都有，我们不关心 1小贴士：现在看不懂上边这些关于checkpoint和LSN的属性的释义是很正常的，我就是想让大家对上边这些属性混个脸熟，后边我们后详细唠叨的。 第三个block未使用，忽略～ checkpoint2：结构和checkpoint1一样。 Log Sequeue Number自系统开始运行，就不断的在修改页面，也就意味着会不断的生成redo日志。redo日志的量在不断的递增，就像人的年龄一样，自打出生起就不断递增，永远不可能缩减了。设计InnoDB的大叔为记录已经写入的redo日志量，设计了一个称之为Log Sequeue Number的全局变量，翻译过来就是：日志序列号，简称lsn。不过不像人一出生的年龄是0岁，设计InnoDB的大叔规定初始的lsn值为8704（也就是一条redo日志也没写入时，lsn的值为8704）。 我们知道在向log buffer中写入redo日志时不是一条一条写入的，而是以一个mtr生成的一组redo日志为单位进行写入的。而且实际上是把日志内容写在了log blcok body处。但是在统计lsn的增长量时，是按照实际写入的日志量加上占用的log block header和log block trailer来计算的。我们来看一个例子： 系统第一次启动后初始化log buffer时，buf_free（就是标记下一条redo日志应该写入到log buffer的位置的变量）就会指向第一个block的偏移量为12字节（log block header的大小）的地方，那么lsn值也会跟着增加12： ![image_1d4v2r59mr10jdl1vs4fk61huv79.png-50.9kB][6] 如果某个mtr产生的一组redo日志占用的存储空间比较小，也就是待插入的block剩余空闲空间能容纳这个mtr提交的日志时，lsn增长的量就是该mtr生成的redo日志占用的字节数，就像这样： ![image_1d4v57vgl1obr1kfcfuunp44bo2t.png-54kB][7] 我们假设上图中mtr_1产生的redo日志量为200字节，那么lsn就要在8716的基础上增加200，变为8916。 如果某个mtr产生的一组redo日志占用的存储空间比较大，也就是待插入的block剩余空闲空间不足以容纳这个mtr提交的日志时，lsn增长的量就是该mtr生成的redo日志占用的字节数加上额外占用的log block header和log block trailer的字节数，就像这样： ![image_1d4v37u011jhc1rpa1fpi5a82ca9.png-99.3kB][8] 我们假设上图中mtr_2产生的redo日志量为1000字节，为了将mtr_2产生的redo日志写入log buffer，我们不得不额外多分配两个block，所以lsn的值需要在8916的基础上增加1000 + 12×2 + 4 × 2 = 1032。 1小贴士：为什么初始的lsn值为8704呢？我也不太清楚，人家就这么规定的。其实你也可以规定你一生下来算1岁，只要保证随着时间的流逝，你的年龄不断增长就好了。 从上边的描述中可以看出来，每一组由mtr生成的redo日志都有一个唯一的LSN值与其对应，LSN值越小，说明redo日志产生的越早。 flushed_to_disk_lsnredo日志是首先写到log buffer中，之后才会被刷新到磁盘上的redo日志文件。所以设计InnoDB的大叔提出了一个称之为buf_next_to_write的全局变量，标记当前log buffer中已经有哪些日志被刷新到磁盘中了。画个图表示就是这样： ![image_1d4q3upvq17n8cargmibugve29.png-84.3kB][9] 我们前边说lsn是表示当前系统中写入的redo日志量，这包括了写到log buffer而没有刷新到磁盘的日志，相应的，设计InnoDB的大叔提出了一个表示刷新到磁盘中的redo日志量的全局变量，称之为flushed_to_disk_lsn。系统第一次启动时，该变量的值和初始的lsn值是相同的，都是8704。随着系统的运行，redo日志被不断写入log buffer，但是并不会立即刷新到磁盘，lsn的值就和flushed_to_disk_lsn的值拉开了差距。我们演示一下： 系统第一次启动后，向log buffer中写入了mtr_1、mtr_2、mtr_3这三个mtr产生的redo日志，假设这三个mtr开始和结束时对应的lsn值分别是： mtr_1：8716 ～ 8916 mtr_2：8916 ～ 9948 mtr_3：9948 ～ 10000 此时的lsn已经增长到了10000，但是由于没有刷新操作，所以此时flushed_to_disk_lsn的值仍为8704，如图： ![image_1d4v3ubbacgm13171s481trb6kj1m.png-88.5kB][10] 随后进行将log buffer中的block刷新到redo日志文件的操作，假设将mtr_1和mtr_2的日志刷新到磁盘，那么flushed_to_disk_lsn就应该增长mtr_1和mtr_2写入的日志量，所以flushed_to_disk_lsn的值增长到了9948，如图： ![image_1d4v40upc1tnt1dpe1l14u2ar4n23.png-100.2kB][11] 综上所述，当有新的redo日志写入到log buffer时，首先lsn的值会增长，但flushed_to_disk_lsn不变，随后随着不断有log buffer中的日志被刷新到磁盘上，flushed_to_disk_lsn的值也跟着增长。如果两者的值相同时，说明log buffer中的所有redo日志都已经刷新到磁盘中了。 1小贴士：应用程序向磁盘写入文件时其实是先写到操作系统的缓冲区中去，如果某个写入操作要等到操作系统确认已经写到磁盘时才返回，那需要调用一下操作系统提供的fsync函数。其实只有当系统执行了fsync函数后，flushed_to_disk_lsn的值才会跟着增长，当仅仅把log buffer中的日志写入到操作系统缓冲区却没有显式的刷新到磁盘时，另外的一个称之为write_lsn的值跟着增长。不过为了大家理解上的方便，我们在讲述时把flushed_to_disk_lsn和write_lsn的概念混淆了起来。 lsn值和redo日志文件偏移量的对应关系因为lsn的值是代表系统写入的redo日志量的一个总和，一个mtr中产生多少日志，lsn的值就增加多少（当然有时候要加上log block header和log block trailer的大小），这样mtr产生的日志写到磁盘中时，很容易计算某一个lsn值在redo日志文件组中的偏移量，如图： ![image_1d4v5sdrj1p1jrhmnfrq4pa073n.png-49.3kB][12] 初始时的LSN值是8704，对应文件偏移量2048，之后每个mtr向磁盘中写入多少字节日志，lsn的值就增长多少。 flush链表中的LSN我们知道一个mtr代表一次对底层页面的原子访问，在访问过程中可能会产生一组不可分割的redo日志，在mtr结束时，会把这一组redo日志写入到log buffer中。除此之外，在mtr结束时还有一件非常重要的事情要做，就是把在mtr执行过程中可能修改过的页面加入到Buffer Pool的flush链表。为了防止大家早已忘记flush链表是个啥，我们再看一下图： ![image_1d4uln1ejrt4cerr6h1tc41uok3k.png-227kB][13] 当第一次修改某个缓存在Buffer Pool中的页面时，就会把这个页面对应的控制块插入到flush链表的头部，之后再修改该页面时由于它已经在flush链表中了，就不再次插入了。也就是说flush链表中的脏页是按照页面的第一次修改时间从大到小进行排序的。在这个过程中会在缓存页对应的控制块中记录两个关于页面何时修改的属性： oldest_modification：如果某个页面被加载到Buffer Pool后进行第一次修改，那么就将修改该页面的mtr开始时对应的lsn值写入这个属性。 newest_modification：每修改一次页面，都会将修改该页面的mtr结束时对应的lsn值写入这个属性。也就是说该属性表示页面最近一次修改后对应的系统lsn值。 我们接着上边唠叨flushed_to_disk_lsn的例子看一下： 假设mtr_1执行过程中修改了页a，那么在mtr_1执行结束时，就会将页a对应的控制块加入到flush链表的头部。并且将mtr_1开始时对应的lsn，也就是8716写入页a对应的控制块的oldest_modification属性中，把mtr_1结束时对应的lsn，也就是8404写入页a对应的控制块的newest_modification属性中。画个图表示一下（为了让图片美观一些，我们把oldest_modification缩写成了o_m，把newest_modification缩写成了n_m）： ![image_1d4v63pct1v9o14l3812gnj11de44.png-31.8kB][14] 接着假设mtr_2执行过程中又修改了页b和页c两个页面，那么在mtr_2执行结束时，就会将页b和页c对应的控制块都加入到flush链表的头部。并且将mtr_2开始时对应的lsn，也就是8404写入页b和页c对应的控制块的oldest_modification属性中，把mtr_2结束时对应的lsn，也就是9436写入页b和页c对应的控制块的newest_modification属性中。画个图表示一下： ![image_1d4v64vte14tq1oc911s1v8gnn51.png-59.4kB][15] 从图中可以看出来，每次新插入到flush链表中的节点都是被放在了头部，也就是说flush链表中前边的脏页修改的时间比较晚，后边的脏页修改时间比较早。 接着假设mtr_3执行过程中修改了页b和页d，不过页b之前已经被修改过了，所以它对应的控制块已经被插入到了flush链表，所以在mtr_2执行结束时，只需要将页d对应的控制块都加入到flush链表的头部即可。所以需要将mtr_3开始时对应的lsn，也就是9436写入页c对应的控制块的oldest_modification属性中，把mtr_3结束时对应的lsn，也就是10000写入页c对应的控制块的newest_modification属性中。另外，由于页b在mtr_3执行过程中又发生了一次修改，所以需要更新页b对应的控制块中newest_modification的值为10000。画个图表示一下： ![image_1d4v68bhl1jb9r8m6vn1b157cn5e.png-110.8kB][16] 总结一下上边说的，就是：flush链表中的脏页按照修改发生的时间顺序进行排序，也就是按照oldest_modification代表的LSN值进行排序，被多次更新的页面不会重复插入到flush链表中，但是会更新newest_modification属性的值。 checkpoint有一个很不幸的事实就是我们的redo日志文件组容量是有限的，我们不得不选择循环使用redo日志文件组中的文件，但是这会造成最后写的redo日志与最开始写的redo日志追尾，这时应该想到：redo日志只是为了系统奔溃后恢复脏页用的，如果对应的脏页已经刷新到了磁盘，也就是说即使现在系统奔溃，那么在重启后也用不着使用redo日志恢复该页面了，所以该redo日志也就没有存在的必要了，那么它占用的磁盘空间就可以被后续的redo日志所重用。也就是说：判断某些redo日志占用的磁盘空间是否可以覆盖的依据就是它对应的脏页是否已经刷新到磁盘里。我们看一下前边一直唠叨的那个例子： ![image_1d4v6epcasjm11u4l131nj41vgs68.png-112.1kB][17] 如图，虽然mtr_1和mtr_2生成的redo日志都已经被写到了磁盘上，但是它们修改的脏页仍然留在Buffer Pool中，所以它们生成的redo日志在磁盘上的空间是不可以被覆盖的。之后随着系统的运行，如果页a被刷新到了磁盘，那么它对应的控制块就会从flush链表中移除，就像这样子： ![image_1d4v6h6kp7311ni21mkn1ejkm397i.png-99.3kB][18] 这样mtr_1生成的redo日志就没有用了，它们占用的磁盘空间就可以被覆盖掉了。设计InnoDB的大叔提出了一个全局变量checkpoint_lsn来代表当前系统中可以被覆盖的redo日志总量是多少，这个变量初始值也是8704。 比方说现在页a被刷新到了磁盘，mtr_1生成的redo日志就可以被覆盖了，所以我们需要进行一个增加checkpoint_lsn的操作，我们把这个过程称之为做一次checkpoint。做一次checkpoint其实可以分为两个步骤： 步骤一：计算一下当前系统中可以被覆盖的redo日志对应的lsn值最大是多少。 redo日志可以被覆盖，意味着它对应的脏页被刷到了磁盘，只要我们计算出当前系统中被最早修改的脏页对应的oldest_modification值，那凡是在系统lsn值小于该节点的oldest_modification值时产生的redo日志都是可以被覆盖掉的，我们就把该脏页的oldest_modification赋值给checkpoint_lsn。 比方说当前系统中页a已经被刷新到磁盘，那么flush链表的尾节点就是页c，该节点就是当前系统中最早修改的脏页了，它的oldest_modification值为8404，我们就把8404赋值给checkpoint_lsn（也就是说在redo日志对应的lsn值小于8404时就可以被覆盖掉）。 步骤二：将checkpoint_lsn和对应的redo日志文件组偏移量以及此次checkpint的编号写到日志文件的管理信息（就是checkpoint1或者checkpoint2）中。 设计InnoDB的大叔维护了一个目前系统做了多少次checkpoint的变量checkpoint_no，每做一次checkpoint，该变量的值就加1。我们前边说过计算一个lsn值对应的redo日志文件组偏移量是很容易的，所以可以计算得到该checkpoint_lsn在redo日志文件组中对应的偏移量checkpoint_offset，然后把这三个值都写到redo日志文件组的管理信息中。 我们说过，每一个redo日志文件都有2048个字节的管理信息，但是上述关于checkpoint的信息只会被写到日志文件组的第一个日志文件的管理信息中。不过我们是存储到checkpoint1中还是checkpoint2中呢？设计InnoDB的大叔规定，当checkpoint_no的值是偶数时，就写到checkpoint1中，是奇数时，就写到checkpoint2中。 记录完checkpoint的信息之后，redo日志文件组中各个lsn值的关系就像这样： ![image_1d4v9cgu21mmcafb1hsp1qtj1di0p.png-79.5kB][19] 批量从flush链表中刷出脏页我们在介绍Buffer Pool的时候说过，一般情况下都是后台的线程在对LRU链表和flush链表进行刷脏操作，这主要因为刷脏操作比较慢，不想影响用户线程处理请求。但是如果当前系统修改页面的操作十分频繁，这样就导致写日志操作十分频繁，系统lsn值增长过快。如果后台的刷脏操作不能将脏页刷出，那么系统无法及时做checkpoint，可能就需要用户线程同步的从flush链表中把那些最早修改的脏页（oldest_modification最小的脏页）刷新到磁盘，这样这些脏页对应的redo日志就没用了，然后就可以去做checkpoint了。 查看系统中的各种LSN值我们可以使用SHOW ENGINE INNODB STATUS命令查看当前InnoDB存储引擎中的各种LSN值的情况，比如： 1mysql&gt; SHOW ENGINE INNODB STATUS\\G(...省略前边的许多状态)LOG---Log sequence number 124476971Log flushed up to 124099769Pages flushed up to 124052503Last checkpoint at 1240524940 pending log flushes, 0 pending chkp writes24 log i/o's done, 2.00 log i/o's/second----------------------(...省略后边的许多状态) 其中： Log sequence number：代表系统中的lsn值，也就是当前系统已经写入的redo日志量，包括写入log buffer中的日志。 Log flushed up to：代表flushed_to_disk_lsn的值，也就是当前系统已经写入磁盘的redo日志量。 Pages flushed up to：代表flush链表中被最早修改的那个页面对应的oldest_modification属性值。 Last checkpoint at：当前系统的checkpoint_lsn值。 innodb_flush_log_at_trx_commit的用法我们前边说为了保证事务的持久性，用户线程在事务提交时需要将该事务执行过程中产生的所有redo日志都刷新到磁盘上。这一条要求太狠了，会很明显的降低数据库性能。如果有的同学对事务的持久性要求不是那么强烈的话，可以选择修改一个称为innodb_flush_log_at_trx_commit的系统变量的值，该变量有3个可选的值： 0：当该系统变量值为0时，表示在事务提交时不立即向磁盘中同步redo日志，这个任务是交给后台线程做的。 这样很明显会加快请求处理速度，但是如果事务提交后服务器挂了，后台线程没有及时将redo日志刷新到磁盘，那么该事务对页面的修改会丢失。 1：当该系统变量值为0时，表示在事务提交时需要将redo日志同步到磁盘，可以保证事务的持久性。1也是innodb_flush_log_at_trx_commit的默认值。 2：当该系统变量值为0时，表示在事务提交时需要将redo日志写到操作系统的缓冲区中，但并不需要保证将日志真正的刷新到磁盘。 这种情况下如果数据库挂了，操作系统没挂的话，事务的持久性还是可以保证的，但是操作系统也挂了的话，那就不能保证持久性了。 崩溃恢复在服务器不挂的情况下，redo日志简直就是个大累赘，不仅没用，反而让性能变得更差。但是万一，我说万一啊，万一数据库挂了，那redo日志可是个宝了，我们就可以在重启时根据redo日志中的记录就可以将页面恢复到系统奔溃前的状态。我们接下来大致看一下恢复过程是个啥样。 确定恢复的起点我们前边说过，checkpoint_lsn之前的redo日志都可以被覆盖，也就是说这些redo日志对应的脏页都已经被刷新到磁盘中了，既然它们已经被刷盘，我们就没必要恢复它们了。对于checkpoint_lsn之后的redo日志，它们对应的脏页可能没被刷盘，也可能被刷盘了，我们不能确定，所以需要从checkpoint_lsn开始读取redo日志来恢复页面。 当然，redo日志文件组的第一个文件的管理信息中有两个block都存储了checkpoint_lsn的信息，我们当然是要选取最近发生的那次checkpoint的信息。衡量checkpoint发生时间早晚的信息就是所谓的checkpoint_no，我们只要把checkpoint1和checkpoint2这两个block中的checkpoint_no值读出来比一下大小，哪个的checkpoint_no值更大，说明哪个block存储的就是最近的一次checkpoint信息。这样我们就能拿到最近发生的checkpoint对应的checkpoint_lsn值以及它在redo日志文件组中的偏移量checkpoint_offset。 确定恢复的终点redo日志恢复的起点确定了，那终点是哪个呢？这个还得从block的结构说起。我们说在写redo日志的时候都是顺序写的，写满了一个block之后会再往下一个block中写： ![image_1d4viej35t9nvld8o3141s8pp.png-69.5kB][20] 普通block的log block header部分有一个称之为LOG_BLOCK_HDR_DATA_LEN的属性，该属性值记录了当前block里使用了多少字节的空间。对于被填满的block来说，该值永远为512。如果该属性的值不为512，那么就是它了，它就是此次奔溃恢复中需要扫描的最后一个block。 怎么恢复确定了需要扫描哪些redo日志进行奔溃恢复之后，接下来就是怎么进行恢复了。假设现在的redo日志文件中有5条redo日志，如图： ![image_1d4vjuf9l17og1papl3e16is1m9f16.png-59.9kB][21] 由于redo 0在checkpoint_lsn后边，恢复时可以不管它。我们现在可以按照redo日志的顺序依次扫描checkpoint_lsn之后的各条redo日志，按照日志中记载的内容将对应的页面恢复出来。这样没什么问题，不过设计InnoDB的大叔还是想了一些办法加快这个恢复的过程： 使用哈希表 根据redo日志的space ID和page number属性计算出散列值，把space ID和page number相同的redo日志放到哈希表的同一个槽里，如果有多个space ID和page number都相同的redo日志，那么它们之间使用链表连接起来，按照生成的先后顺序链接起来的，如图所示： ![image_1d50lj9da176rojd12ja1lodognc.png-156.4kB][22] 之后就可以遍历哈希表，因为对同一个页面进行修改的redo日志都放在了一个槽里，所以可以一次性将一个页面修复好（避免了很多读取页面的随机IO），这样可以加快恢复速度。另外需要注意一点的是，同一个页面的redo日志是按照生成时间顺序进行排序的，所以恢复的时候也是按照这个顺序进行恢复，如果不按照生成时间顺序进行排序的话，那么可能出现错误。比如原先的修改操作是先插入一条记录，再删除该条记录，如果恢复时不按照这个顺序来，就可能变成先删除一条记录，再插入一条记录，这显然是错误的。 跳过已经刷新到磁盘的页面 我们前边说过，checkpoint_lsn之前的redo日志对应的脏页确定都已经刷到磁盘了，但是checkpoint_lsn之后的redo日志我们不能确定是否已经刷到磁盘，主要是因为在最近做的一次checkpoint后，可能后台线程又不断的从LRU链表和flush链表中将一些脏页刷出Buffer Pool。这些在checkpoint_lsn之后的redo日志，如果它们对应的脏页在奔溃发生时已经刷新到磁盘，那在恢复时也就没有必要根据redo日志的内容修改该页面了。 那在恢复时怎么知道某个redo日志对应的脏页是否在奔溃发生时已经刷新到磁盘了呢？这还得从页面的结构说起，我们前边说过每个页面都有一个称之为File Header的部分，在File Header里有一个称之为FIL_PAGE_LSN的属性，该属性记载了最近一次修改页面时对应的lsn值（其实就是页面控制块中的newest_modification值）。如果在做了某次checkpoint之后有脏页被刷新到磁盘中，那么该页对应的FIL_PAGE_LSN代表的lsn值肯定大于checkpoint_lsn的值，凡是符合这种情况的页面就不需要做恢复操作了，所以更进一步提升了奔溃恢复的速度。 遗漏的问题：LOG_BLOCK_HDR_NO是如何计算的我们前边说过，对于实际存储redo日志的普通的log block来说，在log block header处有一个称之为LOG_BLOCK_HDR_NO的属性（忘记了的话回头再看看哈），我们说这个属性代表一个唯一的标号。这个属性是初次使用该block时分配的，跟当时的系统lsn值有关。使用下边的公式计算该block的LOG_BLOCK_HDR_NO值： 1((lsn / 512) &amp; 0x3FFFFFFFUL) + 1 这个公式里的0x3FFFFFFFUL可能让大家有点困惑，其实它的二进制表示可能更亲切一点： ![image_1d4rt3sm81pbe1tij3pm147op9c30.png-36.9kB][23] 从图中可以看出，0x3FFFFFFFUL对应的二进制数的前2位为0，后30位的值都为1。我们刚开始学计算机的时候就学过，一个二进制位与0做与运算（&amp;）的结果肯定是0，一个二进制位与1做与运算（&amp;）的结果就是原值。让一个数和0x3FFFFFFFUL做与运算的意思就是要将该值的前2个比特位的值置为0，这样该值就肯定小于或等于0x3FFFFFFFUL了。这也就说明了，不论lsn多大，((lsn / 512) &amp; 0x3FFFFFFFUL)的值肯定在00x3FFFFFFFUL之间，再加1的话肯定在10x40000000UL之间。而0x40000000UL这个值大家应该很熟悉，这个值就代表着1GB。也就是说系统最多能产生不重复的LOG_BLOCK_HDR_NO值只有1GB个。设计InnoDB的大叔规定redo日志文件组中包含的所有文件大小总和不得超过512GB，一个block大小是512字节，也就是说redo日志文件组中包含的block块最多为1GB个，所以有1GB个不重复的编号值也就够用了。 另外，LOG_BLOCK_HDR_NO值的第一个比特位比较特殊，称之为flush bit，如果该值为1，代表着本block是在某次将log buffer中的block刷新到磁盘的操作中的第一个被刷入的block。","link":"/2021/10/08/MySQL%EF%BC%88%E4%B9%9D%EF%BC%89%E9%87%8D%E5%81%9A%E6%97%A5%E5%BF%97/"},{"title":"Netty（五）锁和内存使用","text":"本篇讲述了Netty中对锁的正确使用和对内存的分配原则 Netty中如何正确的使用锁锁的对象和范围–减少粒度1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950=======ServerBootstrap=======@Override void init(Channel channel) throws Exception { final Map&lt;ChannelOption&lt;?&gt;, Object&gt; options = options0();//无线程安全问题 synchronized (options) { //针对这两种属性来锁 setChannelOptions(channel, options, logger); } final Map&lt;AttributeKey&lt;?&gt;, Object&gt; attrs = attrs0(); synchronized (attrs) { for (Entry&lt;AttributeKey&lt;?&gt;, Object&gt; e: attrs.entrySet()) { @SuppressWarnings(&quot;unchecked&quot;) AttributeKey&lt;Object&gt; key = (AttributeKey&lt;Object&gt;) e.getKey(); channel.attr(key).set(e.getValue()); } } ChannelPipeline p = channel.pipeline(); final EventLoopGroup currentChildGroup = childGroup; final ChannelHandler currentChildHandler = childHandler; final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] currentChildOptions; final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] currentChildAttrs; synchronized (childOptions) { currentChildOptions = childOptions.entrySet().toArray(newOptionArray(0)); } synchronized (childAttrs) { currentChildAttrs = childAttrs.entrySet().toArray(newAttrArray(0)); } p.addLast(new ChannelInitializer&lt;Channel&gt;() { @Override public void initChannel(final Channel ch) throws Exception { final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); if (handler != null) { pipeline.addLast(handler); } ch.eventLoop().execute(new Runnable() { @Override public void run() { pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); } }); } }); } 锁的对象本身大小–减少空间占用12345678910111213141516171819=====ChannelOutboundBuffer===== private static final AtomicLongFieldUpdater&lt;ChannelOutboundBuffer&gt; TOTAL_PENDING_SIZE_UPDATER = AtomicLongFieldUpdater.newUpdater(ChannelOutboundBuffer.class, &quot;totalPendingSize&quot;); private volatile long totalPendingSize; //统计待发送的字节数 private static final AtomicIntegerFieldUpdater&lt;ChannelOutboundBuffer&gt; UNWRITABLE_UPDATER = AtomicIntegerFieldUpdater.newUpdater(ChannelOutboundBuffer.class, &quot;unwritable&quot;); private void incrementPendingOutboundBytes(long size, boolean invokeLater) { if (size == 0) { return; } long newWriteBufferSize = TOTAL_PENDING_SIZE_UPDATER.addAndGet(this, size); if (newWriteBufferSize &gt; channel.config().getWriteBufferHighWaterMark()) { setUnwritable(invokeLater); } } Atomic long VS long： 前者是一个对象，包含对象头（object header）以用来保存 hashcode、lock 等信息，32 位系统占用8字节；64 位系统占 16 字节，所以在 64 位系统情况下： volatile long = 8 bytes AtomicLong = 8 bytes （volatile long）+ 16bytes （对象头）+ 8 bytes (引用) = 32 bytes至少节约 24 字节! 结论：Atomic* objects -&gt; Volatile primary type + Static Atomic*FieldUpdater 锁的速度–提高并发性记录内存分配字节数等功能用到的 LongCounter（io.netty.util.internal.PlatformDependent#newLongCounter() ） 123456789101112131415161718=====PlatformDependent=====public static LongCounter newLongCounter() { if (javaVersion() &gt;= 8) { //判断JDK的版本 return new LongAdderCounter(); } else { return new AtomicLongCounter(); } }//继承JDK中的 LongAdder，在高并发下比较优秀final class LongAdderCounter extends LongAdder implements LongCounter { @Override public long value() { return longValue(); }} 不同场景选择不同的并发包例1：关闭和等待关闭事件执行器（Event Executor）： Object.wait/notify -&gt; CountDownLatch 1234567=====SingleThreadEventExecutor=====private volatile ThreadProperties threadProperties; private final Executor executor; private volatile boolean interrupted; private final CountDownLatch threadLock = new CountDownLatch(1); private final Set&lt;Runnable&gt; shutdownHooks = new LinkedHashSet&lt;Runnable&gt;(); 例2：Nio Event loop中负责存储task的Queue Jdk’s LinkedBlockingQueue (MPMC) -&gt; jctools’ MPSC 123456789io.netty.util.internal.PlatformDependent.Mpsc#newMpscQueue(int)：static &lt;T&gt; Queue&lt;T&gt; newMpscQueue(final int maxCapacity) { // Calculate the max capacity which can not be bigger then MAX_ALLOWED_MPSC_CAPACITY. // This is forced by the MpscChunkedArrayQueue implementation as will try to round it // up to the next power of two and so will overflow otherwise. final int capacity = max(min(maxCapacity, MAX_ALLOWED_MPSC_CAPACITY), MIN_MAX_MPSC_CAPACITY); return USE_MPSC_CHUNKED_ARRAY_QUEUE ? new MpscChunkedArrayQueue&lt;T&gt;(MPSC_CHUNK_SIZE, capacity) : new MpscGrowableAtomicArrayQueue&lt;T&gt;(MPSC_CHUNK_SIZE, capacity); } 避免用锁 Netty 应用场景下：局部串行 + 整体并行 &gt; 一个队列 + 多个线程模式: 降低用户开发难度、逻辑简单、提升处理性能 避免锁带来的上下文切换和并发保护等额外开销 避免用锁：用 ThreadLocal 来避免资源争用，例如 Netty 轻量级的线程池实现 Netty如何使用内存减少对像本身大小 用基本类型就不要用包装类 应该定义成类变量的不要定义为实例变量 对分配内存进行预估 对于已经可以预知固定 size 的 HashMap避免扩容 1234567891011121314=====com.google.common.collect.Maps#newHashMapWithExpectedSize===== public static &lt;K, V&gt; HashMap&lt;K, V&gt; newHashMapWithExpectedSize(int expectedSize) { return new HashMap(capacity(expectedSize)); } static int capacity(int expectedSize) { if (expectedSize &lt; 3) { CollectPreconditions.checkNonnegative(expectedSize, &quot;expectedSize&quot;); return expectedSize + 1; } else { return expectedSize &lt; 1073741824 ? (int)((float)expectedSize / 0.75F + 1.0F) : 2147483647; } } Netty 根据接受到的数据动态调整（guess）下个要分配的 Buffer 的大小 123456789101112131415161718192021222324252627======io.netty.channel.AdaptiveRecvByteBufAllocator====== /** * 接受数据buffer的容量会尽可能的足够大以接受数据 * 也尽可能的小以不会浪费它的空间 * @param actualReadBytes */ private void record(int actualReadBytes) { //尝试是否可以减小分配的空间仍然能满足需求： //尝试方法：当前实际读取的size是否小于或等于打算缩小的尺寸 if (actualReadBytes &lt;= SIZE_TABLE[max(0, index - INDEX_DECREMENT - 1)]) { //decreaseNow: 连续2次尝试减小都可以 if (decreaseNow) { //减小 index = max(index - INDEX_DECREMENT, minIndex); nextReceiveBufferSize = SIZE_TABLE[index]; decreaseNow = false; } else { decreaseNow = true; } //判断是否实际读取的数据大于等于预估的，如果是，尝试扩容 } else if (actualReadBytes &gt;= nextReceiveBufferSize) { index = min(index + INDEX_INCREMENT, maxIndex); nextReceiveBufferSize = SIZE_TABLE[index]; decreaseNow = false; } } Zero-Copy零拷贝 使用逻辑组合，代替实际复制。 12345678910111213141516171819202122232425262728293031323334=====io.netty.handler.codec.ByteToMessageDecoder======public static final Cumulator COMPOSITE_CUMULATOR = new Cumulator() { @Override public ByteBuf cumulate(ByteBufAllocator alloc, ByteBuf cumulation, ByteBuf in) { ByteBuf buffer; try { if (cumulation.refCnt() &gt; 1) { buffer = expandCumulation(alloc, cumulation, in.readableBytes()); buffer.writeBytes(in); } else { CompositeByteBuf composite; //创建composite bytebuf，如果已经创建过，就不用了 if (cumulation instanceof CompositeByteBuf) { composite = (CompositeByteBuf) cumulation; } else { composite = alloc.compositeBuffer(Integer.MAX_VALUE); composite.addComponent(true, cumulation); } //避免内存复制 composite.addComponent(true, in); in = null; buffer = composite; } return buffer; } finally { if (in != null) { // We must release if the ownership was not transferred as otherwise it may produce a leak if // writeBytes(...) throw for whatever release (for example because of OutOfMemoryError). in.release(); } } } }; 使用包装，代替实际复制。 12byte[] bytes = data.getBytes(); ByteBuf byteBuf = Unpooled.wrappedBuffer(bytes); 调用 JDK 的 Zero-Copy 接口。 Netty 中也通过在 DefaultFileRegion 中包装了 NIO 的 FileChannel.transferTo() 方法实现了零拷贝：io.netty.channel.DefaultFileRegion#transferTo 1234567891011121314151617181920212223242526272829@Override public long transferTo(WritableByteChannel target, long position) throws IOException { long count = this.count - position; if (count &lt; 0 || position &lt; 0) { throw new IllegalArgumentException( &quot;position out of range: &quot; + position + &quot; (expected: 0 - &quot; + (this.count - 1) + ')'); } if (count == 0) { return 0L; } if (refCnt() == 0) { throw new IllegalReferenceCountException(0); } // Call open to make sure fc is initialized. This is a no-oop if we called it before. open(); long written = file.transferTo(this.position + position, count, target);//这里调用了 if (written &gt; 0) { transferred += written; } else if (written == 0) { // If the amount of written data is 0 we need to check if the requested count is bigger then the // actual file itself as it may have been truncated on disk. // // See https://github.com/netty/netty/issues/8868 validate(this, position); } return written; } 堆外内存 优点： 更广阔的“空间 ”，缓解店铺内压力 -&gt; 破除堆空间限制，减轻 GC 压力 减少“冗余”细节（假设烧烤过程为了气氛在室外进行：烤好直接上桌：vs 烤好还要进店内）-&gt; 避免复制 缺点： 需要搬桌子 -&gt; 创建速度稍慢 受城管管、风险大 -&gt; 堆外内存受操作系统管理 内存池为什么引入对象池： 创建对象开销大 对象高频率创建且可复用 支持并发又能保护系统 维护、共享有限的资源 如何实现对象池？ 开源实现：Apache Commons Pool Netty 轻量级对象池实现 io.netty.util.Recycler 源码解读12345678910111213141516171819202122232425262728293031323334353637383940//主从模型 EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); //自定义业务逻辑 ChannelHandler final EchoServerHandler serverHandler = new EchoServerHandler(); try { //构造引导器实例ServerBootstrap ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) //设置NIO的channel .option(ChannelOption.SO_BACKLOG, 100) .handler(new LoggingHandler(LogLevel.INFO)) //两种设置keepalive风格 .childOption(ChannelOption.SO_KEEPALIVE, true) .childOption(NioChannelOption.SO_KEEPALIVE, true) //切换到unpooled的方式之一,切换内存池参数之一 .childOption(ChannelOption.ALLOCATOR, UnpooledByteBufAllocator.DEFAULT) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override public void initChannel(SocketChannel ch) throws Exception { ChannelPipeline p = ch.pipeline(); if (sslCtx != null) { p.addLast(sslCtx.newHandler(ch.alloc())); } p.addLast(new LoggingHandler(LogLevel.INFO)); p.addLast(serverHandler); } }); // Start the server. ChannelFuture f = b.bind(PORT).sync(); // Wait until the server socket is closed. f.channel().closeFuture().sync(); } finally { // Shut down all event loops to terminate all threads. bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } 12345678910111213141516171819202122232425262728293031323334353637383940414243=====DefaultChannelConfig====== //默认bytebuf分配器 private volatile ByteBufAllocator allocator = ByteBufAllocator.DEFAULT;======ByteBufUtil====== private static final byte WRITE_UTF_UNKNOWN = (byte) '?'; private static final int MAX_CHAR_BUFFER_SIZE; private static final int THREAD_LOCAL_BUFFER_SIZE; private static final int MAX_BYTES_PER_CHAR_UTF8 = (int) CharsetUtil.encoder(CharsetUtil.UTF_8).maxBytesPerChar(); static final int WRITE_CHUNK_SIZE = 8192; static final ByteBufAllocator DEFAULT_ALLOCATOR; static { //以io.netty.allocator.type为准，没有的话，安卓平台用非池化实现，其他用池化实现 //读取io.netty.allocator.typ，如果没有的话就看是不是安卓平台 String allocType = SystemPropertyUtil.get( &quot;io.netty.allocator.type&quot;, PlatformDependent.isAndroid() ? &quot;unpooled&quot; : &quot;pooled&quot;); allocType = allocType.toLowerCase(Locale.US).trim(); ByteBufAllocator alloc; if (&quot;unpooled&quot;.equals(allocType)) { alloc = UnpooledByteBufAllocator.DEFAULT; logger.debug(&quot;-Dio.netty.allocator.type: {}&quot;, allocType); } else if (&quot;pooled&quot;.equals(allocType)) { alloc = PooledByteBufAllocator.DEFAULT; logger.debug(&quot;-Dio.netty.allocator.type: {}&quot;, allocType); } else { //io.netty.allocator.type设置的不是&quot;unpooled&quot;或者&quot;pooled&quot;，就用池化实现。 alloc = PooledByteBufAllocator.DEFAULT; logger.debug(&quot;-Dio.netty.allocator.type: pooled (unknown: {})&quot;, allocType); } DEFAULT_ALLOCATOR = alloc; //默认使用了池化的技术 THREAD_LOCAL_BUFFER_SIZE = SystemPropertyUtil.getInt(&quot;io.netty.threadLocalDirectBufferSize&quot;, 0); logger.debug(&quot;-Dio.netty.threadLocalDirectBufferSize: {}&quot;, THREAD_LOCAL_BUFFER_SIZE); MAX_CHAR_BUFFER_SIZE = SystemPropertyUtil.getInt(&quot;io.netty.maxThreadLocalCharBufferSize&quot;, 16 * 1024); logger.debug(&quot;-Dio.netty.maxThreadLocalCharBufferSize: {}&quot;, MAX_CHAR_BUFFER_SIZE); } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061===== PooledDirectByteBuf ===== final class PooledDirectByteBuf extends PooledByteBuf&lt;ByteBuffer&gt; { private static final Recycler&lt;PooledDirectByteBuf&gt; RECYCLER = new Recycler&lt;PooledDirectByteBuf&gt;() { @Override protected PooledDirectByteBuf newObject(Handle&lt;PooledDirectByteBuf&gt; handle) { return new PooledDirectByteBuf(handle, 0); } }; //从“池”里借一个用 static PooledDirectByteBuf newInstance(int maxCapacity) { PooledDirectByteBuf buf = RECYCLER.get(); buf.reuse(maxCapacity); return buf; } ===== RECYCLER ===== public final T get() { if (maxCapacityPerThread == 0) { //表明没有开启池化 return newObject((Handle&lt;T&gt;) NOOP_HANDLE); } Stack&lt;T&gt; stack = threadLocal.get(); DefaultHandle&lt;T&gt; handle = stack.pop(); //试图从“池”中取出一个，没有就新建一个 if (handle == null) { handle = stack.newHandle(); handle.value = newObject(handle); } return (T) handle.value; } static final class DefaultHandle&lt;T&gt; implements Handle&lt;T&gt; { private int lastRecycledId; private int recycleId; boolean hasBeenRecycled; private Stack&lt;?&gt; stack; private Object value; DefaultHandle(Stack&lt;?&gt; stack) { this.stack = stack; } @Override public void recycle(Object object) { if (object != value) { throw new IllegalArgumentException(&quot;object does not belong to handle&quot;); } Stack&lt;?&gt; stack = this.stack; if (lastRecycledId != recycleId || stack == null) { throw new IllegalStateException(&quot;recycled already&quot;); } //释放用完的对象到池里面去 stack.push(this); } } 如何切换堆内内存和堆外内存方法 1：参数设置 io.netty.noPreferDirect = true; 方法 2：传入构造参数false ServerBootstrap serverBootStrap = new ServerBootstrap(); UnpooledByteBufAllocator unpooledByteBufAllocator = new UnpooledByteBufAllocator(false); serverBootStrap.childOption(ChannelOption.ALLOCATOR, unpooledByteBufAllocator) 123456789101112public static final UnpooledByteBufAllocator DEFAULT = new UnpooledByteBufAllocator(PlatformDependent.directBufferPreferred());====== PlatformDependent ======// We should always prefer direct buffers by default if we can use a Cleaner to release direct buffers. //使用堆外内存两个条件：1 有cleaner方法去释放堆外内存； 2 io.netty.noPreferDirect 不能设置为true DIRECT_BUFFER_PREFERRED = CLEANER != NOOP &amp;&amp; !SystemPropertyUtil.getBoolean(&quot;io.netty.noPreferDirect&quot;, false);//参数指定 if (logger.isDebugEnabled()) { logger.debug(&quot;-Dio.netty.noPreferDirect: {}&quot;, !DIRECT_BUFFER_PREFERRED); } 堆外内存的分配？ ByteBuffer.allocateDirect(initialCapacity)","link":"/2021/05/25/Netty%EF%BC%88%E4%BA%94%EF%BC%89%E9%94%81%E5%92%8C%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8/"},{"title":"Netty（四）编解码技术","text":"本篇主要讲述了 二次编码 keepalive和idle检测 为什么需要二次编码我们把解决半包和粘包问题的常用三种解码器叫做一次解码器，这层解码的结果是字节。 我们在项目中使用的是对象，需要和字节进行相互转换，所以二次解码器就是将字节转换成对象。相对应的编码器就是将Java对象转换成字节流方便存储或传输。 一次解码器：ByteToMessageDecoder io.netty.buffer.ByteBuf(原始数据流,可能出现粘包或半包) –&gt; io.netty.buffer.ByteBuf(用户数据的字节数组) 二次解码器：MessageToMessageDecoder io.netty.buffer.ByteBuf(用户数据的字节数组) –&gt; java Object 常用的二次编解码方式 Java序列号 XML JSON ProtoBuf Marshaling 选择编解码方式的特点 压缩后的空间大小 编解码的时间速度 可读性 多语言的支持 Protobuf的简介和使用源码解读12345678public class ByteArrayDecoder extends MessageToMessageDecoder&lt;ByteBuf&gt; { @Override protected void decode(ChannelHandlerContext ctx, ByteBuf msg, List&lt;Object&gt; out) throws Exception { // copy the ByteBuf content to a byte array //将netty的ByteBuf 转换成JDK中的字节数组 out.add(ByteBufUtil.getBytes(msg)); }} Keepalive和Idle检测为什么需要keepalive生活场景类比： 订餐电话场景 服务器应用 电话线路 数据连接（TCP 连接） 交谈的话语 数据 通话双方 数据发送和接收方 对比\\场景 订餐电话场景 服务器应用 需要 keepalive 的场景 对方临时着急走开 对端异常“崩溃” 对方在，但是很忙，不知道什么时候忙完 对端在，但是处理不过来 电话线路故障 对端在，但是不可达 不做 keepalive 的后果 线路占用，耽误其他人订餐 连接已坏，但是还浪费资源维持，下次直接用会直接报错 如何设计keepalive？以TCP中为例TCP keepalive 核心参数： # sysctl -a|grep tcp_keepalive net.ipv4.tcp_keepalive_time = 7200 net.ipv4.tcp_keepalive_intvl = 75 net.ipv4.tcp_keepalive_probes = 9 当启用（默认关闭）keepalive 时，TCP 在连接没有数据通过的7200秒后发送 keepalive 消息，当探测没有确认时，按75秒的重试频率重发，一直发 9 个探测包都没有确认，就认定连接失效。 所以总耗时一般为：2 小时 11 分钟 (7200 秒 + 75 秒* 9 次) 为什么应用层还需要keepalive ? 协议分层，各层关注点不同：传输层关注是否“通”，应用层关注是否可服务？ 类比前面的电话订餐例子，电话能通，不代表有人接；服务器连接在，但是不定可以服务（例如服务不过来等）。 TCP 层的 keepalive 默认关闭，且经过路由等中转设备 keepalive 包可能会被丢弃。 TCP 层的 keepalive 时间太长：默认 &gt; 2 小时，虽然可改，但属于系统参数，改动影响所有应用。 HTTP 属于应用层协议，但是常常听到名词“ HTTP Keep-Alive ”指的是对长连接和短连接的选择： • Connection : Keep-Alive 长连接（HTTP/1.1 默认长连接，不需要带这个 header） • Connection : Close 短连接 idle检测是什么 Idle 监测，只是负责诊断，诊断后，做出不同的行为，决定 Idle 监测的最终用途： 发送 keepalive :一般用来配合 keepalive ，减少 keepalive 消息。 Keepalive 设计演进：V1 定时 keepalive 消息 -&gt; V2 空闲监测 + 判定为 Idle 时才发keepalive。 V1：keepalive 消息与服务器正常消息交换完全不关联，定时就发送； V2：有其他数据传输的时候，不发送 keepalive ，无数据传输超过一定时间，判定为 Idle，再发 keepalive 。 如何开启keepalive和idle1234567891011在server端： //构造引导器实例ServerBootstrap ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) //设置NIO的channel .option(ChannelOption.SO_BACKLOG, 100) .handler(new LoggingHandler(LogLevel.INFO)) //两种设置keepalive风格 .childOption(ChannelOption.SO_KEEPALIVE, true) .childOption(NioChannelOption.SO_KEEPALIVE, true)","link":"/2021/05/25/Netty%EF%BC%88%E5%9B%9B%EF%BC%89%E7%BC%96%E8%A7%A3%E7%A0%81%E6%8A%80%E6%9C%AF/"},{"title":"Redis（一）数据结构类型","text":"Redis中的数据结构和对象系统！ 数据结构简单动态字符串SDS结构解析Redis中自己实现了简单动态字符串：分为三部分 12345678struct sdshdr{ int len; // 记录buf数组中已经使用的字节数量 = sds保存字符串的长度 int free; // 记录buf数组中没有使用的字节数量 char buf[]; // 字节数组，用于保存字符串} 比如保存 Redis这个字符串，保存形式为: R , e , d , i , s ,\\0 。 最后一个字节保存了空字符 ‘\\0’; free = 0 len = 5 Buf = [‘R’ , ‘e’ , ‘d’ , ‘i’ , ‘s’ , ‘\\0’] ps: 最后一个空字符不计算在len属性中。这个空字符对于使用者是完全透明的，只是为了重用C字符串中的一些库函数。 sds和c字符串的区别c语言是使用长度为N+1的字符数组来保存长度为N的字符串，且最后一个元素总是空字符 ‘\\0’ 。 sds可以常数的复杂度获取字符串的长度 ，直接读len属性即可，而C字符串需要遍历整个字符串才行。 杜绝了缓冲区溢出： C语言中进行字符串拼接需要先分配足够的内存，否则会发生缓冲区溢出。而SDS中的API会自动的先检查空间是否足够，因为记录了字符串长度，所以比较快。（如果空间不够，不仅会分配足够的空间，还会分配足够的未使用空间） 减少修改字符串带来的内存重分配次数： C字符串的长度和底层数组之间存在关系，所以每次增长或缩短C字符串，都会进行一次内存重分配操作： 如果是增长字符串的操作，需要先通过内存重分配来扩展底层数组的大小，否则会产生缓冲区溢出。 如果是缩短字符串的操作。操作之后需要通过内存重分配来释放字符串不再使用的空间，否则会产生内存泄漏 SDS通过未使用空间来解耦了字符串长度和底层数组的关系：buf数组的长度不一定就是字符数量+1 空间预分配策略：用于优化SDS的字符串增长操作，当空间扩展时不仅分配SDS所必须要的空间，还会分配额外的未使用空间。至于分配多少未使用空间 ，由公司决定，见书籍。 惰性空间释放策略：用于优化SDS的字符串缩短操操作，并不释放多出来的字节。而是用free属性记录下来留待将来使用。通过这个策略避免了缩短字符串带来的内存重分配操作，并为将来的增长操作提供了优化。 ps：sds提供了相应的api让我们有需要时是否未使用空间。 二进制安全：由于空字符被认为是C字符串的末尾，导致C字符串只能保存文本数据。 兼容部分C字符串函数。 链表结构解析123456789101112131415161718192021// 链表节点typedef struct listNode{ struct ListNode *prev; // 前置节点 struct ListNode *next; // 后置节点 void *value; // 节点的值 }// 链表typedef struct list{ ListNode *head; // 表头节点 ListNode *tail; // 表尾节点 unsigned long len; // 节点个数 /** 节点值复制函数 节点值释放函数 节点值对比函数 */} 特性总结： 双向链表 没有环 带表头和表尾指针 带链表长度计数器 链表可以保存不同类型的值 字典实现解析123456789101112131415161718192021//哈希表typedef struct dictht{ dictEntry **table; // 哈希表数组 unsigned long size; // 哈希表大小 unsigned long sizemask; // 哈希表大小掩码，用于计算索引值，总是 = size-1 unsigned long used; // 改哈希表已经使用节点的数量}// 哈希表节点用数组来表示，每个dictEntry结构都保存了一个键值对：typedef struct dictEntry{ void *key; // 键 union{ void *val; // 值 } struct dictEntry *next; // 指向下一个哈希表节点，形成链表。这个指针可以将多个哈希值相同的键值对连接在一起来解决键冲突的问题。} 哈希算法当需要添加新的键值对时，需要先根据键来计算出哈希值和索引值，然后再根据索引值将包含新的键值对的哈希表节点放到哈希表数组的指定索引上。 解决冲突当两个及以上数量的键被分配到了哈希表数组的同一个索引上时，趁这些键发生了哈希冲突。 如何解决：链地址法 每个哈希节点都有一个Next指针，被分配到同一个索引上的多个节点可以用这个单向链表连接起来。 rehash渐进式rehashRedis 会对哈希表做 rehash 操作。rehash 也就是增加现有的哈希桶数量，让逐渐增多的 entry 元素能在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个桶中的冲突。 其实，为了使 rehash 操作更高效，Redis 默认使用了两个全局哈希表：哈希表 1 和哈希表 2。一开始，当你刚插入数据时，默认使用哈希表 1，此时的哈希表 2 并没有被分配空间。随着数据逐步增多，Redis 开始执行 rehash，这个过程分为三步： 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍； 把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中； 释放哈希表 1 的空间。 第二步涉及大量的数据拷贝，如果一次性把哈希表 1 中的数据都迁移完，会造成 Redis 线程阻塞，无法服务其他请求。此时，Redis 就无法快速访问数据了。为了避免这个问题，Redis 采用了渐进式 rehash： 简单来说就是在第二步拷贝数据时，Redis 仍然正常处理客户端请求，每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝到哈希表 2 中；等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的entries。 把一次性大量拷贝的开销，分摊到了多次处理请求的过程中，避免了耗时操作，保证了数据的快速访问。 跳表有序链表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表。具体来说，跳表在链表的基础上，增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位。 整数集合intset实现解析当一个集合中只包含整数值元素且集合元素不多时，就会使用intset作为底层实现。 升级降级压缩列表ziplist压缩列表实际上类似于一个数组，数组中的每一个元素都对应保存一个数据。和数组不同的是，压缩列表在表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的偏移量和列表中的 entry 个数；压缩列表在表尾还有一个 zlend，表示列表结束。 在压缩列表中，如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段的长度直接定位，复杂度是 O(1)。而查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O(N) 了。 压缩列表是一种非常紧凑的数据结构，占用的内存比链表要少。 RedisObject对象Redis并没有直接使用前面介绍的这些数据结构，而是构建了一个对象系统。这个系统包含字符串对象、列表对象、哈希对象、集合对象和有序集合对象5种，每种对象至少用到了一种数据结构。 使用对象的好处有： 根据对象的类型来判断一个对象是否可以执行给定的命令。 针对不同的使用场景，为对象设置多种不同的数据结构实现，从而优化对象在不同场景的使用效率。 对象系统还基于引用计数技术的内存回收机制，当程序不再使用某个对象时，这个对象占用的内存会自动释放。 对象类型和编码Redis使用对象来表示数据库中的键和值，每次当我们在Redis的数据库中新创建一个键值对时，我们至少会创建两个对象：键对象 和 值对象。（键对象总是一个字符串对象，而值对象有多种） 12set msg &quot;hello world&quot;;就是两个字符串对象 每个对象都用redisObject来表示： 12345typedef struct redisObject{ unsigned type:4; //类型 unsigned encoding:4; //编码 void *ptr; // 指向底层实现数据结构的指针} 1234567891011121314151617181920212223242526//当我们对一个数据库键执行 type命令时，返回的是值对象的类型Welcome: set msg helloWorldlocalhost: OKlocalhost: type msglocalhost: string // 字符串对象localhost: rpush numbers 135localhost: (int)1localhost: type numberslocalhost: list // 列表对象localhost: hmset profile name tom age 25 localhost: OKlocalhost: type profilelocalhost: hash // 哈希对象localhost: sadd fruits apple banana cherrylocalhost: (int)3localhost: type fruitslocalhost: set // 集合对象localhost: zadd price 8.5 apple 5.0 banana 6.0 cherrylocalhost: (int)3localhost: type price localhost: zset // 有序集合对象 对象的编码及对应的底层数据结构： 可以看出，每种类型的对象都至少使用了两种不同的编码。 使用 Object encoding 可以查看键的值对象的编码： 1234567891011121314151617181920localhost: set msg &quot;hello world&quot;localhost: OKlocalhost: object encoding msglocalhost: embstrlocalhost: set story &quot;long long ago&quot;localhost: OKlocalhost: object encoding storylocalhost: embstrlocalhost: set ss &quot;long long long long long long long long long ago ...&quot;localhost: OKlocalhost: object encoding sslocalhost: rawlocalhost: sadd nums 1 3 5localhost: (int)3localhost: object encoding numslocalhost: intsetlocalhost: Redis可以通过不同的使用场景来为一个对象设置不同的编码，从而优化对象在某一个场景下的效率。 比如： 在列表对象包含的元素比较少时，Redis使用压缩列表作为列表对象的底层实现:因为压缩列表比双端链表更节约内存，并且在元素数量较少时，在内存中以连续块方式保存的压缩列表比起双端链表可以更快被载人到缓存中; 随着列表对象包含的元素越来越多，使用压缩列表来保存元素的优势逐渐消失时，对象就会将底层实现从压缩列表转向功能更强、也更适合保存大量元素的双端链表上面; 字符串对象编码方式有： int 、 raw 或者 embstr。 如果一个字符串对象保存的是整数值，并且这个整数值可以用long类型来表示，那么字符串对象会将整数值保存在字符串对象结构的ptr属性里面(将void*转换成long),并将字符串对象的编码设置为int。 如果一个字符串对象保存的是一个字符串值，并且这个字符串值的长度大于32字节，那么就会用一个sds来保存，并将对象的编码设置为raw。 如果一个字符串对象保存的是一个字符串值，并且这个字符串值的长度小于32字节，那么就会用embstr编码的方式来保存这个字符串的值。 12345678910111213141516localhost: set number 10086localhost: OKlocalhost: object encoding numberlocalhost: intlocalhost: set story &quot;Long,long,long ago there lived a king ...&quot;localhost: OKlocalhost: object encoding storylocalhost: embstrlocalhost: set ss &quot;long long long long long long long long long ago ...&quot;localhost: OKlocalhost: object encoding sslocalhost: rawlocalhost: strlen storylocalhost: (int)41 embstr编码是专门用于保存短字符串的一种优化编码方式，这种编码和raw编码一样，都使用redi sObj ect结构和sdshdr结构来表示字符串对象，但raw编码会调用两次内存分配函数来分别创建redisObj ect结构和sdshdr结构，而embstr编码则通过调用一次内存分配函数来分配一块连续的空间，空间中依次包含redi sObj ect和sdshdr两个结构，使用embstr编码 有以下好处： embstr编码将创建字符串对象所需的内存分配次数从raw编码的两次降低为一次。 释放embstr编码的字符串对象只需要调用一次内存释放函数，而释放raw编码的字符串对象需要调用两次内存释放函数 字符串对象的所有数据都保存在一块连续的内存里面，所以这种编码的字符串对象比起raw编码的字符串对象能够更好地利用缓存带来的优势。 1234localhost: set msg hellolocalhost: OKlocalhost: object encoding msglocalhost: embstr 浮点数也是通过 embstr编码的字符串格式来保存的。但是做运算的时候会先将字符串转换成浮点数，执行操作后的结果再转换回字符串。 1234567891011localhost: set pi 3.14localhost: OKlocalhost: object encoding pilocalhost: embstrlocalhost: localhost: localhost: incrbyfloat pi 2.0localhost: 5.14000000000000057localhost: object encoding pilocalhost: embstrlocalhost: 列表对象编码方式可以为 ziplist 或者 linkedlist 哈希对象集合对象编码方式可以是intset或者hashtable。 有序集合对象内存回收Redis在自己的对象系统中构建了一个引用计数的方式来实现内存回收机制。 1234typedef struct redisObject{ //... int refcount; //引用计数} 在创建一个新对象时，引用计数的值会被初始化为1; 当对象被一个新程序使用时，它的引用计数值会被增1; 当对象不再被一个程序使用时，它的引用计数值会被减1; 当对象的引用计数值变为0时，对象所占用的内存会被释放。 引用计数还有对象共享的功能： 在Redis中，让多个键共享同-个值对象需要执行以下两个步骤: 将数据库键的值指针指向一个现有的值对象; 将被共享的值对象的引用计数增1 目前来说，Redis会在初始化服务器时，创建- - 万个字符串对象，这些对象包含了从0到9999的所有整数值，当服务器需要用到值为0到9999的字符串对象时，服务器就会使用这些共享对象，而不是新创建对象。 1234localhost: set a 100localhost: OKlocalhost: object refcount alocalhost: (int)2147483647 对象的空转时长1234typedef struct redisObject{ //... unsigned lru:22; // 该属性记录了对象最后一次被命令程序访问的时间} Object idletime 可以打印出给定键的空转时长，就是通过当前时间减去值对象的lru时间计算出来的。 12345678910localhost: set msg &quot;helloWorld&quot;localhost: OKlocalhost: object idletime msglocalhost: (int)18localhost: object idletime msglocalhost: (int)41localhost: get msglocalhost: helloWorldlocalhost: object idletime msglocalhost: (int)7 除了可以被OBJECT IDLETIME命令打印出来之外，键的空转时长还有另外- -项作用:如果服务器打开了maxmemory选项，并且服务器用于回收内存的算法为volatile- lru或者allkeys-1ru,那么当服务器占用的内存数超过了maxmemory选项所设置的上限值时，空转时长较高的那部分键会优先被服务器释放,从而回收内存。 配置文件的maxmemory选项和maxmemory-pol icy选项的说明介绍了关于这方面的更多信息。","link":"/2021/05/13/Redis%EF%BC%88%E4%B8%80%EF%BC%89%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%B1%BB%E5%9E%8B/"},{"title":"Redis（七）统计模式和扩展数据类型","text":"围绕这四种场景： 手机 App 中的每天的用户登录信息：一天对应一系列用户 ID 或移动设备 ID； 电商网站上商品的用户评论列表：一个商品对应了一系列的评论； 用户在手机 App 上的签到打卡信息：一天对应一系列用户的签到记录； 应用网站上的网页访问信息：一个网页对应一系列的访问点击。 集合统计模式聚合统计Set所谓的聚合统计，就是指统计多个集合元素的聚合结果，包括： 统计多个集合的共有元素（交集统计）； 把两个集合相比，统计其中一个集合独有的元素（差集统计）； 统计多个集合的所有元素（并集统计）。 统计每天的新增用户数和第二天的留存用户数，正好对应了聚合统计，用两个集合：一个集合记录所有登录过 App 的用户 ID，同时，用另一个集合记录每一天登录过 App 的用户 ID。然后，再对这两个集合做聚合统计。 举例：userid 为key ，value 为累积所有用户的id userid+日期为可以，value为当日用户id 12345678910111213141516171819//redis127.0.0.1:6379&gt; sadd userid 10001 10002 // 向 userid 的集合里面加上10001，10002(integer) 2127.0.0.1:6379&gt; scard userid //目前累积用户id为2个(integer) 2127.0.0.1:6379&gt; sadd userid:20210515 10023 10024 10025 //20210515的时候有三个用户id访问了，放入一个集合中(integer) 3127.0.0.1:6379&gt; sdiff userid userid:20210515 //比较两个集合的差异1) &quot;10001&quot;2) &quot;10002&quot;127.0.0.1:6379&gt; sunionstore userid userid userid:20210515 //将userid和userid:20210515的集合取并集放入userid中(integer) 5 //目前累积用户数为5个了127.0.0.1:6379&gt; scard userid(integer) 5127.0.0.1:6379&gt; sadd userid:20210516 10001 10023 10025 10031 10032 //20210516又有这些id访问了(integer) 5127.0.0.1:6379&gt; sdiffstore user:new userid userid:20210516 //求0516的新增用户数：取差集并保存在user:new 的集合中(integer) 2127.0.0.1:6379&gt; Set 的差集、并集和交集的计算复杂度较高，在数据量较大的情况下，如果直接执行这些计算，会导致 Redis 实例阻塞。所以，我给你分享一个小建议：你可以从主从集群中选择一个从库，让它专门负责聚合计算，或者是把数据读取到客户端，在客户端来完成聚合统计，这样就可以规避阻塞主库实例和其他从库实例的风险了。 排序统计Sorted Set最新评论列表包含了所有评论中的最新留言，这就要求集合类型能对元素保序，也就是说，集合中的元素可以按序排列，这种对元素保序的集合类型叫作有序集合。 List 是按照元素进入 List 的顺序进行排序的，而 Sorted Set 可以根据元素的权重来排序，我们可以自己来决定每个元素的权重值。比如说，我们可以根据元素插入 Sorted Set的时间确定权重值，先插入的元素权重小，后插入的元素权重大。 List12345678910111213141516171819127.0.0.1:6379&gt; lpush product1 A B C D E F //product1 商品有6个评论分别为 A B C D E F(integer) 5127.0.0.1:6379&gt; lrange product1 0 2 //展示第一页的 3 个评论时1) &quot;F&quot;2) &quot;E&quot;3) &quot;D&quot;127.0.0.1:6379&gt; lrange product1 3 5 //获取第二页的 3 个评论，1) &quot;C&quot;2) &quot;B&quot;3) &quot;A&quot;//此时如果新添加了一个评论G127.0.0.1:6379&gt; lpush product1 G(integer) 7127.0.0.1:6379&gt; lrange product1 3 5 ////获取第二页的 3 个评论就发生变化了1) &quot;D&quot;2) &quot;C&quot;3) &quot;B&quot;127.0.0.1:6379&gt; Sorted Set按评论时间的先后给每条评论设置一个权重值，然后再把评论保存到 Sorted Set中。 1ZRANGEBYSCORE comments N-9 N //返回最新的10条评论 所以，在面对需要展示最新列表、排行榜等场景时，如果数据更新频繁或者需要分页显示，建议你优先考虑使用 Sorted Set。 二值状态统计BitMap在签到统计时，每个用户一天的签到用 1 个 bit 位就能表示，一个月（假设是 31 天）的签到情况用 31 个 bit 位就可以，而一年的签到也只需要用 365 个 bit 位，根本不用太复杂的集合类型。这个时候，我们就可以选择 Bitmap。这是 Redis 提供的扩展数据类型。我来给你解释一下它的实现原理。 基数统计","link":"/2021/05/15/Redis%EF%BC%88%E4%B8%83%EF%BC%89%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%92%8C%E6%89%A9%E5%B1%95%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"},{"title":"Netty（六）流程源码解析","text":"本篇讲述了Netty启动过程中的源码分析： 启动服务 构建连接 接受数据 业务处理 发送数据 断开连接 关闭服务 实例代码1234567891011121314151617181920212223242526272829303132333435363738394041424344public class EchoServer { private int port; public EchoServer(int port) { this.port = port; } public static void main(String[] args) throws Exception { new EchoServer(8833).start(); } public void start() throws Exception { //1.Reactor模型的主、从多线程 EventLoopGroup mainGroup = new NioEventLoopGroup(); EventLoopGroup childGroup = new NioEventLoopGroup(); try { //2.构造引导器实例ServerBootstrap ServerBootstrap b = new ServerBootstrap(); b.group(mainGroup, childGroup) .channel(NioServerSocketChannel.class) //2.1 设置NIO的channel .option(ChannelOption.SO_BACKLOG, 1024) .localAddress(new InetSocketAddress(port)) //2.2 配置本地监听端口 .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { //2.3 初始化channel的时候，配置Handler @Override protected void initChannel(final SocketChannel socketChannel) { socketChannel.pipeline() .addLast(&quot;codec&quot;, new HttpServerCodec()) .addLast(&quot;compressor&quot;, new HttpContentCompressor()) .addLast(&quot;aggregator&quot;, new HttpObjectAggregator(65536)) .addLast(&quot;handler&quot;, new EchoServerHandler()); //2.4 加入自定义业务逻辑ChannelHandler } }); ChannelFuture f = b.bind().sync(); //3.启动监听 System.out.println(&quot;Http Server started， Listening on &quot; + port); f.channel().closeFuture().sync(); } finally { mainGroup.shutdownGracefully().sync(); childGroup.shutdownGracefully().sync(); } }} 启动服务 创建NioEventLoopGroup12EventLoopGroup bossGroup = new NioEventLoopGroup(); //bossGroup 负责的是接受请求EventLoopGroup workerGroup = new NioEventLoopGroup();//workerGroup 负责的是处理请求 EventLoopGroup 就是一个死循环，不停地检测IO事件，处理IO事件，执行任务。 创建selector1Selector selector = sun.nio.ch.SelectorProviderImpl.openSelector() ServerBootstrap 参数配置group()方法传入Reactor模型通过 ServerBootstrap 的方法 group() 传入之后，会设置成为 ServerBootstrap 的 parentGroup 和 childGroup 12 ServerBootstrap b = new ServerBootstrap();b.group(mainGroup, childGroup) 123456789101112131415161718192021=====ServerBootstrap=====public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) { super.group(parentGroup);//切换到 AbstractBootstrap ObjectUtil.checkNotNull(childGroup, &quot;childGroup&quot;); if (this.childGroup != null) { throw new IllegalStateException(&quot;childGroup set already&quot;); } this.childGroup = childGroup; return this; }//AbstractBootstrap 需要使用 parentGroup 来注册监听外部请求（OP_ACCEPT 事件）的 Channel=====AbstractBootstrap=====public B group(EventLoopGroup group) { ObjectUtil.checkNotNull(group, &quot;group&quot;); if (this.group != null) { throw new IllegalStateException(&quot;group set already&quot;); } this.group = group; return self(); } 设置服务端Channel1.channel(NioServerSocketChannel.class) 12345678910111213141516171819这个 channel 实际上是 AbstractBootstrap 抽象类的方法，因为这个是可以共用的。然后 AbstractBootstrap 会交给其成员函数 channelFactory()，通过实例化参数来控制怎么根据 class 来生成 channel。=====AbstractBootstrap=====public B channel(Class&lt;? extends C&gt; channelClass) { return channelFactory(new ReflectiveChannelFactory&lt;C&gt;( ObjectUtil.checkNotNull(channelClass, &quot;channelClass&quot;) )); }=====ReflectiveChannelFactory=====public ReflectiveChannelFactory(Class&lt;? extends T&gt; clazz) { ObjectUtil.checkNotNull(clazz, &quot;clazz&quot;); try { this.constructor = clazz.getConstructor(); //通过反射获取构造器，再创建channel实例 } catch (NoSuchMethodException e) { throw new IllegalArgumentException(&quot;Class &quot; + StringUtil.simpleClassName(clazz) + &quot; does not have a public non-arg constructor&quot;, e); } } 对于服务端来说，创建的一般是public io.netty.channel.socket.nio.NioServerSocketChannel() option方法设置TCP参数1.option(ChannelOption.SO_BACKLOG, 1024)//主要是设置 TCP 的 backlog 参数，这个设置之后主要是调用底层 C 对应的接口 为什么要设置这个参数呢？这个参数实际上指定内核为此套接口排队的最大连接个数。对于给定的套接字接口，内核要维护两个对列：未连接队列和已连接队列。那是因为在 TCP 的三次握手过程中三个分节来分隔这两个队列。下面是整个过程的一个讲解： 如果服务器处于 listen 时，收到客户端的 syn 分节（connect）时在未完成队列中创建一个新的条目，然后用三路握手的第二个分节即服务器的 syn 响应客户端。 新条目会直到第三个分节到达前（客户端对服务器 syn 的 ack）都会一直保留在未完成连接队列中，如果三路握手完成，该条目将从未完成队列搬到已完成队列的尾部。 当进程调用 accept 时，从已完成队列的头部取一条目给进程，当已完成队列为空的时候进程就睡眠，直到有条目在已完成连接队列中才唤醒。 现在说到了重点，backlog 其实是两个队列的总和的最大值，大多数实现默认值为 5。但是高并发的情况之下，并不够用。因为可能客户端 syn 的到达以及等待三路握手第三个分节的到达延时而增大。 所以我们需要根据实际场景和网络状况进行灵活配置 设置服务端的 Handler123456789101112//父类中的 Handler 是客户端新接入的连接 SocketChannel 对应的 ChannelPipeline 的 Handler.handler(new LoggingHandler(LogLevel.INFO)) //子类中的 Handler 是 NioServerSocketChannel 对应的 ChannelPipeline 的 Handler .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { //2.3 初始化channel的时候，配置Handler @Override protected void initChannel(final SocketChannel socketChannel) { socketChannel.pipeline() .addLast(&quot;codec&quot;, new HttpServerCodec()) .addLast(&quot;compressor&quot;, new HttpContentCompressor()) .addLast(&quot;aggregator&quot;, new HttpObjectAggregator(65536)) .addLast(&quot;handler&quot;, new EchoServerHandler()); //2.4 加入自定义业务逻辑ChannelHandler } 上面代码有两个 handler 方法，区别在于 handler() 方法是 NioServerSocketChannel 使用的，所有连接该监听端口的客户端都会执行它；父类 AbstractBootstrap 中的 Handler 是个工厂类，它为每个接入的客户端都创建一个新的 Handler。 绑定本地端口然后启动服务1ChannelFuture f = b.bind().sync(); //3.启动监听 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151=====AbstractBootstrap=====public ChannelFuture bind() { validate();//验证服务启动需要的必要参数 SocketAddress localAddress = this.localAddress; if (localAddress == null) { throw new IllegalStateException(&quot;localAddress not set&quot;); } return doBind(localAddress); } private ChannelFuture doBind(final SocketAddress localAddress) { final ChannelFuture regFuture = initAndRegister(); //初始化一个 channel， final Channel channel = regFuture.channel(); //获取 channel if (regFuture.cause() != null) { return regFuture; } if (regFuture.isDone()) {//如果这个 channel 的注册事件完成了 // At this point we know that the registration was complete and successful. //再产生一个异步任务，进行端口监听 ChannelPromise promise = channel.newPromise(); doBind0(regFuture, channel, localAddress, promise); return promise; } else { // Registration future is almost always fulfilled already, but just in case it's not. //设置一个进度条的任务，等待注册事件完成后，就开始端口的监听 final PendingRegistrationPromise promise = new PendingRegistrationPromise(channel); regFuture.addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture future) throws Exception { Throwable cause = future.cause(); if (cause != null) { // Registration on the EventLoop failed so fail the ChannelPromise directly to not cause an // IllegalStateException once we try to access the EventLoop of the Channel. promise.setFailure(cause); } else { // Registration was successful, so set the correct executor to use. // See https://github.com/netty/netty/issues/2586 promise.registered(); doBind0(regFuture, channel, localAddress, promise); } } }); return promise; } }final ChannelFuture initAndRegister() { Channel channel = null; try { channel = channelFactory.newChannel();//通过 channel 工厂生成一个 channel init(channel); } catch (Throwable t) { if (channel != null) { // channel can be null if newChannel crashed (eg SocketException(&quot;too many open files&quot;)) channel.unsafe().closeForcibly(); // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(channel, GlobalEventExecutor.INSTANCE).setFailure(t); } // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(new FailedChannel(), GlobalEventExecutor.INSTANCE).setFailure(t); } ChannelFuture regFuture = config().group().register(channel);//将这个 channel 注册进 parentEventLoop if (regFuture.cause() != null) { if (channel.isRegistered()) { channel.close(); } else { channel.unsafe().closeForcibly(); } } return regFuture; }===== ServerBootstrap ===== void init(Channel channel) throws Exception { final Map&lt;ChannelOption&lt;?&gt;, Object&gt; options = options0(); synchronized (options) { setChannelOptions(channel, options, logger);//设置 channel 的 option } final Map&lt;AttributeKey&lt;?&gt;, Object&gt; attrs = attrs0(); synchronized (attrs) { for (Entry&lt;AttributeKey&lt;?&gt;, Object&gt; e: attrs.entrySet()) { @SuppressWarnings(&quot;unchecked&quot;) AttributeKey&lt;Object&gt; key = (AttributeKey&lt;Object&gt;) e.getKey(); channel.attr(key).set(e.getValue()); ////设置属性 } } ChannelPipeline p = channel.pipeline(); final EventLoopGroup currentChildGroup = childGroup; final ChannelHandler currentChildHandler = childHandler; final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] currentChildOptions; final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] currentChildAttrs; synchronized (childOptions) { currentChildOptions = childOptions.entrySet().toArray(newOptionArray(0)); } synchronized (childAttrs) { currentChildAttrs = childAttrs.entrySet().toArray(newAttrArray(0)); } p.addLast(new ChannelInitializer&lt;Channel&gt;() { @Override public void initChannel(final Channel ch) throws Exception { final ChannelPipeline pipeline = ch.pipeline(); ////获取 pipeline //这里获取的 handler，对应的是 AbstractBootstrap 的 handler，这个是通过 ServerBootstrap.handler() 方法设置的 ChannelHandler handler = config.handler(); if (handler != null) { pipeline.addLast(handler);//添加进入 pipeline，这个是为了让每个处理的都能首先调用这个 handler } //执行任务，设置子 handler。这里对用的是 ServerBootstrap.childHandler() 方法设置的 handler ch.eventLoop().execute(new Runnable() { @Override public void run() { pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); } }); } }); }=====AbstractBootstrap=====private static void doBind0( final ChannelFuture regFuture, final Channel channel, final SocketAddress localAddress, final ChannelPromise promise) { // This method is invoked before channelRegistered() is triggered. Give user handlers a chance to set up // the pipeline in its channelRegistered() implementation. //执行到这里，说明任务已经被注册到 loopgroup //所以可以开始一个监听端口的任务 channel.eventLoop().execute(new Runnable() { @Override public void run() { if (regFuture.isSuccess()) { channel.bind(localAddress, promise).addListener(ChannelFutureListener.CLOSE_ON_FAILURE); } else { promise.setFailure(regFuture.cause()); } } }); } initAndRegister初始化注册到selectorchannelFactory.newChannel()1234567891011121314151617181920212223242526272829303132final ChannelFuture initAndRegister() { Channel channel = null; try { //1. 将channelClass作为ReflectiveChannelFactory的构造函数创建出一个ReflectiveChannelFactory channel = channelFactory.newChannel(); init(channel); } catch (Throwable t) { if (channel != null) { // channel can be null if newChannel crashed (eg SocketException(&quot;too many open files&quot;)) channel.unsafe().closeForcibly(); // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(channel, GlobalEventExecutor.INSTANCE).setFailure(t); } // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(new FailedChannel(), GlobalEventExecutor.INSTANCE).setFailure(t); } //config().group()指的是 ServerBootstrap 的 parentLoopGroup ChannelFuture regFuture = config().group().register(channel); if (regFuture.cause() != null) { if (channel.isRegistered()) { channel.close(); } else { channel.unsafe().closeForcibly(); } } return regFuture; } new 一个channel，这里的channel是在服务启动的时候创建，我们可以和普通Socket编程中的ServerSocket对应上，表示服务端绑定的时候经过的一条流水线。 将channelClass作为ReflectiveChannelFactory的构造函数创建出一个ReflectiveChannelFactory，最终创建channel相当于调用默认构造函数new出一个 NioServerSocketChannel对象 123456789101112public NioServerSocketChannel() { this(newSocket(DEFAULT_SELECTOR_PROVIDER)); }private static ServerSocketChannel newSocket(SelectorProvider provider) { try { return provider.openServerSocketChannel(); //创建一条server端channel } catch (IOException e) { throw new ChannelException( &quot;Failed to open a server socket.&quot;, e); } } 总结一下，用户调用方法 Bootstrap.bind(port) 第一步就是通过反射的方式new一个NioServerSocketChannel对象，并且在new的过程中创建了一系列的核心组件 init(channel)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253=====ServerBootstrap=====@Override void init(Channel channel) throws Exception { //1.设置option和attr final Map&lt;ChannelOption&lt;?&gt;, Object&gt; options = options0(); synchronized (options) { setChannelOptions(channel, options, logger); } final Map&lt;AttributeKey&lt;?&gt;, Object&gt; attrs = attrs0(); synchronized (attrs) { for (Entry&lt;AttributeKey&lt;?&gt;, Object&gt; e: attrs.entrySet()) { @SuppressWarnings(&quot;unchecked&quot;) AttributeKey&lt;Object&gt; key = (AttributeKey&lt;Object&gt;) e.getKey(); channel.attr(key).set(e.getValue()); } } ChannelPipeline p = channel.pipeline(); //2. 设置新接入channel的option和attr final EventLoopGroup currentChildGroup = childGroup; final ChannelHandler currentChildHandler = childHandler; final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] currentChildOptions; final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] currentChildAttrs; synchronized (childOptions) { currentChildOptions = childOptions.entrySet().toArray(newOptionArray(0)); } synchronized (childAttrs) { currentChildAttrs = childAttrs.entrySet().toArray(newAttrArray(0)); } //3.加入新连接处理器 //向serverChannel的流水线处理器中加入了一个 ServerBootstrapAcceptor(这是一个接入器，专门接受新请求，把新的请求扔给某个事件循环器) p.addLast(new ChannelInitializer&lt;Channel&gt;() { @Override public void initChannel(final Channel ch) throws Exception { final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); if (handler != null) { pipeline.addLast(handler); } ch.eventLoop().execute(new Runnable() { @Override public void run() { pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); } }); } }); } config().group().register(channel)1234=====MultithreadEventLoopGroup======public ChannelFuture register(Channel channel) { return next().register(channel); } 123456=====MultithreadEventExecutorGroup=====@Override public EventExecutor next() { return chooser.next(); }//这里的 chooser 是 MultithreadEventExecutorGroup 的成员属性，它可以对根据目前 ExectuorGroup 中的 EventExecutor 的情况策略选择 EventExecutor。这里默认使用的是 DefaultEventExecutorChooserFactory，这个是基于轮询策略操作的。 PowerOfTwoEventExecutorChooser 按位与(&amp;)操作符 GenericEventExecutorChooser 取模(%)运算符 12345678910111213141516171819202122232425262728===== DefaultEventExecutorChooserFactory =====private static final class PowerOfTwoEventExecutorChooser implements EventExecutorChooser { private final AtomicInteger idx = new AtomicInteger(); private final EventExecutor[] executors; PowerOfTwoEventExecutorChooser(EventExecutor[] executors) { this.executors = executors; } @Override public EventExecutor next() { return executors[idx.getAndIncrement() &amp; executors.length - 1]; } } private static final class GenericEventExecutorChooser implements EventExecutorChooser { private final AtomicInteger idx = new AtomicInteger(); private final EventExecutor[] executors; GenericEventExecutorChooser(EventExecutor[] executors) { this.executors = executors; } @Override public EventExecutor next() { return executors[Math.abs(idx.getAndIncrement() % executors.length)]; } } chooserFactory 最后会选择出 EventExecutor 后，就可以将 Channel 进行注册了。在 Netty 的 NioEventLoopGroup 中 EventExecutor 都是 SingleThreadEventLoop 来承担的（如果你继续跟进代码的话，你会发现其实 EventExecutor 实际上就是一个 Java 原生的线程池，最后实现的是一个 ExecutorService ）。 123456789101112=====SingleThreadEventLoop=====@Override public ChannelFuture register(Channel channel) { return register(new DefaultChannelPromise(channel, this)); } @Override public ChannelFuture register(final ChannelPromise promise) { ObjectUtil.checkNotNull(promise, &quot;promise&quot;); promise.channel().unsafe().register(this, promise); return promise; } 接下来，我们获取到了 EventExecutor 后，就可以让它帮忙注册了。 1234567891011121314151617181920212223242526===== AbstractChannel =====public final void register(EventLoop eventLoop, final ChannelPromise promise) { //...校验省略部分代码 //先将EventLoop事件循环器绑定到该NioServerSocketChannel上 AbstractChannel.this.eventLoop = eventLoop; if (eventLoop.inEventLoop()) { register0(promise); } else { try { eventLoop.execute(new Runnable() { @Override public void run() { register0(promise); } }); } catch (Throwable t) { logger.warn( &quot;Force-closing a channel whose registration task was not accepted by an event loop: {}&quot;, AbstractChannel.this, t); closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); } } } 1234567891011121314151617181920212223242526272829303132333435363738private void register0(ChannelPromise promise) { try { // check if the channel is still open as it could be closed in the mean time when the register // call was outside of the eventLoop if (!promise.setUncancellable() || !ensureOpen(promise)) { return; } boolean firstRegistration = neverRegistered; doRegister(); neverRegistered = false; registered = true; // Ensure we call handlerAdded(...) before we actually notify the promise. This is needed as the // user may already fire events through the pipeline in the ChannelFutureListener. pipeline.invokeHandlerAddedIfNeeded(); safeSetSuccess(promise); //开始通知成功了 pipeline.fireChannelRegistered(); // Only fire a channelActive if the channel has never been registered. This prevents firing // multiple channel actives if the channel is deregistered and re-registered. if (isActive()) { if (firstRegistration) { pipeline.fireChannelActive(); } else if (config().isAutoRead()) { // This channel was registered before and autoRead() is set. This means we need to begin read // again so that we process inbound data. // // See https://github.com/netty/netty/issues/4805 beginRead(); } } } catch (Throwable t) { // Close the channel directly to avoid FD leak. closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); } } 1234567891011121314151617181920212223===== AbstractNioChannel ===== @Override protected void doRegister() throws Exception { boolean selected = false; for (;;) { try { //将当前的channel注册到selector上 selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this); return; } catch (CancelledKeyException e) { if (!selected) { // Force the Selector to select now as the &quot;canceled&quot; SelectionKey may still be // cached and not removed because no Select.select(..) operation was called yet. eventLoop().selectNow(); selected = true; } else { // We forced a select operation on the selector before but the SelectionKey is still cached // for whatever reason. JDK bug ? throw e; } } } } 最终监听 OP_ACCEPT 是通过 bind 完成后的 fireChannelActive() 来触发的。 为什么注册 OP_ACCEPT(16) 到多路复用器上，怎么注册 0 呢？0 表示已注册，但不进行任何操作。这样做的原因是 注册方法是多台的。它既可以被 NioServerSocketChannel 用来监听客户端的连接接入，也可以注册 SocketChannel 用来监听网络读或写操作。 通过 SelectionKey 的 interceptOps(int pos) 可以方便修改监听的操作位。所以，此处注册需要获取 SelectionKey 并给 AbstractNioChannel 的成员变量 selectionKey 赋值。 doRegister完毕之后调用pipeline.invokeHandlerAddedIfNeeded(); 123456789final void invokeHandlerAddedIfNeeded() { assert channel.eventLoop().inEventLoop(); if (firstRegistration) { firstRegistration = false; // We are now registered to the EventLoop. It's time to call the callbacks for the ChannelHandlers, // that were added before the registration was done. callHandlerAddedForAllHandlers(); } } 当注册成功后，触发了 ChannelRegistered 事件，这个事件也是整个 pipeline 都会触发的。 ChannelRegistered 触发完后，就会判断是否 ServerSocketChannel 监听是否成功，如果成功，需要出发 NioServerSocketChannel 的 ChannelActive 事件。 123if(isAcitve()) { pipeline.fireChannelActive();} isAcitve() 方法也是多态。如果服务端判断是否监听启动；如果是客户端查看 TCP 是否连接完成。channelActive() 事件在 ChannelPipeline 中传递，找到了 pipeline.fireChannelActive(); 的发起调用的代码，不巧，刚好就是下面的doBind0()方法 123456789101112131415private static void doBind0( final ChannelFuture regFuture, final Channel channel, final SocketAddress localAddress, final ChannelPromise promise) { //通过包装一个Runnable进行异步化的，关于异步化task channel.eventLoop().execute(new Runnable() { @Override public void run() { if (regFuture.isSuccess()) { channel.bind(localAddress, promise).addListener(ChannelFutureListener.CLOSE_ON_FAILURE); } else { promise.setFailure(regFuture.cause()); } } }); } 123456789101112131415161718===== AbstractChannel =====@Override public ChannelFuture bind(SocketAddress localAddress, ChannelPromise promise) { return pipeline.bind(localAddress, promise); }===== DefaultChannelPipeline =====@Override public final ChannelFuture bind(SocketAddress localAddress, ChannelPromise promise) { return tail.bind(localAddress, promise); }===== HeadContext ======@Override public void bind( ChannelHandlerContext ctx, SocketAddress localAddress, ChannelPromise promise) { unsafe.bind(localAddress, promise); } 1234567891011121314151617181920212223242526272829303132333435363738394041@Override public final void bind(final SocketAddress localAddress, final ChannelPromise promise) { assertEventLoop(); if (!promise.setUncancellable() || !ensureOpen(promise)) { return; } // See: https://github.com/netty/netty/issues/576 if (Boolean.TRUE.equals(config().getOption(ChannelOption.SO_BROADCAST)) &amp;&amp; localAddress instanceof InetSocketAddress &amp;&amp; !((InetSocketAddress) localAddress).getAddress().isAnyLocalAddress() &amp;&amp; !PlatformDependent.isWindows() &amp;&amp; !PlatformDependent.maybeSuperUser()) { // Warn a user about the fact that a non-root user can't receive a // broadcast packet on *nix if the socket is bound on non-wildcard address. logger.warn( &quot;A non-root user can't receive a broadcast packet if the socket &quot; + &quot;is not bound to a wildcard address; binding to a non-wildcard &quot; + &quot;address (&quot; + localAddress + &quot;) anyway as requested.&quot;); } boolean wasActive = isActive(); try { doBind(localAddress); //最终调到了jdk里面的bind方法,如下 } catch (Throwable t) { safeSetFailure(promise, t); closeIfClosed(); return; } if (!wasActive &amp;&amp; isActive()) { invokeLater(new Runnable() { @Override public void run() { pipeline.fireChannelActive(); } }); } safeSetSuccess(promise); } 12345678910===== NioServerSocketChannel =====@Override protected void doBind(SocketAddress localAddress) throws Exception { if (PlatformDependent.javaVersion() &gt;= 7) { //判断JDK到版本 javaChannel().bind(localAddress, config.getBacklog()); } else { javaChannel().socket().bind(localAddress, config.getBacklog()); } } 完成之后根据配置决定是否自动出发 Channel 读操作，下面是代码实现 1234567891011121314151617181920=====DefaultChannelPipeline===== @Override public void channelActive(ChannelHandlerContext ctx) { ctx.fireChannelActive(); //注册读事件，读包括：创建连接/读取数据 readIfIsAutoRead(); }private void readIfIsAutoRead() { if (channel.config().isAutoRead()) { channel.read(); } }@Override public boolean isAutoRead() { return autoRead == 1; } AbstractChannel 的读操作出发了 ChannelPipeline 的读操作，最终调用到 HeadHandler 的读方法，代码如下 12345@Override public Channel read() { pipeline.read(); return this; } 123public void read(ChannelHandlerContext ctx){ unsafe.beginRead();} 继续看 AbstractUnsafe 的 beginRead 方法，代码如下: 由于不同类型的 Channel 对于读操作的处理是不同的，所以合格 beginRead 也算是多态方法。对于 NIO 的 channel，无论是客户端还是服务端，都是修改网络监听操作位为自身感兴趣的shi 123456789101112131415protected void doBeginRead() throws Exception { // Channel.read() or ChannelHandlerContext.read() was called //this.selectionKey就是我们在前面register步骤返回的对象，前面我们在register的时候，注册测ops是0 final SelectionKey selectionKey = this.selectionKey; if (!selectionKey.isValid()) { return; } readPending = true; final int interestOps = selectionKey.interestOps(); if ((interestOps &amp; readInterestOp) == 0) { //readInterestOp 是不是监听了ops=16 selectionKey.interestOps(interestOps | readInterestOp); } } JDK SelectionKey 有四种操作类型，分别为： OP_READ = 1&lt;&lt;0 OP_WRITE = 1&lt;&lt;2 OP_CONNECT = 1&lt;&lt;3 OP_ACCEPT = 1&lt;&lt;4 每个操作位代表一种网络操作类型，分别为 0001，0010，0100，1000，这样做的好处是方便地通过位操作来进行网络操作位的状态判断和状态修改，从而提升操作性能。 总结netty启动一个服务所经过的流程 设置启动类参数，最重要的就是设置channel 创建server对应的channel，创建各大组件，包括ChannelConfig,ChannelId,ChannelPipeline,ChannelHandler,Unsafe等 初始化server对应的channel，设置一些attr，option，以及设置子channel的attr，option，给server的channel添加新channel接入器，并出发addHandler,register等事件 调用到jdk底层做端口绑定，并触发active事件，active触发的时候，真正做服务端口绑定 构建连接 接受连接本质就是对OP_ACCEPT事件对处理，对应于源码就是NioEventLoop的run方法： 断点打在processSelectedKeysOptimized方法中，然后启动服务端，再启动客户端进行debug 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102@Override protected void run() { for (;;) { //死循环监听事件 try { try { switch (selectStrategy.calculateStrategy(selectNowSupplier, hasTasks())) { case SelectStrategy.CONTINUE: continue; case SelectStrategy.BUSY_WAIT: case SelectStrategy.SELECT: select(wakenUp.getAndSet(false)); //select轮询 if (wakenUp.get()) { selector.wakeup(); } // fall through default: } } catch (IOException e) { // If we receive an IOException here its because the Selector is messed up. Let's rebuild // the selector and retry. https://github.com/netty/netty/issues/8566 rebuildSelector0(); handleLoopException(e); continue; } cancelledKeys = 0; needsToSelectAgain = false; final int ioRatio = this.ioRatio; if (ioRatio == 100) { try { processSelectedKeys(); } finally { // Ensure we always run tasks. runAllTasks(); } } else { final long ioStartTime = System.nanoTime(); try { processSelectedKeys(); //处理事件 } finally { // Ensure we always run tasks. final long ioTime = System.nanoTime() - ioStartTime; runAllTasks(ioTime * (100 - ioRatio) / ioRatio); } } } catch (Throwable t) { handleLoopException(t); } // Always handle shutdown even if the loop processing threw an exception. try { if (isShuttingDown()) { closeAll(); if (confirmShutdown()) { return; } } } catch (Throwable t) { handleLoopException(t); } } }private void processSelectedKeys() { if (selectedKeys != null) { processSelectedKeysOptimized(); //优化后的 } else { processSelectedKeysPlain(selector.selectedKeys()); } }private void processSelectedKeysOptimized() { for (int i = 0; i &lt; selectedKeys.size; ++i) { final SelectionKey k = selectedKeys.keys[i]; // null out entry in the array to allow to have it GC'ed once the Channel close // See https://github.com/netty/netty/issues/2363 selectedKeys.keys[i] = null; final Object a = k.attachment(); //这里的attachment就是NioServerSocketChannel if (a instanceof AbstractNioChannel) { processSelectedKey(k, (AbstractNioChannel) a); //处理SelectedKey } else { @SuppressWarnings(&quot;unchecked&quot;) NioTask&lt;SelectableChannel&gt; task = (NioTask&lt;SelectableChannel&gt;) a; processSelectedKey(k, task); } if (needsToSelectAgain) { // null out entries in the array to allow to have it GC'ed once the Channel close // See https://github.com/netty/netty/issues/2363 selectedKeys.reset(i + 1); selectAgain(); i = -1; } } } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) { final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe(); if (!k.isValid()) { final EventLoop eventLoop; try { eventLoop = ch.eventLoop(); } catch (Throwable ignored) { // If the channel implementation throws an exception because there is no event loop, we ignore this // because we are only trying to determine if ch is registered to this event loop and thus has authority // to close ch. return; } // Only close ch if ch is still registered to this EventLoop. ch could have deregistered from the event loop // and thus the SelectionKey could be cancelled as part of the deregistration process, but the channel is // still healthy and should not be closed. // See https://github.com/netty/netty/issues/5125 if (eventLoop != this || eventLoop == null) { return; } // close the channel if the key is not valid anymore unsafe.close(unsafe.voidPromise()); return; } try { int readyOps = k.readyOps(); //什么事件发生了，如果是16代表的就是OP_ACCEPT // We first need to call finishConnect() before try to trigger a read(...) or write(...) as otherwise // the NIO JDK channel implementation may throw a NotYetConnectedException. if ((readyOps &amp; SelectionKey.OP_CONNECT) != 0) { // remove OP_CONNECT as otherwise Selector.select(..) will always return without blocking // See https://github.com/netty/netty/issues/924 int ops = k.interestOps(); ops &amp;= ~SelectionKey.OP_CONNECT; k.interestOps(ops); unsafe.finishConnect(); } // Process OP_WRITE first as we may be able to write some queued buffers and so free memory. if ((readyOps &amp; SelectionKey.OP_WRITE) != 0) { // Call forceFlush which will also take care of clear the OP_WRITE once there is nothing left to write ch.unsafe().forceFlush(); } // Also check for readOps of 0 to workaround possible JDK bug which may otherwise lead // to a spin loop if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) { unsafe.read(); //如果是16，进入这个方法 } } catch (CancelledKeyException ignored) { unsafe.close(unsafe.voidPromise()); } } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869private final class NioMessageUnsafe extends AbstractNioUnsafe { private final List&lt;Object&gt; readBuf = new ArrayList&lt;Object&gt;(); @Override public void read() { assert eventLoop().inEventLoop(); //初始化 final ChannelConfig config = config(); final ChannelPipeline pipeline = pipeline(); final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle(); allocHandle.reset(config); boolean closed = false; Throwable exception = null; try { try { do { int localRead = doReadMessages(readBuf);// if (localRead == 0) { break; } if (localRead &lt; 0) { closed = true; break; } allocHandle.incMessagesRead(localRead); //记录一下创建的次数 } while (allocHandle.continueReading());//判断是不是需要继续去读 } catch (Throwable t) { exception = t; } int size = readBuf.size(); for (int i = 0; i &lt; size; i ++) { readPending = false; //创建连接的结果通过fireChannelRead传播出去，就是pipeline中各种handle的执行 //其中有个关键的就是ServerBootStrapAcceptor 负责初始化创建的SocketChannel的 pipeline.fireChannelRead(readBuf.get(i)); } readBuf.clear(); allocHandle.readComplete(); pipeline.fireChannelReadComplete(); if (exception != null) { closed = closeOnReadError(exception); pipeline.fireExceptionCaught(exception); } if (closed) { inputShutdown = true; if (isOpen()) { close(voidPromise()); } } } finally { // Check if there is a readPending which was not processed yet. // This could be for two reasons: // * The user called Channel.read() or ChannelHandlerContext.read() in channelRead(...) method // * The user called Channel.read() or ChannelHandlerContext.read() in channelReadComplete(...) method // // See https://github.com/netty/netty/issues/2254 if (!readPending &amp;&amp; !config.isAutoRead()) { removeReadOp(); } } } } 123456789101112131415161718192021====== NioServerSocketChannel ======@Override protected int doReadMessages(List&lt;Object&gt; buf) throws Exception { SocketChannel ch = SocketUtils.accept(javaChannel());//接受新连接创建SocketChannel try { if (ch != null) { buf.add(new NioSocketChannel(this, ch)); //创建完SocketChannel之后放入到buf数组中去 return 1; //返回1代表仅仅创建了一个连接 } } catch (Throwable t) { logger.warn(&quot;Failed to create a new channel from an accepted socket.&quot;, t); try { ch.close(); } catch (Throwable t2) { logger.warn(&quot;Failed to close a socket.&quot;, t2); } } return 0; } 1234567891011121314===== SocketUtils =====public static SocketChannel accept(final ServerSocketChannel serverSocketChannel) throws IOException { try { return AccessController.doPrivileged(new PrivilegedExceptionAction&lt;SocketChannel&gt;() { @Override public SocketChannel run() throws IOException { //非阻塞模式下，没有连接请求时返回null return serverSocketChannel.accept();//接受创建连接请求来创建一个SocketChannel } }); } catch (PrivilegedActionException e) { throw (IOException) e.getCause(); } } 1234567@Override public boolean continueReading(UncheckedBooleanSupplier maybeMoreDataSupplier) { return config.isAutoRead() &amp;&amp; (!respectMaybeMoreData || maybeMoreDataSupplier.get()) &amp;&amp; totalMessages &lt; maxMessagePerRead &amp;&amp; //maxMessagePerRead 最大次数16次 totalBytesRead &gt; 0; //读取的字节数为0 } 找到ServerBootStrapAcceptor 12345678910111213141516171819202122232425===== ServerBootStrap ======public void channelRead(ChannelHandlerContext ctx, Object msg) { final Channel child = (Channel) msg; child.pipeline().addLast(childHandler); setChannelOptions(child, childOptions, logger); for (Entry&lt;AttributeKey&lt;?&gt;, Object&gt; e: childAttrs) { child.attr((AttributeKey&lt;Object&gt;) e.getKey()).set(e.getValue()); } try { childGroup.register(child).addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture future) throws Exception { if (!future.isSuccess()) { forceClose(child, future.cause()); } } }); } catch (Throwable t) { forceClose(child, t); } } 123456789101112131415161718 @Override public ChannelFuture register(Channel channel) { return next().register(channel); //选择一个子的元素 }===== SingleThreadEventLoop ===== 此时已经是workereventloop了 @Override public ChannelFuture register(Channel channel) { return register(new DefaultChannelPromise(channel, this)); } @Override public ChannelFuture register(final ChannelPromise promise) { ObjectUtil.checkNotNull(promise, &quot;promise&quot;); promise.channel().unsafe().register(this, promise); return promise; } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102===== AbstractChannel ======@Override public final void register(EventLoop eventLoop, final ChannelPromise promise) { if (eventLoop == null) { throw new NullPointerException(&quot;eventLoop&quot;); } if (isRegistered()) { promise.setFailure(new IllegalStateException(&quot;registered to an event loop already&quot;)); return; } if (!isCompatible(eventLoop)) { promise.setFailure( new IllegalStateException(&quot;incompatible event loop type: &quot; + eventLoop.getClass().getName())); return; } AbstractChannel.this.eventLoop = eventLoop; if (eventLoop.inEventLoop()) { //判断当前是不是eventloop所选定的线程 register0(promise); } else { try { eventLoop.execute(new Runnable() { @Override public void run() { register0(promise); } }); } catch (Throwable t) { logger.warn( &quot;Force-closing a channel whose registration task was not accepted by an event loop: {}&quot;, AbstractChannel.this, t); closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); } } }private void register0(ChannelPromise promise) { try { // check if the channel is still open as it could be closed in the mean time when the register // call was outside of the eventLoop if (!promise.setUncancellable() || !ensureOpen(promise)) { return; } boolean firstRegistration = neverRegistered; doRegister(); neverRegistered = false; registered = true; // Ensure we call handlerAdded(...) before we actually notify the promise. This is needed as the // user may already fire events through the pipeline in the ChannelFutureListener. pipeline.invokeHandlerAddedIfNeeded(); safeSetSuccess(promise); //这里是创建连接，就不会到bind的代码中 pipeline.fireChannelRegistered(); // Only fire a channelActive if the channel has never been registered. This prevents firing // multiple channel actives if the channel is deregistered and re-registered. if (isActive()) { //此时连接已经建立好了，已经是active了 if (firstRegistration) { pipeline.fireChannelActive();// 此时也是第一次注册，走到该方法中，触发HeadCOntent中的read方法 } else if (config().isAutoRead()) { // This channel was registered before and autoRead() is set. This means we need to begin read // again so that we process inbound data. // // See https://github.com/netty/netty/issues/4805 beginRead(); } } } catch (Throwable t) { // Close the channel directly to avoid FD leak. closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); } }===== AbstractNioChannel =====@Override protected void doRegister() throws Exception { boolean selected = false; for (;;) { try { selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this); return; } catch (CancelledKeyException e) { if (!selected) { // Force the Selector to select now as the &quot;canceled&quot; SelectionKey may still be // cached and not removed because no Select.select(..) operation was called yet. eventLoop().selectNow(); selected = true; } else { // We forced a select operation on the selector before but the SelectionKey is still cached // for whatever reason. JDK bug ? throw e; } } } } 12345==== HeadContext =====@Override public void read(ChannelHandlerContext ctx) { unsafe.beginRead(); //和启动服务后面的类似了，接受的是read事件 } selector.select()/selectNow()/select(timeoutMillis) 发现 OP_ACCEPT 事件，处理： SocketChannel socketChannel = serverSocketChannel.accept() selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this); selectionKey.interestOps(OP_READ); 注册OP_READ事件 创建连接的初始化和注册是通过 pipeline.fireChannelRead 在 ServerBootstrapAcceptor 中完成的。 第一次 Register 并不是监听 OP_READ ，而是 0 ： selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this) 。 最终监听 OP_READ 是通过“Register”完成后的fireChannelActive（io.netty.channel.AbstractChannel.AbstractUnsafe#register0中）来触发的 Worker’s NioEventLoop 是通过 Register 操作执行来启动。 接受连接的读操作，不会尝试读取更多次（16次）。 接受数据 读数据技巧 自适应数据大小的分配器（AdaptiveRecvByteBufAllocator）： 发放东西时，拿多大的桶去装？小了不够，大了浪费，所以会自己根据实际装的情况猜一猜下次情况，从而决定下次带多大的桶。 连续读（defaultMaxMessagesPerRead）： 发放东西时，假设拿的桶装满了，这个时候，你会觉得可能还有东西发放，所以直接拿个新桶等着装，而不是回家，直到后面出现没有装上的情况或者装了很多次需要给别人一点机会等原因才停止，回家。 断点位置在NioEventLoop中的697行的read方法中： 12345if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) { //如果是SocketChannel的话，跳转到NioByteChannel //如果是ServerSocketChannel的话，跳转到NioMessageChannel unsafe.read(); } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263===== AbstractNioByteChannel ======@Override public final void read() { final ChannelConfig config = config(); if (shouldBreakReadReady(config)) { clearReadPending(); return; } final ChannelPipeline pipeline = pipeline(); final ByteBufAllocator allocator = config.getAllocator(); //ByteBuf分配器 //AdaptiveRecvByteBufAllocator，帮忙决定下次分配多少 final RecvByteBufAllocator.Handle allocHandle = recvBufAllocHandle(); allocHandle.reset(config); ByteBuf byteBuf = null; boolean close = false; try { do { //经可能分配合适的大小 byteBuf = allocHandle.allocate(allocator); //读并且记录读了多少，如果读满了，下次continue的话就直接扩容 allocHandle.lastBytesRead(doReadBytes(byteBuf)); if (allocHandle.lastBytesRead() &lt;= 0) { // nothing was read. release the buffer. byteBuf.release(); byteBuf = null; close = allocHandle.lastBytesRead() &lt; 0; if (close) { // There is nothing left to read as we received an EOF. readPending = false; } break; } allocHandle.incMessagesRead(1); readPending = false; //在pipeline上执行，业务逻辑就是在这个地方 pipeline.fireChannelRead(byteBuf); byteBuf = null; } while (allocHandle.continueReading()); //判断是不是要继续读，此时不是创建连接的时候了，最后一个判断满足 //记录这次读事件总共读了多少数据，计算下次分配大小 allocHandle.readComplete(); pipeline.fireChannelReadComplete(); //相当于完成本次读事件的处理 if (close) { closeOnRead(pipeline); } } catch (Throwable t) { handleReadException(pipeline, byteBuf, t, close, allocHandle); } finally { // Check if there is a readPending which was not processed yet. // This could be for two reasons: // * The user called Channel.read() or ChannelHandlerContext.read() in channelRead(...) method // * The user called Channel.read() or ChannelHandlerContext.read() in channelReadComplete(...) method // // See https://github.com/netty/netty/issues/2254 if (!readPending &amp;&amp; !config.isAutoRead()) { removeReadOp(); } } } } 1234@Override public ByteBuf allocate(ByteBufAllocator alloc) { return alloc.ioBuffer(guess()); //猜，一开始是1024 } 1234567===== NioSocketChannel ======@Override protected int doReadBytes(ByteBuf byteBuf) throws Exception { final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle(); allocHandle.attemptedBytesRead(byteBuf.writableBytes()); return byteBuf.writeBytes(javaChannel(), allocHandle.attemptedBytesRead()); } 12345678910===== AbstractByteBuf ======@Override public int writeBytes(ScatteringByteChannel in, int length) throws IOException { ensureWritable(length); int writtenBytes = setBytes(writerIndex, in, length); if (writtenBytes &gt; 0) { writerIndex += writtenBytes; } return writtenBytes; } 123456789=====PooledByteBuf======@Override public final int setBytes(int index, ScatteringByteChannel in, int length) throws IOException { try { return in.read(internalNioBuffer(index, length)); } catch (ClosedChannelException ignored) { return -1; } } 123456789101112131415@Override public boolean continueReading(UncheckedBooleanSupplier maybeMoreDataSupplier) { return config.isAutoRead() &amp;&amp; (!respectMaybeMoreData || maybeMoreDataSupplier.get()) &amp;&amp; //maybeMoreDataSupplier.get()判断是不是满载而归 totalMessages &lt; maxMessagePerRead &amp;&amp; totalBytesRead &gt; 0; }private final UncheckedBooleanSupplier defaultMaybeMoreSupplier = new UncheckedBooleanSupplier() { @Override public boolean get() { return attemptedBytesRead == lastBytesRead; } }; 总结读取数据本质：sun.nio.ch.SocketChannelImpl#read(java.nio.ByteBuffer) NioSocketChannel read() 是读数据， NioServerSocketChannel read() 是创建连接 pipeline.fireChannelReadComplete(); 一次读事件处理完成 pipeline.fireChannelRead(byteBuf); 一次读数据完成，一次读事件处理可能会包含多次读数据操作。 为什么最多只尝试读取 16 次？“雨露均沾” AdaptiveRecvByteBufAllocator 对 bytebuf 的猜测：放大果断，缩小谨慎（需要连续 2 次判断） 业务处理 还是从NioEventLoop中的unsafe.read()开始 123if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) { unsafe.read(); } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354===== AbstractNioByteChannel =====@Override public final void read() { final ChannelConfig config = config(); if (shouldBreakReadReady(config)) { clearReadPending(); return; } final ChannelPipeline pipeline = pipeline(); final ByteBufAllocator allocator = config.getAllocator(); final RecvByteBufAllocator.Handle allocHandle = recvBufAllocHandle(); allocHandle.reset(config); ByteBuf byteBuf = null; boolean close = false; try { do { byteBuf = allocHandle.allocate(allocator); allocHandle.lastBytesRead(doReadBytes(byteBuf)); if (allocHandle.lastBytesRead() &lt;= 0) { // nothing was read. release the buffer. byteBuf.release(); byteBuf = null; close = allocHandle.lastBytesRead() &lt; 0; if (close) { // There is nothing left to read as we received an EOF. readPending = false; } break; } allocHandle.incMessagesRead(1); readPending = false; pipeline.fireChannelRead(byteBuf); //在这里处理业务逻辑 byteBuf = null; } while (allocHandle.continueReading()); allocHandle.readComplete(); pipeline.fireChannelReadComplete(); if (close) { closeOnRead(pipeline); } } catch (Throwable t) { handleReadException(pipeline, byteBuf, t, close, allocHandle); } finally { if (!readPending &amp;&amp; !config.isAutoRead()) { removeReadOp(); } } } } 123456789101112131415161718192021222324252627282930313233343536373839404142====== DefaultChannelPipeline ======@Override public final ChannelPipeline fireChannelRead(Object msg) { AbstractChannelHandlerContext.invokeChannelRead(head, msg); return this; }===== AbstractChannelHandlerContext =====static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) { final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, &quot;msg&quot;), next); EventExecutor executor = next.executor(); //默认是NioEventLoop，可以指定 if (executor.inEventLoop()) { next.invokeChannelRead(m); } else { executor.execute(new Runnable() { @Override public void run() { next.invokeChannelRead(m); } }); } }private void invokeChannelRead(Object msg) { if (invokeHandler()) { try { ((ChannelInboundHandler) handler()).channelRead(this, msg); //channelRead方法可以跳转到业务逻辑那里 } catch (Throwable t) { notifyHandlerException(t); } } else { fireChannelRead(msg); } }====== DefaultChannelPipeline ======@Override public void channelRead(ChannelHandlerContext ctx, Object msg) { ctx.fireChannelRead(msg); } 1234567891011121314151617181920212223242526272829303132===== AbstractChannelHandlerContext =====@Override public ChannelHandlerContext fireChannelRead(final Object msg) { //找head的下面一个可以执行CHANNEL_READ的方法 invokeChannelRead(findContextInbound(MASK_CHANNEL_READ), msg); return this; }private AbstractChannelHandlerContext findContextInbound(int mask) { AbstractChannelHandlerContext ctx = this; do { ctx = ctx.next; //先next一下 } while ((ctx.executionMask &amp; mask) == 0); //判断是不是有执行的资格 return ctx; }static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) { final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, &quot;msg&quot;), next); EventExecutor executor = next.executor(); if (executor.inEventLoop()) { next.invokeChannelRead(m); } else { executor.execute(new Runnable() { @Override public void run() { next.invokeChannelRead(m); } }); } } 1234567public class EchoServerHandler extends ChannelInboundHandlerAdapter { @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { //这里写业务逻辑 ctx.write(msg); } 总结 处理业务本质：数据在 pipeline 中所有的 handler 的 channelRead() 执行过程Handler 要实现 io.netty.channel.ChannelInboundHandler#channelRead (ChannelHandlerContext ctx, Object msg)，且不能加注解 @Skip 才能被执行到。中途可退出，不保证执行到 Tail Handler。 默认处理线程就是 Channel 绑定的 NioEventLoop 线程，也可以设置其他： pipeline.addLast(new UnorderedThreadPoolEventExecutor(10), serverHandler) 发送数据写数据的三种方式 快递场景（包裹） Netty 写数据（数据） 揽收到仓库 write：写到一个 buffer 从仓库发货 flush: 把 buffer 里的数据发送出去 揽收到仓库并立马发货 （加急件） writeAndFlush：写到 buffer，立马发送 揽收与发货之间有个缓冲的仓库 Write 和 Flush 之间有个 ChannelOutboundBuffer 对方仓库爆仓时，送不了的时候，会停止送，协商等电话通知什么时候好了，再送。 Netty 写数据，写不进去时，会停止写，然后注册一个 OP_WRITE 事件，来通知什么时候可以写进去了再写。 发送快递时，对方仓库都直接收下，这个时候再发送快递时，可以尝试发送更多的快递试试，这样效果更好。 Netty 批量写数据时，如果想写的都写进去了，接下来的尝试写更多（调整 maxBytesPerGatheringWrite） 发送快递时，发到某个地方的快递特别多，我们会连续发，但是快递车毕竟有限，也会考虑下其他地方。 Netty 只要有数据要写，且能写的出去，则一直尝试，直到写不出去或者满 16 次（writeSpinCount）。 揽收太多，发送来不及时，爆仓，这个时候会出个告示牌：收不下了，最好过 2 天再来邮寄吧。 Netty 待写数据太多，超过一定的水位线（**writeBufferWaterMark.high()**），会将可写的标志位改成 false ，让应用端自己做决定要不要发送数据了。 ​ 源码分析接着上面的断点： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354@Override public ChannelFuture write(Object msg) { return write(msg, newPromise()); } @Override public ChannelFuture write(final Object msg, final ChannelPromise promise) { write(msg, false, promise); return promise; }===== AbstractChannelHandlerContext ===== private void write(Object msg, boolean flush, ChannelPromise promise) { ObjectUtil.checkNotNull(msg, &quot;msg&quot;); try { if (isNotValidPromise(promise, true)) { ReferenceCountUtil.release(msg); // cancelled return; } } catch (RuntimeException e) { ReferenceCountUtil.release(msg); throw e; } //当前的handle是我们自己写的业务逻辑处理handle，这里找下一个handle final AbstractChannelHandlerContext next = findContextOutbound(flush ? (MASK_WRITE | MASK_FLUSH) : MASK_WRITE); final Object m = pipeline.touch(msg, next); EventExecutor executor = next.executor(); if (executor.inEventLoop()) { if (flush) { next.invokeWriteAndFlush(m, promise); } else { next.invokeWrite(m, promise); } } else { final AbstractWriteTask task; if (flush) { task = WriteAndFlushTask.newInstance(next, m, promise); } else { task = WriteTask.newInstance(next, m, promise); //这里wirte } if (!safeExecute(executor, task, promise, m)) { // We failed to submit the AbstractWriteTask. We need to cancel it so we decrement the pending bytes // and put it back in the Recycler for re-use later. // // See https://github.com/netty/netty/issues/8343. task.cancel(); } } } 1234567891011121314151617181920private void invokeWrite(Object msg, ChannelPromise promise) { if (invokeHandler()) { invokeWrite0(msg, promise); } else { write(msg, promise); } } private void invokeWrite0(Object msg, ChannelPromise promise) { try { ((ChannelOutboundHandler) handler()).write(this, msg, promise); } catch (Throwable t) { notifyOutboundHandlerException(t, promise); } }@Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) { unsafe.write(msg, promise); } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public final void write(Object msg, ChannelPromise promise) { assertEventLoop(); ChannelOutboundBuffer outboundBuffer = this.outboundBuffer; //类比前面发快递的仓库了 if (outboundBuffer == null) { // If the outboundBuffer is null we know the channel was closed and so // need to fail the future right away. If it is not null the handling of the rest // will be done in flush0() // See https://github.com/netty/netty/issues/2362 safeSetFailure(promise, newClosedChannelException(initialCloseCause)); // release message now to prevent resource-leak ReferenceCountUtil.release(msg); return; } int size; try { msg = filterOutboundMessage(msg); size = pipeline.estimatorHandle().size(msg); if (size &lt; 0) { size = 0; } } catch (Throwable t) { safeSetFailure(promise, t); ReferenceCountUtil.release(msg); return; } outboundBuffer.addMessage(msg, size, promise);//把消息放入buffer中 }===== ChannelOutboundBuffer =====public void addMessage(Object msg, int size, ChannelPromise promise) { Entry entry = Entry.newInstance(msg, size, total(msg), promise); if (tailEntry == null) { flushedEntry = null; } else { Entry tail = tailEntry; tail.next = entry; } tailEntry = entry; if (unflushedEntry == null) { unflushedEntry = entry; //追加到队尾 } // increment pending bytes after adding message to the unflushed arrays. // See https://github.com/netty/netty/issues/1619 incrementPendingOutboundBytes(entry.pendingSize, false); //根据前面算的size来更改buffer中还有多少数据未处理 }private void incrementPendingOutboundBytes(long size, boolean invokeLater) { if (size == 0) { return; } long newWriteBufferSize = TOTAL_PENDING_SIZE_UPDATER.addAndGet(this, size); if (newWriteBufferSize &gt; channel.config().getWriteBufferHighWaterMark()) { setUnwritable(invokeLater); } } write完之后就到了发送flush过程 1234567891011public class EchoServerHandler extends ChannelInboundHandlerAdapter { @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { ctx.write(msg); } @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { ctx.flush(); //到了这里了 } 1234567891011121314151617@Override public ChannelHandlerContext flush() { //找到下一级的handle final AbstractChannelHandlerContext next = findContextOutbound(MASK_FLUSH); EventExecutor executor = next.executor(); if (executor.inEventLoop()) { next.invokeFlush(); } else { Tasks tasks = next.invokeTasks; if (tasks == null) { next.invokeTasks = tasks = new Tasks(next); } safeExecute(executor, tasks.invokeFlushTask, channel().voidPromise(), null); } return this; } 123456789101112131415private void invokeFlush() { if (invokeHandler()) { invokeFlush0(); } else { flush(); } } private void invokeFlush0() { try { ((ChannelOutboundHandler) handler()).flush(this); } catch (Throwable t) { notifyHandlerException(t); } } 123456789101112131415161718@Override public void flush(ChannelHandlerContext ctx) { unsafe.flush(); }@Override public final void flush() { assertEventLoop(); ChannelOutboundBuffer outboundBuffer = this.outboundBuffer; if (outboundBuffer == null) { return; } outboundBuffer.addFlush(); flush0(); } 12345678910111213141516171819202122232425public void addFlush() { // There is no need to process all entries if there was already a flush before and no new messages // where added in the meantime. // // See https://github.com/netty/netty/issues/2577 Entry entry = unflushedEntry; //将没有flush的数据加入到数组中 if (entry != null) { if (flushedEntry == null) { // there is no flushedEntry yet, so start with the entry flushedEntry = entry; } do { flushed ++; if (!entry.promise.setUncancellable()) { // Was cancelled so make sure we free up memory and notify about the freed bytes int pending = entry.cancel(); decrementPendingOutboundBytes(pending, false, true); } entry = entry.next; } while (entry != null); // All flushed so reset unflushedEntry unflushedEntry = null; } } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546protected void flush0() { if (inFlush0) { // Avoid re-entrance return; } final ChannelOutboundBuffer outboundBuffer = this.outboundBuffer; if (outboundBuffer == null || outboundBuffer.isEmpty()) { return; } inFlush0 = true; // Mark all pending write requests as failure if the channel is inactive. if (!isActive()) { try { if (isOpen()) { outboundBuffer.failFlushed(new NotYetConnectedException(), true); } else { // Do not trigger channelWritabilityChanged because the channel is closed already. outboundBuffer.failFlushed(newClosedChannelException(initialCloseCause), false); } } finally { inFlush0 = false; } return; } try { doWrite(outboundBuffer); //在这里进入 } catch (Throwable t) { if (t instanceof IOException &amp;&amp; config().isAutoClose()) { initialCloseCause = t; close(voidPromise(), t, newClosedChannelException(t), false); } else { try { shutdownOutput(voidPromise(), t); } catch (Throwable t2) { initialCloseCause = t; close(voidPromise(), t2, newClosedChannelException(t), false); } } } finally { inFlush0 = false; } } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263===== NioSocketChannel =====protected void doWrite(ChannelOutboundBuffer in) throws Exception { SocketChannel ch = javaChannel(); int writeSpinCount = config().getWriteSpinCount(); do { if (in.isEmpty()) { // All written so clear OP_WRITE clearOpWrite(); // Directly return here so incompleteWrite(...) is not called. return; } // Ensure the pending writes are made of ByteBufs only. int maxBytesPerGatheringWrite = ((NioSocketChannelConfig) config).getMaxBytesPerGatheringWrite(); //最多返回1024个数据，总的size尽量不超过 maxBytesPerGatheringWrite ByteBuffer[] nioBuffers = in.nioBuffers(1024, maxBytesPerGatheringWrite); int nioBufferCnt = in.nioBufferCount(); // Always us nioBuffers() to workaround data-corruption. // See https://github.com/netty/netty/issues/2761 switch (nioBufferCnt) { case 0: // We have something else beside ByteBuffers to write so fallback to normal writes. writeSpinCount -= doWrite0(in); break; case 1: { // Only one ByteBuf so use non-gathering write // Zero length buffers are not added to nioBuffers by ChannelOutboundBuffer, so there is no need // to check if the total size of all the buffers is non-zero. ByteBuffer buffer = nioBuffers[0]; int attemptedBytes = buffer.remaining(); final int localWrittenBytes = ch.write(buffer); if (localWrittenBytes &lt;= 0) { incompleteWrite(true); return; } adjustMaxBytesPerGatheringWrite(attemptedBytes, localWrittenBytes, maxBytesPerGatheringWrite); in.removeBytes(localWrittenBytes); --writeSpinCount; break; } default: { // Zero length buffers are not added to nioBuffers by ChannelOutboundBuffer, so there is no need // to check if the total size of all the buffers is non-zero. // We limit the max amount to int above so cast is safe long attemptedBytes = in.nioBufferSize(); final long localWrittenBytes = ch.write(nioBuffers, 0, nioBufferCnt); if (localWrittenBytes &lt;= 0) { incompleteWrite(true); return; } // Casting to int is safe because we limit the total amount of data in the nioBuffers to int above. adjustMaxBytesPerGatheringWrite((int) attemptedBytes, (int) localWrittenBytes, maxBytesPerGatheringWrite); in.removeBytes(localWrittenBytes); --writeSpinCount; break; } } } while (writeSpinCount &gt; 0); incompleteWrite(writeSpinCount &lt; 0); } 总结 写的本质： Single write: sun.nio.ch.SocketChannelImpl#write(java.nio.ByteBuffer) gathering write：sun.nio.ch.SocketChannelImpl#write(java.nio.ByteBuffer[], int, int) 写数据写不进去时，会停止写，注册一个 OP_WRITE 事件，来通知什么时候可以写进去了。 OP_WRITE 不是说有数据可写，而是说可以写进去，所以正常情况，不能注册，否则一直触发。 批量写数据时，如果尝试写的都写进去了，接下来会尝试写更多（maxBytesPerGatheringWrite）。 只要有数据要写，且能写，则一直尝试，直到 16 次（writeSpinCount），写 16 次还没有写完，就直接 schedule 一个 task 来继续写，而不是用注册写事件来触发，更简洁有力。 待写数据太多，超过一定的水位线（writeBufferWaterMark.high()），会将可写的标志位改成 false ，让应用端自己做决定要不要继续写。 channelHandlerContext.channel().write() ：从 TailContext 开始执行； channelHandlerContext.write() : 从当前的 Context 开始。 ​ ​ 断开连接后续更新～ 关闭服务","link":"/2021/05/25/Netty%EF%BC%88%E5%85%AD%EF%BC%89%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"},{"title":"Redis（三）过期键删除和持久化","text":"Redis过期键的删除和持久化策略 单机数据库服务器中的数据库123456struct redisServer{ //... redisDB *db; //一个数组，保存着服务器中的所有数据库 int dbnum; // 服务器中的数据库数量，默认是16个} 切换数据库客户端可以通过执行select命令来切换目标数据库。 12345678910111213localhost: localhost: set msg &quot;hello&quot;localhost: OKlocalhost: get msglocalhost: hellolocalhost: localhost: localhost: select 2 // 切换数据库localhost: OKlocalhost: get msglocalhost: (nil)localhost: 1234typedef struct redisClient{ //.. redisDb *db; //记录客户端当前正在使用的数据库，指向的是redisServer.db数组中的某一个元素} 数据库键空间Redis是一个键值对的数据库服务器，服务器中的每个数据库都是由redisDB结构表示，结构中的dict字典保存了数据库中的所有键值对，将这个字典称为键空间key space。 1234typedef struct redisDB{ //.. dict *dict;//数据库键空间，保存着数据库中的所有键值对，可以理解为Map&lt;String , RedisObject&gt;,其中key就是键，value就是对应的值。键空间就是map.keySet} 12345678910111213localhost: dbsize //返回数据库键的数量，就是取键空间的数量localhost: (int)17localhost: randomkeylocalhost: userid:20210515localhost: randomkeylocalhost: pilocalhost: flushdblocalhost: OKlocalhost: dbsizelocalhost: (int)0localhost: 设置键的过期时间通过expire命令 + 键可以设置键的生存时间，当经过指定的秒数之后，服务器就会自动删除生存时间为0的键。用ttl命令可以查看键的剩余生存时间。 123456789101112131415161718192021localhost: set key valuelocalhost: OKlocalhost: expire key 5localhost: (int)1localhost: get keylocalhost: valuelocalhost: get keylocalhost: (nil)localhost: localhost: set a blocalhost: OKlocalhost: expire a 10localhost: (int)1localhost: ttl alocalhost: (int)7localhost: ttl alocalhost: (int)5localhost: ttl alocalhost: (int)2localhost: 如何保存所有键的过期时间 redisDB结构的expires字典保存了数据库中所有键的过期时间，称为过期字典。 12345typedef struct redisDb{ // 过期字典，保存着所有键的过期时间 dict *expires; // 用Java的语言就是 Map&lt;键，过期时间&gt;} 移除过期时间命令：persist 计算并返回剩余时间：ttl命令通过计算当前时间和键的过期时间之差来实现的。 过期键的删除策略可能存在的策略 定时删除：在设置键的过期时间的同时，创建一个定时器，定时器在键的过期时间来临时删除键 惰性删除：放任键不管，但是每次从键空间中获取键的时候，都检查取得的键是否过期，如果过期就删除 定期删除：每隔一段时间，程序就对数据库进行检查，删除里面的过期键，至于要删除多少过期键，以及要检查多少个数据库则由算法决定 定时删除对内存最友好，但是对CPU是不友好的。 惰性删除对内存不友好，可能存在内存泄漏的风险。 Redis采用的策略Redis服务器实际使用的是惰性删除和定期删除两种策略结合，来达到CPU时间和内存空间之间的平衡。 惰性删除策略的实现：所有读写数据库的命令都要先看键是否过期。 定期删除策略的实现： Redis会周期性的去执行一个函数，遍历每个数据库从中取出一定数量的随机键来进行检查。 AOF、RDB和复制功能时对过期键的处理生成RDB文件时： 在使用SAVE命令或者BGSAVE命令创建一个RDB文件时，会对数据库中的键检查，已经过期的键不会加入到数据库中。 载入RDB文件时： 载入RDB文件时，主从服务器不同： 主服务器在载入的时候会对文件中保存的键检查，过期键会被直接忽略 从服务器在载入的时候，文件中保存的所有键无论是否过期都载入到数据库中。但是主从同步的时候，从服务器的数据库就会被清空 AOF写入的时候，如果这个键已经被删除了，则追加一个显示的del命令。 AOF重写的时候会对过期的键进行检查，已过期的键不会写入。 主从复制： 从服务器的过期删除完全是由主服务器来控制的： 主服务器删除一个过期键之后，会显示的向所有从服务器发送一个DEL命令。 从服务器只有在接受到主服务器的DEL命令时才会删除过期键。 这样主要是为了保持主从服务器的数据一致性。 持久化 Redis 用做缓存时，直接从内存中读取数据，响应速度会非常快。但是一旦服务器宕机，内存中的数据将全部丢失。 从后端数据库恢复这些数据，但这种方式存在两个问题： 一是，需要频繁访问数据库，会给数据库带来巨大的压力； 二是，这些数据是从慢速数据库中读取出来的，性能肯定比不上从 Redis 中读取，导致使用这些数据的应用程序响应变慢。 所以，对 Redis 来说，实现数据的持久化，避免从后端数据库中进行恢复，是至关重要的。 RDB（Redis DataBase ）内存快照 RDB文件的创建和载入Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave save：在主线程中执行，会导致阻塞； bgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是Redis RDB 文件生成的默认配置。 避免阻塞和正常处理写操作并不是一回事！！！。此时，主线程的确没有阻塞，可以正常接收请求，但是，为了保证快照完整性，它只能处理读操作，因为不能修改正在执行快照的数据。为了快照而暂停写操作，肯定是不能接受的。所以这个时候，Redis 就会借助操作系统提供的写时复制技术（Copy-On-Write, COW），在执行快照的同时，正常处理写操作。 简单来说，bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。此时，如果主线程对这些数据也都是读操作（例如图中的键值对 A），那么，主线程和bgsave 子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对 C），那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave 子进程会把这个副本数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据。 主要注意的是： 因为AOF文件的更新频率比RDB高，所有优先使用AOF文件来还原数据库。 服务器在启动时如果检测到RDB文件，就会自动载入RDB文件，在载入期间，服务器会一直处于阻塞状态。 自动间隔性保存可以指定配置： RDB文件结构给哪些内存数据做快照？Redis 的数据都在内存中，为了提供所有数据的可靠性保证，它执行的是全量快照，也就是说，把内存中的所有数据都记录到磁盘中 快照频率如何确定如果频繁地执行全量快照，也会带来两方面的开销。 一方面，频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。 另一方面，bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了。 我们可以做增量快照，就是指，做了一次全量快照后，后续的快照只对修改的数据进行快照记录，这样可以避免每次全量快照的开销。 它需要我们使用额外的元数据信息去记录哪些数据被修改了，这会带来额外的空间开销问题。 虽然跟 AOF 相比，快照的恢复速度快，但是，快照的频率不好把握，如果频率太低，两次快照间一旦宕机，就可能有比较多的数据丢失。如果频率太高，又会产生额外开销，那么，还有什么方法既能利用 RDB 的快速恢复，又能以较小的开销做到尽量少丢数据呢？ AOF（Append Only File）为什么是写后日志之前学习MySQL的时候接触的都是数据库的写前日志（Write Ahead Log, WAL），也就是说，在实际写数据前，先把修改的数据记到日志文件中，以便故障时进行恢复。 但是在Redis中却恰恰相反， Redis 是先执行命令，把数据写入内存，然后才记录日志。 那么为什么要这样设置呢？ 传统数据库的日志，例如 redo log（重做日志），记录的是修改后的数据，而 AOF 里记录的是 Redis 收到的每一条命令，这些命令是以文本形式保存的。 “*3”表示当前命令有三个部分，每部分都是由“$+数字”开头，后面紧跟着具体的命令、键或值。这里，“数字”表示这部分中的命令、键或值一共有多少字节。例如，“$3set”表示这部分有 3 个字节，也就是“set”命令。 为了避免额外的检查开销，Redis 在向 AOF 里面记录日志的时候，并不会先去对这些命令进行语法检查。所以，如果先记日志再执行命令的话，日志中就有可能记录了错误的命令，Redis 在使用日志恢复数据时，就可能会出错。而写后日志这种方式，就是先让系统执行命令，只有命令能执行成功，才会被记录到日志中，否则，系统就会直接向客户端报错。所以，Redis 使用写后日志这一方式的一大好处是，可以避免出现记录错误命令的情况。除此之外，AOF 还有一个好处：它是在命令执行后才记录日志，所以不会阻塞当前的写操作。 有两个缺陷： 如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数据就有丢失的风险。 AOF 虽然避免了对当前命令的阻塞，但可能会给下一个操作带来阻塞风险。这是因为，AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就会导致写盘很慢，进而导致后续的操作也无法执行了。 三种写回策略在AOF的配置文件中，AOF 机制给我们提供了三个选择，也就是 AOF 配置项appendfsync 的三个可选值。 Always，同步写回：每个写命令执行完，立马同步地将日志写回磁盘； Everysec，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘； No，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。 “同步写回”可以做到基本不丢数据，但是它在每一个写命令后都有一个慢速的落盘操作，不可避免地会影响主线程性能；虽然“操作系统控制的写回”在写完缓冲区后，就可以继续执行后续的命令，但是落盘的时机已经不在 Redis 手中了，只要 AOF 记录没有写回磁盘，一旦宕机对应的数据就丢失了；“每秒写回”采用一秒写回一次的频率，避免了“同步写回”的性能开销，虽然减少了对系统性能的影响，但是如果发生宕机，上一秒内未落盘的命令操作仍然会丢失。所以，这只能算是，在避免影响主线程性能和避免数据丢失两者间取了个折中。 想要获得高性能，就选择 No 策略；如果想要得到高可靠性保证，就选择Always 策略；如果允许数据有一点丢失，又希望性能别受太大影响的话，那么就选择Everysec 策略。 AOF文件重写AOF 文件过大会带来一系列性能问题，主要在于以下三个方面： 文件系统本身对文件大小有限制，无法保存过大的文件； 如果文件太大，之后再往里面追加命令记录的话，效率也会变低； 如果发生宕机，AOF 中记录的命令要一个个被重新执行，用于故障恢复，如果日志文件太大，整个恢复过程就会非常缓慢，这就会影响到 Redis 的正常使用。 AOF 重写机制就是在重写时，Redis 根据数据库的现状创建一个新的 AOF 文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入。比如说，当读取了键值对“testkey”:“testvalue”之后，重写机制会记录 settestkey testvalue 这条命令。 我们知道，AOF 文件是以追加的方式，逐一记录接收到的写命令的。当一个键值对被多条写命令反复修改时，AOF 文件会记录相应的多条命令。但是，在重写的时候，是根据这个键值对当前的最新状态，为它生成对应的写入命令。这样一来，一个键值对在重写日志中只用一条命令就行了，而且，在日志恢复时，只用执行这条命令，就可以直接完成这个键值对的写入了。 AOF重写会阻塞主线程吗AOF 日志重写的时候，是由 bgrewriteaof 子进程来完成的，不用主线程参与，这也是为了避免阻塞主线程，导致数据库性能下降。 每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程。此时，子进程这里面就包含了数据库的最新数据。然后，bgrewriteaof 子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。 所谓fork子进程指的是，子进程是会拷贝父进程的页表，即虚实映射关系，而不会拷贝物理内存。子进程复制了父进程页表，也能共享访问父进程的内存数据了，此时，类似于有了父进程的所有内存数据。 因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指正在使用的 AOF 日志，Redis 会把这个操作写到它的缓冲区。这样一来，即使宕机了，这个 AOF 日志的操作仍然是齐全的，可以用于恢复。而第二处日志，就是指新的 AOF 重写日志。这个操作也会被写到重写日志的缓冲区。这样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以用新的 AOF 文件替代旧文件了。 AOF 日志和内存快照混合Redis 4.0 中提出了一个混合使用 AOF 日志和内存快照的方法。简单来说，内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。 这样一来，快照不用很频繁地执行，这就避免了频繁 fork 对主线程的影响。而且，AOF日志也只用记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现文件过大的情况了，也可以避免重写开销。 关于 AOF 和 RDB 的选择问题，我想再给你提三点建议： 数据不能丢失时，内存快照和 AOF 的混合使用是一个很好的选择； 如果允许分钟级别的数据丢失，可以只使用 RDB； 如果只用 AOF，优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个平衡。","link":"/2021/05/13/Redis%EF%BC%88%E4%B8%89%EF%BC%89%E8%BF%87%E6%9C%9F%E9%94%AE%E5%88%A0%E9%99%A4%E5%92%8C%E6%8C%81%E4%B9%85%E5%8C%96/"},{"title":"Redis（二）IO模型","text":"Redis 是单线程，主要是指Redis 的网络 IO和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程，但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。 为什么用单线程多线程的开销日常写程序时，我们经常会听到一种说法:“使用多线程，可以增加系统吞吐率，或是可以增加系统扩展性。”的确，对于一个多线程的系统来说，在有合理的资源分配的情况下，可以增加系统中处理请求操作的资源实体，进而提升系统能够同时处理的请求数，即吞吐率。下面的左图是我们采用多线程时所期待的结果。 系统中通常会存在被多线程同时访问的共享资源，比如一个共享的数据结构。当有多个线程要修改这个共享资源时，为了保证共享资源的正确性，就需要有额外的机制进行保证，而这个额外的机制，就会带来额外的开销。 单线程为什么快Redis 采用了多路复用机制，使其在网络 IO 操作中能并发处理大量的客户端请求，实现高吞吐率。 基本的IO模型和阻塞的地方 当 Redis监听到一个客户端有连接请求，但一直未能成功建立起连接时，会阻塞在 accept() 函数这里，导致其他客户端无法和 Redis 建立连接。 当 Redis 通过 recv() 从一个客户端读取数据时，如果数据一直没有到达，Redis 也会一直阻塞在 recv()。 非阻塞模式Socket 网络模型的非阻塞模式设置，主要体现在三个关键的函数调用上，如果想要使用socket 非阻塞模式，就必须要了解这三个函数的调用返回类型和设置模式： socket() 方法会返回主动套接字，然后调用 listen() 方法，将主动套接字转化为监听套接字，此时，可以监听来自客户端的连接请求。最后，调用 accept() 方法接收到达的客户端连接，并返回已连接套接字。 针对监听套接字，我们可以设置非阻塞模式：当 Redis 调用 accept() 但一直未有连接请求到达时，Redis 线程可以返回处理其他操作，而不用一直等待。但是，你要注意的是，调用 accept() 时，已经存在监听套接字了。 针对已连接套接字设置非阻塞模式：Redis 调用 recv() 后，如果已连接套接字上一直没有数据到达，Redis 线程同样可以返回处理其他操作。我们也需要有机制继续监听该已连接套接字，并在有数据达到时通知 Redis。 基于多路复用的高性能IO模型Linux 中的 IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的select/epoll 机制。简单来说，在 Redis 只运行单线程的情况下，该机制允许内核中，同时存在多个监听套接字和已连接套接字。内核会一直监听这些套接字上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个IO 流的效果。 Redis 网络框架调用 epoll 机制，让内核监听这些套接字。此时，Redis 线程不会阻塞在某一个特定的监听或已连接套接字上，也就是说，不会阻塞在某一个特定的客户端请求处理上。正因为此，Redis 可以同时和多个客户端连接并处理请求，从而提升并发性。 为了在请求到达时能通知到 Redis 线程，select/epoll 提供了基于事件的回调机制，即针对不同事件的发生，调用相应的处理函数。那么，回调机制是怎么工作的呢？ 其实，select/epoll 一旦监测到 FD 上有请求到达时，就会触发相应的事件。这些事件会被放进一个事件队列，Redis 单线程对该事件队列不断进行处理。这样一来，Redis 无需一直轮询是否有请求实际发生，这就可以避免造成 CPU 资源浪费。同时，Redis 在对事件队列中的事件进行处理时，会调用相应的处理函数，这就实现了基于事件的回调。因为 Redis 一直在对事件队列进行处理，所以能及时响应客户端请求，提升Redis 的响应性能。 以连接请求和读数据请求为例： 这两个请求分别对应 Accept 事件和 Read 事件，Redis 分别对这两个事件注册 accept 和get 回调函数。当 Linux 内核监听到有连接请求或读数据请求时，就会触发 Accept 事件和 Read 事件，此时，内核就会回调 Redis 相应的 accept 和 get 函数进行处理。 这就像病人去医院瞧病。在医生实际诊断前，每个病人（等同于请求）都需要先分诊、测体温、登记等。如果这些工作都由医生来完成，医生的工作效率就会很低。所以，医院都设置了分诊台，分诊台会一直处理这些诊断前的工作（类似于 Linux 内核监听请求），然后再转交给医生做实际诊断。这样即使一个医生（相当于 Redis 单线程），效率也能提升","link":"/2021/05/13/Redis%EF%BC%88%E4%BA%8C%EF%BC%89IO%E6%A8%A1%E5%9E%8B/"},{"title":"Redis（六）集群模式","text":"Redis在使用 RDB 进行持久化时，会 fork 子进程来完成，而fork 操作的用时和 Redis 的数据量是正相关的， fork 在执行时会阻塞主线程。数据量越大，fork 操作造成的主线程阻塞的时间越长。 所以，在使用 RDB 对 几十G的数据进行持久化时，后台运行的子进程在 fork 创建时阻塞了主线程，于是就导致Redis 响应变慢了。因此需要进行切片集群 数据量越来越大该怎么办 纵向扩展：升级单个 Redis 实例的资源配置，包括增加内存容量、增加磁盘容量、使用更高配置的 CPU。就像下图中，原来的实例内存是 8GB，硬盘是 50GB，纵向扩展后，内存增加到 24GB，磁盘增加到 150GB。 横向扩展：横向增加当前 Redis 实例的个数，就像下图中，原来使用 1 个 8GB 内存、50GB 磁盘的实例，现在使用三个相同配置的实例。 Redis ClusterRedis Cluster 方案采用哈希槽（Hash Slot）来处理数据和实例之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。 数据映射到哈希槽过程分为下面两步： 根据键值对的 key，按照CRC16 算法计算一个 16 bit的值； 再用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。 哈希槽映射到redis实例分配过程： 我们在部署 Redis Cluster 方案时，可以使用 cluster create 命令创建集群，此时，Redis会自动把这些槽平均分布在集群实例上。例如，如果集群中有 N 个实例，那么，每个实例上的槽个数为 16384/N 个。 我们也可以使用 cluster meet 命令手动建立实例间的连接，形成集群，再使用cluster addslots 命令，指定每个实例上的哈希槽个数 假设集群中不同 Redis 实例的内存大小配置不一，如果把哈希槽均分在各个实例上，在保存相同数量的键值对时，和内存大的实例相比，内存小的实例就会有更大的容量压力。遇到这种情况时，你可以根据不同实例的资源配置情况，使用 cluster addslots命令手动分配哈希槽。 客户端如何定位到哪个redis实例中呢一般来说，客户端和集群实例建立连接后，实例就会把哈希槽的分配信息发给客户端。但是，在集群刚刚创建的时候，每个实例只知道自己被分配了哪些哈希槽，是不知道其他实例拥有的哈希槽信息的。 那么，客户端为什么可以在访问任何一个实例时，都能获得所有的哈希槽信息呢？这是因为，Redis 实例会把自己的哈希槽信息发给和它相连接的其它实例，来完成哈希槽分配信息的扩散。当实例之间相互连接后，每个实例就有所有哈希槽的映射关系了。客户端收到哈希槽信息后，会把哈希槽信息缓存在本地。当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了。 但是在集群中实例和哈希槽的对应关系会发生变化： 在集群中，实例有新增或删除，Redis 需要重新分配哈希槽； 为了负载均衡，Redis 需要把哈希槽在所有实例上重新分布一遍。 客户端是无法主动感知这些变化的。这就会导致，它缓存的分配信息和最新的分配信息就不一致了， 重定向机制MOVED所谓的“重定向”，就是指，客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令。 那客户端又是怎么知道重定向时的新实例的访问地址呢？当客户端把一个键值对的操作请求发给一个实例时，如果这个实例上并没有这个键值对映射的哈希槽，那么，这个实例就会给客户端返回下面的 MOVED 命令响应结果，这个结果中就包含了新实例的访问地址。 12GET hello:key(error) MOVED 13320 172.16.19.5:6379 MOVED 命令表示，客户端请求的键值对所在的哈希槽 13320，实际是在172.16.19.5 这个实例上。通过返回的 MOVED 命令，就相当于把哈希槽所在的新实例的信息告诉给客户端了。这样一来，客户端就可以直接和 172.16.19.5 连接，并发送操作请求了。 ASK在实际应用时，如果 Slot 2 中的数据比较多，就可能会出现一种情况：客户端向实例 2 发送请求，但此时，Slot 2 中的数据只有一部分迁移到了实例 3，还有部分数据没有迁移。在这种迁移部分完成的情况下，客户端就会收到一条 ASK 报错信息， 12GET hello:key(error) ASK 13320 172.16.19.5:6379 这个结果中的 ASK 命令就表示，客户端请求的键值对所在的哈希槽 13320，在172.16.19.5 这个实例上，但是这个哈希槽正在迁移。此时，客户端需要先给 172.16.19.5这个实例发送一个 ASKING 命令。这个命令的意思是，让这个实例允许执行客户端接下来发送的命令。然后，客户端再向这个实例发送 GET 命令，以读取数据。 在下图中，Slot 2 正在从实例 2 往实例 3 迁移，key1 和 key2 已经迁移过去，key3 和key4 还在实例 2。客户端向实例 2 请求 key2 后，就会收到实例 2 返回的 ASK 命令。 ASK 命令表示两层含义：第一，表明 Slot 数据还在迁移中；第二，ASK 命令把客户端所请求数据的最新实例地址返回给客户端，此时，客户端需要给实例 3 发送 ASKING 命令，然后再发送操作命令。 和 MOVED 命令不同，ASK 命令并不会更新客户端缓存的哈希槽分配信息。所以，在上图中，如果客户端再次请求 Slot 2 中的数据，它还是会给实例 2 发送请求。这也就是说，ASK 命令的作用只是让客户端能给新实例发送一次请求，而不像 MOVED 命令那样，会更改本地缓存，让后续所有命令都发往新实例。 问题：为什么 Redis 不直接用一个表，把键值对和实例的对应关系记录下来？ 如果使用表记录键值对和实例的对应关系，一旦键值对和实例的对应关系发生了变化（例如实例有增减或者数据重新分布），就要修改表。如果是单线程操作表，那么所有操作都要串行执行，性能慢；如果是多线程操作表，就涉及到加锁开销。此外，如果数据量非常大，使用表记录键值对和实例的对应关系，需要的额外存储空间也会增加。 基于哈希槽计算时，虽然也要记录哈希槽和实例的对应关系，但是哈希槽的个数要比键值对的个数少很多，无论是修改哈希槽和实例的对应关系，还是使用额外空间存储哈希槽和实例的对应关系，都比直接记录键值对和实例的关系的开销小得多。","link":"/2021/05/15/Redis%EF%BC%88%E5%85%AD%EF%BC%89%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/"},{"title":"Redis（四）主从复制","text":"Redis 具有高可靠性，又是什么意思呢？其实，这里有两层含义： 一是数据尽量少丢失，通过RDB和AOF来保证 二是服务尽量少中断，增加副本冗余量，将一份数据同时保存在多个实例上。 主从复制的作用 数据的热备份 故障恢复 读写分离 高可用的基石 主从之间的第一次同步的三个阶段当我们启动多个 Redis 实例的时候，它们相互之间就可以通过 replicaof（Redis 5.0 之前使用 slaveof）命令形成主库和从库的关系，之后会按照三个阶段完成数据的第一次同步。 第一阶段是主从库间建立连接、协商同步的过程，主要是为全量复制做准备。在这一步，从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了。 具体来说，从库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。 psync 命令包含了主库的 runID和复制进度 offset两个参数。 runID，是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例。当从库和主库第一次复制时，因为不知道主库的 runID，所以将 runID 设为“？”。 offset，此时设为 -1，表示第一次复制。 FULLRESYNC 响应表示第一次复制采用的全量复制，也就是说，主库会把当前所有的数据都复制给从库 在第二阶段，主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载。这个过程依赖于内存快照生成的 RDB 文件。 具体来说，主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。从库接收到RDB 文件后，会先清空当前数据库，然后加载 RDB 文件。这是因为从库在通过 replicaof命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把当前数据库清空。 在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。否则，Redis 的服务就被中断了。但是，这些请求中的写操作并没有记录到刚刚生成的 RDB 文件中。为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录RDB 文件生成后收到的所有写操作。 第三个阶段，主库会把第二阶段执行过程中新收到的写命令，再发送给从库。具体的操作是，当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了。 主从级联模式 在一次全量复制中，对于主库来说，需要完成两个耗时的操作：生成 RDB 文件和传输 RDB 文件。 如果从库数量很多，而且都要和主库进行全量复制的话，就会导致主库忙于 fork 子进程生成 RDB 文件，进行数据全量同步。fork 这个操作会阻塞主线程处理正常请求，从而导致主库响应应用程序的请求速度变慢。此外，传输 RDB 文件也会占用主库的网络带宽，同样会给主库的资源使用带来压力。那么，有没有好的解决方法可以分担主库压力呢？ 我们可以通过“主 - 从 - 从”模式将主库生成 RDB 和传输 RDB 的压力，以级联的方式分散到从库上 一旦主从库完成了全量复制，它们之间就会一直维护一个网络连接，主库会通过这个连接将后续陆续收到的命令操作再同步给从库，这个过程也称为基于长连接的命令传播，可以避免频繁建立连接的开销。 网络中断后如何实现增量复制复制偏移量master/slave_repl_offset主从服务器都会维护一个复制偏移量，记录存储数据的字节数。 刚开始的时候，主库和从库的写读位置在一起，这算是它们的起始位置。随着主库不断接收新的写操作，它在缓冲区中的写位置会逐步偏离起始位置，我们通常用偏移量来衡量这个偏移距离的大小，对主库来说，对应的偏移量就是 master_repl_offset。 主库接收的新写操作越多，这个值就会越大。同样，从库在复制完写操作命令后，它在缓冲区中的读位置也开始逐步偏移刚才的起始位置，此时，从库已复制的偏移量 slave_repl_offset 也在不断增加。正常情况下，这两个偏移量基本相等。 复制积压缓冲区repl_backlog_buffer当主从库断连后，主库会把断连期间收到的写操作命令，写入 replication buffer，同时也会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区。 repl_backlog_buffer 是一个环形缓冲区，主库会记录自己写到的位置，从库则会记录自己已经读到的位置。 当网络中断重新连接后，从库首先会给主库发送 psync 命令，并把自己当前的slave_repl_offset 发给主库，主库会判断自己的 master_repl_offset 和 slave_repl_offset之间的差距。在网络断连阶段，主库可能会收到新的写操作命令，所以，一般来说，master_repl_offset会大于 slave_repl_offset。此时，主库只用把 master_repl_offset 和 slave_repl_offset之间的命令操作同步给从库就行。 因为 repl_backlog_buffer 是一个环形缓冲区，所以在缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致。 我们可以调整repl_backlog_size这个参数。这个参数和所需的缓冲空间大小有关。 一个 Redis 实例的数据库不要太大，一个实例大小在几 GB 级别比较合适，这样可以减少 RDB 文件生成、传输和重新加载的开销。另外，为了避免多个从库同时和主库进行全量复制，给主库过大的同步压力，我们也可以采用“主 - 从 - 从”这一级联模式，来缓解主库的压力。 有三种模式：全量复制、基于长连接的命令传播，以及增量复制。 命令传播阶段命令传播阶段，从服务器默认每过一秒就会发送replconf ack + 复制偏移量 给主服务器 为什么全量复制用RDB文件而不是AOF RDB是经过压缩之后的二进制文件，无论是把RDB写入磁盘还是通过网络传输，IO效率都比AOF高 RDB恢复数据的效率要高于AOF","link":"/2021/05/13/Redis%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"},{"title":"RocketMQ概念和设计","text":"","link":"/2021/07/13/RocketMQ%E6%A6%82%E5%BF%B5%E5%92%8C%E8%AE%BE%E8%AE%A1/"},{"title":"Redis（五）哨兵模式","text":"如果主库挂了，如何才能不间断的提供服务呢？ 哨兵机制是实现主从库自动切换的关键机制，它有效地解决了主从复制模式下故障转移的这三个问题。 哨兵的主要任务由一个或多个哨兵实例组成的哨兵系统可用监视多个主服务器及下属的所有从服务器，主服务器下线后会自动将从服务器升级成主服务器实现故障转移。哨兵本质上就是一个运行在特殊模式下的 Redis 进程，主从库实例运行的同时，它也在运行。哨兵主要负责的就是三个任务：监控、选主（选择主库）和通知。 监控监控是指哨兵进程在运行时，周期性地给所有的主从库发送 PING 命令，检测它们是否仍然在线运行。如果从库没有在规定时间内响应哨兵的 PING 命令，哨兵就会把它标记为“下线状态”；同样，如果主库也没有在规定时间内响应哨兵的 PING 命令，哨兵就会判定主库下线，然后开始自动切换主库的流程。 选主主库挂了以后，哨兵就需要从很多个从库里，按照一定的规则选择一个从库实例，把它作为新的主库。这一步完成后，现在的集群里就有了新主库。 通知在执行通知任务时，哨兵会把新主库的连接信息发给其他从库，让它们执行 replicaof 命令，和新主库建立连接，并进行数据复制。同时，哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上。 主服务下线的两种状态判定主观下线：哨兵默认每秒向创建命令连接的实例（主从服务器和其他哨兵）发送ping命令，根据回复判定是否在线。在配置文件中配置了down- after-minlliseconds属性，超过了这个时间没有回复就将实例的标识属性计作主观下线（不同哨兵对同一个实例的在线状态判定可能并不相同） 通常会采用多实例组成的集群模式进行部署，这也被称为哨兵集群。引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。 客观下线：哨兵认为主服务器主观下线时，还会向其他哨兵询问，看有多少哨兵也认为这个主服务器下线了，如果超过一个阈值就认为该主服务器客观下线，进行故障转移。 如何选定新主库筛选机制在选主时，除了要检查从库的当前在线状态，还要判断它之前的网络连接状态。如果从库总是和主库断连，而且断连次数超出了一定的阈值，我们就有理由相信，这个从库的网络状况并不是太好，就可以把这个从库筛掉了。 具体怎么判断呢？你使用配置项 down-after-milliseconds * 10。其中，down-after\u0002milliseconds 是我们认定主从库断连的最大连接超时时间。如果在 down-after\u0002milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了。如果发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主库。 打分机制 优先级最高的从库得分高。用户可以通过 slave-priority 配置项，给不同的从库设置不同优先级。比如，你有两个从库，它们的内存大小不一样，你可以手动给内存大的实例设置一个高优先级。在选主时，哨兵会给优先级高的从库打高分，如果有一个从库优先级最高，那么它就是新主库了。如果从库的优先级都一样，那么哨兵开始第二轮打分。 和旧主库同步程度最接近的从库得分高。主从库同步时有个命令传播的过程。在这个过程中，主库会用master_repl_offset 记录当前的最新写操作在 repl_backlog_buffer 中的位置，而从库会用 slave_repl_offset 这个值记录当前的复制进度。此时，我们想要找的从库，它的 slave_repl_offset 需要最接近 master_repl_offset。 ID 号小的从库得分高。在优先级和复制进度都相同的情况下，ID 号最小的从库得分最高，会被选为新主库。 哨兵集群在配置哨兵的信息时，我们只需要用到下面的这个配置项，设置主库的 IP和端口，并没有配置其他哨兵的连接信息。 sentinelmonitor 哨兵实例既然都不知道彼此的地址，又是怎么组成集群的呢？ pub/sub机制Redis的消息订阅发布机制为了区分不同应用的消息，Redis 会以频道的形式，对这些消息进行分门别类的管理。所谓的频道，实际上就是消息的类别。当消息类别相同时，它们就属于同一个频道。反之，就属于不同的频道。只有订阅了同一个频道的应用，才能通过发布的消息进行信息交换。 在主从集群中，主库上有一个名为“sentinel:hello”的频道，不同哨兵就是通过它来相互发现，实现互相通信的。 在下图中，哨兵 1 把自己的 IP（172.16.19.3）和端口（26579）发布到“sentinel:hello”频道上，哨兵 2 和 3 订阅了该频道。那么此时，哨兵 2 和 3 就可以从这个频道直接获取哨兵 1 的 IP 地址和端口号。然后，哨兵 2、3 可以和哨兵 1 建立网络连接。通过这个方式，哨兵 2 和 3 也可以建立网络连接，这样一来，哨兵集群就形成了。它们相互间可以通过网络连接进行通信，比如对主库有没有下线这件事儿进行判断和协商。 哨兵如何和从库建立连接在哨兵的监控任务中，它需要对主从库都进行心跳判断，而且在主从库切换完成后，它还需要通知从库，让它们和新主库进行同步。 哨兵是如何知道从库的 IP 地址和端口的呢？ 这是由哨兵向主库发送 INFO 命令来完成的。就像下图所示，哨兵 2 给主库发送 INFO 命令，主库接受到这个命令后，就会把从库列表返回给哨兵。接着，哨兵就可以根据从库列表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控。哨兵 1和 3 可以通过相同的方法和从库建立连接。 客户端如何感知发生了主库的切换呢每个哨兵实例也提供 pub/sub 机制，客户端可以从哨兵订阅消息。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件。 客户端读取哨兵的配置文件后，可以获得哨兵的地址和端口，和哨兵建立网络连接。 在客户端执行订阅命令，来获取不同的事件消息。 SUBSCRIBE+odown. //订阅“所有实例进入客观下线状态的事件”： PSUBSCRIBE * //订阅所有的事件 当哨兵把新主库选择出来后，客户端就会看到下面的 switch-master 事件。这个事件表示主库已经切换了，新主库的 IP 地址和端口信息已经有了。 switch-master 有了这些事件通知，客户端不仅可以在主从切换后得到新主库的连接信息，还可以监控到主从库切换过程中发生的各个重要事件。这样，客户端就可以知道主从切换进行到哪一步了，有助于了解切换进度。 哨兵的选举过程主库故障以后，哨兵集群有多个实例，那怎么确定由哪个哨兵来进行实际的主从切换呢？ 任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 is-master-down\u0002by-addr 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票。 一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”。这个所需的赞成票数是通过哨兵配置文件中的 quorum 配置项设定的。例如，现在有 5 个哨兵，quorum 配置的是 3，那么，一个哨兵需要 3 张赞成票，就可以标记主库为“客观下线”了。这 3 张赞成票包括哨兵自己的一张赞成票和另外两个哨兵的赞成票。此时，这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为“Leader 选举”。因为最终执行主从切换的哨兵称为 Leader，投票过程就是确定 Leader。 总而言之，任何一个想成为 Leader 的哨兵，要满足两个条件： 第一，拿到半数以上的赞成票； 第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。以 3 个哨兵为例，假设此时的 quorum 设置为 2，那么，任何一个想成为 Leader 的哨兵只要拿到2 张赞成票，就可以了。 在 T1 时刻，S1 判断主库为“客观下线”，它想成为 Leader，就先给自己投一张赞成票，然后分别向 S2 和 S3 发送命令，表示要成为 Leader。 在 T2 时刻，S3 判断主库为“客观下线”，它也想成为 Leader，所以也先给自己投一张赞成票，再分别向 S1 和 S2 发送命令，表示要成为 Leader。 在 T3 时刻，S1 收到了 S3 的 Leader 投票请求。因为 S1 已经给自己投了一票 Y，所以它不能再给其他哨兵投赞成票了，所以 S1 回复 N 表示不同意。同时，S2 收到了 T2 时 S3发送的 Leader 投票请求。因为 S2 之前没有投过票，它会给第一个向它发送投票请求的哨兵回复 Y，给后续再发送投票请求的哨兵回复 N，所以，在 T3 时，S2 回复 S3，同意 S3成为 Leader。 在 T4 时刻，S2 才收到 T1 时 S1 发送的投票命令。因为 S2 已经在 T3 时同意了 S3 的投票请求，此时，S2 给 S1 回复 N，表示不同意 S1 成为 Leader。发生这种情况，是因为S3 和 S2 之间的网络传输正常，而 S1 和 S2 之间的网络传输可能正好拥塞了，导致投票请求传输慢了。 在 T5 时刻，S1 得到的票数是来自它自己的一票 Y 和来自 S2 的一票 N。而 S3 除了自己的赞成票 Y 以外，还收到了来自 S2 的一票 Y。此时，S3 不仅获得了半数以上的Leader 赞成票，也达到预设的 quorum 值（quorum 为 2），所以它最终成为了Leader。接着，S3 会开始执行选主操作，而且在选定新主库后，会给其他从库和客户端通知新主库的信息。 如果 S3 没有拿到 2 票 Y，那么这轮投票就不会产生 Leader。哨兵集群会等待一段时间（也就是哨兵故障转移超时时间的 2 倍），再重新选举。这是因为，哨兵集群能够进行成功投票，很大程度上依赖于选举命令的正常网络传播。如果网络压力较大或有短时堵塞，就可能导致没有一个哨兵能拿到半数以上的赞成票。所以，等到网络拥塞好转之后，再进行投票选举，成功的概率就会增加。","link":"/2021/05/13/Redis%EF%BC%88%E4%BA%94%EF%BC%89%E5%93%A8%E5%85%B5%E6%A8%A1%E5%BC%8F/"},{"title":"SpringBoot（三）整合视图层","text":"","link":"/2021/08/10/SpringBoot%EF%BC%88%E4%B8%89%EF%BC%89%E6%95%B4%E5%90%88%E8%A7%86%E5%9B%BE%E5%B1%82/"},{"title":"SpringBoot（一）入门","text":"SpringBoot入门主要讲了简介和如何快速使用！ SpringBoot简介HelloWorld 新建一个SpringBoot项目：IDEA -&gt; File -&gt; New -&gt;New Project -&gt; Spring Initializr(https://start.spring.io) -&gt;next即可。 依赖分析： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!--spring-boot-starter-parent指定了当前项目为一个Spring Boot项目，它提供了诸多的默认Maven依赖--&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.5.3&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.binshow&lt;/groupId&gt; &lt;artifactId&gt;Chapter2_DataManager&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;Chapter2_DataManager&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;!--properties标签可以手动的改依赖的版本--&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--maven会自动下载spring-boot-starter-web模块所包含的jar文件--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 快速的搭建一个web服务： 12345678910111213@RestController@SpringBootApplicationpublic class DemoApplication { @RequestMapping(&quot;/&quot;) String index() { return &quot;hello spring boot&quot;; } public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); }} 访问http://localhost:8080/看到hello spring boot说明启动成功了。 基础配置全局配置文件application.properties在src/main/resources目录下，Spring Boot提供了一个名为application.properties的全局配置文件，可对一些默认配置的配置值进行修改 不仅仅可以配置所有官方属性，还可以自定义属性： 在application.properties定义一些属性： 12binshow.name = shengbinbinbinshow.age = 18 自己编写一个Bean：通过Value注解取到配置文件中的值 12345678910@Component@Datapublic class BinshowPropeties { //通过 @Value(&quot;${属性名}&quot;) 来加载配置文件中的属性值 @Value(&quot;${binshow.name}&quot;) private String name; @Value(&quot;${binshow.age}&quot;) private int age;} 编写Controller，查看是否注入到属性了 12345678910@RestControllerpublic class IndexController { @Autowired private BinshowPropeties binshowPropeties; @RequestMapping(&quot;/&quot;) String index() { return binshowPropeties.getName()+&quot;，&quot;+binshowPropeties.getAge(); }} http://localhost:8080/ 看到shengbinbin，18。说明值已经被取出来了。 在属性非常多的情况下，也可以定义一个和配置文件对应的Bean： 12345678910@ConfigurationProperties(prefix = &quot;binshow&quot;) // 注入配置文件中前缀为 binshow的属性@Configuration@Datapublic class ConfigBean { private String name; private int age; private String address;} 配置文件中的属性之间还可以相互引用： 1234binshow.name = shengbinbinbinshow.age = 18binshow.address = anhuiWuhubinshow.info = ${binshow.name}--${binshow.age} 自定义配置文件在src/main/resources目录下新建一个test.properties: 12test.name=zkdtest.age=25 测试： 123456789@Data@Configuration@ConfigurationProperties(prefix = &quot;test&quot;)@PropertySource(&quot;classpath:test.properties&quot;) // 指定要用到的配置文件public class TestConfigBean { private String name; private int age;} 1234567891011121314151617181920212223242526@RestControllerpublic class IndexController { @Autowired private BinshowPropeties binshowPropeties; @Autowired private ConfigBean configBean; @Autowired private TestConfigBean testConfigBean; @RequestMapping(&quot;/&quot;) String index() { return binshowPropeties.getName()+&quot;，&quot;+binshowPropeties.getAge(); } @RequestMapping(&quot;/config&quot;) String indexConfig() { return configBean.getName()+&quot;, &quot;+configBean.getAge() + &quot;, &quot; + configBean.getAddress(); } @RequestMapping(&quot;/test&quot;) String indexTest() { return testConfigBean.getName()+&quot;, &quot;+testConfigBean.getAge(); }}","link":"/2021/08/10/SpringBoot%EF%BC%88%E4%B8%80%EF%BC%89%E5%85%A5%E9%97%A8/"},{"title":"Spring中的AOP","text":"","link":"/2021/10/08/Spring%E4%B8%AD%E7%9A%84AOP/"},{"title":"SpringBoot（二）整合持久层","text":"本节主要讲述的是SpringBoot对数据层的操作，包括： 对数据源的自动装配 对jdbc的整合 对mybatis的整合 对mybatis-plus的整合 以最新的Spring-boot 2.5.3 为例： 一、SQL数据源的自动配置-HikariDataSource导入JDBC场景123456789101112131415 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jdbc&lt;/artifactId&gt; &lt;/dependency&gt;查看这个依赖整合了哪些：spring-boot-starter-data-jdbc： - spring-boot-start-jdbc -HikariCP：数据源 -Spring-jdbc - spring-data-jdbc 数据库驱动？ 为什么导入JDBC场景，官方不导入驱动？官方不知道我们接下要操作什么数据库。 数据库版本和驱动版本对应： 12345678910 &lt;!--导入mysql驱动--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt;默认版本： &lt;version&gt;8.0.26&lt;/version&gt;可以自定义修改版本： 分析自动配置自动配置的类： DataSourceAutoConfiguration：数据源的自动配置 1234567891011121314151617181920212223242526272829package org.springframework.boot.autoconfigure.jdbc;@Configuration(proxyBeanMethods = false)@ConditionalOnClass({ DataSource.class, EmbeddedDatabaseType.class })@ConditionalOnMissingBean(type = &quot;io.r2dbc.spi.ConnectionFactory&quot;)@EnableConfigurationProperties(DataSourceProperties.class)@Import({ DataSourcePoolMetadataProvidersConfiguration.class, DataSourceInitializationConfiguration.InitializationSpecificCredentialsDataSourceInitializationConfiguration.class, DataSourceInitializationConfiguration.SharedCredentialsDataSourceInitializationConfiguration.class })public class DataSourceAutoConfiguration { @Configuration(proxyBeanMethods = false) @Conditional(EmbeddedDatabaseCondition.class) @ConditionalOnMissingBean({ DataSource.class, XADataSource.class }) @Import(EmbeddedDataSourceConfiguration.class) protected static class EmbeddedDatabaseConfiguration { } //数据库连接池的配置 @Configuration(proxyBeanMethods = false) @Conditional(PooledDataSourceCondition.class) @ConditionalOnMissingBean({ DataSource.class, XADataSource.class }) @Import({ DataSourceConfiguration.Hikari.class, DataSourceConfiguration.Tomcat.class, DataSourceConfiguration.Dbcp2.class, DataSourceConfiguration.OracleUcp.class, DataSourceConfiguration.Generic.class, DataSourceJmxConfiguration.class }) protected static class PooledDataSourceConfiguration { } 12@ConfigurationProperties(prefix = &quot;spring.datasource&quot;)public class DataSourceProperties implements BeanClassLoaderAware, InitializingBean { 修改数据源的相关配置：spring.datasource 数据库连接池的配置，是自己容器中没有DataSource才自动配置的 底层配置好的连接池是：HikariDataSource DataSourceTransactionManagerAutoConfiguration：事务管理器的自动配置 JdbcTemplateAutoConfiguration：JdbcTemplate的自动配置，可以来对数据库进行crud 123456789101112package org.springframework.boot.autoconfigure.jdbc;@Configuration(proxyBeanMethods = false)@ConditionalOnClass({ DataSource.class, JdbcTemplate.class })@ConditionalOnSingleCandidate(DataSource.class)@AutoConfigureAfter(DataSourceAutoConfiguration.class)@EnableConfigurationProperties(JdbcProperties.class)@Import({ DatabaseInitializationDependencyConfigurer.class, JdbcTemplateConfiguration.class, NamedParameterJdbcTemplateConfiguration.class })public class JdbcTemplateAutoConfiguration {} 123@ConfigurationProperties(prefix = &quot;spring.jdbc&quot;)public class JdbcProperties { 修改jdbcTemplate的相关配置：spring.jdbc JndiDataSourceAutoConfiguration： jndi的自动配置 XADataSourceAutoConfiguration： 分布式事务相关的 修改配置项123456spring: datasource: url: jdbc:mysql://localhost:3306/db_account username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver 测试：123456789101112131415161718@Slf4j@SpringBootTestclass Boot05WebAdminApplicationTests { @Autowired JdbcTemplate jdbcTemplate; @Test void contextLoads() {// jdbcTemplate.queryForObject(&quot;select * from account_tbl&quot;)// jdbcTemplate.queryForList(&quot;select * from account_tbl&quot;,) Long aLong = jdbcTemplate.queryForObject(&quot;select count(*) from account_tbl&quot;, Long.class); log.info(&quot;记录总数：{}&quot;,aLong); }} 使用Druid数据源官方文档：https://github.com/alibaba/druid 自定义使用步骤 引入Druid的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; 编写自己的配置类： 1234567891011121314151617/** * @author shengbinbin * @description: Druid 数据源的配置类 * @date 2021/8/118:19 下午 */@Configurationpublic class MyDataSourceConfig { // SpringBoot 默认的自动配置类 是先判断容器中有没有DataSource.class,没有才会自动配置 @ConfigurationProperties(&quot;spring.datasource&quot;) // 将配置文件中 spring.datasource 前缀的值相注入到dataSource中 @Bean public DataSource dataSource(){ DruidDataSource druidDataSource = new DruidDataSource(); return druidDataSource; } } 编写yaml配置文件，以spring.datasource输入一些属性注入数据源： 123456spring: datasource: url: jdbc:mysql://localhost:3306/binshow username: root password: bin123456 driver-class-name: com.mysql.cj.jdbc.Driver 编写测试类，观察现在容器中的数据源是什么：(发现已经是Druid了) 12345678910111213@SpringBootTest@Slf4jclass ApplicationTests { @Autowired DataSource dataSource; @Test void contextLoads() { log.info(&quot;容器中的数据源类型为：{}&quot;,dataSource.getClass()); //容器中的数据源类型为：class com.alibaba.druid.pool.DruidDataSource }} 查看监控页添加一个Servlet 组件，StatViewServlet的用途包括： 提供监控信息展示的html页面 提供监控信息的JSON API 添加一个Servlet：ServletRegistrationBean，并设置druidDataSource的Filter的属性 12345678910111213141516171819202122232425@Configurationpublic class MyDataSourceConfig { // SpringBoot 默认的自动配置类 是先判断容器中有没有DataSource.class,没有才会自动配置 @ConfigurationProperties(&quot;spring.datasource&quot;) // 将配置文件中 spring.datasource 前缀的值相注入到dataSource中 @Bean public DataSource dataSource() throws SQLException { DruidDataSource druidDataSource = new DruidDataSource(); // 开启监控功能 druidDataSource.setFilters(&quot;stat&quot;); return druidDataSource; } /** * 配置Druid的监控页功能 * @return 返回注入的Bean */ @Bean public ServletRegistrationBean statViewServlet(){ StatViewServlet statViewServlet = new StatViewServlet(); ServletRegistrationBean&lt;StatViewServlet&gt; registrationBean = new ServletRegistrationBean&lt;&gt;(statViewServlet, &quot;/druid/*&quot;); return registrationBean; }} 开启Web服务，访问http://127.0.0.1:8080/druid/ 发现是可以正常访问的。 随便开启一个sql查询，访问：http://127.0.0.1:8080/sql 123456789101112@Controllerpublic class IndexController { @Autowired private JdbcTemplate jdbcTemplate; @GetMapping(&quot;/sql&quot;) @ResponseBody public String getFromDb(){ Long res = jdbcTemplate.queryForObject(&quot;select count(*) from user&quot;, Long.class); return res.toString(); }} StatFilter用于统计监控信息；如SQL监控、URI监控 123需要给数据源中配置如下属性；可以允许多个filter，多个用，分割；如：&lt;property name=&quot;filters&quot; value=&quot;stat,slf4j&quot; /&gt; 系统中所有filter： 别名 Filter类名 default com.alibaba.druid.filter.stat.StatFilter stat com.alibaba.druid.filter.stat.StatFilter mergeStat com.alibaba.druid.filter.stat.MergeStatFilter encoding com.alibaba.druid.filter.encoding.EncodingConvertFilter log4j com.alibaba.druid.filter.logging.Log4jFilter log4j2 com.alibaba.druid.filter.logging.Log4j2Filter slf4j com.alibaba.druid.filter.logging.Slf4jLogFilter commonlogging com.alibaba.druid.filter.logging.CommonsLogFilter 慢SQL记录配置 123456&lt;bean id=&quot;stat-filter&quot; class=&quot;com.alibaba.druid.filter.stat.StatFilter&quot;&gt; &lt;property name=&quot;slowSqlMillis&quot; value=&quot;10000&quot; /&gt; &lt;property name=&quot;logSlowSql&quot; value=&quot;true&quot; /&gt;&lt;/bean&gt;使用 slowSqlMillis 定义慢SQL的时长 使用官方starter方式引入druid-starter12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.17&lt;/version&gt;&lt;/dependency&gt; 分析自动配置 扩展配置项 spring.datasource.druid DruidSpringAopConfiguration.class, 监控SpringBean的；配置项：spring.datasource.druid.aop-patterns DruidStatViewServletConfiguration.class, 监控页的配置：spring.datasource.druid.stat-view-servlet；默认开启 DruidWebStatFilterConfiguration.class, web监控配置；spring.datasource.druid.web-stat-filter；默认开启 DruidFilterConfiguration.class}) 所有Druid自己filter的配置 12345678private static final String FILTER_STAT_PREFIX = &quot;spring.datasource.druid.filter.stat&quot;;private static final String FILTER_CONFIG_PREFIX = &quot;spring.datasource.druid.filter.config&quot;;private static final String FILTER_ENCODING_PREFIX = &quot;spring.datasource.druid.filter.encoding&quot;;private static final String FILTER_SLF4J_PREFIX = &quot;spring.datasource.druid.filter.slf4j&quot;;private static final String FILTER_LOG4J_PREFIX = &quot;spring.datasource.druid.filter.log4j&quot;;private static final String FILTER_LOG4J2_PREFIX = &quot;spring.datasource.druid.filter.log4j2&quot;;private static final String FILTER_COMMONS_LOG_PREFIX = &quot;spring.datasource.druid.filter.commons-log&quot;;private static final String FILTER_WALL_PREFIX = &quot;spring.datasource.druid.filter.wall&quot;; 配置示例1234567891011121314151617181920212223242526272829303132spring: datasource: url: jdbc:mysql://localhost:3306/db_account username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver druid: aop-patterns: com.atguigu.admin.* #监控SpringBean filters: stat,wall # 底层开启功能，stat（sql监控），wall（防火墙） stat-view-servlet: # 配置监控页功能 enabled: true login-username: admin login-password: admin resetEnable: false web-stat-filter: # 监控web enabled: true urlPattern: /* exclusions: '*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*' filter: stat: # 对上面filters里面的stat的详细配置 slow-sql-millis: 1000 logSlowSql: true enabled: true wall: enabled: true config: drop-table-allow: false SpringBoot配置示例 https://github.com/alibaba/druid/tree/master/druid-spring-boot-starter 配置项列表https://github.com/alibaba/druid/wiki/DruidDataSource%E9%85%8D%E7%BD%AE%E5%B1%9E%E6%80%A7%E5%88%97%E8%A1%A8 整合MyBatis操作https://github.com/mybatis 1234&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt; 配置模式 全局配置文件 SqlSessionFactory: 自动配置好了 SqlSession：自动配置了 SqlSessionTemplate 组合了SqlSession @Import(AutoConfiguredMapperScannerRegistrar.class）； Mapper： 只要我们写的操作MyBatis的接口标准了 @Mapper 就会被自动扫描进来 123456@EnableConfigurationProperties(MybatisProperties.class) ： MyBatis配置项绑定类。@AutoConfigureAfter({ DataSourceAutoConfiguration.class, MybatisLanguageDriverAutoConfiguration.class })public class MybatisAutoConfiguration{}@ConfigurationProperties(prefix = &quot;mybatis&quot;)public class MybatisProperties 可以修改配置文件中 mybatis 开始的所有： 12345678910111213141516# 配置mybatis规则mybatis: config-location: classpath:mybatis/mybatis-config.xml #全局配置文件位置 mapper-locations: classpath:mybatis/mapper/*.xml #sql映射文件位置 Mapper接口---&gt;绑定Xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.atguigu.admin.mapper.AccountMapper&quot;&gt;&lt;!-- public Account getAcct(Long id); --&gt; &lt;select id=&quot;getAcct&quot; resultType=&quot;com.atguigu.admin.bean.Account&quot;&gt; select * from account_tbl where id=#{id} &lt;/select&gt;&lt;/mapper&gt; 配置 private Configuration configuration; mybatis.configuration下面的所有，就是相当于改mybatis全局配置文件中的值 12345678# 配置mybatis规则mybatis:# config-location: classpath:mybatis/mybatis-config.xml mapper-locations: classpath:mybatis/mapper/*.xml configuration: map-underscore-to-camel-case: true 可以不写全局；配置文件，所有全局配置文件的配置都放在configuration配置项中即可 导入mybatis官方starter 编写mapper接口。标准@Mapper注解 编写sql映射文件并绑定mapper接口 在application.yaml中指定Mapper配置文件的位置，以及指定全局配置文件的信息 （建议；配置在mybatis.configuration） 2、注解模式123456789@Mapperpublic interface CityMapper { @Select(&quot;select * from city where id=#{id}&quot;) public City getById(Long id); public void insert(City city);} 3、混合模式123456789@Mapperpublic interface CityMapper { @Select(&quot;select * from city where id=#{id}&quot;) public City getById(Long id); public void insert(City city);} 最佳实战： 引入mybatis-starter 配置application.yaml中，指定mapper-location位置即可 编写Mapper接口并标注@Mapper注解 简单方法直接注解方式 复杂方法编写mapper.xml进行绑定映射 @MapperScan(“com.atguigu.admin.mapper”) 简化，其他的接口就可以不用标注@Mapper注解 整合MyBatisPlus操作二、NoSQL整合Redis","link":"/2021/08/10/SpringBoot%EF%BC%88%E4%BA%8C%EF%BC%89%E6%95%B4%E5%90%88%E6%8C%81%E4%B9%85%E5%B1%82/"},{"title":"Spring中的IOC","text":"源码分析IOC的过程和常见面试题 IOC概述依赖倒置原则全称是（Dependency Inversion Principle ），这是软件设计的一种重要思想。 那么如何理解这种设计原则呢？ 比如说我们现在要设计一个小汽车，可能一开始的设计方案是这样的： 轮胎 → 底盘 → 车身 → 小汽车 这个时候如果用户突然说要改轮胎的尺寸，那么我们所有的设计都需要去修改了，非常麻烦！ 如果换一种设计方案呢： 小汽车 → 车身 → 底盘 → 轮胎 这样的话如果要修改轮胎就不会牵一发而动全身了。 这就是依赖倒置原则：把原本的高层建筑依赖底层建筑“倒置”过来，变成底层建筑依赖高层建筑。高层建筑决定需要什么，底层去实现这样的需求，但是高层并不用管底层是怎么实现的。 控制反转IoC（Inverse of Control:控制反转）就是基于依赖倒置原则的一种代码设计的思路。就是 将原本在程序中手动创建对象的控制权，交由Spring框架来管理。 依靠的方法就是 依赖注入（Dependency Injection） IoC 容器是 Spring 用来实现 IoC 的载体，将对象之间的相互依赖关系交给 IOC 容器来管理，并由 IOC 容器完成对象属性的注入。 两个好处： 通过配置文件对代码初始化，解决对象之间的依赖关系。 创建实例的时候不需要了解里面的细节。 总而言之，就是通过配置文件来解耦对象创建和属性注入的两个关键步骤。 IOC容器启动源码解析面试问题总结Spring中的单例Bean的线程安全问题是如何解决的？采用的是ThreadLocal线程级变量存放的BeanDefinition。 Spring中bean的生命周期 IOC容器先找到配置文件中Bean的定义，加载到容器中形成一个beanDefiniton 实例化BeanFactoryPostProcess这个类，执行它的post方法。 利用反射创建Bean对象 注入属性，这里会有一些实现Aware接口的方法，比如BeanNameAware 和 BeanFactoryAware 等的方法都会执行。 执行BeanPostProcess中的前置处理方法。 检测是否有 调用 标签下的 init-method 方法执行初始化方法 BeanPostProcess 的后置处理方法 可以使用了~ 使用完之后调用DisposableBean中的destory方法或者 配置了自定义的destory方法。 Spring中Bean的作用域singleton : 唯一 bean 实例，Spring 中的 bean 默认都是单例的。 prototype : 每次请求都会创建一个新的 bean 实例。 request : 每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效。 session : 每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效。 global-session： 全局session作用域，仅仅在基于portlet的web应用中才有意义，Spring5已经没有了。Portlet是能够生成语义代码(例如：HTML)片段的小型Java Web插件。它们基于portlet容器，可以像servlet一样处理HTTP请求。但是，与 servlet 不同，每个 portlet 都有不同的会话 FactoryBean 和BeanFactory的区别ApplicationContext和BeanFactory的区别Spring中用到了哪些设计模式工厂设计模式 : Spring使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。","link":"/2021/10/08/Spring%E4%B8%AD%E7%9A%84IOC/"},{"title":"effectiveJava&lt;类与接口&gt;","text":"本章主要讲述的是关于类和接口的几个设计原则： 设计接口的时候预留后面的变动 只能用接口来定义类型，不能定义常量 类的层次结构优先于标签类 静态成员类优于非静态成员类 限制源文件为单个顶级类 Item21 设计接口的时候预留后面的变动为后面的变动来设计接口 在Java8之前，是不可能在已经存在实现类的接口中添加方法的，因为一旦在接口中添加的话，所有的实现类都要进行修改。 在Java8之后，接口中引入了默认方法构造，以此来给存在实现类的接口中添加方法，但是这还是比较有风险的。 123456789101112131415161718192021222324252627282930313233public interface UserService { //1. addUser 和 deleteUser 方法所有UserService的实现类都要重写 int addUser(); void deleteUser(); //2. updateUser 属于默认方法，实现类可以选择重写，也可以选择不重写 // 有一个问题就是无法保证 default void updateUser(){ //提供一些默认的方法实现 }}public class UserServiceImpl implements UserService{ @Override public int addUser() { return 0; } @Override public void deleteUser() { }}public static void main(String[] args) { UserServiceImpl userService = new UserServiceImpl(); // UserServiceImpl 这个实现类并没有实现 userService 接口的 updateUser 方法，但是也可以调用 // 这里就存在一个问题： 默认的方法实现不一定保证在所有的实现类中都是有效的！ userService.updateUser();// } 默认的方法实现便利了lambda表达式，但不一定保证在所有的实现类中都是有效的！ 1234567891011121314//java.util.Collection#removeIf //Collection 接口中的默认方法default boolean removeIf(Predicate&lt;? super E&gt; filter) { Objects.requireNonNull(filter); boolean removed = false; final Iterator&lt;E&gt; each = iterator(); //1. 用迭代器遍历集合 while (each.hasNext()) { if (filter.test(each.next())) { //2. 用断言来确定是否要移除该元素 each.remove(); removed = true; } } return removed; } 这个默认实现已经是 最好的通用方式了，但是在下面的包中的某些实现类来说并不能正常工作 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-collections4&lt;/artifactId&gt; &lt;version&gt;4.4&lt;/version&gt; &lt;/dependency&gt; 123456789101112131415161718//org.apache.commons.collections4.collection.SynchronizedCollection //所有的方法在委托给包装的集合之前，都先在锁定对象上进行同步。public class SynchronizedCollection&lt;E&gt; implements Collection&lt;E&gt;, Serializable { private static final long serialVersionUID = 2412805092710877986L; private final Collection&lt;E&gt; collection; protected final Object lock; //Apache的版本添加了一个可以使用客户端提供的对象来锁定的功能 //... //已经覆盖了这个方法，如果没有覆盖的话就会调用默认的方法实现（接口里面的），这样的话就对锁object的情况不知道，。 //如果调用了一个SynchronizedCollection实例的removeIf方法，而另一个线程又并发地修改了这个集合，就会导致ConcurrentModificationException或者其他异常行为 public boolean removeIf(Predicate&lt;? super E&gt; filter) { synchronized(this.lock) { return this.decorated().removeIf(filter); } } } 为了防止类似的事情出现在Java平台类库的实现里，比如Collections.synchronizedCollection返回的包级私有类里，JDK的维护者必须要覆盖默认的removeIf方法，以及其他的类似需要在调用默认方法前执行必要同步的方法。而不属于Java平台的已经存在的类就没有机会和接口做同步的修改，有些现在已经修改了。 结论： 除非必要，否则应该避免使用默认方法来给已经存在的接口添加新的方法。在添加的时候，也必须认真努力思考，是否有已存在的接口实现可能会被新增的默认方法实现破坏。 Item22 只能用接口来定义类型，不能定义常量只用接口来定义类型，而不能定义常量 12345// 常量接口，是明令禁止的。public interface PhysicalConstants { static final double AVOGADROS_NUMBER = 6.022; static final double ELECTRON_MASS = 6.022;} 如果确实要定义某些常量，一般有以下三种方式： 如果常量只和某些类或接口有关系，就定义到类或接口中。比如Integer中就封装了MIN_VALUE和MAX_VALUE 如果这些常量可以很好地看做是枚举类型的成员，那就应该用枚举类型（Item34)来导出它们。 用一个不可实例化的工具类（Item4）来导出这些常量 12345678910public class PhysicalConstants { private PhysicalConstants() { } // Prevents instantiation public static final double AVOGADROS_NUMBER = 6.022_140_857e23; public static final double BOLTZMANN_CONST =1.380_648_52e-23; public static final double ELECTRON_MASS = 9.109_383_56e-31;}// 通过 类名.常量名 来访问这个常量 double avogadrosNumber = PhysicalConstants.AVOGADROS_NUMBER; Item23 类的层次结构优先于标签类类的层次性要优于标签类 Item24 静态成员类优于非静态成员类嵌套类就是定义在其他类里面的类。嵌套类存在的目的只是为其外围类提供服务。 内部类分为以下四种：静态成员类、非静态成员类、匿名类、和局部类 静态成员类: 静态成员类是最简单的嵌套类。最好是把它看做普通的类，只是碰巧声明在了另一个类内部，同时可以访问外围类的所有成员，包括哪些声明为私有的成员。静态成员类也是其外围类的一个静态成员，和其他的静态成员遵守相同的访问规则。如果它被声明为私有的，那么他就只能被外围类访问，等等。 一个静态成员类的常见用法是作为公有的辅助类，只有和其外围类一起合作才有用。 1234567891011121314151617181920212223242526272829303132333435363738//String的compareToIgnoreCase方法public int compareToIgnoreCase(String str) { return CASE_INSENSITIVE_ORDER.compare(this, str);} // 外围类这里做了一个封装。public static final Comparator&lt;String&gt; CASE_INSENSITIVE_ORDER = new CaseInsensitiveComparator(); // 这个私有静态内部类的功能只有一个，就是提供一个忽略大小写的字符串比较的功能 // 并不需要使用到外围类String的属性或者方法。所以设计为静态成员类 private static class CaseInsensitiveComparator implements Comparator&lt;String&gt;, java.io.Serializable { // use serialVersionUID from JDK 1.2.2 for interoperability private static final long serialVersionUID = 8575799808933029326L; public int compare(String s1, String s2) { int n1 = s1.length(); int n2 = s2.length(); int min = Math.min(n1, n2); for (int i = 0; i &lt; min; i++) { char c1 = s1.charAt(i); char c2 = s2.charAt(i); if (c1 != c2) { c1 = Character.toUpperCase(c1); c2 = Character.toUpperCase(c2); if (c1 != c2) { c1 = Character.toLowerCase(c1); c2 = Character.toLowerCase(c2); if (c1 != c2) { // No overflow because of numeric promotion return c1 - c2; } } } } return n1 - n2; } 非静态成员类: 从语法上，静态和非静态成员类之间的区别只在于其声明中是否有static修饰符。 但是这两种内部类是非常不同的。每一个非静态内部类都隐式地和一个指向外围类实例相关联。在非静态成员类的实例方法中，你可以使用修饰过的this来获得外围实例的引用，从而调用外围实例的方法。如果一个嵌套类的实例，可以与其外部类实例分开而独立存在，那么这个嵌套类就必须是静态成员类。在不存在外围类实例的情况下，无法创建非静态成员类的实例。 当这个非静态成员类的实例被创建的时候，该实例和其外部类实例之间的关联就建立起来了，并且，这种关联关系不能被修改了。通常情况下，这种关联是通过外围类的实例方法调用非静态成员类的构造器，自动创建的。也可以通过表达式“enclosingInstance.new MemberClass(args)”手动地创建这中关系，但实际中很少这么做。正如你所想的那样，这种关联会占用非静态成员类实例的空间，并且会增加其构造的时间。 非静态成员类的一个常见用法是定义一个Adapter。允许一个外围类的实例被看做是一些不相关的实例类的实例。比如Set和List，通常也使用非静态成员类来实现他们的迭代器。如下： 如果内嵌类的实例需要用到外围类实例的属性或方法的时候，使用非静态成员类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445//ArrayList的迭代器方法——iterator() // 使用到了外围类的属性和方法，符合使用非静态成员类的条件 ,所以此处不应该加static来修饰 // 没有ArrayList对象，独立于ArrayList对象的new Itr()没有意义 private class Itr implements Iterator&lt;E&gt; { int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such int expectedModCount = modCount; // modCount 为外围类的属性 Itr() {} public boolean hasNext() { return cursor != size; } @SuppressWarnings(&quot;unchecked&quot;) public E next() { checkForComodification(); int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i]; } public void remove() { if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try { ArrayList.this.remove(lastRet); // 这里的this也是外围类的实例 cursor = lastRet; lastRet = -1; expectedModCount = modCount; } catch (IndexOutOfBoundsException ex) { throw new ConcurrentModificationException(); } } //。。。 } 如果你声明的成员类不需要访问其外围类实例，那么就应该在声明中添加static修饰符，让这个成员类是静态的而不是非静态的。如果你省略了这个修饰符，每一个内部类实例都会有一个额外的隐藏的指向外部类实例的引用。正如前面提到的那样，保存这个引用会花费时间和空间。更严重地是，它可能导致一个本该被垃圾回收的外部类实例被保留下来，这个问题导致的内存泄露问题是灾难性的。但是却难以发现，因为这个引用时不可见的。 私有静态成员类的常见的用法是代表外围类代表的对象组件。以Map实例为例，它把键值关联起来，在很多的Map的实现中，都有一个内部Entry对象来表示map中的每一个键值对。虽然每一个entry都和map相关联，但是其方法（getKey, getValue, 和setValue）都不需要访问map。因此，使用非静态成员类来表示entry是比较浪费的，私有静态成员类就是最好的选择。如果你不小心漏掉了entry声明前的stati修饰符，这个map也能工作，但是它的每一个entry都包含一个指向map的多余的引用，既浪费时间又浪费空间。 匿名类: 匿名类是没有名字的。它也不是其外围类的一个成员。在使用的时候同时声明和实例化，而不是和其他成员一起被声明。匿名类可以在代码中任何地方出现，只要其表达式是正确的。在lambda被加入到Java之前（Chapter6），匿名类是用来创建小的函数对象和过程对象的最佳方法，但是现在lambda表达式更受欢迎了（Item42）。另外一个匿名类的常见用法是在静态工厂方法的实现中（见Item20里的intArrayAsList）。 局部类: 局部类在四种嵌套类中使用频率最低。一个局部类几乎可以在任何本地变量可以声明的地方进行声明，并且遵守和本地变量相同的作用域规则。局部类和其他嵌套类有都一点共同的属性。和成员类一样，局部类有名字，也可以被重复使用；和匿名类样，局部类也只有定义在非静态环境中才拥有外围实例对象，也不能包含静态成员；和匿名类一样，局部类为了不破坏代码可读性，也应该很短。 Item25 限制源文件为单个顶级类永远都不要在一个源文件里写多个顶级类或者接口","link":"/2021/07/27/effectiveJava-%E7%B1%BB%E4%B8%8E%E6%8E%A5%E5%8F%A3/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/05/06/hello-world/"},{"title":"优惠券项目","text":"讲一下优惠券项目 项目介绍 主要提供了对发券活动、券信息、发券规则的管理能力 提供了通过不同类型比如券id、发券规则、发券活动等查询优惠券的能力 提供了手动发券和补发券的能力 对订单支付时间的MQ的处理发券 定时job 计算人工发券规则和系统发券规则的使用率变化情况 实体类解析优惠券的管理主要分为三个维度：优惠券活动、优惠券信息、发券规则 核心资源： CouponActivity 优惠券活动：一次向用户发放优惠券的活动。比如说我们续报期就会发券来激励用户续报班课。 一个优惠券活动表主要有id、创建人名称、活动名称、活动描述和ctime、utime。其中我们会在名称和创建人哪里建索引。方便查找 12345678910111213CREATE TABLE IF NOT EXISTS `coupon_activity` ( `id` BIGINT(11) NOT NULL AUTO_INCREMENT COMMENT '优惠券活动id', `name` VARCHAR(255) NOT NULL COMMENT '优惠券活动名称', `desc` VARCHAR(255) NOT NULL COMMENT '优惠券活动描述', `ldap` VARCHAR(20) NOT NULL COMMENT '优惠券活动ldap', `createdTime` BIGINT(20) NOT NULL COMMENT '创建时间', `updatedTime` BIGINT(20) NOT NULL COMMENT '更新时间', PRIMARY KEY (`id`), KEY `nameIndex` (`name`), KEY `ldapIndex` (`ldap`)) ENGINE=InnoDB;ALTER TABLE `coupon_activity` MODIFY COLUMN `id` BIGINT AUTO_INCREMENT NOT NULL COMMENT '优惠券活动id'; CouponJobBaseInfo 券信息：包含 发券活动 和 券模板两个id 一张优惠券的基础信息，包括了两个重要的字段： activityId（所属优惠券活动）、 couponTemplateId（所发优惠券的模板 - CouponTemplate） CouponTemplate 优惠券模板：被包含在 CouponJobBaseInfo 中，是优惠券的模板，发券时服务端根据 CouponTemplate 生成具体的优惠券 123456789101112131415161718CREATE TABLE `coupon_job_base_info` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `name` VARCHAR(255) NOT NULL COMMENT '基础信息名称', `activityId` INT(11) NOT NULL DEFAULT -1 comment '发券活动Id', `couponTemplateId` INT(11) NOT NULL DEFAULT -1 comment '发券模板Id', `type` TINYINT NOT NULL DEFAULT -1 comment '发券基础信息类型，1：人工发券， 2: 系统发券', `dbctime` DATETIME(3) DEFAULT CURRENT_TIMESTAMP(3) COMMENT '创建时间', `dbutime` DATETIME(3) DEFAULT CURRENT_TIMESTAMP(3) ON UPDATE CURRENT_TIMESTAMP(3) COMMENT '更新时间', PRIMARY KEY (`id`))ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;ALTER TABLE `coupon_job_base_info` ADD COLUMN `link` varchar(255) NOT NULL DEFAULT '' COMMENT '落地页链接' AFTER `type`;ALTER TABLE `coupon_job_base_info` MODIFY COLUMN `id` BIGINT NOT NULL AUTO_INCREMENT;ALTER TABLE `coupon_job_base_info` MODIFY COLUMN `activityId` BIGINT NOT NULL DEFAULT -1 comment '发券活动Id';ALTER TABLE `coupon_job_base_info` MODIFY COLUMN `couponTemplateId` BIGINT NOT NULL DEFAULT -1 comment '发券模板Id';--- int 升 longALTER TABLE `coupon_job_base_info` ADD COLUMN `ldap` varchar(32) NOT NULL DEFAULT '' COMMENT '创建者' AFTER `link`; CouponJobEx 规则信息：主要是发放条件，给谁发、什么时候发、发券的有效期、发券成功之后是否发送通知等等。 发券的规则：发放条件，何时发，给谁发，券有效期，发券成功后的通知等。创建一个发券规则就意味着向一批目标用户发放一批优惠券 CouponJobEx 的 “Ex” 是因为二期重构前已经存在了名为 CouponJob 的资源（重构后没有了），为了做区分在定义 VO 时加上了 “Ex” CouponJobEx 根据其 fetchCondition 字段（发放条件）的取值，实际上分为了两类：人工发券和系统发券 123456789101112131415161718192021222324252627282930313233343536373839CREATE TABLE `coupon_job_ex` ( `id` BIGINT NOT NULL AUTO_INCREMENT, `name` VARCHAR(255) NOT NULL DEFAULT '' COMMENT '规则名称', `desc` VARCHAR(512) NOT NULL DEFAULT '' COMMENT '描述', `ldap` VARCHAR(32) NOT NULL DEFAULT '' COMMENT '创建人', `status` INT(11) NOT NULL DEFAULT 0 COMMENT '状态', `reason` VARCHAR(512) NOT NULL COMMENT '原因', `baseInfoId` BIGINT NOT NULL DEFAULT 0 COMMENT '券基础信息 id', `couponTemplateId` BIGINT NOT NULL DEFAULT 0 COMMENT '券模板 id', `couponActivityId` BIGINT NOT NULL DEFAULT 0 COMMENT '券活动 id', `scheduledTime` BIGINT NOT NULL DEFAULT 0 COMMENT '发券规则定时执行时间', `couponEndTime` BIGINT NOT NULL DEFAULT 0 COMMENT '优惠券过期时间', `couponStartTime` BIGINT NOT NULL DEFAULT 0 COMMENT '优惠券 生效时间', `defaultValidPeriod` BIGINT NOT NULL DEFAULT 0 COMMENT '优惠券可用时间毫秒数', `useTimeType` int(11) NOT NULL DEFAULT '0' COMMENT '使用时间类型（1:defaultValidPeriod，2:couponJobEndTime）', `couponJobStartTime` BIGINT NOT NULL DEFAULT 0 COMMENT '规则生效时间', `couponJobEndTime` BIGINT NOT NULL DEFAULT 0 COMMENT '规则结束时间', `source` INT(11) NOT NULL DEFAULT 0 COMMENT '来源', `sourceInfo` longtext NOT NULL DEFAULT '' COMMENT '来源信息', `fetchCondition` INT(11) NOT NULL DEFAULT 0 COMMENT '发放条件', `fetchConditionInfo` longtext NOT NULL DEFAULT '' COMMENT '发放条件具体信息', `lessonMatcherIdFromSource` INT(11) NOT NULL DEFAULT 0 COMMENT '发放用户指定 班课匹配 id', `lessonMatcherIdFromFetchCondition` INT(11) NOT NULL DEFAULT 0 COMMENT '到课/买课发放条件下 指定班课', `createdTime` BIGINT NOT NULL DEFAULT 0 COMMENT '创建时间', `updatedTime` BIGINT NOT NULL DEFAULT 0 COMMENT '更新时间', `notificationInfo` longtext NOT NULL DEFAULT '' COMMENT '通知详细信息', `dbctime` DATETIME(3) DEFAULT CURRENT_TIMESTAMP(3) COMMENT '创建时间', `dbutime` DATETIME(3) DEFAULT CURRENT_TIMESTAMP(3) ON UPDATE CURRENT_TIMESTAMP(3) COMMENT '更新时间', PRIMARY KEY (`id`), KEY (`baseInfoId`), KEY (`couponTemplateId`), KEY (`couponActivityId`), KEY (`createdTime`), KEY (`couponJobStartTime`, `couponJobEndTime`), KEY (`fetchCondition`), KEY (`lessonMatcherIdFromFetchCondition`))ENGINE=InnoDB AUTO_INCREMENT=100000000 DEFAULT CHARSET=utf8mb4;alter table coupon_job_ex add column testUser varchar(512) COMMENT '测试用户'; Coupon 优惠券 学生领到手里的优惠券：Coupon 是根据 CouponJobBaseInfo 的 CouponTemplate 生成的 12345678910111213141516171819CREATE TABLE `coupon` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, `userId` int(11) NOT NULL, `type` int(11) NOT NULL, /* 1表示1对1代金券 */ `value` decimal(30,4) NOT NULL, `code` varchar(255) DEFAULT NULL, `used` int(11) NOT NULL DEFAULT '0', `usedTime` bigint(20) NOT NULL, /* 原来叫paidTime */ `expiredTime` bigint(20) NOT NULL, `createdTime` bigint(20) NOT NULL, label VARCHAR(64), matcherId int(11) NOT NULL DEFAULT 0, couponJobId int(11) NOT NULL DEFAULT 0, desc varchar(255) NOT NULL DEFAULT '', PRIMARY KEY (`id`), UNIQUE KEY `code` (`code`), //优惠券有一个code唯一索引 KEY (`userId`)) ENGINE=InnoDB; CouponTemplate 优惠券模板 ： 优惠券模板定义了优惠券的通用字段，比如 适用的班课群组、默认的有效期、是否适用于联报。 1234567891011121314CREATE TABLE `coupon_template` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `name` VARCHAR(255) NOT NULL DEFAULT '', `value` DECIMAL(30,4) NOT NULL, `desc` VARCHAR(255) NOT NULL DEFAULT '', `matcherId` INT(11) NOT NULL, `defaultValidPeriod` BIGINT(20) NOT NULL, `incompossibleActivityTypes` text, `createdTime` BIGINT(20) NOT NULL, `updatedTime` BIGINT(20) NOT NULL, PRIMARY KEY (`id`), KEY `name` (`name`), KEY `matcherId` (`matcherId`))ENGINE=InnoDB DEFAULT CHARSET=utf8; LessonMatcher：满足一些条件的班课：比如说年级、科目、产品线、阶段、难度等。这个lessonMatcher有很多的使用场景，不仅仅在发券中。usage区分使用的场景。 通用的匹配器，用来匹配一些满足条件的班课；LessonMatcher 可以用于多种场景，其 usage 字段用于区分使用的场景 优惠券发送记录： 123456789101112struct CouponJobSendRecordEx { 1: optional i64 couponJobId, 2: optional i64 userId, 3: optional i64 couponId, 4: optional i64 createdTime, 5: optional i64 couponTemplateId, 6: optional i32 index, // 支持一个 job 给同一个用户发多张券。正常情况下，index = 0；当输入发券时选择输入班课 ids 且设置不去重时，index &gt; 0 7: optional i64 orderId, 8: optional i64 couponBaseInfoId, 9: optional i64 couponActivityId, 10: optional i64 id;} 券规则状态： 12345678910@Datapublic class CouponJobStat { private int sentCount; // required 发送数量 private int usedCount; // required 使用数量 private long id; private long couponActivityId; // required 优惠券活动id private long couponTemplateId; // 优惠券模板id private long couponJobId; // 发券规则id private long baseInfoId; // 券信息id} 提供的能力券活动、券信息、券规则的管理功能主要是查询优惠券和发券的能力查询优惠券的能力构造了一个request，其中有一个字段type表示根据不同的类型来查询优惠券 发券的能力配置券规则的时候可以选择 是 发放 还是 用户领取 发放的时候有一些场景的约束，无、下单、到课、调班等。 对外提供RPC的发券使用的一个发券的request： 12345678public class GrantCouponRequest { private long userId; // optional private long couponJobId; // optional private String encodedCouponActivityId; // optional private long couponActivityId; // optional private int requestType; // optional：有发券规则、 发券活动、 调班三种 private TGrantCouponParamsOnTransfer grantCouponParamsOnTransfer; // 如果是调班发券，这个参数不为空} 发券也有类型的发券： 根据发券规则来发券 根据优惠券活动来发券 调班发券 发券的结果有下面5种： 忽略、重试、成功、失败、已领取 1public class GrantResult { /** * 发券的结果有下面5种： * 忽略、重试、成功、失败、已领取 */ public static final GrantResult IGNORE_RESULT = GrantResult.builder().requestMode(GrantCouponRequestMode.IGNORE).build(); public static final GrantResult RETRY_RESULT = GrantResult.builder().requestMode(GrantCouponRequestMode.RETRY).build(); public static final GrantResult SUCCESS_RESULT = GrantResult.builder().requestMode(GrantCouponRequestMode.SUCCESS).build(); public static final GrantResult FAIL_RESULT = GrantResult.builder().requestMode(GrantCouponRequestMode.FAIL).build(); public static final GrantResult FETCH_RESULT = GrantResult.builder().requestMode(GrantCouponRequestMode.FETCHED).build(); 无论根据什么类型发券： 都会拿到发券规则 和 userId 。 调班不一样，单独讲 分布式锁加锁 ： 用 userID为key ， 当前线程的id + 系统当前时间 组合 作为 value ， 默认锁30秒 构造条件发券参数：如果是根据发券规则发券，一个 发券规则 对应 一个发券参数 如果是根据优惠券活动，还需要先对发券规则筛选一波。再对复合的发券规则构造对应的发券参数。 1// 条件发券参数public class ConditionalCouponJobGrantParam { private long userId; private long couponJobId; // 发券规则 private int fetchCondition; // 发放条件 private long orderId; // 订单id private boolean isEarnestCoupon; private Optional&lt;Integer&gt; lessonId; private Optional&lt;Integer&gt; episodeId;} 走发券的统一接口：分为校验和 先对 发券参数 进行校验： 校验优惠券是否已经被领取了，内部是看优惠券的code是否已经存在（查表）来判断 校验发券规则的状态和发券时间： 看当前时间是不是在发券规则的开始和结束时间之内、发券规则的状态是否是已取消 如果是购课发券场景 是否已发券。同一个规则, 同一个订单 id, 再次下单报错, 校验发券参数：判断用户是否满足发券规则、判断班课是否满足 校验完之后返回： GrantCouponParam 校验完之后开始发券： 先生成优惠券的Code：先根据 券规则id + 用户id + 50位的随机字符串（如果发券规则限制次数就用1位的随机字符串，如果不限制就是50位，如果发券规则规定不止发一张就用 几张+1个字符串）生成 code再在上面的基础上 加上第几张。表示同一个券规则下的第几张优惠券 对每个优惠券code生成对应的Coupon实体类，批量写入数据库。意味着发券完成。 记录 发券记录写入表中 看发券规则判断是否要发通知。如果要发就调用相关的能力。 推荐优惠券的能力有两个入口： 用户支付订单时的入口，会带来一个orderId 用户点进去我的优惠券入口。 构造优惠券推荐请求： 1public class CouponRecommendRequest { private CouponRecommendRequestType requestType; // 是订单还是商品 private CouponFilterType filterType; // 是所有优惠券 还是 有效的优惠券 private long userId; private Boolean includeLock; private boolean isSuggest; // 是否是推荐的请求 private long orderId; private List&lt;Long&gt; productIds;} 根据请求的类型是 订单还是商品 拿到对应的推荐服务 组装优惠券推荐的上下文 1public class CouponRecommendContext { private CouponRecommendRequest request; private OrderV2 order; private TransferOrderDetail transferOrderDetail; // 调班订单详情 Map&lt;Long, Set&lt;Long&gt;&gt; couponId2MatchedProductIds; // 优惠券id to 匹配的商品id private List&lt;Coupon&gt; selectedCoupons; // 所有的优惠券 private Set&lt;Long&gt; matchedCouponIds; // 所有能用的优惠券 private Set&lt;Long&gt; matchedProductIds; // 所有匹配能用优惠券的商品 check上下文，其实就是属性填充。如果是根据订单推荐就塞入订单属性等 通过userId拿到所有的优惠券，塞入selectedCoupons属性中 通过订单的类型 来将 步骤 5里面的的所有优惠券过滤一遍，重新塞入selectedCoupons 获取所有订单项的 productId，这个商品满足的所有 matcherId，拿到每个商品的价格 判断订单是否是班课订单 拿到所有优惠券 对应的优惠券模板模板 遍历所有的优惠券，通过优惠券模板拿到可以适用的班课。 计算所有班课的价格总和是否满足优惠券模板的约束。 到这里就知道这个优惠券能不能用在这个订单上了。设置上下文的相应属性 对这个用户所有的优惠券进行排序： 对所有优惠券排序:可用优惠券排在不可用优惠券前面 , 实际优惠金额都大于订单实际价值，则优惠面额小的在前面 ,否则实际优惠金额大的在前面 ， 优惠金额相等， 则过期时间早的放在前面，过期时间相同， 则优惠券类型优先级高的放在前面 对有效的优惠券排序： 对无效的优惠券排序： 项目介绍一下主要负责的是猿辅导APP下的优惠券模块，将优惠券分为三个维度：活动、券信息（模板）、发券规则。活动维度就是一次向用户发放优惠券的活动，券信息就是 连接券模板和券活动，券规则主要就是发券的规则，主要分为圈人圈课两个方面。 然后这个项目主要提供了对三个维度的管理功能、对券的查询功能（根据券id、券规则和券活动等不同条件）、提供了发券的接口、用MQ感知一系列事件比如订单支付后发券等等，然后还会有一个定时job来计算某个券规则下的优惠券使用情况。 toC的那一块主要是提供的券推荐的能力，也就是用户下订单时给他推荐最合适的优惠券。 数据库是如何设计的coupon_activity、coupon_job_base_info、coupon_job_ex 优惠券：有userId表示创建人、type看是什么类型、value表示数值、是否已经使用和使用时间、发券规则id、适用班课、创建时间和过期时间 1CREATE TABLE `coupon` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, `userId` int(11) NOT NULL, `type` int(11) NOT NULL, /* 1表示1对1代金券 */ `value` decimal(30,4) NOT NULL, `code` varchar(255) DEFAULT NULL, `used` int(11) NOT NULL DEFAULT '0', `usedTime` bigint(20) NOT NULL, /* 原来叫paidTime */ `expiredTime` bigint(20) NOT NULL, `createdTime` bigint(20) NOT NULL, label VARCHAR(64), matcherId int(11) NOT NULL DEFAULT 0, couponJobId int(11) NOT NULL DEFAULT 0, desc varchar(255) NOT NULL DEFAULT '', PRIMARY KEY (`id`), UNIQUE KEY `code` (`code`), //优惠券有一个code唯一索引 KEY (`userId`)) ENGINE=InnoDB; 券模板：定义了优惠券的通用字段：比如适用班课、默认有效时间。 1CREATE TABLE `coupon_template` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `name` VARCHAR(255) NOT NULL DEFAULT '', `value` DECIMAL(30,4) NOT NULL, `desc` VARCHAR(255) NOT NULL DEFAULT '', `matcherId` INT(11) NOT NULL, `defaultValidPeriod` BIGINT(20) NOT NULL, `incompossibleActivityTypes` text, `createdTime` BIGINT(20) NOT NULL, `updatedTime` BIGINT(20) NOT NULL, PRIMARY KEY (`id`), KEY `name` (`name`), KEY `matcherId` (`matcherId`))ENGINE=InnoDB DEFAULT CHARSET=utf8; 发券规则：主要是userId创建人、券信息id、券模板id、券活动id、券规则执行时间、优惠券的有效时间、发放条件（用户群组、指定班课用户）、是否有通知消息、 1CREATE TABLE `coupon_job_ex` ( `id` BIGINT NOT NULL AUTO_INCREMENT, `name` VARCHAR(255) NOT NULL DEFAULT '' COMMENT '规则名称', `desc` VARCHAR(512) NOT NULL DEFAULT '' COMMENT '描述', `ldap` VARCHAR(32) NOT NULL DEFAULT '' COMMENT '创建人', `status` INT(11) NOT NULL DEFAULT 0 COMMENT '状态', `reason` VARCHAR(512) NOT NULL COMMENT '原因', `baseInfoId` BIGINT NOT NULL DEFAULT 0 COMMENT '券基础信息 id', `couponTemplateId` BIGINT NOT NULL DEFAULT 0 COMMENT '券模板 id', `couponActivityId` BIGINT NOT NULL DEFAULT 0 COMMENT '券活动 id', `scheduledTime` BIGINT NOT NULL DEFAULT 0 COMMENT '发券规则定时执行时间', `couponEndTime` BIGINT NOT NULL DEFAULT 0 COMMENT '优惠券过期时间', `couponStartTime` BIGINT NOT NULL DEFAULT 0 COMMENT '优惠券 生效时间', `defaultValidPeriod` BIGINT NOT NULL DEFAULT 0 COMMENT '优惠券可用时间毫秒数', `useTimeType` int(11) NOT NULL DEFAULT '0' COMMENT '使用时间类型（1:defaultValidPeriod，2:couponJobEndTime）', `couponJobStartTime` BIGINT NOT NULL DEFAULT 0 COMMENT '规则生效时间', `couponJobEndTime` BIGINT NOT NULL DEFAULT 0 COMMENT '规则结束时间', `source` INT(11) NOT NULL DEFAULT 0 COMMENT '来源', `sourceInfo` longtext NOT NULL DEFAULT '' COMMENT '来源信息', `fetchCondition` INT(11) NOT NULL DEFAULT 0 COMMENT '发放条件', `fetchConditionInfo` longtext NOT NULL DEFAULT '' COMMENT '发放条件具体信息', `lessonMatcherIdFromSource` INT(11) NOT NULL DEFAULT 0 COMMENT '发放用户指定 班课匹配 id', `lessonMatcherIdFromFetchCondition` INT(11) NOT NULL DEFAULT 0 COMMENT '到课/买课发放条件下 指定班课', `createdTime` BIGINT NOT NULL DEFAULT 0 COMMENT '创建时间', `updatedTime` BIGINT NOT NULL DEFAULT 0 COMMENT '更新时间', `notificationInfo` longtext NOT NULL DEFAULT '' COMMENT '通知详细信息', `dbctime` DATETIME(3) DEFAULT CURRENT_TIMESTAMP(3) COMMENT '创建时间', `dbutime` DATETIME(3) DEFAULT CURRENT_TIMESTAMP(3) ON UPDATE CURRENT_TIMESTAMP(3) COMMENT '更新时间', PRIMARY KEY (`id`), KEY (`baseInfoId`), KEY (`couponTemplateId`), KEY (`couponActivityId`), KEY (`createdTime`), KEY (`couponJobStartTime`, `couponJobEndTime`), KEY (`fetchCondition`), KEY (`lessonMatcherIdFromFetchCondition`))ENGINE=InnoDB AUTO_INCREMENT=100000000 DEFAULT CHARSET=utf8mb4;alter table coupon_job_ex add column testUser varchar(512) COMMENT '测试用户'; 还有一些其他的比如优惠券发放记录、券规则状态等等。 发券流程是啥样的别的服务RPC来调用我们的服务发券时\u0010： 对外封装了一个接口，提供根据发券活动、发券规则等发券 加分布式锁： 用 userID为key ， 当前线程的id + 系统当前时间 组合 作为 value ， 默认锁30秒 构造发券参数：主要是券规则id + userId 走统一的发券接口： 校验： 校验优惠券是否已经被领取了，内部是看优惠券的code是否已经存在（查表）来判断 校验发券规则的状态和发券时间： 看当前时间是不是在发券规则的开始和结束时间之内、发券规则的状态是否是已取消 如果是购课发券场景 是否已发券。同一个规则, 同一个订单 id, 再次下单报错, 校验发券参数：判断用户是否满足发券规则、判断班课是否满足 校验完之后返回一个发券： GrantCouponParam 生成对应的优惠券码 对每个优惠券码都生成优惠券实体 对每个优惠券发券，拿出每个优惠券的模板，调用中台的接口:其实就是存数据库、搞监控、处理老逻辑之类的。 记录一下发券信息 如果是手动执行一个发券规则的话： 拿到发券规则 和目标用户id 分批 用CountDownLatch 来控制一起发放 遍历每个需要发券的记录、 生成优惠券的 code、每个code 生成一个coupon实例、调用中台存起来。 优惠券码是怎么生成的根据发券规则下面的 每次发几张 来决定： 不限制次数：券规则id + 用户id + 50位的随机字符串（如果发券规则限制次数就用1位）生成 code + 当前是第几张（每次发几张） 次数为1：券规则id + 用户id + 1位的随机字符串生成 code + 当前是第几张 次数大于1张：券规则id + 用户id + 已经发送了几张的次数生成 code + 当前是第几张 1if (fetchLimit == 0) { //不限制次数 oldCode = null;//因为是随机值,无法按照源算法算出 code //次数位置使用 随机 code // 根据 券规则id + 用户id + 50位的随机字符串（如果发券规则限制次数就用1位）生成 code // 再在上面的基础上 加上第几张。表示同一个券规则下的第几张优惠券 newCodes = appendNumAfterCode( genCode(couponJobId, userId, wrapTimes(randomCode())), fetchSize); maxTimes = Integer.MAX_VALUE; } if (fetchLimit == 1) { oldCode = genCode(couponJobId, userId); newCodes = appendNumAfterCode( genCode(couponJobId, userId, wrapTimes(1)), fetchSize); maxTimes = 1; } if (fetchLimit &gt; 1) { maxTimes = fetchLimit; oldCode = null; // 领券 不存在多次场景 newCodes = appendNumAfterCode( genCode(couponJobId, userId, wrapTimes(sentTimes + 1)), fetchSize); } 优惠券推荐是怎么实现的","link":"/2021/10/09/%E4%BC%98%E6%83%A0%E5%88%B8%E9%A1%B9%E7%9B%AE/"},{"title":"操作系统（一）概述","text":"讲一下内核和中断 内核由来计算机是由各种外部硬件设备组成的，比如CPU、内存和硬盘等，如果让每个应用对和这些硬件设备对接通信协议的话很麻烦，所以就出现了一个连接硬件设备的桥梁，称为内核。应用程序只需要关心和内核的交互，而不需要关心硬件的细节了。 作用 管理进程、线程，决定哪个进程、线程使⽤ CPU，也就是进程调度的能⼒； 管理内存，决定内存的分配和回收，也就是内存管理的能⼒； 管理硬件设备，为进程与硬件设备之间提供通信能⼒，也就是硬件通信能⼒； 提供系统调⽤，如果应⽤程序要运⾏更⾼权限运⾏的服务，那么就需要有系统调⽤，它是⽤户程序与操作系统之间的接⼝。 如何工作的内核具有很⾼的权限，可以控制 cpu、内存、硬盘等硬件，⽽应⽤程序具有的权限很⼩，因此⼤多数操作系统，把内存分成了两个区域： 内核空间，这个内存空间只有内核程序可以访问； ⽤户空间，这个内存空间专⻔给应⽤程序使⽤； 用户空间的代码被限制了只能使用一个局部的内存空间，称为用户态；而内核空间的代码可以访问所有内存，称为内核态。 系统调用过程应⽤程序如果需要进⼊内核空间，就需要通过系统调⽤，下⾯来看看系统调⽤的过程： 内核程序执⾏在内核态，⽤户程序执⾏在⽤户态。当应⽤程序使⽤系统调⽤时，会产⽣⼀个中断。发⽣中断后， CPU 会中断当前在执⾏的⽤户程序，转⽽跳转到中断处理程序，也就是开始执⾏内核程序。内核处理完后，主动触发中断，把 CPU 执⾏权限交回给⽤户程序，回到⽤户态继续⼯作。 线程的实现方式一个应用程序启动后会在内存中创建一个执行副本，称为进程。 Linux的内核是一个宏内核，可以看做一个进程。 用户态进程如果要执行程序，是否也需要向内核申请呢？ 主要有三种线程的实现⽅式： ⽤户线程（User Thread）：在⽤户空间实现的线程，不是由内核管理的线程，是由⽤户态的线程库来完成线程的管理； 内核线程（Kernel Thread）：在内核中实现的线程，是由内核管理的线程； 轻量级进程（LightWeight Process）：在内核中来⽀持⽤户线程； 用户态线程当线程的概念出现时，操作系统厂商可不能直接就去修改操作系统的内核，为了保证系统的稳定性！所以当时研究人员就写了一个线程函数库来完成线程的创建，包括线程的创建、终⽌、同步和调度等。 这个线程函数库是位于用户态的，也就是说操作系统对这个库是一无所知的。 也就是说：我用线程库写的一个多线程进程，只能一次在一个 CPU 核心上运行 同时：如果AB是同一个进程的两个线程的话，A 正在运行的时候，线程 B 想要运行的话，只能等待 A 主动放弃 CPU，也就是主动调用 pthread_yield 函数。（一个CPU同时只能执行这个进程的一个线程） 用户态线程无法做到进程那样的轮转调度。 如果进程中的某一个线程阻塞了会发生什么？ 在操作系统眼里，是进程阻塞了，那么整个进程就会进入阻塞态，在阻塞操作结束前，这个进程都无法得到 CPU 资源。那就相当于，所有的线程都被阻塞了 为了解决这个问题，就出现了一种解决方案：jacket：就是把一个产生阻塞的系统调用转化成一个非阻塞的系统调用。 小白惊讶地问：“这怎么做得到？该阻塞的调用，还能变得不阻塞？” 小明答道：“我来举个例子吧，不是直接调用一个系统 I/O 例程，而是调用一个应用级别的 I/O jacket 例程，这个 jacket 例程中的代码会检查并且确定 I/O 设备是不是正忙，如果忙的话，就在用户态下将该线程阻塞，然后把控制权交给另一个线程。隔一段时间后再次检查 I/O 设备。就像你说的，最后还是会执行阻塞调用，但使用 jacket 可以缩短被阻塞的时间。不过有些情况下是可以不被阻塞的，取决于具体的实现。 用户线程的优势： 创建开销小 切换成本低 缺点： 与内核协作成本高，比如发生IO时需要频繁的在用户态和内核态之间切换 线程间协作成本高，线程通信需要IO，IO需要系统调用 无法利用多核优势，这些线程只能占用一个CPU核，无法并行加速。 操作系统是无法感知到用户态线程的，也就不会进行线程的切换。 内核态线程本质上就是操作系统能够看到的线程。 许多操作系统都已经支持内核级线程了。为了实现线程，内核里就需要有用来记录系统里所有线程的线程表。当需要创建一个新线程的时候，就需要进行一个系统调用，然后由操作系统进行线程表的更新。 内核态线程的优点： 操作系统内核如果知道线程的存在，就可以像调度多个进程一样，把这些线程放在好几个 CPU 核心上，就能做到实际上的并行 假如线程 A 阻塞了，与他同属一个进程的线程也不会被阻塞。这是内核级线程的绝对优势 内核态线程的缺点： 让操作系统进行线程调度，那意味着每次切换线程，就需要「陷入」内核态，而操作系统从用户态到内核态的转变是有开销的，所以说内核级线程切换的代价要比用户级线程大。 线程表是存放在操作系统固定的表格空间或者堆栈空间里，所以内核级线程的数量是有限的，扩展性比不上用户级线程 java中的线程是内核态线程还是用户态线程呢？ 中断是什么中断（interrupt）是计算机系统中的基本机制之一。即：在计算机运行过程中，当发生某个事件后，CPU 会停止当前程序流，转而去处理该事件，并在处理完毕后继续执行原程序流。 相关概念 描述 中断分类 硬中断（Hardware Interrupt）&amp; 软中断（Software Interrupt） 中断向量表（Interrupt Vector Table） 记录了中断号与中断服务程序内存地址的映射关系 中断服务程序 / 中断处理程序（Interrupt Service） 通过中断向量表定位到的特定处理程序 这样做的好处是化被动为主动，如果没有中断机制的话，就需要CPU轮询来判断某个条件是否成立，增加系统的开销。 分类硬中断：硬中断由外部设备（例如：磁盘，网卡，键盘，时钟）产生，用来通知操作系统外设状态变化 时钟中断： 一种硬中断，用于定期打断 CPU 执行的线程，以便切换给其他线程以得到执行机会。 处理流程： 1、外设 将中断请求发送给中断控制器； 2、中断控制器 根据中断优先级，有序地将中断传递给 CPU； 3、CPU 终止执行当前程序流，将 CPU 所有寄存器的数值保存到栈中； 4、CPU 根据中断向量，从中断向量表中查找中断处理程序的入口地址，执行中断处理程序； 5、CPU 恢复寄存器中的数值，返回原程序流停止位置继续执行。 软中断：是一条 CPU 指令，由当前正在运行的进程产生。流程就和上面的345一样。 系统调用： 是一种软中断处理程序，用于让程序从用户态陷入内核态，以执行相应的操作。","link":"/2021/10/06/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%80%EF%BC%89%E6%A6%82%E8%BF%B0/"},{"title":"操作系统（三）文件和设备管理","text":"","link":"/2021/10/10/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%89%EF%BC%89%E6%96%87%E4%BB%B6%E5%92%8C%E8%AE%BE%E5%A4%87%E7%AE%A1%E7%90%86/"},{"title":"算法题","text":"哈哈 第一部分：四种基本情况 1. 无重复数字的二分查找https://leetcode-cn.com/problems/binary-search/ 123456789101112class Solution { public int search(int[] nums, int target) { int n = nums.length; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r + 1 &gt;&gt; 1; if(nums[mid] &gt; target) r = mid-1; else l = mid; } return nums[l] == target ? l : -1; }} 2. 有重复数字的二分查找第一个位置和最后一个位置https://leetcode-cn.com/problems/find-first-and-last-position-of-element-in-sorted-array/ 12345678910111213141516171819202122232425262728class Solution { public int[] searchRange(int[] nums, int target) { int[] res = new int[2]; int n = nums.length; if(n == 0) return new int[]{-1,-1}; //找到第一个出现的位置 int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; if(nums[mid] &lt; target) l = mid+1; else r = mid; } if(nums[l] != target) return new int[]{-1,-1}; else res[0] = l; //找到最后一个出现的位置 l = 0; r = n-1; while(l &lt; r){ int mid = l + r +1 &gt;&gt; 1; if(nums[mid] &gt; target) r = mid-1; else l = mid; } res[1] = l; return res; }} 3. 搜索插入位置https://leetcode-cn.com/problems/search-insert-position/ 12345678910111213141516class Solution { public int searchInsert(int[] nums, int target) { int n = nums.length; //1. 注意如果最后一个数小于target的话，就返回数组长度 if (n == 0 || nums[n-1] &lt; target) return n; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; if(nums[mid] &lt; target) l = mid + 1; else r = mid; } return l; }} 4. x的平方根（只保留整数部分）12345678910111213 class Solution { public int mySqrt(int x) { int l = 0 , r = x; while(l &lt; r){ int mid = l + r + 1 &gt;&gt; 1; if(mid &gt; x / mid) r = mid-1; else l = mid; } return l; }} 5. 寻找重复的数https://leetcode-cn.com/problems/find-the-duplicate-number/ 123456789101112131415161718192021class Solution { public int findDuplicate(int[] nums) { int n = nums.length; //对值的范围进而二分 int l = 0 , r = n-1; while (l &lt; r){ int mid = l+r &gt;&gt;1; //看一下数组中比mid小的数有多少 int count = 0; for (int num : nums){ if (num &lt;= mid) count++; } //比mid小的数大于mid，说明在左边,可能是mid if (count &gt; mid) r = mid; else l = mid+1; } return l; }} 6. 实现Pow(x,n)https://leetcode-cn.com/problems/powx-n/comments/ 12345678910111213class Solution { public double myPow(double x, int n) { double res = 1.0; for(int i = n ; i != 0 ; i /=2){ if(i % 2 != 0) res = res * x; x *= x; } return n &lt; 0 ? 1/res : res; }} 7. 寻找两个排序数组的中位数https://leetcode-cn.com/problems/median-of-two-sorted-arrays/ 1234567891011121314151617181920212223242526class Solution { public double findMedianSortedArrays(int[] nums1, int[] nums2) { int n = nums1.length; int m = nums2.length; int l = (n + m + 1)/2; int r = (n + m + 2)/2; return (getK(nums1, 0 , n-1 , nums2 , 0 , m-1 , l) + getK(nums1 , 0 ,n-1 , nums2 ,0,m-1,r))/2.0; } //从两个正序数组中获取第k大的数 int getK(int[] nums1 , int s1 , int e1 , int[] nums2 , int s2 , int e2 , int k){ int len1 = e1 - s1 + 1; int len2 = e2 - s2 + 1; if(len1 &gt; len2) return getK(nums2 , s2 , e2 , nums1 , s1 , e1 , k); if(len1 == 0) return nums2[s2 + k -1]; if(k == 1) return Math.min(nums1[s1] , nums2[s2]); int i = s1 + Math.min(len1 , k/2)-1; //每次取一半的值 int j = s2 + Math.min(len2 , k/2)-1; //每一轮都将较小的那半组数据舍去 if(nums1[i] &gt; nums2[j]) return getK(nums1 , s1 ,e1 , nums2 ,j+1, e2 ,k-(j-s2+1)); else return getK(nums1, i+1 ,e1 , nums2 , s2 , e2 , k-(i-s1+1)); }} 第二部分：旋转排序数组1. 寻找旋转排序数组中的最小值(无重复值)https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array/ 1234567891011121314class Solution { public int findMin(int[] nums) { int n = nums.length; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; //34512 if(nums[mid] &gt; nums[r]) l = mid+1; else r = mid; } return nums[l]; }} 2. 寻找旋转排序数组中的最小值(有重复值)https://leetcode-cn.com/problems/find-minimum-in-rotated-sorted-array-ii/ 12345678910111213class Solution { public int findMin(int[] nums) { int n = nums.length; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; if(nums[mid] &gt; nums[r]) l = mid+1; else if(nums[mid] &lt; nums[r]) r = mid; else r--; } return nums[l]; }} 3. 寻找旋转排序数组中的指定值(无重复值)https://leetcode-cn.com/problems/search-in-rotated-sorted-array/ 123456789101112131415161718192021class Solution { public int search(int[] nums, int target) { int n = nums.length; if(n == 0) return -1; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; //34512 if(nums[mid] &gt; nums[r]){ if(target &gt;= nums[l] &amp;&amp; target &lt;= nums[mid]) r = mid; else l = mid+1; }else{ //561234 if(target &gt; nums[mid] &amp;&amp; target &lt;= nums[r]) l = mid+1; else r = mid; } } return nums[l] == target ? l : -1; }} 第三部分：二维矩阵1. 搜索二维矩阵https://leetcode-cn.com/problems/search-a-2d-matrix/ 123456789101112131415class Solution { public boolean searchMatrix(int[][] matrix, int target) { int m = matrix.length; //行数 int n = matrix[0].length; //列数 int l = 0 , r = m *n -1; while(l &lt; r){ int mid = l + r + 1 &gt;&gt; 1; //将一维的数组转换成二维的坐标 if(matrix[mid / n][ mid % n] &gt; target) r = mid-1; else l = mid; } return matrix[l/n][l%n] == target; }} 2. 搜索二维矩阵2https://leetcode-cn.com/problems/search-a-2d-matrix-ii/submissions/ 1234567891011121314class Solution { public boolean searchMatrix(int[][] matrix, int target) { int m = matrix.length; //行数 int n = matrix[0].length; //列数 int i = 0 , j = n-1; while(i &lt;= m-1 &amp;&amp; j &gt;= 0){ if(matrix[i][j] &gt; target) j--; else if(matrix[i][j] &lt; target) i++; else return true; } return false; }} 3. 有序矩阵中第k小的数https://leetcode-cn.com/problems/kth-smallest-element-in-a-sorted-matrix/","link":"/2021/05/06/%E7%AE%97%E6%B3%95%E9%A2%98/"},{"title":"git之基本操作","text":"git是工作用到的分布式的版本控制工具 –学自极客时间git专栏 使用Git之前的最小配置配置用户名和邮箱12345678910shengbinbin@192 ~ % git versiongit version 2.24.3 (Apple Git-128)//配置用户名和邮箱 shengbinbin@192 ~ % git config --global user.name 'binshow'shengbinbin@192 ~ % git config --global user.email '1157024800@qq.com'--local ： 只对某个仓库有效--global 对当前用户的所有仓库有效--system 对系统所有登陆的用户有效，基本不用 123456shengbinbin@192 ~ % git config --list //显示git的配置credential.helper=osxkeychainuser.name=binshowuser.mail=1157024800@qq.comuser.email=1157024800@qq.comshengbinbin@192 ~ % 创建第一个仓库并配置local用户信息123456789101112131415161718192021222324252627282930313233343536shengbinbin@chengbinbindeMacBook-Pro Code % pwd/Users/shengbinbin/Documents/Codeshengbinbin@chengbinbindeMacBook-Pro Code % git init git_learning //1.创建好一个git仓库文件夹，命名为 git_learninghint: Using 'master' as the name for the initial branch. This default branch namehint: is subject to change. To configure the initial branch name to use in allhint: of your new repositories, which will suppress this warning, call:hint:hint: git config --global init.defaultBranch &lt;name&gt;hint:hint: Names commonly chosen instead of 'master' are 'main', 'trunk' andhint: 'development'. The just-created branch can be renamed via this command:hint:hint: git branch -m &lt;name&gt;Initialized empty Git repository in /Users/shengbinbin/Documents/Code/git_learning/.git/shengbinbin@chengbinbindeMacBook-Pro Code % cd git_learningshengbinbin@chengbinbindeMacBook-Pro git_learning % ls -altotal 0drwxr-xr-x 3 shengbinbin staff 96 6 22 21:47 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 9 shengbinbin staff 288 6 22 21:47 .git //2. .git是核心文件夹 //3.设置local的相关信息shengbinbin@chengbinbindeMacBook-Pro git_learning % git config --local user.name 'shengbinbin'shengbinbin@chengbinbindeMacBook-Pro git_learning % git config --local user.email '1157024800@qq.com'shengbinbin@chengbinbindeMacBook-Pro git_learning % git config --local --list //4.查看local的相关信息core.repositoryformatversion=0core.filemode=truecore.bare=falsecore.logallrefupdates=truecore.ignorecase=truecore.precomposeunicode=trueuser.name=shengbinbinuser.email=1157024800@qq.com 添加第一个文件readMe1234567891011121314151617181920212223242526272829303132333435363738394041 //1. 将材料中的readMe拷贝到当前目录下shengbinbin@chengbinbindeMacBook-Pro git_learning % cp ../0-material/readme .shengbinbin@chengbinbindeMacBook-Pro git_learning % ls -altotal 8drwxr-xr-x 4 shengbinbin staff 128 6 22 21:51 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 9 shengbinbin staff 288 6 22 21:50 .git-rw-r--r--@ 1 shengbinbin staff 51 6 22 21:51 readme //2. 直接进行commit提交会报错，（-m表示这次提交的意义是什么）shengbinbin@chengbinbindeMacBook-Pro git_learning % git commit -m'Add readMe'On branch masterInitial commitUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) readmenothing added to commit but untracked files present (use &quot;git add&quot; to track) //3. 先用git add 将文件加入暂存区shengbinbin@chengbinbindeMacBook-Pro git_learning % git add readmeshengbinbin@chengbinbindeMacBook-Pro git_learning % git status //4. 查看状态On branch masterNo commits yetChanges to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: readmeshengbinbin@chengbinbindeMacBook-Pro git_learning % git commit -m'Add readMe' //5. 最后成功提交[master (root-commit) d59544f] Add readMe 1 file changed, 2 insertions(+) create mode 100644 readmeshengbinbin@chengbinbindeMacBook-Pro git_learning % git logcommit d59544f7fba30a55f9511993709de2403c9cfbe5 (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt; //local的配置参数优先级较高Date: Tue Jun 22 21:52:44 2021 +0800 Add readMeshengbinbin@chengbinbindeMacBook-Pro git_learning % 工作区和暂存区 添加index + logo 123456789101112131415161718192021222324shengbinbin@chengbinbindeMacBook-Pro git_learning % cp ../0-material/index.html.01 index.html //复制一个网页过来shengbinbin@chengbinbindeMacBook-Pro git_learning % cp -r ../0-material/images . //复制整个文件夹过来shengbinbin@chengbinbindeMacBook-Pro git_learning % git statusOn branch masterUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) images/ index.htmlnothing added to commit but untracked files present (use &quot;git add&quot; to track)shengbinbin@chengbinbindeMacBook-Pro git_learning % git add index.html images //3.将两个文件加入到暂存区shengbinbin@chengbinbindeMacBook-Pro git_learning % git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) new file: images/git-logo.png new file: index.htmlshengbinbin@chengbinbindeMacBook-Pro git_learning % git commit -m 'ADD index + logo'[master 2a76eaa] ADD index + logo 2 files changed, 49 insertions(+) create mode 100644 images/git-logo.png create mode 100644 index.htmlshengbinbin@chengbinbindeMacBook-Pro git_learning % 添加css： 1234567891011121314151617181920212223shengbinbin@chengbinbindeMacBook-Pro git_learning % mkdir stylesshengbinbin@chengbinbindeMacBook-Pro git_learning % cp ../0-material/styles/style.css.01 styles/style.cssshengbinbin@chengbinbindeMacBook-Pro git_learning % ls -altotal 16drwxr-xr-x 7 shengbinbin staff 224 6 22 22:00 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 12 shengbinbin staff 384 6 22 21:57 .gitdrwxr-xr-x@ 3 shengbinbin staff 96 6 22 21:57 images-rw-r--r--@ 1 shengbinbin staff 1303 6 22 21:57 index.html-rw-r--r--@ 1 shengbinbin staff 51 6 22 21:51 readmedrwxr-xr-x 3 shengbinbin staff 96 6 22 22:00 stylesshengbinbin@chengbinbindeMacBook-Pro git_learning % git statusOn branch masterUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) styles/nothing added to commit but untracked files present (use &quot;git add&quot; to track)shengbinbin@chengbinbindeMacBook-Pro git_learning % git add stylesshengbinbin@chengbinbindeMacBook-Pro git_learning % git commit -m 'ADD style.css'[master 9bd6521] ADD style.css 1 file changed, 50 insertions(+) create mode 100644 styles/style.css 添加js： 123456789101112131415161718192021222324252627282930313233343536373839404142shengbinbin@chengbinbindeMacBook-Pro git_learning % cp -r ../0-material/js .shengbinbin@chengbinbindeMacBook-Pro git_learning % ls -altotal 16drwxr-xr-x 8 shengbinbin staff 256 6 22 22:04 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 12 shengbinbin staff 384 6 22 22:01 .gitdrwxr-xr-x@ 3 shengbinbin staff 96 6 22 21:57 images-rw-r--r--@ 1 shengbinbin staff 1303 6 22 21:57 index.htmldrwxr-xr-x@ 3 shengbinbin staff 96 6 22 22:04 js-rw-r--r--@ 1 shengbinbin staff 51 6 22 21:51 readmedrwxr-xr-x 3 shengbinbin staff 96 6 22 22:00 stylesshengbinbin@chengbinbindeMacBook-Pro git_learning % git add jsshengbinbin@chengbinbindeMacBook-Pro git_learning % git commit -m 'ADD js'[master 7a430c9] ADD js 1 file changed, 15 insertions(+) create mode 100644 js/script.jsshengbinbin@chengbinbindeMacBook-Pro git_learning % git log //查看git 提交日志commit 7a430c9c43daee3f83b4b3b607a8ef82e2061a46 (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:06:20 2021 +0800 ADD jscommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caaAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMe: 文件重命名git mv123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263shengbinbin@chengbinbindeMacBook-Pro git_learning % ls -altotal 16drwxr-xr-x 8 shengbinbin staff 256 6 22 22:04 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 12 shengbinbin staff 384 6 22 22:06 .gitdrwxr-xr-x@ 3 shengbinbin staff 96 6 22 21:57 images-rw-r--r--@ 1 shengbinbin staff 1303 6 22 21:57 index.htmldrwxr-xr-x@ 3 shengbinbin staff 96 6 22 22:04 js-rw-r--r--@ 1 shengbinbin staff 51 6 22 21:51 readmedrwxr-xr-x 3 shengbinbin staff 96 6 22 22:00 stylesshengbinbin@chengbinbindeMacBook-Pro git_learning % mv readMe readMe.md //1.本地目录重命名shengbinbin@chengbinbindeMacBook-Pro git_learning % ls -altotal 16drwxr-xr-x 8 shengbinbin staff 256 6 22 22:13 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 12 shengbinbin staff 384 6 22 22:06 .gitdrwxr-xr-x@ 3 shengbinbin staff 96 6 22 21:57 images-rw-r--r--@ 1 shengbinbin staff 1303 6 22 21:57 index.htmldrwxr-xr-x@ 3 shengbinbin staff 96 6 22 22:04 js-rw-r--r--@ 1 shengbinbin staff 51 6 22 21:51 readMe.mddrwxr-xr-x 3 shengbinbin staff 96 6 22 22:00 stylesshengbinbin@chengbinbindeMacBook-Pro git_learning % git status //2. git 认为是删除了readMe 新建了readMe.mdOn branch masterChanges not staged for commit: (use &quot;git add/rm &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) deleted: readmeUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) readMe.mdno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)shengbinbin@chengbinbindeMacBook-Pro git_learning % git add readMe.md //3.将readMe.md 加到暂存区shengbinbin@chengbinbindeMacBook-Pro git_learning % git rm readme //4. 删除git上的readmerm 'readme'shengbinbin@chengbinbindeMacBook-Pro git_learning % git status //5. git就识别出来了On branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) renamed: readme -&gt; readMe.mdshengbinbin@chengbinbindeMacBook-Pro git_learning % git reset --hard //6.清除本次提交，包括本地的修改HEAD is now at 7a430c9 ADD jsshengbinbin@chengbinbindeMacBook-Pro git_learning % git mv readme readme.md //7.在git上直接进行重命名shengbinbin@chengbinbindeMacBook-Pro git_learning % git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) renamed: readme -&gt; readme.md //8. 提交shengbinbin@chengbinbindeMacBook-Pro git_learning % git commit readme.md -m &quot;rename readme to readme.md&quot;[master aeb4213] rename readme to readme.md 1 file changed, 2 insertions(+) create mode 100644 readme.mdshengbinbin@chengbinbindeMacBook-Pro git_learning % 查看历史 git log123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104shengbinbin@chengbinbindeMacBook-Pro git_learning % git log --oneline //1. aeb4213 (HEAD -&gt; master) rename readme to readme.md7a430c9 ADD js9bd6521 ADD style.css2a76eaa ADD index + logod59544f Add readMeshengbinbin@chengbinbindeMacBook-Pro git_learning % git log -n2 --oneline // 看最近2次的提交历史aeb4213 (HEAD -&gt; master) rename readme to readme.md7a430c9 ADD jsshengbinbin@chengbinbindeMacBook-Pro git_learning % git branch -v //查看本地分支* master aeb4213 rename readme to readme.mdshengbinbin@chengbinbindeMacBook-Pro git_learning % git checkout -b temp 9bd6521 //创建一个新的分支D readmeSwitched to a new branch 'temp'shengbinbin@chengbinbindeMacBook-Pro git_learning % git branch -v master aeb4213 rename readme to readme.md* temp 9bd6521 ADD style.cssshengbinbin@chengbinbindeMacBook-Pro git_learning % ls -altotal 16drwxr-xr-x 7 shengbinbin staff 224 6 22 22:37 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 13 shengbinbin staff 416 6 22 22:37 .gitdrwxr-xr-x@ 3 shengbinbin staff 96 6 22 21:57 images-rw-r--r-- 1 shengbinbin staff 1303 6 22 22:36 index.html-rw-r--r-- 1 shengbinbin staff 51 6 22 22:37 readmedrwxr-xr-x 3 shengbinbin staff 96 6 22 22:00 stylesshengbinbin@chengbinbindeMacBook-Pro git_learning % vim readme // temp 分支修改了一下readmeshengbinbin@chengbinbindeMacBook-Pro git_learning % git commit -am'Add test' //直接提交（-am）[temp 2a5925a] Add test 1 file changed, 1 insertion(+)shengbinbin@chengbinbindeMacBook-Pro git_learning % git branch -av master aeb4213 rename readme to readme.md* temp 2a5925a Add testshengbinbin@chengbinbindeMacBook-Pro git_learning % git log //查看当前分支的提交记录commit 2a5925a9f096a881a838238a5ced6a5ff3b7f52a (HEAD -&gt; temp)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:38:52 2021 +0800 Add testcommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caaAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMe shengbinbin@chengbinbindeMacBook-Pro git_learning % git log --all --graph //查看全部分支的提交历史，图形化显示* commit 2a5925a9f096a881a838238a5ced6a5ff3b7f52a (HEAD -&gt; temp)| Author: shengbinbin &lt;1157024800@qq.com&gt;| Date: Tue Jun 22 22:38:52 2021 +0800|| Add test|| * commit aeb4213f9fe55e5ffa1a866df58a519c9dcc18e4 (master)| | Author: shengbinbin &lt;1157024800@qq.com&gt;| | Date: Tue Jun 22 22:23:50 2021 +0800| || | rename readme to readme.md| || * commit 7a430c9c43daee3f83b4b3b607a8ef82e2061a46|/ Author: shengbinbin &lt;1157024800@qq.com&gt;| Date: Tue Jun 22 22:06:20 2021 +0800|| ADD js|* commit 9bd65211b66a0a27503644f8fe66a66a428d4ecb| Author: shengbinbin &lt;1157024800@qq.com&gt;| Date: Tue Jun 22 22:01:17 2021 +0800|| ADD style.css|* commit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caa| Author: shengbinbin &lt;1157024800@qq.com&gt;| Date: Tue Jun 22 21:57:59 2021 +0800|| ADD index + logo|* commit d59544f7fba30a55f9511993709de2403c9cfbe5 Author: shengbinbin &lt;1157024800@qq.com&gt; Date: Tue Jun 22 21:52:44 2021 +0800 Add readMe shengbinbin@chengbinbindeMacBook-Pro git_learning % git log --all --graph --oneline* 2a5925a (HEAD -&gt; temp) Add test| * aeb4213 (master) rename readme to readme.md| * 7a430c9 ADD js|/* 9bd6521 ADD style.css* 2a76eaa ADD index + logo* d59544f Add readMeshengbinbin@chengbinbindeMacBook-Pro git_learning % gitk可视化工具 探密.git裸仓库HEAD 和 config12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667shengbinbin@192 git_learning % ls -al //1.查看初始化的仓库中有什么total 16drwxr-xr-x 7 shengbinbin staff 224 6 22 22:38 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 14 shengbinbin staff 448 6 22 22:45 .git //核心文件夹drwxr-xr-x@ 3 shengbinbin staff 96 6 22 21:57 images-rw-r--r-- 1 shengbinbin staff 1303 6 22 22:36 index.html-rw-r--r-- 1 shengbinbin staff 57 6 22 22:38 readmedrwxr-xr-x 3 shengbinbin staff 96 6 22 22:00 stylesshengbinbin@192 git_learning % cd .gitshengbinbin@192 .git % ls -al //2. 进入核心文件夹 .git 查看内容total 56drwxr-xr-x 14 shengbinbin staff 448 6 22 22:45 .drwxr-xr-x 7 shengbinbin staff 224 6 22 22:38 ..-rw-r--r-- 1 shengbinbin staff 9 6 22 22:38 COMMIT_EDITMSG-rw-r--r-- 1 shengbinbin staff 21 6 22 22:34 HEAD-rw-r--r-- 1 shengbinbin staff 41 6 22 22:22 ORIG_HEAD-rw-r--r-- 1 shengbinbin staff 191 6 22 21:50 config-rw-r--r-- 1 shengbinbin staff 73 6 22 21:47 description-rw-r--r-- 1 shengbinbin staff 461 6 22 22:45 gitk.cachedrwxr-xr-x 15 shengbinbin staff 480 6 22 21:47 hooks-rw-r--r-- 1 shengbinbin staff 447 6 22 22:38 indexdrwxr-xr-x 3 shengbinbin staff 96 6 22 21:47 infodrwxr-xr-x 4 shengbinbin staff 128 6 22 21:52 logsdrwxr-xr-x 24 shengbinbin staff 768 6 22 22:38 objectsdrwxr-xr-x 4 shengbinbin staff 128 6 22 21:47 refsshengbinbin@192 .git % cat HEAD //3. 观察发现HEAD是一个引用，目前正指在 temp分支上ref: refs/heads/tempshengbinbin@192 .git % cd ..shengbinbin@192 git_learning % git checkout master //4. 切换分支Switched to branch 'master'shengbinbin@192 git_learning % cat .git/HEAD //5. 观察HEAD的引用也随之发生了变化ref: refs/heads/mastershengbinbin@192 git_learning % cat .git/config //6. 观察发现 config中存放的都是本地相关的配置[core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true[user] name = shengbinbin email = 1157024800@qq.comshengbinbin@192 git_learning % vim .git/config //7. 直接修改这个配置文件中的usernameshengbinbin@192 git_learning % git config --local user.name //8.发现username确实发生了变化zhangkedanshengbinbin@192 git_learning % git config --local user.name 'shengbinbin' //9.还原user.nameshengbinbin@192 git_learning % git config --local user.nameshengbinbinshengbinbin@192 git_learning % cat .git/config[core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true[user] name = shengbinbin email = 1157024800@qq.comshengbinbin@192 git_learning %shengbinbin@192 git_learning % ref123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172shengbinbin@192 git_learning % cd .gitshengbinbin@192 .git % ls -altotal 56drwxr-xr-x 14 shengbinbin staff 448 6 26 23:29 .drwxr-xr-x 9 shengbinbin staff 288 6 26 23:25 ..-rw-r--r-- 1 shengbinbin staff 9 6 22 22:38 COMMIT_EDITMSG-rw-r--r-- 1 shengbinbin staff 23 6 26 23:25 HEAD-rw-r--r-- 1 shengbinbin staff 41 6 22 22:22 ORIG_HEAD-rw-r--r-- 1 shengbinbin staff 191 6 26 23:29 config-rw-r--r-- 1 shengbinbin staff 73 6 22 21:47 description-rw-r--r-- 1 shengbinbin staff 461 6 22 22:45 gitk.cachedrwxr-xr-x 15 shengbinbin staff 480 6 22 21:47 hooks-rw-r--r-- 1 shengbinbin staff 626 6 26 23:25 indexdrwxr-xr-x 3 shengbinbin staff 96 6 22 21:47 infodrwxr-xr-x 4 shengbinbin staff 128 6 22 21:52 logsdrwxr-xr-x 24 shengbinbin staff 768 6 22 22:38 objectsdrwxr-xr-x 4 shengbinbin staff 128 6 22 21:47 refsshengbinbin@192 .git % cd refsshengbinbin@192 refs % ls -altotal 0drwxr-xr-x 4 shengbinbin staff 128 6 22 21:47 .drwxr-xr-x 14 shengbinbin staff 448 6 26 23:29 ..drwxr-xr-x 4 shengbinbin staff 128 6 22 22:38 heads //1. 存放的分支信息drwxr-xr-x 3 shengbinbin staff 96 6 22 22:53 tagsshengbinbin@192 refs % cd headsshengbinbin@192 heads % ls -altotal 16drwxr-xr-x 4 shengbinbin staff 128 6 22 22:38 .drwxr-xr-x 4 shengbinbin staff 128 6 22 21:47 ..-rw-r--r-- 1 shengbinbin staff 41 6 22 22:23 master-rw-r--r-- 1 shengbinbin staff 41 6 22 22:38 tempshengbinbin@192 heads % pwd/Users/shengbinbin/Documents/Code/git_learning/.git/refs/headsshengbinbin@192 heads % cat master //2. 查看master的内容（就是一个哈希码）aeb4213f9fe55e5ffa1a866df58a519c9dcc18e4shengbinbin@192 heads % git cat-file -t aeb4213f9 //3. 使用git的命令看一下这个哈希码代表了什么东西commitshengbinbin@192 heads % git branch -av* master aeb4213 rename readme to readme.md temp 2a5925a Add test shengbinbin@192 heads % cat temp2a5925a9f096a881a838238a5ced6a5ff3b7f52ashengbinbin@192 heads % cd ..shengbinbin@192 refs % ls tags //4. tags文件中存放的就是之前在gitk中加上的tag。（里程碑的概念）js01shengbinbin@192 refs % cd tagsshengbinbin@192 tags % ls -altotal 8drwxr-xr-x 3 shengbinbin staff 96 6 22 22:53 .drwxr-xr-x 4 shengbinbin staff 128 6 22 21:47 ..-rw-r--r-- 1 shengbinbin staff 41 6 22 22:53 js01shengbinbin@192 tags % cat js01 //5. tag本身有一个哈希码966bd62acdabd98c6aaf60f2cdd4f9517c810915shengbinbin@192 tags % git cat-file -t 966bd6tagshengbinbin@192 tags % git cat-file -p 966bd6 //6. 查看tag的内容，里面存放有一个object对象object 7a430c9c43daee3f83b4b3b607a8ef82e2061a46type committag js01tagger shengbinbin &lt;1157024800@qq.com&gt; 1624373582 +0800first jsshengbinbin@192 tags % git cat-file -t 7a430c9 //7.这个object对象其实就是一个提交commitshengbinbin@192 tags % object1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495shengbinbin@192 .git % cd objectsshengbinbin@192 objects % ls -altotal 0drwxr-xr-x 24 shengbinbin staff 768 6 22 22:38 .drwxr-xr-x 14 shengbinbin staff 448 6 26 23:29 ..drwxr-xr-x 3 shengbinbin staff 96 6 22 21:57 01drwxr-xr-x 4 shengbinbin staff 128 6 22 22:38 2adrwxr-xr-x 3 shengbinbin staff 96 6 22 21:52 31drwxr-xr-x 3 shengbinbin staff 96 6 22 21:52 4bdrwxr-xr-x 3 shengbinbin staff 96 6 22 21:57 6adrwxr-xr-x 3 shengbinbin staff 96 6 22 22:06 7adrwxr-xr-x 3 shengbinbin staff 96 6 22 21:52 7cdrwxr-xr-x 3 shengbinbin staff 96 6 22 22:06 87drwxr-xr-x 3 shengbinbin staff 96 6 22 22:01 91drwxr-xr-x 4 shengbinbin staff 128 6 22 22:53 96drwxr-xr-x 3 shengbinbin staff 96 6 22 22:23 99drwxr-xr-x 4 shengbinbin staff 128 6 22 22:38 9bdrwxr-xr-x 3 shengbinbin staff 96 6 22 22:06 abdrwxr-xr-x 4 shengbinbin staff 128 6 22 22:23 aedrwxr-xr-x 3 shengbinbin staff 96 6 22 22:06 b7drwxr-xr-x 3 shengbinbin staff 96 6 22 22:23 cddrwxr-xr-x 3 shengbinbin staff 96 6 22 21:52 d5drwxr-xr-x 3 shengbinbin staff 96 6 22 21:57 dadrwxr-xr-x 3 shengbinbin staff 96 6 22 22:01 efdrwxr-xr-x 3 shengbinbin staff 96 6 22 22:38 f8drwxr-xr-x 2 shengbinbin staff 64 6 22 21:47 infodrwxr-xr-x 2 shengbinbin staff 64 6 22 21:47 packshengbinbin@192 objects % cd ae //1. 先随便进入一个文件夹查看里面有什么shengbinbin@192 ae % ls -altotal 16drwxr-xr-x 4 shengbinbin staff 128 6 22 22:23 .drwxr-xr-x 24 shengbinbin staff 768 6 22 22:38 ..-r--r--r-- 1 shengbinbin staff 164 6 22 22:23 b4213f9fe55e5ffa1a866df58a519c9dcc18e4-r--r--r-- 1 shengbinbin staff 53 6 22 22:01 e37060401d19e7bd9f80b7b33920a000e96b5bshengbinbin@192 ae % git cat-file -t aeb4213f9fe55e5ffa1a866df58a519c9dcc18e4 //2.将ae和后面的哈希码拼接之后发现类型commitshengbinbin@192 ae % git cat-file -t aee37060401treeshengbinbin@192 ae % git cat-file -p aee37060401 //3. 查看这个tree里面有什么内容。发现有个blob100644 blob ef3f137d8af338a8604544a3e482090684321d93 style.cssshengbinbin@192 ae % git cat-file -p ef3f137d8 //4. 查看这个blob，发现就是之前加入的cssbody{ background-color: orange; font-family: 'Monaco', sans-serif; color: white;}body a{ color: white;}header{ text-align: center; margin-top: 50px;}h3{ color: red;}header-img{ width: 400px;}header-words{ line-height: 10px; font-size: 50px; font-family: 'Monaco', sans-serif; margin-bottom: 75px;}section{ padding: 0 50px 0px 50px; text-align: left;}div.accordion { cursor: pointer; border: none; outline: none;}div.accordion.active, div.accordion:hover { background-color: white; color: #1D2031;}div.panel { padding: 0 18px 0 0; display: none;} 在git中，只要文件内容相同，就被视为是同一个blog！！！ commit/tree/blob三个对象的关系一个commit就代表了一棵树 blob指的就是某个具体的文件 分离头指针状态指的是commit提交的时候没有基于某个分支来做，HEAD并未指向任何分支，可能会被git清理掉。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111shengbinbin@192 git_learning % git logcommit aeb4213f9fe55e5ffa1a866df58a519c9dcc18e4 (HEAD -&gt; master) //1. 一开始的HEAD指向master分支Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800 rename readme to readme.mdcommit 7a430c9c43daee3f83b4b3b607a8ef82e2061a46 (tag: js01)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:06:20 2021 +0800 ADD jscommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caabody{Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMeshengbinbin@192 git_learning % git checkout 9bd6521 //2. 切换分支，切换到某次commit上的时候，Note: switching to '9bd6521'. //3. 提示当前处于分离头指针的情况You are in 'detached HEAD' state. You can look around, make experimentalchanges and commit them, and you can discard any commits you make in thisstate without impacting any branches by switching back to a branch.If you want to create a new branch to retain commits you create, you maydo so (now or later) by using -c with the switch command. Example: git switch -c &lt;new-branch-name&gt;Or undo this operation with: git switch -Turn off this advice by setting config variable advice.detachedHead to falseHEAD is now at 9bd6521 ADD style.cssshengbinbin@192 git_learning % ls -altotal 16drwxr-xr-x 7 shengbinbin staff 224 6 27 16:02 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 14 shengbinbin staff 448 6 27 16:02 .gitdrwxr-xr-x@ 3 shengbinbin staff 96 6 22 21:57 images-rw-r--r-- 1 shengbinbin staff 1303 6 22 22:36 index.html-rw-r--r-- 1 shengbinbin staff 51 6 26 23:25 readmedrwxr-xr-x 3 shengbinbin staff 96 6 22 22:00 stylesshengbinbin@192 git_learning % vim styles/style.css //4. 在分离头指针的情况下进行修改shengbinbin@192 git_learning % git status HEAD detached at 9bd6521Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: styles/style.cssno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)shengbinbin@192 git_learning % git commit -am'Background to green'[detached HEAD fcad9c2] Background to green 1 file changed, 1 insertion(+), 1 deletion(-)shengbinbin@192 git_learning % git logcommit fcad9c2c62b980debc083ff34ef0496512ab2756 (HEAD)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Sun Jun 27 16:03:31 2021 +0800 Background to greencommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caaAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMe shengbinbin@192 git_learning % git checkout master // 5. 切换分支的时候提示你是否对当前提交进行保存Warning: you are leaving 1 commit behind, not connected toany of your branches: fcad9c2 Background to greenIf you want to keep it by creating a new branch, this may be a good timeto do so with: git branch &lt;new-branch-name&gt; fcad9c2Switched to branch 'master'shengbinbin@192 git_learning % git branch fix_css fcad9c2 //6. 将当前分支修改的内容用分支 fix_css来保存下来shengbinbin@192 git_learning % git的基本操作比较两次提交的差异123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//比较两次提交的差异 git diff + 两个 commitIdshengbinbin@192 git_learning % git checkout -b fix_readme fix_cssSwitched to a new branch 'fix_readme'shengbinbin@192 git_learning % git log -n1commit fcad9c2c62b980debc083ff34ef0496512ab2756 (HEAD -&gt; fix_readme, fix_css)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Sun Jun 27 16:03:31 2021 +0800 Background to greenshengbinbin@192 git_learning % gitk --allshengbinbin@192 git_learning % cat .git/HEADref: refs/heads/fix_readmeshengbinbin@192 git_learning % cat .git/refs/heads/fix_readmefcad9c2c62b980debc083ff34ef0496512ab2756shengbinbin@192 git_learning % git cat-file -t fcad9c2commitshengbinbin@192 git_learning % git logcommit fcad9c2c62b980debc083ff34ef0496512ab2756 (HEAD -&gt; fix_readme, fix_css)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Sun Jun 27 16:03:31 2021 +0800 Background to greencommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caaAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMe shengbinbin@192 git_learning % git diff fcad9c2 9bd6521 //比较两次提交的差异diff --git a/styles/style.css b/styles/style.cssindex 4c6bc45..ef3f137 100644--- a/styles/style.css+++ b/styles/style.css@@ -1,5 +1,5 @@ body{- background-color: green;+ background-color: orange; font-family: 'Monaco', sans-serif; color: white; }shengbinbin@192 git_learning % 删除不需要的分支 123456789101112shengbinbin@192 git_learning % git branch -av fix_css fcad9c2 Background to green* fix_readme fcad9c2 Background to green master aeb4213 rename readme to readme.md temp 2a5925a Add testshengbinbin@192 git_learning % gitk --allshengbinbin@192 git_learning % git branch -d fix_css // 删除 fix_css这个分支Deleted branch fix_css (was fcad9c2).shengbinbin@192 git_learning % gitk --allshengbinbin@192 git_learning % 修改commit的message修改上一次提交的commit，使用 git commit –amend 1234567891011121314151617181920shengbinbin@192 git_learning % git log -n1commit aeb4213f9fe55e5ffa1a866df58a519c9dcc18e4 (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800 rename readme to readme.mdshengbinbin@192 git_learning % git commit --amend 'Move filename readme to readme.md'error: pathspec 'Move filename readme to readme.md' did not match any file(s) known to gitshengbinbin@192 git_learning % git commit --amend[master 76459c3] Mobe filename readme to readme.md Date: Tue Jun 22 22:23:50 2021 +0800 1 file changed, 2 insertions(+) create mode 100644 readme.mdshengbinbin@192 git_learning % git log -n1commit 76459c3e75e7a91af40e3fcd340e4c488f4ed589 (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800 Mobe filename readme to readme.mdshengbinbin@192 git_learning % 修改之前的commit信息，不是上一个的：git rebase -i 变基式操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344shengbinbin@192 git_learning % git log -3commit 76459c3e75e7a91af40e3fcd340e4c488f4ed589 (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800 Mobe filename readme to readme.mdcommit 7a430c9c43daee3f83b4b3b607a8ef82e2061a46 (tag: js01)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:06:20 2021 +0800 ADD jscommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.cssshengbinbin@192 git_learning % git rebase -i 9bd65211[detached HEAD 95a2ce3] ADD a js Date: Tue Jun 22 22:06:20 2021 +0800 1 file changed, 15 insertions(+) create mode 100644 js/script.jsSuccessfully rebased and updated refs/heads/master.shengbinbin@192 git_learning % git log -n3commit f8367296bd1af13c79e6e23f1ade3cef425bb26a (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800 Mobe filename readme to readme.mdcommit 95a2ce35bfe6ea75912d0ab6ee6ad793e090657bAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:06:20 2021 +0800 ADD a jscommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.css 1234567891011121314151617181920212223242526pick 7a430c9 ADD js //将pick改成reword，保存退出pick 76459c3 Mobe filename readme to readme.md# Rebase 9bd6521..76459c3 onto 9bd6521 (2 commands)## Commands:# p, pick &lt;commit&gt; = use commit# r, reword &lt;commit&gt; = use commit, but edit the commit message# e, edit &lt;commit&gt; = use commit, but stop for amending# s, squash &lt;commit&gt; = use commit, but meld into previous commit# f, fixup &lt;commit&gt; = like &quot;squash&quot;, but discard this commit's log message# x, exec &lt;command&gt; = run command (the rest of the line) using shell# b, break = stop here (continue rebase later with 'git rebase --continue')# d, drop &lt;commit&gt; = remove commit# l, label &lt;label&gt; = label current HEAD with a name# t, reset &lt;label&gt; = reset HEAD to a label# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]# . create a merge commit using the original merge commit's# . message (or the oneline, if no original merge commit was# . specified). Use -c &lt;commit&gt; to reword the commit message.## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted. 1234567891011121314151617ADD a js //此时修改message# Please enter the commit message for your changes. Lines starting# with '#' will be ignored, and an empty message aborts the commit.## Date: Tue Jun 22 22:06:20 2021 +0800## interactive rebase in progress; onto 9bd6521# Last command done (1 command done):# reword 7a430c9 ADD js# Next command to do (1 remaining command):# pick 76459c3 Mobe filename readme to readme.md# You are currently editing a commit while rebasing branch 'master' on '9bd6521'.## Changes to be committed:# new file: js/script.js# 将连续的commit整理成一个12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364shengbinbin@192 git_learning % git log // 查看历史commit f8367296bd1af13c79e6e23f1ade3cef425bb26a (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800 Mobe filename readme to readme.mdcommit 95a2ce35bfe6ea75912d0ab6ee6ad793e090657bAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:06:20 2021 +0800 ADD a jscommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caaAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMeshengbinbin@192 git_learning % git rebase -i d59544f7 //将这个commit 后面的三个commit合并成一个[detached HEAD 88f9280] create a complete web page ADD index + logo ADD style.css ADD a js Date: Tue Jun 22 21:57:59 2021 +0800 4 files changed, 114 insertions(+) create mode 100644 images/git-logo.png create mode 100644 index.html create mode 100644 js/script.js create mode 100644 styles/style.cssSuccessfully rebased and updated refs/heads/master.shengbinbin@192 git_learning % git log //合并完之后再看一下logcommit ea5ccd12fd5b46a7536a92fec3cf6619dbe993f3 (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800 Mobe filename readme to readme.mdcommit 88f92801204a631a4eb1239a997b5457bc4f8c34Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 create a complete web page ADD index + logo ADD style.css ADD a jscommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMeshengbinbin@192 git_learning % 1234567891011121314151617181920212223242526272829pick 2a76eaa ADD index + logopick 9bd6521 ADD style.css //将pick改成 spick 95a2ce3 ADD a js //将pick改成spick f836729 Mobe filename readme to readme.md# Rebase d59544f..f836729 onto d59544f (4 commands)## Commands:# p, pick &lt;commit&gt; = use commit# r, reword &lt;commit&gt; = use commit, but edit the commit message# e, edit &lt;commit&gt; = use commit, but stop for amending# s, squash &lt;commit&gt; = use commit, but meld into previous commit# f, fixup &lt;commit&gt; = like &quot;squash&quot;, but discard this commit's log message# x, exec &lt;command&gt; = run command (the rest of the line) using shell# b, break = stop here (continue rebase later with 'git rebase --continue')# d, drop &lt;commit&gt; = remove commit# l, label &lt;label&gt; = label current HEAD with a name# t, reset &lt;label&gt; = reset HEAD to a label# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]# . create a merge commit using the original merge commit's# . message (or the oneline, if no original merge commit was# . specified). Use -c &lt;commit&gt; to reword the commit message.## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted.# 1234567891011121314151617181920212223242526272829303132# This is a combination of 3 commits.# This is the 1st commit message:create a complete web page //新加一个commit信息ADD index + logo# This is the commit message #2:ADD style.css# This is the commit message #3:ADD a js# Please enter the commit message for your changes. Lines starting# with '#' will be ignored, and an empty message aborts the commit.## Date: Tue Jun 22 21:57:59 2021 +0800## interactive rebase in progress; onto d59544f# Last commands done (3 commands done):# squash 9bd6521 ADD style.css# squash 95a2ce3 ADD a js# Next command to do (1 remaining command):# pick f836729 Mobe filename readme to readme.md# You are currently rebasing branch 'master' on 'd59544f'.## Changes to be committed:# new file: images/git-logo.png# new file: index.html# new file: js/script.js# new file: styles/style.css# 将间隔的commit整理成一个123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172shengbinbin@192 git_learning % git logcommit ea5ccd12fd5b46a7536a92fec3cf6619dbe993f3 (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:23:50 2021 +0800# This is a combination of 2 commits. Mobe filename readme to readme.md // 1 commit 88f92801204a631a4eb1239a997b5457bc4f8c34Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 create a complete web page ADD index + logo ADD style.css ADD a jscommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMe //2 ,将这个和上面的1合并在一起 shengbinbin@192 git_learning % git rebase -i d59544fSuccessfully rebased and updated refs/heads/master.shengbinbin@192 git_learning % git rebase -i d59544fThe previous cherry-pick is now empty, possibly due to conflict resolution.If you wish to commit it anyway, use: git commit --allow-emptyOtherwise, please use 'git rebase --skip'interactive rebase in progress; onto d59544fLast command done (1 command done): pick d59544fNext commands to do (2 remaining commands): squash ea5ccd1 Mobe filename readme to readme.md pick 88f9280 create a complete web page ADD index + logo ADD style.css ADD a js (use &quot;git rebase --edit-todo&quot; to view and edit)You are currently rebasing branch 'master' on 'd59544f'. (all conflicts fixed: run &quot;git rebase --continue&quot;)nothing to commit, working tree cleanCould not apply d59544f...shengbinbin@192 git_learning % git rebase --continue[detached HEAD 0a8f323] Add a new readme Date: Tue Jun 22 21:52:44 2021 +0800 2 files changed, 4 insertions(+) create mode 100644 readme create mode 100644 readme.mdSuccessfully rebased and updated refs/heads/master.shengbinbin@192 git_learning % git logcommit 01b4b51917535a1b2cdafef8fc275ab17e8b4a1d (HEAD -&gt; master)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 create a complete web page ADD index + logo ADD style.css ADD a jscommit 0a8f323893329d23eb7bc2e68049391ee3957e53Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add a new readme Add readMe Mobe filename readme to readme.md 1234567891011121314151617181920212223242526272829pick d59544fs ea5ccd1 Mobe filename readme to readme.md //改成spick 88f9280 create a complete web page ADD index + logo ADD style.css ADD a js# Rebase d59544f..ea5ccd1 onto d59544f (2 commands)## Commands:# p, pick &lt;commit&gt; = use commit# r, reword &lt;commit&gt; = use commit, but edit the commit message# e, edit &lt;commit&gt; = use commit, but stop for amending# s, squash &lt;commit&gt; = use commit, but meld into previous commit# f, fixup &lt;commit&gt; = like &quot;squash&quot;, but discard this commit's log message# x, exec &lt;command&gt; = run command (the rest of the line) using shell# b, break = stop here (continue rebase later with 'git rebase --continue')# d, drop &lt;commit&gt; = remove commit# l, label &lt;label&gt; = label current HEAD with a name# t, reset &lt;label&gt; = reset HEAD to a label# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]# . create a merge commit using the original merge commit's# . message (or the oneline, if no original merge commit was# . specified). Use -c &lt;commit&gt; to reword the commit message.## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted.# 123456789101112131415161718192021222324252627282930# This is a combination of 2 commits.Add a new readme# This is the 1st commit message:Add readMe# This is the commit message #2:Mobe filename readme to readme.md# Please enter the commit message for your changes. Lines starting# with '#' will be ignored, and an empty message aborts the commit.## Date: Tue Jun 22 21:52:44 2021 +0800## interactive rebase in progress; onto d59544f# Last commands done (2 commands done):# pick d59544f# squash ea5ccd1 Mobe filename readme to readme.md# Next command to do (1 remaining command):# pick 88f9280 create a complete web page ADD index + logo ADD style.css ADD a js# You are currently rebasing branch 'master' on 'd59544f'.### Initial commit## Changes to be committed:# new file: readme# new file: readme.md# 暂存区和HEAD做比较head 为已经提交过后的状态 12345678910111213141516171819202122232425shengbinbin@192 git_learning % git statusOn branch masternothing to commit, working tree cleanshengbinbin@192 git_learning % vi index.html shengbinbin@192 git_learning % git add index.html shengbinbin@192 git_learning % git diff --cached //比较暂存区和Head最新的提交的差异diff --git a/index.html b/index.htmlindex 6ad4c68..0f0ee94 100644--- a/index.html+++ b/index.html@@ -14,7 +14,7 @@ &lt;div class=&quot;accordion&quot;&gt;&lt;h1&gt;Terminologys&lt;/h1&gt;&lt;/div&gt; &lt;div class=&quot;panel&quot;&gt; &lt;ol&gt;- &lt;li&gt;&lt;/li&gt;+ &lt;li&gt;add&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; shengbinbin@192 git_learning % git commit -m'Add the first git command with config'[master 6cff2f6] Add the first git command with config 1 file changed, 1 insertion(+), 1 deletion(-) 暂存区和工作区文件的比较123456789101112131415161718192021222324body{shengbinbin@192 git_learning % git branch fix_readme* master tempshengbinbin@192 git_learning % vim index.htmlshengbinbin@192 git_learning % git add index.html //加入到暂存区shengbinbin@192 git_learning % vim styles/style.cssshengbinbin@192 git_learning % git diff //默认比较的就是工作区和暂存区diff --git a/styles/style.css b/styles/style.cssindex ef3f137..f2a08e0 100644--- a/styles/style.css+++ b/styles/style.css@@ -1,7 +1,7 @@ body{ background-color: orange; font-family: 'Monaco', sans-serif;- color: white;+ color: black; } body a{shengbinbin@192 git_learning % 将暂存区恢复成HEAD一样123456789101112131415161718192021222324252627282930313233343536shengbinbin@192 git_learning % git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: index.htmlChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: styles/style.cssshengbinbin@192 git_learning % git add styles/style.cssshengbinbin@192 git_learning % git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: index.html modified: styles/style.cssshengbinbin@192 git_learning % git reset HEAD //恢复暂存区Unstaged changes after reset:M index.htmlM styles/style.cssshengbinbin@192 git_learning % git statusOn branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: index.html modified: styles/style.cssno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)shengbinbin@192 git_learning % git diff --cachedshengbinbin@192 git_learning % 将工作区恢复成暂存区12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849shengbinbin@192 git_learning % git statusOn branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: index.html modified: styles/style.cssno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)shengbinbin@192 git_learning % git add index.htmlshengbinbin@192 git_learning % git diff --cacheddiff --git a/index.html b/index.htmlindex 0f0ee94..5ec8fa6 100644--- a/index.html+++ b/index.html@@ -15,7 +15,7 @@ &lt;div class=&quot;panel&quot;&gt; &lt;ol&gt; &lt;li&gt;add&lt;/li&gt;- &lt;li&gt;&lt;/li&gt;+ &lt;li&gt;bare repo&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt;shengbinbin@192 git_learning % vim index.htmlshengbinbin@192 git_learning % git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: index.htmlChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: index.html modified: styles/style.cssshengbinbin@192 git_learning % git restore index.htmlshengbinbin@192 git_learning % git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: index.htmlChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: styles/style.css 回退到之前的commit状态12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758shengbinbin@192 git_learning % git branch -av fix_readme fcad9c2 Background to green* master 6cff2f6 Add the first git command with config temp 2a5925a Add testshengbinbin@192 git_learning % git checkout tempSwitched to branch 'temp'shengbinbin@192 git_learning % git logcommit 2a5925a9f096a881a838238a5ced6a5ff3b7f52a (HEAD -&gt; temp)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:38:52 2021 +0800 Add testcommit 9bd65211b66a0a27503644f8fe66a66a428d4ecbAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caaAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMeshengbinbin@192 git_learning % gitk --allshengbinbin@192 git_learning % git reset --hard 9bd65211 //回退到指定的commitHEAD is now at 9bd6521 ADD style.cssshengbinbin@192 git_learning % gitk --allshengbinbin@192 git_learning % git logcommit 9bd65211b66a0a27503644f8fe66a66a428d4ecb (HEAD -&gt; temp)Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 22:01:17 2021 +0800 ADD style.csscommit 2a76eaa0b2bbcda5419e741bd16fefa58afc9caaAuthor: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:57:59 2021 +0800 ADD index + logocommit d59544f7fba30a55f9511993709de2403c9cfbe5Author: shengbinbin &lt;1157024800@qq.com&gt;Date: Tue Jun 22 21:52:44 2021 +0800 Add readMeshengbinbin@192 git_learning % 查看不同commit的指定文件的差异1234567891011121314151617181920shengbinbin@192 git_learning % git branch -av fix_readme fcad9c2 Background to green master 6cff2f6 Add the first git command with config* temp 9bd6521 ADD style.cssshengbinbin@192 git_learning % git diff 9bd6521 6cff2f6 -- index.html //查看不同提交指定文件的差异diff --git a/index.html b/index.htmlindex 6ad4c68..0f0ee94 100644--- a/index.html+++ b/index.html@@ -14,7 +14,7 @@ &lt;div class=&quot;accordion&quot;&gt;&lt;h1&gt;Terminologys&lt;/h1&gt;&lt;/div&gt; &lt;div class=&quot;panel&quot;&gt; &lt;ol&gt;- &lt;li&gt;&lt;/li&gt;+ &lt;li&gt;add&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt;shengbinbin@192 git_learning % 删除文件123456789101112131415161718192021222324252627282930shengbinbin@192 git_learning % ls -altotal 16drwxr-xr-x 7 shengbinbin staff 224 6 28 09:49 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 14 shengbinbin staff 448 6 28 09:49 .gitdrwxr-xr-x 3 shengbinbin staff 96 6 27 21:59 images-rw-r--r-- 1 shengbinbin staff 1303 6 28 09:47 index.html-rw-r--r-- 1 shengbinbin staff 51 6 28 09:49 readmedrwxr-xr-x 3 shengbinbin staff 96 6 28 09:46 stylesshengbinbin@192 git_learning % git branch fix_readme master* tempshengbinbin@192 git_learning % git rm readme //git 删除指定文件rm 'readme'shengbinbin@192 git_learning % git statusOn branch tempChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) deleted: readmeshengbinbin@192 git_learning % ls -altotal 8drwxr-xr-x 6 shengbinbin staff 192 6 28 09:54 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 14 shengbinbin staff 448 6 28 09:54 .gitdrwxr-xr-x 3 shengbinbin staff 96 6 27 21:59 images-rw-r--r-- 1 shengbinbin staff 1303 6 28 09:47 index.htmldrwxr-xr-x 3 shengbinbin staff 96 6 28 09:46 stylesshengbinbin@192 git_learning % 指定不需要git管理的文件12345678910111213141516171819202122232425262728293031shengbinbin@192 git_learning % echo 'I am a file' &gt;docshengbinbin@192 git_learning % ls -altotal 32drwxr-xr-x 9 shengbinbin staff 288 6 28 10:03 .drwxr-xr-x@ 7 shengbinbin staff 224 6 22 21:47 ..drwxr-xr-x 14 shengbinbin staff 448 6 28 10:02 .git-rw-r--r-- 1 shengbinbin staff 5 6 28 10:02 .gitignore-rw-r--r-- 1 shengbinbin staff 12 6 28 10:03 docdrwxr-xr-x 3 shengbinbin staff 96 6 27 21:59 images-rw-r--r-- 1 shengbinbin staff 1303 6 28 09:47 index.html-rw-r--r-- 1 shengbinbin staff 51 6 28 09:56 readmedrwxr-xr-x 3 shengbinbin staff 96 6 28 09:46 stylesshengbinbin@192 git_learning % cat .gitignoredoc/shengbinbin@192 git_learning % git statusOn branch tempUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) .gitignore docnothing added to commit but untracked files present (use &quot;git add&quot; to track)shengbinbin@192 git_learning % vi .gitignoreshengbinbin@192 git_learning % git statusOn branch tempUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) .gitignorenothing added to commit but untracked files present (use &quot;git add&quot; to track)shengbinbin@192 git_learning % Git的备份常用的传输协议： 哑协议和智能协议的区别： 直观区别：哑协议传输进度不可见，而智能协议传输是可见的。 传输速度：智能协议比哑协议传输速度更快 123456789101112131415161718192021222324shengbinbin@192 git_learning % pwd/Users/shengbinbin/Documents/Code/git_learningshengbinbin@192 git_learning % cd ..shengbinbin@192 Code % mkdir backupshengbinbin@192 Code % cd backupshengbinbin@192 backup % pwd/Users/shengbinbin/Documents/Code/backupshengbinbin@192 backup % cd ..shengbinbin@192 Code % pwd/Users/shengbinbin/Documents/Codeshengbinbin@192 Code % git clone --bare /Users/shengbinbin/Documents/Code/git_learning/.git ya.gitCloning into bare repository 'ya.git'...done.shengbinbin@192 Code % git clone --bare file:///Users/shengbinbin/Documents/Code/git_learning/.git zhineng.gitCloning into bare repository 'zhineng.git'...remote: Enumerating objects: 28, done.remote: Counting objects: 100% (28/28), done.remote: Compressing objects: 100% (23/23), done.remote: Total 28 (delta 8), reused 0 (delta 0), pack-reused 0Receiving objects: 100% (28/28), 22.06 KiB | 22.06 MiB/s, done.Resolving deltas: 100% (8/8), done.shengbinbin@192 Code % 123456789shengbinbin@192 git_learning % git remote -vshengbinbin@192 git_learning % git remote add zhineng file:///Users/shengbinbin/Documents/Code/backup/zhineng.gitshengbinbin@192 git_learning % git branch -av fix_readme fcad9c2 Background to green master 6cff2f6 Add the first git command with config* temp 9bd6521 ADD style.css//可以将当前文件夹的内容备份到本地的另外一个文件夹 GitHub的交互过程配置SSH的公私钥https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account 检查一下是否已经存在了SSH密钥：https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/checking-for-existing-ssh-keys 1234567891011shengbinbin@chengbinbindeMacBook-Pro ~ % cd ~/.sshshengbinbin@chengbinbindeMacBook-Pro .ssh % ls -altotal 24drwx------ 5 shengbinbin staff 160 5 6 15:26 .drwxr-xr-x+ 35 shengbinbin staff 1120 6 22 10:25 ..-rw------- 1 shengbinbin staff 2602 5 6 15:22 id_rsa-rw-r--r--@ 1 shengbinbin staff 571 5 6 15:22 id_rsa.pub //已经有了-rw-r--r-- 1 shengbinbin staff 1197 5 6 16:51 known_hostsshengbinbin@chengbinbindeMacBook-Pro .ssh % cat id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDjEseQlMpM7ANSuie198dnHoopBWLaNEuIkBSwSA1NeDHY1a0gKXp8TZkujdLPUIJDcd7AOXPTZ4AmhMmHN7eC5XNR2MuXMZxbzWgdd/nTuwbeve8DmvQbgBQ4Jn6mqDdmnJO62r4XlO1CvScyQ5i+HW5LcLBXtdzUjMpaUb5mnzqIqg7QAR6m3j2oYyMzf4ccfoMEu1XbdTAV2opeRdOadUwpN3STq5X5JM5ECbcXfitMrVz1FnNNvH+UEmKqA/I4Ea4vB7npeaaECb4krMfo4InozLIdD0reZSyvHG1AP2/OdHD6Aw4/3XA8XTKv9JWwV8Xhce3B3VFjL2ajDwl0CJVFuQ6GZd778T4/hlPSSOmmQJOilsvOEqRq4EoojJyBx3TvU/GwlkwpRgbeQvKHwiV095VDAV8DKB2qMFmgQ1cvOTLRyTPgvuO6CkkZjR0UD9OQ3+uWSz3B8TA9ULALsbYsWP9LaACSkI2PyGwfLWuyCQ14ckir84sub/Bzw4s= 1157024800@qq.comshengbinbin@chengbinbindeMacBook-Pro .ssh % 将公钥粘到github上去。 创建个人仓库 将本地仓库同步到GitHub123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960shengbinbin@192 git_learning % git remote -v //查看远程分支信息 ,现在是有2个本地的备份zhineng file:///Users/shengbinbin/Documents/Code/backup/zhineng.git (fetch)zhineng file:///Users/shengbinbin/Documents/Code/backup/zhineng.git (push) ////新增远程的github仓库地址，命名为githubshengbinbin@192 git_learning % git remote add github git@github.com:binshow/git_learning.gitshengbinbin@192 git_learning % git remote -vgithub git@github.com:binshow/git_learning.git (fetch)github git@github.com:binshow/git_learning.git (push)zhineng file:///Users/shengbinbin/Documents/Code/backup/zhineng.git (fetch)zhineng file:///Users/shengbinbin/Documents/Code/backup/zhineng.git (push)shengbinbin@192 git_learning % git push github --all //将本地的所以分支push到github上Enumerating objects: 25, done.Counting objects: 100% (25/25), done.Delta compression using up to 8 threadsCompressing objects: 100% (20/20), done.Writing objects: 100% (25/25), 21.83 KiB | 7.28 MiB/s, done.Total 25 (delta 6), reused 0 (delta 0), pack-reused 0remote: Resolving deltas: 100% (6/6), done.remote:remote: Create a pull request for 'fix_readme' on GitHub by visiting:remote: https://github.com/binshow/git_learning/pull/new/fix_readmeremote:To github.com:binshow/git_learning.git * [new branch] fix_readme -&gt; fix_readme ! [rejected] master -&gt; master (fetch first) ! [rejected] temp -&gt; temp (fetch first)error: failed to push some refs to 'github.com:binshow/git_learning.git'hint: Updates were rejected because the remote contains work that you dohint: not have locally. This is usually caused by another repository pushinghint: to the same ref. You may want to first integrate the remote changeshint: (e.g., 'git pull ...') before pushing again.hint: See the 'Note about fast-forwards' in 'git push --help' for details.shengbinbin@192 git_learning % git pull github masterhint: Pulling without specifying how to reconcile divergent branches ishint: discouraged. You can squelch this message by running one of the followinghint: commands sometime before your next pull:hint:hint: git config pull.rebase false # merge (the default strategy)hint: git config pull.rebase true # rebasehint: git config pull.ff only # fast-forward onlyhint:hint: You can replace &quot;git config&quot; with &quot;git config --global&quot; to set a defaulthint: preference for all repositories. You can also pass --rebase, --no-rebase,hint: or --ff-only on the command line to override the configured default perhint: invocation.remote: Enumerating objects: 13, done.remote: Counting objects: 100% (13/13), done.remote: Compressing objects: 100% (8/8), done.remote: Total 13 (delta 2), reused 13 (delta 2), pack-reused 0Unpacking objects: 100% (13/13), 20.76 KiB | 2.59 MiB/s, done.From github.com:binshow/git_learning * branch master -&gt; FETCH_HEAD * [new branch] master -&gt; github/masterfatal: refusing to merge unrelated historiesshengbinbin@192 git_learning % 协同开发的常见操作不同人修改了不同文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106shengbinbin@chengbinbindeMacBook-Pro git_learn % git clone git@github.com:binshow/git_learning.gitCloning into 'git_learning'...remote: Enumerating objects: 20, done.remote: Counting objects: 100% (20/20), done.remote: Compressing objects: 100% (12/12), done.remote: Total 20 (delta 3), reused 14 (delta 3), pack-reused 0Receiving objects: 100% (20/20), 22.62 KiB | 279.00 KiB/s, done.Resolving deltas: 100% (3/3), done.shengbinbin@chengbinbindeMacBook-Pro git_learn % ls -altotal 16drwxr-xr-x 8 shengbinbin staff 256 6 22 14:45 .drwx------@ 29 shengbinbin staff 928 6 21 09:09 ..-rw-r--r--@ 1 shengbinbin staff 6148 6 22 09:23 .DS_Storedrwxr-xr-x@ 8 shengbinbin staff 256 11 25 2018 0-materialdrwxr-xr-x 4 shengbinbin staff 128 6 22 13:02 backupdrwxr-xr-x@ 8 shengbinbin staff 256 6 22 14:14 git_leanrningdrwxr-xr-x 5 shengbinbin staff 160 6 22 14:45 git_learningdrwxr-xr-x 11 shengbinbin staff 352 6 22 09:17 git_study//两个分支shengbinbin@chengbinbindeMacBook-Pro git_learn % git clone git@github.com:binshow/git_learning.git git_learning_02Cloning into 'git_learning_02'...remote: Enumerating objects: 20, done.remote: Counting objects: 100% (20/20), done.remote: Compressing objects: 100% (12/12), done.remote: Total 20 (delta 3), reused 14 (delta 3), pack-reused 0Receiving objects: 100% (20/20), 22.62 KiB | 11.31 MiB/s, done.Resolving deltas: 100% (3/3), done.shengbinbin@chengbinbindeMacBook-Pro git_learn % clearshengbinbin@chengbinbindeMacBook-Pro git_learn % ls -altotal 16drwxr-xr-x 9 shengbinbin staff 288 6 22 14:45 .drwx------@ 29 shengbinbin staff 928 6 21 09:09 ..-rw-r--r--@ 1 shengbinbin staff 6148 6 22 09:23 .DS_Storedrwxr-xr-x@ 8 shengbinbin staff 256 11 25 2018 0-materialdrwxr-xr-x 4 shengbinbin staff 128 6 22 13:02 backupdrwxr-xr-x@ 8 shengbinbin staff 256 6 22 14:14 git_leanrningdrwxr-xr-x 5 shengbinbin staff 160 6 22 14:45 git_learningdrwxr-xr-x 5 shengbinbin staff 160 6 22 14:45 git_learning_02drwxr-xr-x 11 shengbinbin staff 352 6 22 09:17 git_studyshengbinbin@chengbinbindeMacBook-Pro git_learn % cd git_learning_02shengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git config --add --local user.name 'zkd'shengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git config --add --local user.eamil 'zkd@qq.com'shengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git config --local --listcore.repositoryformatversion=0core.filemode=truecore.bare=falsecore.logallrefupdates=truecore.ignorecase=truecore.precomposeunicode=trueremote.origin.url=git@github.com:binshow/git_learning.gitremote.origin.fetch=+refs/heads/*:refs/remotes/origin/*branch.main.remote=originbranch.main.merge=refs/heads/mainuser.name=zkduser.eamil=zkd@qq.comshengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git branch -av* main 7e5f6d1 Create README.md remotes/origin/HEAD -&gt; origin/main remotes/origin/feature/add_git_commands 7e5f6d1 Create README.md remotes/origin/main 7e5f6d1 Create README.md remotes/origin/master 8773e48 ADD js remotes/origin/temp a01c150 ADD js //基于远地的分支创建一个本地的分支shengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git checkout -b feature/add_git_commands origin/feature/add_git_commandsBranch 'feature/add_git_commands' set up to track remote branch 'feature/add_git_commands' from 'origin'.Switched to a new branch 'feature/add_git_commands'//第一个人修改了readMeshengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git branch* feature/add_git_commands mainshengbinbin@chengbinbindeMacBook-Pro git_learning_02 % ls -altotal 16drwxr-xr-x 5 shengbinbin staff 160 6 22 14:45 .drwxr-xr-x 9 shengbinbin staff 288 6 22 14:45 ..drwxr-xr-x 12 shengbinbin staff 384 6 22 14:50 .git-rw-r--r-- 1 shengbinbin staff 1064 6 22 14:45 LICENSE-rw-r--r-- 1 shengbinbin staff 35 6 22 14:45 README.mdshengbinbin@chengbinbindeMacBook-Pro git_learning_02 % vim README.mdshengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git add README.mdshengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git statusOn branch feature/add_git_commandsYour branch is up to date with 'origin/feature/add_git_commands'.Changes to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: README.mdshengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git commit -m'ADD git commands description in readMe'[feature/add_git_commands 1a26577] ADD git commands description in readMe 1 file changed, 2 insertions(+)shengbinbin@chengbinbindeMacBook-Pro git_learning_02 % git pushEnumerating objects: 5, done.Counting objects: 100% (5/5), done.Delta compression using up to 8 threadsCompressing objects: 100% (3/3), done.Writing objects: 100% (3/3), 350 bytes | 350.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0), pack-reused 0To github.com:binshow/git_learning.git 7e5f6d1..1a26577 feature/add_git_commands -&gt; feature/add_git_commands shengbinbin@chengbinbindeMacBook-Pro git_learning_02 %","link":"/2021/06/22/git%E4%B9%8B%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"},{"title":"操作系统（二）进程和线程","text":"主要分为四个部分： 进程基础知识 线程基础知识 进程间通信方式 线程间通信方式 进程调度算法 多线程同步 死锁问题 进程基础知识我们编写的代码只是⼀个存储在硬盘的静态⽂件，通过编译后就会⽣成⼆进制可执⾏⽂件，当我们运⾏这个可执⾏⽂件后，它会被装载到内存中，接着 CPU 会执⾏程序中的每⼀条指令，那么这个运⾏中的程序，就被称为「进程」（Process） CPU是支持多个程序交替执行的思想，这种执行方式称为并发执行。 比如一个程序需要去硬盘中读取数据，那么运行到读取文件的指令时，就会去硬盘中读取数据，但是硬盘的读写速度是⾮常慢的，那么在这个时候，如果 CPU傻傻的等硬盘返回数据的话，那 CPU 的利⽤率是⾮常低的。 你烧开水的时候会一直等到水烧开吗？肯定不会，中间可以去干别的事情。 虽然单核的 CPU 在某⼀个瞬间，只能运⾏⼀个进程。但在 1 秒钟期间，它可能会运⾏多个进程，这样就产⽣并⾏的错觉，实际上这是并发： 进程发展历程先从进程线程的发展过程开始说起： 很久以前，CPU只能挨个挨个的处理程序，程序A处理完了才能继续处理程序B，此时叫批处理系统 随着CPU的迅速发展，运行速度越来越快，出现了程序的执行速度和磁盘的IO速度不匹配的矛盾。当程序A需要从硬盘读取数据的时候，CPU要进行等待，非常浪费。所以当程序A读取数据的时候，先对A的执行现场进行保存，再对程序B进行处理，等程序A读取完毕的时候再通过中断机制使得CPU继续执行他。 当程序A被再次调度执行的时候，IO操作还没有完成，只能卡住不动。所以程序A内部出现了多个线程，一个线程去读取IO操作，另外的线程去处理其他的事，比如响应用户等等 每个线程又可以执行多个协程，操作系统内核完全不用参与，相当于用户态线程了，开销非常小 总而言之： 进程是系统进行资源分配和调度的基本单位。 进程存在的作用就是合理的隔离资源、运行环境、提升资源利用率 注意进程和程序的区别： 程序是指指令和数据的有序集合，是一个静态的概念。 进程是程序的一次运行，是一个具有独立功能的程序关于某个数据集合的一次运行活动，是系统进行资源分配和调度的基本单位。 单个CPU可以被若干进程共享，它使用某种调度算法决定何时停止一个进程的工作，并转而为另外一个进程提供服务。另外需要注意的是，如果一个进程运行了两遍，则被认为是两个进程。 举例区分： 进程和程序之间的区别是非常微妙的，但是通过一个例子可以让你加以区分：想想一位会做饭的计算机科学家正在为他的女儿制作生日蛋糕。他有做生日蛋糕的食谱，厨房里有所需的原谅：面粉、鸡蛋、糖、香草汁等。在这个比喻中，做蛋糕的食谱就是程序、计算机科学家就是 CPU、而做蛋糕的各种原谅都是输入数据。进程就是科学家阅读食谱、取来各种原料以及烘焙蛋糕等一系例了动作的总和。 进程的状态尽管每个进程是一个独立的实体，有其自己的程序计数器和内部状态，但是，进程之间仍然需要相互帮助。例如，一个进程的结果可以作为另一个进程的输入，在 shell 命令中 1cat chapter1 chapter2 chapter3 | grep tree 第一个进程是 cat，将三个文件级联并输出。第二个进程是 grep，它从输入中选择具有包含关键字 tree 的内容，根据这两个进程的相对速度（这取决于两个程序的相对复杂度和各自所分配到的 CPU 时间片），可能会发生下面这种情况，grep 准备就绪开始运行，但是输入进程还没有完成，于是必须阻塞 grep 进程，直到输入完毕。 当一个进程开始运行时，它可能会经历下面三状态模型： 图中会涉及三种状态 运行态，运行态指的就是进程实际占用 CPU 时间片运行时 就绪态，就绪态指的是可运行，但因为其他进程正在运行而处于就绪状态 阻塞态，除非某种外部事件发生，否则进程不能运行 逻辑上来说，运行态和就绪态是很相似的。这两种情况下都表示进程可运行，但是第二种情况没有获得 CPU 时间分片。第三种状态与前两种状态不同的原因是这个进程不能运行，CPU 空闲时也不能运行。 状态变迁： NULL -&gt; 创建状态：⼀个新进程被创建时的第⼀个状态； 创建状态 -&gt; 就绪状态：当进程被创建完成并初始化后，⼀切就绪准备运⾏时，变为就绪状态，这个过程是很快的； 就绪态 -&gt; 运⾏状态：处于就绪状态的进程被操作系统的进程调度器选中后，就分配给CPU 正式运⾏该进程； 运⾏状态 -&gt; 结束状态：当进程已经运⾏完成或出错时，会被操作系统作结束状态处理； 运⾏状态 -&gt; 就绪状态：处于运⾏状态的进程在运⾏过程中，由于分配给它的运⾏时间⽚⽤完，操作系统会把该进程变为就绪态，接着从就绪态选中另外⼀个进程运⾏； 运⾏状态 -&gt; 阻塞状态：当进程请求某个事件且必须等待时，例如请求 I/O 事件； 阻塞状态 -&gt; 就绪状态：当进程要等待的事件完成时，它从阻塞状态变到就绪状态； 如果有⼤量处于阻塞状态的进程，进程可能会占⽤着物理内存空间，显然不是我们所希望的，毕竟物理内存空间是有限的，被阻塞状态的进程占⽤着物理内存就⼀种浪费物理内存的⾏为。 所以，在虚拟内存管理的操作系统中，通常会把阻塞状态的进程的物理内存空间换出到硬盘，等需要再次运⾏的时候，再从硬盘换⼊到物理内存。 因此就需要一个新的状态来描述这种进程没有占⽤实际的物理内存空间的情况，称为挂起。 阻塞挂起状态：进程在外存（硬盘）并等待某个事件的出现； 就绪挂起状态：进程在外存（硬盘），但只要进⼊内存，即刻⽴刻运⾏； 通过 sleep 让进程间歇性挂起，其工作原理是设置一个定时器，到期后唤醒进程。 用户希望挂起一个程序的执行，比如在 Linux 中用 Ctrl+Z 挂起进程； 进程的控制结构PCBPCB 是进程存在的唯⼀标识，这意味着⼀个进程的存在，必然会有⼀个 PCB，如果进程消失了，那么 PCB 也会随之消失 PCB包含的内容： 进程描述信息：进程标识符和⽤户标识符等 进程控制和管理信息：进程当前状态和进程优先级等 资源分配清单：有关内存地址空间或虚拟地址空间的信息，所打开⽂件的列表和所使⽤的 I/O 设备信息 CPU 相关信息：CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB中，以便进程重新执⾏时，能从断点处继续执⾏。 PCB的组织方式： 通常是通过链表的⽅式进⾏组织，把具有相同状态的进程链在⼀起，组成各种队列。⽐如： 将所有处于就绪状态的进程链在⼀起，称为就绪队列； 把所有因等待某事件⽽处于等待状态的进程链在⼀起就组成各种阻塞队列； 另外，对于运⾏队列在单核 CPU 系统中则只有⼀个运⾏指针了，因为单核 CPU 在某个时间，只能运⾏⼀个程序。 进程的控制进程的创建操作系统允许⼀个进程创建另⼀个进程，⽽且允许⼦进程继承⽗进程所拥有的资源，当⼦进程被终⽌时，其在⽗进程处继承的资源应当还给⽗进程。同时，终⽌⽗进程时同时也会终⽌其所有的⼦进程。 Linux 操作系统对于终⽌有⼦进程的⽗进程，会把⼦进程交给 1 号进程接管。 创建过程： 为新进程分配⼀个唯⼀的进程标识号，并申请⼀个空⽩的 PCB，PCB 是有限的，若申请失败则创建失败； 为进程分配资源，此处如果资源不⾜，进程就会进⼊等待状态，以等待资源；初始化 PCB； 如果进程的调度队列能够接纳新进程，那就将进程插⼊到就绪队列，等待被调度运⾏； 对于虚拟空间地址来说，子进程会拷贝父进程的虚拟地址空间。所以，fork后子进程的用户区与父进程的用户区相同，也会拷贝内核区内容，仅仅是进程的 pid不同。 实际上，准确的来说，Linux的fork 是通过 写时拷贝 (copy-on-write)实现。 写时拷贝是一种可以推迟甚至不用避免拷贝的技术。 更具体来讲，在执行fork语句后，内核并不复制父进程的整个地址空间，而是父子进程共享父进程的地址空间（此时父子进程对于地址空间是只读指令），在父进程或者子进程进行写指令时，子进程才会复制一份地址空间，从而使得父子进程拥有自己的虚拟地址空间，在自己的地址空间进行写操作。也就是说，资源的复制是在需要写入时才会进行，在此之前，只会以只读方式进行共享。 进程的终止 正常退出：当编译器完成了所给定程序的编译之后，编译器会执行一个系统调用告诉操作系统它完成了工作。这个调用在 UNIX 中是 exit ，在 Windows 中是 ExitProcess 错误退出：比如编译时发现文件不存在，这时候应用程序通常会弹出一个对话框告知用户发生了系统错误，是需要重试还是退出 严重错误：通常是由于程序中的错误所导致的。例如，执行了一条非法指令，引用不存在的内存，或者除数是 0 等 被其他进程杀死：某个进程执行系统调用告诉操作系统杀死某个进程。在 UNIX 中，这个系统调用是 kill 查找需要终⽌的进程的 PCB； 如果处于执⾏状态，则⽴即终⽌该进程的执⾏，然后将 CPU 资源分配给其他进程； 如果其还有⼦进程，则应将其所有⼦进程终⽌； 将该进程所拥有的全部资源都归还给⽗进程或操作系统； 将其从 PCB 所在队列中删除； 进程的上下文切换各个进程之间是共享 CPU 资源的，在不同的时候进程之间需要切换，让不同的进程可以在CPU 执⾏，那么这个⼀个进程切换到另⼀个进程运⾏，称为进程的上下⽂切换。 任务是交给 CPU 运⾏的，那么在每个任务运⾏前，CPU 需要知道任务从哪⾥加载，⼜从哪⾥开始运⾏。所以，操作系统需要事先帮 CPU 设置好 CPU 寄存器和程序计数器 CPU 寄存器是 CPU 内部⼀个容量⼩，但是速度极快的内存（缓存）。我举个例⼦，寄存器像是你的⼝袋，内存像你的书包，硬盘则是你家⾥的柜⼦，如果你的东⻄存放到⼝袋，那肯定是⽐你从书包或家⾥柜⼦取出来要快的多。 程序计数器则是⽤来存储 CPU 正在执⾏的指令位置、或者即将执⾏的下⼀条指令位置。 进程是由内核管理和调度的，所以进程的切换只能发⽣在内核态。 所以，进程的上下⽂切换不仅包含了虚拟内存、栈、全局变量等⽤户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。 通常，会把交换的信息保存在进程的 PCB，当要运⾏另外⼀个进程的时候，我们需要从这个进程的 PCB 取出上下⽂，然后恢复到 CPU 中，这使得这个进程可以继续执⾏，如下图所示： 发生线程切换的常见场景： 为了保证所有进程可以得到公平调度，CPU 时间被划分为⼀段段的时间⽚，这些时间⽚再被轮流分配给各个进程。这样，当某个进程的时间⽚耗尽了，进程就从运⾏状态变为就绪状态，系统从就绪队列选择另外⼀个进程运⾏； 进程在系统资源不⾜（⽐如内存不⾜）时，要等到资源满⾜后才可以运⾏，这个时候进程也会被挂起，并由系统调度其他进程运⾏； 当进程通过睡眠函数 sleep 这样的⽅法将⾃⼰主动挂起时，⾃然也会重新调度； 当有优先级更⾼的进程运⾏时，为了保证⾼优先级进程的运⾏，当前进程会被挂起，由⾼优先级进程来运⾏； 发⽣硬件中断时，CPU 上的进程会被中断挂起，转⽽执⾏内核中的中断服务程序； 线程基础知识线程的由来为什么要在进程的基础上再创建一个线程的概念？ 多线程之间会共享同一块地址空间和所有可用数据的能力，这是进程所不具备的 线程要比进程更轻量级，由于线程更轻，所以它比进程更容易创建，也更容易撤销。 第三个原因可能是性能方面的探讨，如果多个线程都是 CPU 密集型的，那么并不能获得性能上的增强，但是如果存在着大量的计算和大量的 I/O 处理，拥有多个线程能在这些活动中彼此重叠进行，从而会加快应用程序的执行速度 线程可以看做进程当中的⼀条执⾏流程。 同⼀个进程内多个线程之间可以共享代码段、数据段、打开的⽂件等资源，但每个线程各⾃都有⼀套独⽴的寄存器和栈，这样可以确保线程的控制流是相对独⽴的。 进程和线程的区别我们编写的代码只是一个存储在硬盘的静态文件，通过编译之后生成二进制的可执行文件，当运行这个可执行文件的时候，会被装载到内存中，接着CPU会执行程序的每一条指令，那么这个运行中的程序就称为进程。 Ⅰ 拥有资源：进程是操作系统资源分配的基本单位，但是线程不拥有资源，线程可以访问隶属进程的资源 Ⅱ 调度：线程是任务调度和执行的基本单位，一个进程中可以有多个线程，它们共享进程资源 在操作系统中能同时运行多个进程（程序）；而在同一个进程（程序）中有多个线程同时执行（通过CPU调度，在每个时间片中只有一个线程执行） 在同一进程中，线程的切换不会引起进程切换，从一个进程中的线程切换到另一个进程中的线程时，会引起进程切换。 QQ 和浏览器是两个进程，浏览器进程里面有很多线程，例如 HTTP 请求线程、事件响应线程、渲染线程等等，线程的并发执行使得在浏览器中点击一个新链接从而发起 HTTP 请求时，浏览器还可以响应用户的其它事件。 Ⅲ 系统开销：由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O 设备等，所付出的开销远大于创建或撤销线程时的开销。类似地，在进行进程切换时，因为线程具有相同的地址空间（虚拟内存共享），这意味着同⼀个进程的线程都具有同⼀个⻚表，那么在切换的时候不需要切换⻚表。⽽对于进程之间的切换，切换的时候要把⻚表给切换掉，⽽⻚表的切换过程开销是⽐较⼤的； Ⅳ 通信方面：线程间可以通过直接读写同一进程中的数据进行通信，但是进程通信需要借助 IPC，需要通过内核。 个人理解：操作系统给进程分配的资源有文件资源、内存资源、CPU资源。而进程把CPU资源更细粒度化的给了该进程下的所有线程，所以线程变成了CPU调度和执行的基本单位。 线程的上下文切换 当两个线程不是属于同⼀个进程，则切换的过程就跟进程上下⽂切换⼀样； 当两个线程是属于同⼀个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据； 线程的三种实现主要有三种实现方式 在用户空间中实现线程； 在内核空间中实现线程； 在用户和内核空间中混合实现线程。 下面我们分开讨论一下 用户线程第一种方法是把整个线程包放在用户空间中，内核对线程一无所知，它不知道线程的存在。所有的这类实现都有同样的通用结构 用户线程的优点： 每个进程都需要有它私有的线程控制块（TCB）列表，⽤来跟踪记录它各个线程状态信息（PC、栈指针、寄存器），TCB 由⽤户级线程库函数来维护，线程的调度都是在用户空间完成，不需要切换到内核，效率很高。 可以允许进程自己实现调度算法：比如JVM中的垃圾收集的线程可以自己实现。 用户线程扩展性比较好，不需要占用内核线程的空间。 用户线程的劣势： 一个进程下面的所有用户进程其实都是运行在一个CPU上面的，无法利用多核优势，也不能由操作系统调度。如果⼀个线程发起了系统阻塞调⽤或者缺页中断⽽阻塞，那进程所包含的⽤户线程都不能执⾏了。 内核线程内核线程是由操作系统管理的，线程对应的 TCB ⾃然是放在操作系统⾥的，这样线程的创建、终⽌和管理都是由操作系统负责。 当某个线程希望创建一个新线程或撤销一个已有线程时，它会进行一个系统调用，这个系统调用通过对线程表的更新来完成线程创建或销毁工作。 内核线程的模型就是所谓的1对1模型，也就是一个用户线程对应一个内核线程。 内核线程的优点： 在⼀个进程当中，如果某个内核线程发起系统调⽤⽽被阻塞，并不会影响其他内核线程的运⾏；所有能够阻塞的调用都会通过系统调用的方式来实现，当一个线程阻塞时，内核可以进行选择，是运行在同一个进程中的另一个线程（如果有就绪线程的话）还是运行一个另一个进程中的线程 内核线程的缺点： 线程的创建、终⽌和切换都是通过系统调⽤的⽅式来进⾏，因此对于系统来说，系统开销⽐较⼤； 混合实现结合用户空间和内核空间的优点，设计人员采用了一种内核级线程的方式，然后将用户级线程与某些或者全部内核线程多路复用起来 在这种模型中，编程人员可以自由控制用户线程和内核线程的数量，具有很大的灵活度。采用这种方法，内核只识别内核级线程，并对其进行调度。其中一些内核级线程会被多个用户级线程多路复用。 进程间通信方式每个进程各自有不同的用户地址空间，任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核，在内核中开辟一块缓冲区，进程1把数据从用户空间拷到内核缓冲区，进程2再从内核缓冲区把数据读走，内核提供的这种机制称为进程间通信（IPC，InterProcess Communication） 管道/命名管道12ps -ef | grep mysql这个Linux命令中的 | 其实就是管道：它的功能是将前⼀个命令（ ps auxf ）的输出，作为后⼀个命令（ grep mysql ）的输⼊ 管道是单向传输的，只能够支持父子进程或兄弟进程之间的通信。 由于普通管道文件没有文件名，所以进程无法使用open函数打开文件，从而得到文件描述符，所以只有一种办法。那就是父进程先调用pipe创建出管道，并得到管道的文件描述符号。然后fork出子进程，让子进程继承父进程打开的文件描述符，父子进程就能通过同一管道，从而实现通信。 管道的实质是一个内核缓冲区，进程以先进先出的方式从缓冲区存取数据，管道一端的进程顺序的将数据写入缓冲区，另一端的进程则顺序的读出数据。 通信的数据是⽆格式的流并且⼤⼩受限 **2.*命名管道 对于命名管道，它可以在不相关的进程间也能相互通信。因为命名管道，提前创建了⼀个类型为管道的设备⽂件，在进程⾥只要使⽤这个设备⽂件，就可以相互通信。 12345678910111213# shengbinbin @ binshow in ~/Documents/test [13:26:34]$ mkfifo myPipe # 创建一个管道# shengbinbin @ binshow in ~/Documents/test [13:26:42]$ ls -l # p表示 pipe管道的意思total 0prw-r--r-- 1 shengbinbin staff 0 10 4 13:26 myPipe# shengbinbin @ binshow in ~/Documents/test [13:26:50]$ echo &quot;hello&quot; &gt; myPipe # 将数据写进管道 ，到这会一直卡住，因为管道中的内容一直没有被读取 # 知道打开另外一个终端， 执行 cat &lt; myPipe // 读取管道⾥的数据才结束 匿名管道的创建： 1234int pipe(int fd[2]) //通过这个系统调用 //这⾥表示创建⼀个匿名管道，并返回了两个描述符， //⼀个是管道的读取端描述符 fd[0] ， //另⼀个是管道的写⼊端描述符 fd[1] 。注意，这个匿名管道是特殊的⽂件，只存在于内存，不存于⽂件系统中。 管道的实质是一个内核缓冲区 这两个描述符都是在⼀个进程⾥⾯，并没有起到进程间通信的作⽤，怎么样才能使得管道是跨过两个进程的呢? 我们可以使⽤ fork 创建⼦进程，创建的⼦进程会复制⽗进程的⽂件描述符，这样就做到了两个进程各有两个「 fd[0] 与 fd[1] 」，两个进程就可以通过各⾃的 fd 写⼊和读取同⼀个管道⽂件实现跨进程通信了。 管道只能⼀端写⼊，另⼀端读出，所以上⾯这种模式容易造成混乱，因为⽗进程和⼦进程都可以同时写⼊，也都可以读出。那么，为了避免这种情况，通常的做法是： ⽗进程关闭读取的 fd[0]，只保留写⼊的 fd[1]； ⼦进程关闭写⼊的 fd[1]，只保留读取的 fd[0]； 但实际在 shell ⾥⾯执⾏ A | B 命令的时候，A 进程和 B 进程都是 shell 创建出来的⼦进程，A 和 B之间不存在⽗⼦关系，它俩的⽗进程都是 shell： 所以说，在 shell ⾥通过「 | 」匿名管道将多个命令连接在⼀起，实际上也就是创建了多个⼦进程，那么在我们编写 shell 脚本时，能使⽤⼀个管道搞定的事情，就不要多⽤⼀个管道，这样可以减少创建⼦进程的系统开销。 对于匿名管道，它的通信范围是存在⽗⼦关系的进程。因为管道没有实体，也就是没有管道⽂件，只能通过 fork 来复制⽗进程 fd ⽂件描述符，来达到通信的⽬的。 对于命名管道，它可以在不相关的进程间也能相互通信。因为命令管道，提前创建了⼀个类型为管道的设备⽂件，在进程⾥只要使⽤这个设备⽂件，就可以相互通信。 不管是匿名管道还是命名管道，进程写⼊的数据都是缓存在内核中，另⼀个进程读取数据时候⾃然也是从内核中获取，同时通信数据都遵循先进先出原则，不⽀持 lseek 之类的⽂件定位操作。 消息队列**消息队列(Message Queuing)**：是存放在内核中的消息链表，每个消息队列由消息队列标识符表示。 消息队列可以解决管道的问题，⽐如，A 进程要给 B 进程发送消息，A 进程把数据放在对应的消息队列后就可以正常返回了，B 进程需要的时候再去读取数据就可以了。同理，B 进程要给 A 进程发送消息也是如此。 消息队列是保存在内核中的消息链表，在发送数据时，会分成⼀个⼀个独⽴的数据单元，也就是消息体（数据块），消息体是⽤户⾃定义的数据类型，消息的发送⽅和接收⽅要约定好消息体的数据类型，所以每个消息体都是固定⼤⼩的存储块，不像管道是⽆格式的字节流数据。如果进程从消息队列中读取了消息体，内核就会把这个消息体删除。 管道是随进程的创建⽽建⽴，随进程的结束⽽销毁，而消息队列⽣命周期随内核，如果没有释放消息队列或者没有关闭操作系统，消息队列会⼀直存在 消息队列的限制： 消息队列不适合⽐较⼤数据的传输,因为消息体有长度的限制。 通信过程中存在⽤户态与内核态之间的数据拷⻉开销 共享内存共享内存是为了解决消息队列中存在的⽤户态与内核态之间的数据拷⻉开销： 共享内存的机制，就是从通信的两个进程中拿出⼀块虚拟地址空间来，映射到相同的物理内存中。这样这个进程写⼊的东⻄，另外⼀个进程⻢上就能看到了，都不需要拷⻉来拷⻉去，传来传去，⼤⼤提⾼了进程间通信的速度。 由于多个进程共享一段内存，可能会有并发安全问题，因此需要依靠某种同步机制（如信号量）来达到进程间的同步及互斥。 信号量共享内存的缺陷就是：如果多个进程同时修改同⼀个共享内存，很有可能就冲突了。 为了防⽌多进程竞争共享资源，⽽造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被⼀个进程访问。正好，信号量就实现了这⼀保护机制。 信号量其实是⼀个整型的计数器，主要⽤于实现进程间的互斥与同步。 信号量表示资源的数量，控制信号量的⽅式有两种原⼦操作： ⼀个是 P 操作，这个操作会把信号量减去 1，相减后如果信号量 &lt; 0，则表明资源已被占⽤，进程需阻塞等待；相减后如果信号量 &gt;= 0，则表明还有资源可使⽤，进程可正常继续执⾏。 另⼀个是 V 操作，这个操作会把信号量加上 1，相加后如果信号量 &lt;= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运⾏；相加后如果信号量 &gt; 0，则表明当前没有阻塞中的进程； P 操作是⽤在进⼊共享资源之前，V 操作是⽤在离开共享资源之后，这两个操作是必须成对出现的。 举例： 两个进程互斥访问共享内存，信号量为1： 具体的过程如下： 进程 A 在访问共享内存前，先执⾏了 P 操作，由于信号量的初始值为 1，故在进程 A 执 ⾏ P 操作后信号量变为 0，表示共享资源可⽤，于是进程 A 就可以访问共享内存。 若此时，进程 B 也想访问共享内存，执⾏了 P 操作，结果信号量变为了 -1，这就意味着临界资源已被占⽤，因此进程 B 被阻塞。 直到进程 A 访问完共享内存，才会执⾏ V 操作，使得信号量恢复为 0，接着就会唤醒阻塞中的线程 B，使得进程 B 可以访问共享内存，最后完成共享内存的访问后，执⾏ V 操作，使信号量恢复到初始值 1。 可以发现，信号初始化为 1 ，就代表着是互斥信号量，它可以保证共享内存在任何时刻只有⼀个进程在访问，这就很好的保护了共享内存。 两个进程同步的方式：例如，进程 A 是负责⽣产数据，⽽进程 B 是负责读取数据，可以初始化信号量为0： 具体过程： 如果进程 B ⽐进程 A 先执⾏了，那么执⾏到 P 操作时，由于信号量初始值为 0，故信号量会变为 -1，表示进程 A 还没⽣产数据，于是进程 B 就阻塞等待； 接着，当进程 A ⽣产完数据后，执⾏了 V 操作，就会使得信号量变为 0，于是就会唤醒阻塞在 P 操作的进程 B； 最后，进程 B 被唤醒后，意味着进程 A 已经⽣产了数据，于是进程 B 就可以正常读取数据了。 可以发现，信号初始化为 0 ，就代表着是同步信号量，它可以保证进程 A 应在进程 B 之前执⾏。 信号信号和信号量完全不一样!!! 在 Linux 操作系统中， 为了响应各种各样的事件，提供了⼏⼗种信号，分别代表不同的意义。我们可以通过 kill -l 命令，查看所有的信号： 123# shengbinbin @ binshow in ~ [13:56:41]$ kill -lHUP INT QUIT ILL TRAP ABRT EMT FPE KILL BUS SEGV SYS PIPE ALRM TERM URG STOP TSTP CONT CHLD TTIN TTOU IO XCPU XFSZ VTALRM PROF WINCH INFO USR1 USR2 我们可以通过键盘输⼊某些组合键的时候，给进程发送信号。例如 Ctrl+C 产⽣ SIGINT 信号，表示终⽌该进程； Ctrl+Z 产⽣ SIGTSTP 信号，表示停⽌该进程，但还未结束； 如果进程在后台运⾏，可以通过 kill 命令的⽅式给进程发送信号，但前提需要知道运⾏中的进程 PID 号，例如： kill -9 1050 ，表示给 PID 为 1050 的进程发送 SIGKILL 信号，⽤来⽴即结束该进程 信号来源 信号是软件层次上对中断机制的一种模拟，是一种异步通信方式，，信号可以在用户空间进程和内核之间直接交互，内核可以利用信号来通知用户空间的进程发生了哪些系统事件，信号事件主要有两个来源： 硬件来源：用户按键输入Ctrl+C退出、硬件异常如无效的存储访问等。 软件终止：终止进程信号、其他进程调用kill函数、软件异常产生信号。 信号是进程间通信机制中唯⼀的异步通信机制，因为可以在任何时候发送信号给某⼀进程，⼀旦有信号产⽣，我们就有下⾯这⼏种，⽤户进程对信号的处理⽅式。 **1.**执⾏默认操作。Linux 对每种信号都规定了默认操作，例如，上⾯列表中的 SIGTERM 信号，就是终⽌进程的意思。 **2.**捕捉信号。我们可以为信号定义⼀个信号处理函数。当信号发⽣时，我们就执⾏相应的信号处理函数。 **3.**忽略信号。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应⽤进程⽆法捕捉和忽略的，即 SIGKILL 和 SEGSTOP ，它们⽤于在任何时候中断或结束某⼀进程。 信号生命周期和处理流程 （1）信号被某个进程产生，并设置此信号传递的对象（一般为对应进程的pid），然后传递给操作系统； （2）操作系统根据接收进程的设置（是否阻塞）而选择性的发送给接收者，如果接收者阻塞该信号（且该信号是可以阻塞的），操作系统将暂时保留该信号，而不传递，直到该进程解除了对此信号的阻塞（如果对应进程已经退出，则丢弃此信号），如果对应进程没有阻塞，操作系统将传递此信号。 （3）目的进程接收到此信号后，将根据当前进程对此信号设置的预处理方式，暂时终止当前代码的执行，保护上下文（主要包括临时寄存器数据，当前程序位置以及当前CPU的状态）、转而执行中断服务程序，执行完成后在回复到中断的位置。当然，对于抢占式内核，在中断返回时还将引发新的调度。 套接字Socket跨⽹络与不同主机上的进程之间通信，就需要 Socket 通信了。 创建一个Socket的系统调用： 12345int socket(int domain, int type, int protocal)//domain 参数⽤来指定协议族，⽐如 AF_INET ⽤于 IPV4、AF_INET6 ⽤于 IPV6、AF_LOCAL/AF_UNIX ⽤于本机； //type 参数⽤来指定通信特性，⽐如 SOCK_STREAM 表示的是字节流，对应 TCP、SOCK_DGRAM 表示的是数据报，对应 UDP、SOCK_RAW 表示的是原始套接字 //protocal:数原本是⽤来指定通信协议的，但现在基本废弃。因为协议已经通过前⾯两个参数指定完成，protocol ⽬前⼀般写成 0 即可 根据创建 socket 类型的不同，通信的⽅式也就不同： 实现 TCP 字节流通信： socket 类型是 AF_INET 和 SOCK_STREAM； 实现 UDP 数据报通信：socket 类型是 AF_INET 和 SOCK_DGRAM； 实现本地进程间通信： 「本地字节流 socket 」类型是 AF_LOCAL 和 SOCK_STREAM，「本地数据报 socket 」类型是 AF_LOCAL 和 SOCK_DGRAM。另外，AF_UNIX 和AF_LOCAL 是等价的，所以 AF_UNIX 也属于本地 socket； TCP的Socket模型 服务端和客户端初始化 socket ，得到⽂件描述符； 服务端调⽤ bind ，将绑定在 IP 地址和端⼝; 服务端调⽤ listen ，进⾏监听； 服务端调⽤ accept ，等待客户端连接； 客户端调⽤ connect ，向服务器端的地址和端⼝发起连接请求； 服务端 accept 返回⽤于传输的 socket 的⽂件描述符； 客户端调⽤ write 写⼊数据；服务端调⽤ read 读取数据； 客户端断开连接时，会调⽤ close ，那么服务端 read 读取数据的时候，就会读取到了EOF ，待处理完数据后，服务端调⽤ close ，表示连接关闭。 所以，监听的 socket 和真正⽤来传送数据的 socket，是「两个」 socket，⼀个叫作监听socket**，⼀个叫作已完成连接 **socket。 成功连接建⽴之后，双⽅开始通过 read 和 write 函数来读写数据，就像往⼀个⽂件流⾥⾯写东⻄⼀样。 UDP的Socket模型 UDP 是没有连接的，所以不需要三次握⼿，也就不需要像 TCP 调⽤ listen 和 connect，但是UDP 的交互仍然需要 IP 地址和端⼝号，因此也需要 bind。 对于 UDP 来说，不需要要维护连接，那么也就没有所谓的发送⽅和接收⽅，甚⾄都不存在客户端和服务端的概念，只要有⼀个 socket 多台机器就可以任意通信，因此每⼀个 UDP 的socket 都需要 bind。 另外，每次通信时，调⽤ sendto 和 recvfrom，都要传⼊⽬标主机的 IP 地址和端⼝。 本地字节流 socket 和 本地数据报 socket 在 bind 的时候，不像 TCP 和 UDP 要绑定 IP 地址和端⼝，⽽是绑定⼀个本地⽂件，这也就是它们之间的最⼤区别。 线程间的通信方式同个进程下的线程之间都是共享进程的资源，只要是共享变量都可以做到线程间通信，⽐如全局变量，所以对于线程间关注的不是通信⽅式，⽽是关注多线程竞争共享资源的问题，信号量也同样可以在线程间实现互斥与同步： 互斥的⽅式，可保证任意时刻只有⼀个线程访问共享资源； 同步的⽅式，可保证线程 A 应在线程 B 之前执⾏； 进程调度算法当 CPU 空闲时，操作系统就选择内存中的某个「就绪状态」的进程，并给其分配 CPU。 当一个计算机是多道程序设计系统时，会频繁的有很多进程或者线程来同时竞争 CPU 时间片。当两个或两个以上的进程/线程处于就绪状态时，就会发生这种情况。如果只有一个 CPU 可用，那么必须选择接下来哪个进程/线程可以运行。操作系统中有一个叫做 调度程序(scheduler) 的角色存在，它就是做这件事儿的，该程序使用的算法叫做 调度算法(scheduling algorithm) 。 尽管有一些不同，但许多适用于进程调度的处理方法同样也适用于线程调度。当内核管理线程的时候，调度通常会以线程级别发生，很少或者根本不会考虑线程属于哪个进程。下面我们会首先专注于进程和线程的调度问题，然后会明确的介绍线程调度以及它产生的问题。 调度原则在所有的情况中，公平是很重要的。对一个进程给予相较于其他等价的进程更多的 CPU 时间片对其他进程来说是不公平的。当然，不同类型的进程可以采用不同的处理方式。 与公平有关的是系统的强制执行，什么意思呢？如果某公司的薪资发放系统计划在本月的15号，那么碰上了疫情大家生活都很拮据，此时老板说要在14号晚上发放薪资，那么调度程序必须强制使进程执行 14 号晚上发放薪资的策略。 另一个共同的目标是保持系统的所有部分尽可能的忙碌。如果 CPU 和所有的 I/O 设备能够一直运行，那么相对于让某些部件空转而言，每秒钟就可以完成更多的工作。例如，在批处理系统中，调度程序控制哪个作业调入内存运行。在内存中既有一些 CPU 密集型进程又有一些 I/O 密集型进程是一个比较好的想法，好于先调入和运行所有的 CPU 密集型作业，然后在它们完成之后再调入和运行所有 I/O 密集型作业的做法。使用后者这种方式会在 CPU 密集型进程启动后，争夺 CPU ，而磁盘却在空转，而当 I/O 密集型进程启动后，它们又要为磁盘而竞争，CPU 却又在空转。。。。。。显然，通过结合 I/O 密集型和 CPU 密集型，能够使整个系统运行更流畅，效率更高。 先来先服务调度算法每次从就绪队列选择最先进⼊队列的进程，然后⼀直运⾏，直到进程退出或被阻塞，才会继续从队列中选择第⼀个进程接着运⾏。 对长作业有利，适用于CPU密集型的系统。 最短作业优先调度算法它会优先选择运⾏时间最短的进程来运⾏，这有助于提⾼系统的吞吐量。 ⾼响应⽐优先调度算法⾼响应⽐优先（Highest Response Ratio Next, HRRN）调度算法主要是权衡了短作业和⻓作业。 时间⽚轮转调度算法每个进程被分配⼀个时间段，称为时间⽚（Quantum），即允许该进程在该时间段中运⾏。如果时间⽚⽤完，进程还在运⾏，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外⼀个进程；如果该进程在时间⽚结束前阻塞或结束，则 CPU ⽴即进⾏切换； 最⾼优先级调度算法但是，对于多⽤户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度程序能从就绪队列中选择最⾼优先级的进程进⾏运⾏，这称为最⾼优先级（Highest Priority First，HPF）调度算法。 多级反馈队列调度算法多级反馈队列（Multilevel Feedback Queue）调度算法是「时间⽚轮转算法」和「最⾼优先级算法」的综合和发展。 「多级」表示有多个队列，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短。 「反馈」表示如果有新的进程加⼊优先级⾼的队列时，⽴刻停⽌当前正在运⾏的进程，转⽽去运⾏优先级⾼的队列； 工作原理： 设置了多个队列，赋予每个队列不同的优先级，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短； 新的进程会被放⼊到第⼀级队列的末尾，按先来先服务的原则排队等待被调度，如果在第⼀级队列规定的时间⽚没运⾏完成，则将其转⼊到第⼆级队列的末尾，以此类推，直⾄完成； 当较⾼优先级的队列为空，才调度较低优先级的队列中的进程运⾏。如果进程运⾏时，有新进程进⼊较⾼优先级的队列，则停⽌当前运⾏的进程并将其移⼊到原队列末尾，接着让较⾼优先级的进程运⾏； 可以发现，对于短作业可能可以在第⼀级队列很快被处理完。对于⻓作业，如果在第⼀级队列处理不完，可以移⼊下次队列等待被执⾏，虽然等待的时间变⻓了，但是运⾏时间也会更⻓了，所以该算法很好的兼顾了⻓短作业，同时有较好的响应时间。 多线程同步 时分复用： 前面说过：在单核CPU的时候，为了实现多个程序同时运⾏的假象，操作系统通常以时间⽚调度的⽅式，让每个进程执⾏每次执⾏⼀个时间⽚，时间⽚⽤完了，就切换下⼀个进程运⾏，由于这个时间⽚的时间很短，于是就造成了「并发」的现象。 空分复用： 如果⼀个程序只有⼀个执⾏流程，也代表它是单线程的。当然⼀个程序可以有多个执⾏流程，也就是所谓的多线程程序，线程是调度的基本单位，进程则是资源分配的基本单位。线程之间是可以共享进程之间的资源的，比如代码段、堆空间、数据段等，每个线程也有自己独立的栈空间。 这样的话就会造成问题：多个线程竞争共享的资源，就会造成并发安全的问题。 互斥当多个线程操作共享变量的代码可以会导致竞争状态，这段代码称为临界区。 互斥指的就是 保证一个线程在临界区执行的时候其他的线程会被阻止进入。 同步互斥解决的是并发线程/进程对临界区的访问问题。 而在多线程中，每个线程并不一定是顺序执行的，它们基本都是各自独立的以不可预知的速度往前推进，这个时候我们又出现了新的需求：就是希望多个线程能密切合作来完成一个共同的任务。 而同步的概念就是为了解决这个问题，在一些并发进程/线程的关键点上进行相互等待和消息互通。 总结一下： 同步就好⽐：「操作 A 应在操作 B 之前执⾏」，「操作 C 必须在操作 A 和操作 B 都完成之后才能执⾏」等； 互斥就好⽐：「操作 A 和操作 B 不能在同⼀时刻执⾏」； 实现和使用主要有两种： 加锁：可以实现互斥。 信号量：PV操作，操作系统执行的时候已经是具有原子性的。 信号量通常表示资源的数量，一个整型数值： 通过操作系统的两个系统调用函数来控制信号量： P 操作：将 sem 减 1 ，相减后，如果 sem &lt; 0 ，则进程/线程进⼊阻塞等待，否则继续，表明 P 操作可能会阻塞； V 操作：将 sem 加 1 ，相加后，如果 sem &lt;= 0 ，唤醒⼀个等待中的进程/线程，表明V 操作不会阻塞； 操作系统实现PV操作 信号量如何实现互斥为每类共享资源设置⼀个信号量 s ，其初值为 1 ，表示该临界资源未被占⽤。 只要把进⼊临界区的操作置于 P(s) 和 V(s) 之间，即可实现进程/线程互斥。 此时，任何想进⼊临界区的线程，必先在互斥信号量上执⾏ P 操作，在完成对临界资源的访 问后再执⾏ V 操作。由于互斥信号量的初始值为 1，故在第⼀个线程执⾏ P 操作后 s 值变为 0，表示临界资源为空闲，可分配给该线程，使之进⼊临界区。 若此时⼜有第⼆个线程想进⼊临界区，也应先执⾏ P 操作，结果使 s 变为负值，这就意味着 临界资源已被占⽤，因此，第⼆个线程被阻塞。 并且，直到第⼀个线程执⾏ V 操作，释放临界资源⽽恢复 s 值为 0 后，才唤醒第⼆个线程， 使之进⼊临界区，待它完成临界资源的访问后，⼜执⾏ V 操作，使 s 恢复到初始值 1。 信号量如何实现同步同步的⽅式是设置⼀个信号量，其初值为 0 。 经典同步问题哲学家就餐问题 5 个⽼⼤哥哲学家，闲着没事做，围绕着⼀张圆桌吃⾯； 巧就巧在，这个桌⼦只有 5 ⽀叉⼦，每两个哲学家之间放⼀⽀叉⼦； 哲学家围在⼀起先思考，思考中途饿了就会想进餐； 奇葩的是，这些哲学家要两⽀叉⼦才愿意吃⾯，也就是需要拿到左右两边的叉⼦才进餐； 吃完后，会把两⽀叉⼦放回原处，继续思考； 如何保证他们的动作时有序的进程，不会出现有人永远拿不到叉子呢？ 信号量的方式： 不过，这种解法存在⼀个极端的问题：假设五位哲学家同时拿起左边的叉⼦，桌⾯上就没有叉⼦了， 这样就没有⼈能够拿到他们右边的叉⼦，也就说每⼀位哲学家都会在 P(fork[(i + 1) % P(fork[(i + 1) %N ]) N ]) 这条语句阻塞了，很明显这发⽣了死锁的现象。 方案二：在拿叉子前加上一个互斥信号量： 这样问题虽然解决了，但是同一时间只有一个哲学家可以吃饭，效率比较低。 方案三：解决方案一的问题：根据哲学家的编号的不同，⽽采取不同的动作 即让偶数编号的哲学家「先拿左边的叉⼦后拿右边的叉⼦」，奇数编号的哲学家「先拿右边的叉⼦后拿左边的叉⼦」 上⾯的程序，在 P 操作时，根据哲学家的编号不同，拿起左右两边叉⼦的顺序不同。另外，V操作是不需要分⽀的，因为 V 操作是不会阻塞的。 读者-写者问题死锁问题死锁概念当两个线程为了保护两个不同的共享资源⽽使⽤了两个互斥锁，那么这两个互斥锁应⽤不当的时候，可能会造成两个线程都在等待对⽅释放锁，在没有外⼒的作⽤下，这些线程会⼀直相互等待，就没办法继续运⾏，这种情况就是发⽣了死锁。 比如线程A已经获得了资源1，此时线程B已经获得了资源2。 而线程A又想要去获得资源2，线程B想要获得资源1，这样就会形成死锁~如下代码所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class DeadLockTest { public static String obj1 = &quot;obj1&quot;; public static String obj2 = &quot;obj2&quot;; public static void main(String[] args) { LockA la = new LockA(); new Thread(la).start(); LockB lb = new LockB(); new Thread(lb).start(); }}class LockA implements Runnable{ public void run() { try { System.out.println(new Date().toString() + &quot; LockA 开始执行&quot;); while(true){ synchronized (DeadLockTest.obj1) { System.out.println(new Date().toString() + &quot; LockA 锁住 obj1&quot;); Thread.sleep(3000); // 此处等待是给B能锁住机会 synchronized (DeadLockTest.obj2) { System.out.println(new Date().toString() + &quot; LockA 锁住 obj2&quot;); Thread.sleep(60 * 1000); // 为测试，占用了就不放 } } } } catch (Exception e) { e.printStackTrace(); } }}class LockB implements Runnable{ public void run() { try { System.out.println(new Date().toString() + &quot; LockB 开始执行&quot;); while(true){ synchronized (DeadLockTest.obj2) { System.out.println(new Date().toString() + &quot; LockB 锁住 obj2&quot;); Thread.sleep(3000); // 此处等待是给A能锁住机会 synchronized (DeadLockTest.obj1) { System.out.println(new Date().toString() + &quot; LockB 锁住 obj1&quot;); Thread.sleep(60 * 1000); // 为测试，占用了就不放 } } } } catch (Exception e) { e.printStackTrace(); } }} 死锁发生的4个条件 互斥：多个线程不能同时使用同一个资源。 占用并等待：线程A持有资源1之后又想去申请资源2，发现资源2被其他线程拿走了，需要等待。在等待的过程中不会释放自己已经持有的资源。 不可剥夺：线程持有资源后在使用完之前不能被其他线程获取。 环路等待：在死锁发生的时候，两个线程获取资源的顺序构成了环形链。 死锁如何预防破坏上面的4个条件其中一个即可~ 如何排查死锁todo","link":"/2021/10/06/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BA%8C%EF%BC%89%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/"},{"title":"消息队列基础问题","text":"消息队列基础篇，主要描述了四个问题： 为什么要用消息队列 如何保证消息不丢失 如何保证消息不重复消费 如何处理消息的积压 消息队列的使用场景异步处理我们以常见的电商秒杀场景为例，秒杀系统需要解决的核心问题是，如何利用有限的服务器资源，尽可能多地处理短时间内的海量请求。 一个秒杀通常包含了一下几个步骤： 风险控制 库存锁定 生成订单 短信通知 更新统计数据等等。。 如果没有任何优化处理的话，正常的处理流程是：App 将请求发送给网关，依次调用上述 5 个流程，然后将结果返回给 APP。但实际上，用户能否秒杀成功主要是看风险控制和库存锁定两步，也就是说当服务端完成前面 2 个步骤，确定本次请求的秒杀结果后，就可以马上给用户返回响应，剩余的步骤都可以放入消息队列中，异步的进行后续的处理。 这样处理之后，在秒杀期间我们可以用更多的服务器资源来处理秒杀请求，秒杀结束后再异步的处理后续的步骤。 在这个场景中，消息队列被用于实现服务的异步处理。这样做的好处是： 可以更快地返回结果； 减少等待，自然实现了步骤之间的并发，提升系统总体的性能。 流量削峰还是以秒杀场景为例，我们已经通过部分异步处理提高了用户的响应效率，下一个问题是如何避免瞬间过多的请求压垮我们的秒杀系统？ 一个设计健壮的程序有自我保护的能力，也就是说，它应该可以在海量的请求下，还能在自身能力范围内尽可能多地处理请求，拒绝处理不了的请求并且保证自身运行正常。不幸的是，现实中很多程序并没有那么“健壮”，而直接拒绝请求返回错误对于用户来说也是不怎么好的体验。 使用消息队列隔离网关和后端服务，以达到流量控制和保护后端服务的目的。 加入消息队列后，整个秒杀流程变为： 网关在收到请求后，将请求放入请求消息队列； 后端服务从请求消息队列中获取 APP 请求，完成后续秒杀处理过程，然后返回结果。 秒杀开始后，当短时间内大量的秒杀请求到达网关时，不会直接冲击到后端的秒杀服务，而是先堆积在消息队列中，后端服务按照自己的最大处理能力，从消息队列中消费请求进行处理。 对于超时的请求可以直接丢弃，APP 将超时无响应的请求处理为秒杀失败即可。运维人员还可以随时增加秒杀服务的实例数量进行水平扩容，而不用对系统的其他部分做任何更改。 这种设计的优点是：能根据下游的处理能力自动调节流量，达到“削峰填谷”的作用。但这样做同样是有代价的： 增加了系统调用链环节，导致总体的响应时延变长。 上下游系统都要将同步调用改为异步消息，增加了系统的复杂度。 消息队列实现令牌桶进行限流令牌桶控制流量的原理是：单位时间内只发放固定数量的令牌到令牌桶中，规定服务在处理请求之前必须先从令牌桶中拿出一个令牌，如果令牌桶中没有令牌，则拒绝请求。这样就保证单位时间内，能处理的请求不超过发放令牌的数量，起到了流量控制的作用。 网关在处理 APP 请求时增加一个获取令牌的逻辑。 令牌桶可以简单地用一个有固定容量的消息队列加一个“令牌发生器”来实现：令牌发生器按照预估的处理能力，匀速生产令牌并放入令牌队列（如果队列满了则丢弃令牌），网关在收到请求时去令牌队列消费一个令牌，获取到令牌则继续调用后端秒杀服务，如果获取不到令牌则直接返回秒杀失败。 服务解耦订单信息是电商系统中比较核心的数据，当一个新订单创建时： 支付系统需要发起支付流程； 风控系统需要审核订单的合法性； 客服系统需要给用户发短信告知用户； 经营分析系统需要更新统计数据； 这些订单下游的系统都需要实时获得订单数据。随着业务不断发展，这些订单下游系统不断的增加，不断变化，并且每个系统可能只需要订单数据的一个子集，负责订单服务的开发团队不得不花费很大的精力，应对不断增加变化的下游系统，不停地修改调试订单系统与这些下游系统的接口。任何一个下游系统接口变更，都需要订单模块重新进行一次上线，对于一个电商的核心服务来说，这几乎是不可接受的。 所有的电商都选择用消息队列来解决类似的系统耦合过于紧密的问题。引入消息队列后，订单服务在订单变化时发送一条消息到消息队列的一个主题 Order 中，所有下游系统都订阅主题 Order，这样每个下游系统都可以获得一份实时完整的订单数据。 如何保证消息不丢失如何检测消息是否丢失 如果是 IT 基础设施比较完善的公司，一般都有分布式链路追踪系统，使用类似的追踪系统可以很方便地追踪每一条消息。 如果没有这样的追踪系统，这里我提供一个比较简单的方法，来检查是否有消息丢失的情况。我们可以利用消息队列的有序性来验证是否有消息丢失。原理非常简单，在 Producer 端，我们给每个发出的消息附加一个连续递增的序号，然后在 Consumer 端来检查这个序号的连续性。如果没有消息丢失，Consumer 收到消息的序号必然是连续递增的，或者说收到的消息，其中的序号必然是上一条消息的序号 +1。如果检测到序号不连续，那就是丢消息了。还可以通过缺失的序号来确定丢失的是哪条消息，方便进一步排查原因。 大多数消息队列的客户端都支持拦截器机制，你可以利用这个拦截器机制，在 Producer 发送消息之前的拦截器中将序号注入到消息中，在 Consumer 收到消息的拦截器中检测序号的连续性，这样实现的好处是消息检测的代码不会侵入到你的业务代码中，待你的系统稳定后，也方便将这部分检测的逻辑关闭或者删除。 分布式系统中需要注意的点 首先，像 Kafka 和 RocketMQ 这样的消息队列，它是不保证在 Topic 上的严格顺序的，只能保证分区上的消息是有序的，所以我们在发消息的时候必须要指定分区，并且，在每个分区单独检测消息序号的连续性。 如果你的系统中 Producer 是多实例的，由于并不好协调多个 Producer 之间的发送顺序，所以也需要每个 Producer 分别生成各自的消息序号，并且需要附加上 Producer 的标识，在 Consumer 端按照每个 Producer 分别来检测序号的连续性。 Consumer 实例的数量最好和分区数量一致，做到 Consumer 和分区一一对应，这样会比较方便地在 Consumer 内检测消息序号的连续性。 如何确保消息可靠传递 生产阶段: 在这个阶段，从消息在 Producer 创建出来，经过网络传输发送到 Broker 端。 存储阶段: 在这个阶段，消息在 Broker 端存储，如果是集群，消息会在这个阶段被复制到其他的副本上。 消费阶段: 在这个阶段，Consumer 从 Broker 上拉取消息，经过网络传输发送到 Consumer 上。 1. 生产阶段： 在生产阶段，消息队列通过最常用的请求确认机制，来保证消息的可靠传递：当你的代码调用发消息方法时，消息队列的客户端会把消息发送到 Broker，Broker 收到消息后（最好写入到磁盘中才返回确认消息），会给客户端返回一个确认响应，表明消息已经收到了。客户端收到响应后，完成了一次正常消息的发送。 12345678你在编写发送消息代码时，需要注意，正确处理返回值或者捕获异常，就可以保证这个阶段的消息不会丢失。 try { RecordMetadata metadata = producer.send(record).get(); System.out.println(&quot; 消息发送成功。&quot;);} catch (Throwable e) { System.out.println(&quot; 消息发送失败！&quot;); System.out.println(e);} 异步发送时，则需要在回调方法里进行检查。这个地方是需要特别注意的，很多丢消息的原因就是，我们使用了异步发送，却没有在回调中检查发送结果。 12345678producer.send(record, (metadata, exception) -&gt; { if (metadata != null) { System.out.println(&quot; 消息发送成功。&quot;); } else { System.out.println(&quot; 消息发送失败！&quot;); System.out.println(exception); }}); 2.存储阶段： 在存储阶段正常情况下，只要 Broker 在正常运行，就不会出现丢失消息的问题，但是如果 Broker 出现了故障，比如进程死掉了或者服务器宕机了，还是可能会丢失消息的。 对于单个节点的 Broker，需要配置 Broker 参数，在收到消息后，将消息写入磁盘后再给 Producer 返回确认响应，这样即使发生宕机，由于消息已经被写入磁盘，就不会丢失消息，恢复后还可以继续消费。例如，在 RocketMQ 中，需要将刷盘方式 flushDiskType 配置为 SYNC_FLUSH 同步刷盘。 如果是 Broker 是由多个节点组成的集群，需要将 Broker 集群配置成：至少将消息发送到 2 个以上的节点，再给客户端回复发送确认响应。这样当某个 Broker 宕机时，其他的 Broker 可以替代宕机的 Broker，也不会发生消息丢失。 3.消费阶段 消费阶段采用和生产阶段类似的确认机制来保证消息的可靠传递，客户端从 Broker 拉取消息后，执行用户的消费业务逻辑，成功后，才会给 Broker 发送消费确认响应。如果 Broker 没有收到消费确认响应，下次拉消息的时候还会返回同一条消息，确保消息不会在网络传输过程中丢失，也不会因为客户端在执行消费逻辑中出错导致丢失。 你在写消费代码时需要注意的是，不要在收到消息后就立即发送消费确认，而是应该在执行完所有消费业务逻辑之后，再发送消费确认。 如何保证消费过程中的重复消息（幂等性）在消息传递过程中，如果出现传递失败的情况，发送方会执行重试，重试的过程中就有可能会产生重复的消息。对使用消息队列的业务系统来说，如果没有对重复消息进行处理，就有可能会导致系统的数据出现错误 消息重复的情况必然存在在 MQTT 协议中，给出了三种传递消息时能够提供的服务质量标准，这三种服务质量从低到高依次是： At most once: 至多一次。消息在传递时，最多会被送达一次。换一个说法就是，没什么消息可靠性保证，允许丢消息。一般都是一些对消息可靠性要求不太高的监控场景使用，比如每分钟上报一次机房温度数据，可以接受数据少量丢失。 At least once: 至少一次。消息在传递时，至少会被送达一次。也就是说，不允许丢消息，但是允许有少量重复消息出现。 Exactly once：恰好一次。消息在传递时，只会被送达一次，不允许丢失也不允许重复，这个是最高的等级。 这个服务质量标准不仅适用于 MQTT，对所有的消息队列都是适用的。我们现在常用的绝大部分消息队列提供的服务质量都是 At least once，包括 RocketMQ、RabbitMQ 和 Kafka 都是这样。也就是说，消息队列很难保证消息不重复。 用幂等性解决重复消息问题一个幂等操作的特点是，其任意多次执行所产生的影响均与一次执行的影响相同。 利用数据库的唯一约束实现幂等例如我们刚刚提到的那个不具备幂等特性的转账的例子：将账户 X 的余额加 100 元。在这个例子中，我们可以通过改造业务逻辑，让它具备幂等性。 首先，我们可以限定，对于每个转账单每个账户只可以执行一次变更操作，在分布式系统中，这个限制实现的方法非常多，最简单的是我们在数据库中建一张转账流水表，这个表有三个字段：转账单 ID、账户 ID 和变更金额，然后给转账单 ID 和账户 ID 这两个字段联合起来创建一个唯一约束，这样对于相同的转账单 ID 和账户 ID，表里至多只能存在一条记录。 基于这个思路，不光是可以使用关系型数据库，只要是支持类似“INSERT IF NOT EXIST”语义的存储类系统都可以用于实现幂等，比如，你可以用 Redis 的 SETNX 命令来替代数据库中的唯一约束，来实现幂等消费。 为更新的数据设置前置条件给数据变更设置一个前置条件，如果满足条件就更新数据，否则拒绝更新数据，在更新数据的时候，同时变更前置条件中需要判断的数据。这样，重复执行这个操作时，由于第一次更新数据的时候已经变更了前置条件中需要判断的数据，不满足前置条件，则不会重复执行更新数据操作。 比如，刚刚我们说过，“将账户 X 的余额增加 100 元”这个操作并不满足幂等性，我们可以把这个操作加上一个前置条件，变为：“如果账户 X 当前的余额为 500 元，将余额加 100 元”，这个操作就具备了幂等性。对应到消息队列中的使用时，可以在发消息时在消息体中带上当前的余额，在消费的时候进行判断数据库中，当前余额是否与消息中的余额相等，只有相等才执行变更操作。 更加通用的方法是，给你的数据增加一个版本号属性，每次更数据前，比较当前数据的版本号是否和消息中的版本号一致，如果不一致就拒绝更新数据，更新数据的同时将版本号 +1，一样可以实现幂等更新 记录并检查操作我们还有一种通用性最强，适用范围最广的实现幂等性方法：记录并检查操作，也称为“Token 机制或者 GUID（全局唯一 ID）机制”，实现的思路特别简单：在执行数据更新操作之前，先检查一下是否执行过这个更新操作。 具体的实现方法是，在发送消息时，给每条消息指定一个全局唯一的 ID，消费时，先根据这个 ID 检查这条消息是否有被消费过，如果没有消费过，才更新数据，然后将消费状态置为已消费。 更加麻烦的是，在“检查消费状态，然后更新数据并且设置消费状态”中，三个操作必须作为一组操作保证原子性，才能真正实现幂等，否则就会出现 Bug 比如说，对于同一条消息：“全局 ID 为 8，操作为：给 ID 为 666 账户增加 100 元”，有可能出现这样的情况： t0 时刻：Consumer A 收到条消息，检查消息执行状态，发现消息未处理过，开始执行“账户增加 100 元”； t1 时刻：Consumer B 收到条消息，检查消息执行状态，发现消息未处理过，因为这个时刻，Consumer A 还未来得及更新消息执行状态。 这样就会导致账户被错误地增加了两次 100 元，这是一个在分布式系统中非常容易犯的错误，一定要引以为戒。 对于这个问题，当然我们可以用事务来实现，也可以用锁来实现，但是在分布式系统中，无论是分布式事务还是分布式锁都是比较难解决问题。 消息积压了该怎么办优化性能来避免消息积压发送端性能优化优化消息收发性能，预防消息积压的方法有两种，增加批量或者是增加并发，在发送端这两种方法都可以使用。 Producer 发送消息的过程，Producer 发消息给 Broker，Broker 收到消息后返回确认响应，这是一次完整的交互。假设这一次交互的平均时延是 1ms，我们把这 1ms 的时间分解开，它包括了下面这些步骤的耗时： 发送端准备数据、序列化消息、构造请求等逻辑的时间，也就是发送端在发送网络请求之前的耗时； 发送消息和返回响应在网络传输中的耗时； Broker 处理消息的时延 无论是增加每次发送消息的批量大小，还是增加并发，都能成倍地提升发送性能 比如说，你的消息发送端是一个微服务，主要接受 RPC 请求处理在线业务。很自然的，微服务在处理每次请求的时候，就在当前线程直接发送消息就可以了，因为所有 RPC 框架都是多线程支持多并发的，自然也就实现了并行发送消息。并且在线业务比较在意的是请求响应时延，选择批量发送必然会影响 RPC 服务的时延。这种情况，比较明智的方式就是通过并发来提升发送性能。 如果你的系统是一个离线分析系统，离线系统在性能上的需求是什么呢？它不关心时延，更注重整个系统的吞吐量。发送端的数据都是来自于数据库，这种情况就更适合批量发送，你可以批量从数据库读取数据，然后批量来发送消息，同样用少量的并发就可以获得非常高的吞吐量。 消费端性能优化使用消息队列的时候，大部分的性能问题都出现在消费端，如果消费的速度跟不上发送端生产消息的速度，就会造成消息积压。如果这种性能倒挂的问题只是暂时的，那问题不大，只要消费端的性能恢复之后，超过发送端的性能，那积压的消息是可以逐渐被消化掉的。 我们在设计系统的时候，一定要保证消费端的消费性能要高于生产端的发送性能，这样的系统才能健康的持续运行。 消费端的性能优化除了优化消费业务逻辑以外，也可以通过水平扩容，增加消费端的并发数来提升总体的消费性能。特别需要注意的一点是，在扩容 Consumer 的实例数量的同时，必须同步扩容主题中的分区（也叫队列）数量，确保 Consumer 的实例数和分区数量是相等的。如果 Consumer 的实例数量超过分区数量，这样的扩容实际上是没有效果的。原因我们之前讲过，因为对于消费者来说，在每个分区上实际上只能支持单线程消费 消息积压了该如何处理？能导致积压突然增加，最粗粒度的原因，只有两种：要么是发送变快了，要么是消费变慢了。 大部分消息队列都内置了监控的功能，只要通过监控数据，很容易确定是哪种原因。如果是单位时间发送的消息增多，比如说是赶上大促或者抢购，短时间内不太可能优化消费端的代码来提升消费性能，唯一的方法是通过扩容消费端的实例数来提升总体的消费能力。 如果短时间内没有足够的服务器资源进行扩容，没办法的办法是，将系统降级，通过关闭一些不重要的业务，减少发送方发送的数据量，最低限度让系统还能正常运转，服务一些重要业务。 还有一种不太常见的情况，你通过监控发现，无论是发送消息的速度还是消费消息的速度和原来都没什么变化，这时候你需要检查一下你的消费端，是不是消费失败导致的一条消息反复消费这种情况比较多，这种情况也会拖慢整个系统的消费速度。 总结","link":"/2021/05/11/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%9F%BA%E7%A1%80%E9%97%AE%E9%A2%98/"},{"title":"算法题（一）链表","text":"链表专题 反转链表问题206. 反转链表123456789101112131415class Solution { public ListNode reverseList(ListNode head) { if(head == null) return head; ListNode pre = null; ListNode cur = head; while(cur != null){ ListNode help = cur.next; // pre cur help cur.next = pre; pre = cur; cur = help; } return pre; }} 25. K 个一组翻转链表123456789101112131415161718192021222324252627282930class Solution { public ListNode reverseKGroup(ListNode head, int k) { if(head == null || head.next == null) return head; ListNode cur = head; // 先找到 cur for(int i = 0 ; i &lt; k ; i++){ if(cur == null) break; cur = cur.next; } // cur就是第一个翻转的尾节点的 后面一个节点 1234以3为单位翻转 ，cur 此时就是4 ListNode newHead = reverse(head , cur); head.next = reverseKGroup(cur , k); // 继续递归翻转后续的节点 return newHead; } ListNode reverse(ListNode head , ListNode tail){ ListNode pre = null; ListNode cur = head; while(cur != tail){ // pre cur help ListNode help = cur.next; cur.next = pre; pre = cur; cur = help; } return pre; }} 92. 反转链表 II12345678910111213141516171819202122232425262728293031323334class Solution { public ListNode reverseBetween(ListNode head, int l, int r) { if(head == null) return head; ListNode dummy = new ListNode(-1); dummy.next = head; ListNode tem = dummy; for(int i = 1 ; i &lt; l ; i++) tem = tem.next; // 待翻转的头指针 ListNode cur = tem.next; ListNode tail = tem.next; //翻转之后的尾节点，用tail指针保存起来 //开始翻转 ListNode pre = null; for(int i = l ; i &lt;= r ; i++){ // pre cur help ListNode help = cur.next; cur.next = pre; pre = cur; cur = help; } //翻转之后的头指针为 pre ， 尾指针为cur // 12 345 6 翻转完之后 tem = 2 , pre = 5 , cur = 6 ,tail = 3 // 3 指向 6 ， 2 指向 5 tail.next = cur; tem.next = pre; return dummy.next; }} 24. 两两交换链表中的节点123456789101112131415161718192021222324252627282930class Solution { public ListNode swapPairs(ListNode head) { //1. 数据校验 if(head == null || head.next == null) return head; //2. 定义一个虚拟头结点，使其next指针指向head ListNode dummy = new ListNode(0); dummy.next = head; //3. 开始翻转 ListNode cur = dummy; //因为要保证后面还有2个结点可以交换，所以后面两个结点不为空 while(cur.next != null &amp;&amp; cur.next.next != null){ //定义两个指针保存位置 ListNode a = cur.next; ListNode b = cur.next.next; //开始交换 // cur a b ListNode c = b.next; cur.next = b; b.next = a; a.next = c; cur = a; } return dummy.next; }} 234. 回文链表123456789101112131415161718192021222324252627282930313233343536373839class Solution { public boolean isPalindrome(ListNode head) { if(head == null || head.next == null) return true; //快慢指针找中点 ListNode fast = head; ListNode slow = head; while(fast.next != null &amp;&amp; fast.next.next != null){ fast = fast.next.next; slow = slow.next; } //slow 为中点 ListNode a = slow.next; slow.next = null; ListNode newHead = reverse(a); ListNode cur = head; while(cur != null &amp;&amp; newHead != null){ if(cur.val != newHead.val) return false; cur =cur.next; newHead = newHead.next; } return true; } ListNode reverse(ListNode head){ ListNode pre = null; ListNode cur = head; while(cur != null){ ListNode help = cur.next; cur.next = pre; pre = cur; cur = help; } return pre; }} 143. 重排链表12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution { public void reorderList(ListNode head) { if(head == null) return; //1. 先找到中间的节点 ListNode fast = head; ListNode slow = head; while(fast.next != null &amp;&amp; fast.next.next != null){ fast = fast.next.next; slow = slow.next; } //2. 将后半个链表翻转 ListNode b = reverse(slow.next); slow.next = null; //3. 合并两个链表 ListNode a = head; while(a != null &amp;&amp; b != null){ ListNode c = a.next; ListNode d = b.next; // a c // b d a.next = b; b.next = c; a = c; b = d; } } ListNode reverse(ListNode head){ ListNode pre = null; ListNode cur = head; while(cur != null){ // pre cur help ListNode help = cur.next; cur.next = pre; pre = cur; cur = help; } return pre; }} 328. 奇偶链表123456789101112131415161718192021222324class Solution { public ListNode oddEvenList(ListNode head) { if(head == null || head.next == null) return head; ListNode o1 = head; ListNode e1 = head.next; ListNode a = head; ListNode b = head.next; while(a.next != null &amp;&amp; b.next != null){ // 注意这个while条件 ListNode c = b.next; ListNode d = c.next; // a b c d a.next = c; b.next = d; a = c; b = d; } a.next = e1; return head; }} 160. 相交链表123456789101112public class Solution { public ListNode getIntersectionNode(ListNode headA, ListNode headB) { if(headA == null || headB == null) return headA == null ? headB : headA; ListNode l1 = headA , l2 = headB; while(l1 != l2){ l1 = l1==null ? headB : l1.next; l2 = l2==null ? headA : l2.next; } return l1; }} 61. 旋转链表12345678910111213141516171819202122232425class Solution { public ListNode rotateRight(ListNode head, int k) { if(head == null || k == 0) return head; //1. 求出链表长度 int len = 1; ListNode cur = head; while(cur.next != null){ cur = cur.next; len++; } //2. 首尾相连，并找出x cur.next = head; k = k % len; int x = len - k; //3. 找到第x个节点，后面那个就是要返回的头结点 while(x &gt; 0){ cur = cur.next; x--; } ListNode res = cur.next; cur.next = null; return res; }} 环形链表问题141. 环形链表1234567891011121314public class Solution { public boolean hasCycle(ListNode head) { if(head == null || head.next == null) return false; ListNode fast = head; ListNode slow = head; while(fast.next != null &amp;&amp; fast.next.next != null){ fast = fast.next.next; slow = slow.next; if(fast == slow) return true; } return false; }} 142. 环形链表 II123456789101112131415161718192021public class Solution { public ListNode detectCycle(ListNode head) { if(head == null) return head; ListNode fast = head; ListNode slow = head; while(fast.next != null &amp;&amp; fast.next.next != null){ fast = fast.next.next; slow = slow.next; if(fast == slow){ fast = head; while(fast != slow){ fast = fast.next; slow = slow.next; } return fast; } } return null; }} 合并排序链表1234567891011121314151617181920212223class Solution { public ListNode mergeTwoLists(ListNode l1, ListNode l2) { if(l1 == null || l2 == null) return l1 == null ? l2 : l1; ListNode dummy = new ListNode(-1); ListNode cur = dummy; while(l1 != null &amp;&amp; l2 != null){ if(l1.val &lt; l2.val){ cur.next = l1; cur = cur.next; l1 = l1.next; }else{ cur.next = l2; cur = cur.next; l2 = l2.next; } } if(l1 == null) cur.next = l2; if(l2 == null) cur.next = l1; return dummy.next; }} 23. 合并K个升序链表123456789101112131415161718192021class Solution { public ListNode mergeKLists(ListNode[] lists) { //1. 建立小根堆 Queue&lt;ListNode&gt; heap = new PriorityQueue&lt;&gt;((a , b) -&gt; (a.val - b.val)); //2. 将所有头结点放入堆中，这样堆顶就是最小的结点 for(ListNode node : lists){ if(node != null) heap.add(node); } //3. 建立虚拟结点 ListNode dummy = new ListNode(-1); ListNode cur = dummy; //4. 一直弹出堆顶的结点，放入cur的后面，并将弹出的结点的下一个结点再压入堆中 while(!heap.isEmpty()){ ListNode minNode = heap.poll(); cur.next = minNode; cur = cur.next; if(minNode.next != null) heap.add(minNode.next); } return dummy.next; }} 快慢指针找倒数第n个节点剑指 Offer 22. 链表中倒数第k个节点1234567891011121314151617class Solution { public ListNode getKthFromEnd(ListNode head, int k) { ListNode fast = head; while(k &gt; 0){ fast = fast.next; k--; } ListNode slow = head; while(fast != null){ fast = fast.next; slow = slow.next; } return slow; }} 19. 删除链表的倒数第 N 个结点1234567891011121314151617181920212223class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(-1); dummy.next = head; ListNode fast = dummy; while(n &gt; 0){ fast = fast.next; n--; } ListNode slow = dummy; while(fast.next != null){ fast = fast.next; slow = slow.next; } slow.next = slow.next.next; return dummy.next; }} 876. 链表的中间结点1234567891011121314class Solution { //如果有两个中间结点，则返回第二个中间结点。 public ListNode middleNode(ListNode head) { if(head == null) return head; ListNode fast = head; ListNode slow = head; while(fast != null &amp;&amp; fast.next != null){ fast = fast.next.next; slow = slow.next; } return slow; }} 链表排序148. 排序链表12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution { public ListNode sortList(ListNode head) { if(head == null) return head; return mergeSort(head); } ListNode mergeSort(ListNode head){ //1. 递归终止条件， 注意这里 head.next == null 也递归终止了 if(head == null || head.next == null) return head; //2. 快慢指针找中点 ListNode fast = head; ListNode slow = head; while(fast.next != null &amp;&amp; fast.next.next != null){ fast = fast.next.next; slow = slow.next; } //3. 递归排序右边 ListNode r = mergeSort(slow.next); slow.next = null; //4. 递归排序左边 ListNode l = mergeSort(head); //5. 合并两个链表 return merge(l,r); } ListNode merge(ListNode l , ListNode r){ ListNode dummy = new ListNode(-1); ListNode cur = dummy; while(l != null &amp;&amp; r != null){ if(l.val &lt;= r.val){ cur.next = l; l = l.next; cur = cur.next; }else{ cur.next = r; r = r.next; cur = cur.next; } } cur.next = l == null ? r : l; return dummy.next; }} 147. 对链表进行插入排序12345678910111213141516171819202122232425class Solution { public ListNode insertionSortList(ListNode head) { ListNode dummy = new ListNode(0), pre; dummy.next = head; while(head != null &amp;&amp; head.next != null) { //需要一个指针指向当前已排序的最后一个位置，这里用的是head指针 if(head.val &lt;= head.next.val) { head = head.next; continue; } pre = dummy; //需要另外一个指针pre,每次从表头循环，这里用的是dummy表头指针。 //每次拿出未排序的节点，先和前驱比较，如果大于或者等于前驱，就不用排序了，直接进入下一次循环 //如果前驱小，则进入内层循环，依次和pre指针比较，插入对应位置即可。 while (pre.next.val &lt; head.next.val) pre = pre.next; ListNode curr = head.next; head.next = curr.next; curr.next = pre.next; pre.next = curr; } return dummy.next; }} 删除链表中的重复元素83. 删除排序链表中的重复元素123456789101112class Solution { public ListNode deleteDuplicates(ListNode head) { if(head == null) return head; ListNode fast = head; while(fast.next != null){ // 一次遍历 if(fast.val == fast.next.val) fast.next = fast.next.next; else fast = fast.next; } return head; }} 82. 删除排序链表中的重复元素 II123456789101112131415161718192021222324class Solution { public ListNode deleteDuplicates(ListNode head) { if(head == null) return head; ListNode dummy = new ListNode(-1); dummy.next = head; // fast 在 slow 后面一个节点,方便slow连接 ListNode fast = head; ListNode slow = dummy; while(fast != null){ // 如果 fast 和 fast的下一个节点值相同，则进入while循环 if(fast.next != null &amp;&amp; fast.val == fast.next.val){ // 12223 while(fast.next != null &amp;&amp; fast.val == fast.next.val) fast = fast.next; fast = fast.next; slow.next = fast; }else{ //1234 fast = fast.next; slow = slow.next; } } return dummy.next; }} 复制链表问题138. 复制带随机指针的链表1234567891011121314151617181920212223242526272829303132class Solution { public Node copyRandomList(Node head) { if(head == null) return head; //1. 复制每个节点 Node cur = head; while(cur != null){ Node tem = new Node(cur.val); tem.next = cur.next; cur.next = tem; cur = cur.next.next; } //2. 复制随机节点 cur = head; while(cur != null){ if(cur.random != null) cur.next.random = cur.random.next; cur = cur.next.next; } //3. 分离奇偶链表 Node res = head.next; cur = head; while(cur.next != null){ // cur help Node help = cur.next; cur.next = help.next; cur = help; } return res; }} 86. 分隔链表1234567891011121314151617181920212223242526class Solution { public ListNode partition(ListNode head, int x) { ListNode dummy1 = new ListNode(-1); ListNode dummy2 = new ListNode(-1); ListNode a = dummy1; ListNode b = dummy2; ListNode cur = head; while(cur != null){ if(cur.val &lt; x){ a.next = cur; a = a.next; }else { b.next = cur; b = b.next; } cur = cur.next; } a.next = dummy2.next; b.next = null; // 注意这里要把 b.next 指向null避免循环 return dummy1.next; }} 725. 分隔链表12345678910111213141516171819202122232425262728293031323334353637class Solution { public ListNode[] splitListToParts(ListNode head, int k) { if(head == null) return new ListNode[k]; //1. 先拿到链表的长度 int n = 1; ListNode cur = head; while(cur.next != null){ cur = cur.next; n++; } //2. 看要分成多少份 链表 int size = n / k; int count = n % k ; // 有 count 个链表可以分配 size + 1 个节点 ListNode[] res = new ListNode[k]; cur = head; for(int i = 0 ; i &lt; k &amp;&amp; cur != null ; i++){ //3. 将当前链表放入到数组中去, curSize 为当前链表的长度 res[i] = cur; int curSize = size + (count &gt; 0 ? 1 : 0); count--; // 1 2 3 4 5 6 7 8 9 10 k = 3 // 那么就是 1234 , 567 , 8910 for(int j = 1 ; j &lt; curSize ; j++) cur = cur.next; // 注意是从1开始的 //4. cur.next 要置空， 且 cur.next 就是下一个循环的起点 ListNode tem = cur.next; cur.next = null; cur = tem; } return res; }} 链表计算问题2. 两数相加1234567891011121314151617181920class Solution { public ListNode addTwoNumbers(ListNode l1, ListNode l2) { if(l1 == null || l2 == null) return l1 == null ? l2 : l1; ListNode dummy = new ListNode(-1); ListNode cur = dummy; int carry = 0; while(l1 != null || l2 != null){ int n1 = l1 == null ? 0 : l1.val; int n2 = l2 == null ? 0 : l2.val; int sum = n1 + n2 + carry; cur.next = new ListNode(sum % 10); cur = cur.next; carry = sum / 10; if(l1 != null) l1 = l1.next; if(l2 != null) l2 = l2.next; } if(carry == 1) cur.next = new ListNode(1); return dummy.next; }} 146. LRU 缓存机制1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586class LRUCache { // 用一个双向链表和Map来实现 Map&lt;Integer, ListNode&gt; map = new HashMap&lt;&gt;(); ListNode dummyHead; ListNode dummyTail; int capacity; // 容量 //注意是静态内部类，且节点中存在 key 和 value两个值 static class ListNode{ int key; int val; ListNode next; ListNode pre; public ListNode(){} public ListNode(int key , int val){ this.key = key; this.val = val; } } // 初始化容量和连接虚拟头节点、尾节点 public LRUCache(int capacity) { this.capacity = capacity; dummyHead = new ListNode(-1,-1); dummyTail = new ListNode(-1,-1); dummyHead.next = dummyTail; dummyTail.pre = dummyHead; } // 先看map中是否含有key，如果有就取出来然后将哪个节点放到最前面 public int get(int key) { if(map.containsKey(key)){ int res = map.get(key).val; moveToHead(key); // 将 key 所在的节点移动到链表头部 return res; }else return -1; } // 先看map中是否含有key，如果有就更新然后将哪个节点放到最前面。 // 再看容量是否满了 public void put(int key, int value) { if(map.containsKey(key)){ ListNode cur = map.get(key); cur.val = value; moveToHead(key); return; } if(map.size() == capacity){ ListNode tail = removeTail(); map.remove(tail.key); } ListNode head = new ListNode(key , value); addHead(head); map.put(key , head); // 注意这里 map 和链表都要添加进去和删除 } void moveToHead(int key){ ListNode cur = map.get(key); //将cur的前后节点连接在一起 cur.next.pre = cur.pre; cur.pre.next = cur.next; addHead(cur); // 将当前节点加入到链表头部 } void addHead(ListNode cur){ //取出旧的头节点 ListNode oldhead = dummyHead.next; dummyHead.next = cur; cur.pre = dummyHead; cur.next = oldhead; oldhead.pre = cur; } //移除链表尾节点 ListNode removeTail(){ ListNode tail = dummyTail.pre; ListNode newTail = tail.pre; newTail.next = dummyTail; dummyTail.pre = newTail; return tail; }}","link":"/2021/09/29/%E7%AE%97%E6%B3%95%E9%A2%98%EF%BC%88%E4%B8%80%EF%BC%89%E9%93%BE%E8%A1%A8/"},{"title":"操作系统（四）网络管理","text":"网络管理： Linux系统是如何收发网络包 零拷贝是啥 IO多路复用 Reacter 零拷贝是啥磁盘可以说是计算机系统最慢的硬件之⼀，读写速度相差内存 10 倍以上。所以对磁盘的读写有很多的优化措施： DMA是啥在没有 DMA 技术前，I/O 的过程是这样的： CPU 发出对应的指令给磁盘控制器，然后返回； 磁盘控制器收到指令后，于是就开始准备数据，会把数据放⼊到磁盘控制器的内部缓冲区中，然后产⽣⼀个中断； CPU 收到中断信号后，停下⼿头的⼯作，接着把磁盘控制器的缓冲区的数据⼀次⼀个字节地读进⾃⼰的寄存器，然后再把寄存器⾥的数据写⼊到内存，⽽在数据传输的期间CPU 是⽆法执⾏其他任务的。 可以看到，整个数据的传输过程，都要需要 CPU 亲⾃参与搬运数据的过程，⽽且这个过程，CPU 是不能做其他事情的。 什么是 DMA 技术？ 简单理解就是，在进⾏ I/O 设备和内存的数据传输的时候，数据搬运的⼯作全部交给 DMA 控制器，⽽ CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。 IO多路复用模型基本的Socket模型客户端和服务端之间能在网络中通信，就必须要使用Socket变成。那Socket编程的基本流程是啥样的呢： 服务端的Socket编程流程： 服务端⾸先调⽤ socket() 函数，创建⽹络协议为 IPv4，以及传输协议为 TCP 的 Socket ，接着调⽤ bind() 函数，给这个 Socket 绑定⼀个 IP 地址和端⼝。 绑定端⼝的⽬的：当内核收到 TCP 报⽂，通过 TCP 头⾥⾯的端⼝号，来找到我们的应⽤程序，然后把数据传递给我们。 绑定 IP 地址的⽬的：⼀台机器是可以有多个⽹卡的，每个⽹卡都有对应的 IP 地址，当绑定⼀个⽹卡时，内核在收到该⽹卡上的包，才会发给我们； 以调⽤ listen() 函数进⾏监听 服务端进⼊了监听状态后，通过调⽤ accept() 函数，来从内核获取客户端的连接，如果没有客户端连接，则会阻塞等待客户端连接的到来。 客户端的Socket编程流程： 客户端在创建好 Socket 后，调⽤ connect() 函数发起连接，该函数的参数要指明服务端的 IP 地址和端⼝号， 开始发生TCP三次握手进行TCP连接 在 TCP 连接的过程中，服务器的内核实际上为每个 Socket 维护了两个队列： ⼀个是还没完全建⽴连接的队列，称为 TCP 半连接队列，这个队列都是没有完成三次握⼿的连接，此时服务端处于 syn_rcvd 的状态； ⼀个是⼀件建⽴连接的队列，称为 TCP 全连接队列，这个队列都是完成了三次握⼿的连接，此时服务端处于 established 状态； 当 TCP 全连接队列不为空后，服务端的 accept() 函数，就会从内核中的 TCP 全连接队列⾥拿出⼀个已经完成连接的 Socket 返回应⽤程序，后续数据传输都⽤这个 Socket。 注意，监听的 Socket 和真正⽤来传数据的 Socket 是两个： ⼀个叫作监听 Socket； ⼀个叫作已连接 Socket； 连接建⽴后，客户端和服务端就开始相互传输数据了，双⽅都可以通过 read() 和write() 函数来读写数据。如下： 服务更多的客户端前面的Socket模型只能实现1对1的通信，因为使用的是同步阻塞的方式。当服务端在还没处理完⼀个客户端的⽹络 I/O 时，或者 读写操作发⽣阻塞时，其他客户端是⽆法与服务端连接的。 你知道服务器单机理论最⼤能连接多少个客户端？ 相信你知道 TCP 连接是由四元组唯⼀确认的，这个四元组就是：本机IP, 本机端⼝, 对端IP, 对端端⼝。 服务器作为服务⽅，通常会在本地固定监听⼀个端⼝，等待客户端的连接。因此服务器的本地 IP 和端⼝是固定的，于是对于服务端 TCP 连接的四元组只有对端 IP 和端⼝是会变化的，所以最⼤ TCP 连接数 = 客户端 IP 数×客户端端⼝数。 对于 IPv4，客户端的 IP 数最多为 2 的 32 次⽅，客户端的端⼝数最多为 2 的 16 次⽅，也就是服务端单机最⼤ TCP 连接数约为 2 的 48 次⽅。 服务器肯定承载不了那么⼤的连接数，主要会受两个⽅⾯的限制： ⽂件描述符，Socket 实际上是⼀个⽂件，也就会对应⼀个⽂件描述符。在 Linux 下，单个进程打开的⽂件描述符数是有限制的，没有经过修改的值⼀般都是 1024，不过我们可以通过 ulimit 增⼤⽂件描述符的数⽬； 系统内存，每个 TCP 连接在内核中都有对应的数据结构，意味着每个连接都是会占⽤⼀定内存的； 那如果服务器的内存只有 2 GB，⽹卡是千兆的，能⽀持并发 1 万请求吗？ 并发 1 万请求，也就是经典的 C10K 问题 ，C 是 Client 单词⾸字⺟缩写，C10K 就是单机同时处理 1 万个请求的问题。 从硬件资源⻆度看，对于 2GB 内存千兆⽹卡的服务器，如果每个请求处理占⽤不到 200KB的内存和 100Kbit 的⽹络带宽就可以满⾜并发 1 万个请求。 不过，要想真正实现 C10K 的服务器，要考虑的地⽅在于服务器的⽹络 I/O 模型，效率低的模型，会加重系统开销，从⽽会离 C10K 的⽬标越来越远。 多进程模型基于最原始的阻塞⽹络 I/O， 如果服务器要⽀持多个客户端，其中⽐较传统的⽅式，就是使⽤多进程模型，也就是为每个客户端分配⼀个进程来处理请求。 服务器的主进程负责监听客户的连接，⼀旦与客户端连接完成，accept() 函数就会返回⼀个「已连接 Socket」，这时就通过 fork() 函数创建⼀个⼦进程，实际上就把⽗进程所有相关的东⻄都复制⼀份，包括⽂件描述符、内存地址空间、程序计数器、执⾏的代码等。这两个进程刚复制完的时候，⼏乎⼀摸⼀样。不过，会根据返回值来区分是⽗进程还是⼦进程，如果返回值是 0，则是⼦进程；如果返回值是其他的整数，就是⽗进程。 正因为⼦进程会复制⽗进程的⽂件描述符，于是就可以直接使⽤「已连接 Socket 」和客户端通信了，可以发现，⼦进程不需要关⼼「监听 Socket」，只需要关⼼「已连接 Socket」；⽗进程则相反，将客户服务交给⼦进程来处理，因此⽗进程不需要关⼼「已连接 Socket」，只需要关⼼「监听 Socket」。 因为进程的切换开销太大了，所以当客户端特别多的时候也不够给力。 多线程模型线程是运⾏在进程中的⼀个“逻辑流”，单进程中可以运⾏多个线程，同进程⾥的线程可以共享进程的部分资源的，⽐如⽂件描述符列表、进程空间、代码、全局数据、堆、共享库等， 当服务器与客户端 TCP 完成连接后，通过 pthread_create() 函数创建线程，然后将「已连接 Socket」的⽂件描述符传递给线程函数，接着在线程⾥和客户端进⾏通信，从⽽达到并发处理的⽬的。 如果每来⼀个连接就创建⼀个线程，线程运⾏完后，还得操作系统还得销毁线程，虽说线程切换的上写⽂开销不⼤，但是如果频繁创建和销毁线程，系统开销也是不⼩的。那么，我们可以使⽤线程池的⽅式来避免线程的频繁创建和销毁，所谓的线程池，就是提前创建若⼲个线程，这样当由新连接建⽴时，将这个已连接的 Socket 放⼊到⼀个队列⾥，然后线程池⾥的线程负责从队列中取出已连接 Socket 进程处理。 需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要加锁。上⾯基于进程或者线程模型的，其实还是有问题的。新到来⼀个 TCP 连接，就需要分配⼀个进程或者线程，那么如果要达到 C10K，意味着要⼀台机器维护 1 万个连接，相当于要维护 1万个进程/线程，操作系统就算死扛也是扛不住的。 IO多路复用前面说的这些方法最终只有一个结论： 给每个请求分配一个进程或者线程是不合理的。 所以我们需要找到一个方法：只用一个进程来维护多个Socket，这就是IO多路复用。 ⼀个进程虽然任⼀时刻只能处理⼀个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉⻓来看，多个请求复⽤了⼀个进程，这就是多路复⽤，这种思想很类似⼀个 CPU 并发多个进程，所以也叫做时分多路复⽤。 而我们所谓的select、poll 和epoll 都是内核提供给用户态的多路复用系统调用，进程可以通过一个系统调用函数来从内核中获取多个事件。 select/poll/epoll 是如何获取⽹络事件的呢？ 在获取事件时，先把所有连接（⽂件描述符）传给内核，再由内核返回产⽣了事件的连接，然后在⽤户态中再处理这些连接对应的请求即可。 Select/pollselect 实现多路复⽤的⽅式是: 将进程关心的所有Socket放到一个文件描述符集合中去，调用select函数就是将这个集合拷贝到内核中去，让内核来检查是否有网络事件发生，其实就是遍历所有的Socket文件描述符，当有事件发生后，就对那个Socket文件描述符上面打一个标记，遍历完之后再把整个集合拷贝到用户态里面，然后程序在用户态还需要再遍历一遍找到对应可读或可写的Socket，再进行处理。 所以，对于 select 这种⽅式 进⾏ 2 次「遍历」⽂件描述符集合，⼀次是在内核态⾥，⼀个次是在⽤户态⾥ ， 发⽣ 2 次「拷⻉」⽂件描述符集合，先从⽤户空间传⼊内核空间，由内核修改后，再传出到⽤户空间中。 select 使⽤固定⻓度的 BitsMap，表示⽂件描述符集合，⽽且所⽀持的⽂件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最⼤值为 1024 ，只能监听 0~1023 的⽂件描述符。 poll 不再⽤ BitsMap 来存储所关注的⽂件描述符，取⽽代之⽤动态数组，以链表形式来组织，突破了 select 的⽂件描述符个数限制，当然还会受到系统⽂件描述符限制。 但是 poll 和 select 并没有太⼤的本质区别，都是使⽤「线性结构」存储进程关注的 Socket集合，因此都需要遍历⽂件描述符集合来找到可读或可写的 Socket，时间复杂度为 **O(n)**，⽽且也需要在⽤户态与内核态之间拷⻉⽂件描述符集合，这种⽅式随着并发数上来，性能的损耗会呈指数级增⻓。 epoll的改进第⼀点，epoll 在内核⾥使⽤红⿊树来跟踪进程所有待检测的⽂件描述字，把需要监控的socket 通过 epoll_ctl() 函数加⼊内核中的红⿊树⾥，红⿊树是个⾼效的数据结构，增删查⼀般时间复杂度是 O(logn) ，通过对这棵⿊红树进⾏操作，这样就不需要像 select/poll 每次操作时都传⼊整个 socket 集合，只需要传⼊⼀个待检测的 socket，减少了内核和⽤户空间⼤量的数据拷⻉和内存分配。 第⼆点， epoll 使⽤事件驱动的机制，内核⾥维护了⼀个链表来记录就绪事件，当某个socket 有事件发⽣时，通过回调函数内核会将其加⼊到这个就绪事件列表中，当⽤户调⽤epoll_wait() 函数时，只会返回有事件发⽣的⽂件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，⼤⼤提⾼了检测的效率。 epoll ⽀持两种事件触发模式，分别是边缘触发（edge-triggered，ET）和⽔平触发（level\u0002triggered，LT）。这两个术语还挺抽象的，其实它们的区别还是很好理解的。 使⽤边缘触发模式时，当被监控的 Socket 描述符上有可读事件发⽣时，服务器端只会从epoll_wait 中苏醒⼀次，即使进程没有调⽤ read 函数从内核读取数据，也依然只苏醒⼀次，因此我们程序要保证⼀次性将内核缓冲区的数据读取完； 使⽤⽔平触发模式时，当被监控的 Socket 上有可读事件发⽣时，服务器端不断地从epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束，⽬的是告诉我们有数据需要读取； 举个例⼦，你的快递被放到了⼀个快递箱⾥，如果快递箱只会通过短信通知你⼀次，即使你⼀直没有去取，它也不会再发送第⼆条短信提醒你，这个⽅式就是边缘触发；如果快递箱发现你的快递没有被取出，它就会不停地发短信通知你，直到你取出了快递，它才消停，这个就是⽔平触发的⽅式。 这就是两者的区别，⽔平触发的意思是只要满⾜事件的条件，⽐如内核中有数据需要读，就⼀直不断地把这个事件传递给⽤户；⽽边缘触发的意思是只有第⼀次满⾜条件的时候才触发，之后就不会再传递同样的事件了。 如果使⽤⽔平触发模式，当内核通知⽂件描述符可读写时，接下来还可以继续去检测它的状态，看它是否依然可读或可写。所以在收到通知后，没必要⼀次执⾏尽可能多的读写操作。 如果使⽤边缘触发模式，I/O 事件发⽣时只会通知⼀次，⽽且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会循环从⽂件描述符读写数据，那么如果⽂件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那⾥，程序就没办法继续往下执⾏。 所以，边缘触发模式⼀般和⾮阻塞 I/O 搭配使⽤，程序会⼀直执⾏ I/O 操作，直到系统调⽤（如 read 和 write ）返回错误，错误类型为EAGAIN 或EWOULDBLOCK 。 ⼀般来说，边缘触发的效率⽐⽔平触发的效率要⾼，因为边缘触发可以减少 epoll_wait 的系统调⽤次数，系统调⽤也是有⼀定的开销的的，毕竟也存在上下⽂的切换。 select/poll 只有⽔平触发模式，epoll 默认的触发模式是⽔平触发，但是可以根据应⽤场景设置为边缘触发模式。","link":"/2021/10/10/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86/"},{"title":"算法题（三）二分查找","text":"二分查找专题 第一部分：四种基本情况704. 二分查找123456789101112class Solution { public int search(int[] nums, int target) { int n = nums.length; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r + 1 &gt;&gt; 1; if(nums[mid] &gt; target) r = mid-1; else l = mid; } return nums[l] == target ? l : -1; }} 34. 在排序数组中查找元素的第一个和最后一个位置123456789101112131415161718192021222324252627class Solution { public int[] searchRange(int[] nums, int target) { int[] res = new int[2]; int n = nums.length; if(n == 0) return new int[]{-1,-1}; //找到第一个出现的位置 int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; if(nums[mid] &lt; target) l = mid+1; else r = mid; } if(nums[l] != target) return new int[]{-1,-1}; else res[0] = l; //找到最后一个出现的位置 l = 0; r = n-1; while(l &lt; r){ int mid = l + r +1 &gt;&gt; 1; if(nums[mid] &gt; target) r = mid-1; else l = mid; } res[1] = l; return res; }} 35. 搜索插入位置1234567891011121314151617// 相当于找数字出现的第一个位置class Solution { public int searchInsert(int[] nums, int target) { int n = nums.length; //1. 注意如果最后一个数小于target的话，就返回数组长度 if (n == 0 || nums[n-1] &lt; target) return n; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; if(nums[mid] &lt; target) l = mid + 1; else r = mid; } return l; }} 69. Sqrt(x)12345678910111213 class Solution { public int mySqrt(int x) { int l = 0 , r = x; while(l &lt; r){ int mid = l + r + 1 &gt;&gt; 1; if(mid &gt; x / mid) r = mid-1; else l = mid; } return l; }} 287. 寻找重复数123456789101112131415161718192021class Solution { public int findDuplicate(int[] nums) { int n = nums.length; //对值的范围进而二分 int l = 0 , r = n-1; while (l &lt; r){ int mid = l+r &gt;&gt;1; //看一下数组中比mid小的数有多少 int count = 0; for (int num : nums){ if (num &lt;= mid) count++; } //比mid小的数大于mid，说明在左边,可能是mid if (count &gt; mid) r = mid; else l = mid+1; } return l; }} 50. Pow(x, n)12345678910111213class Solution { public double myPow(double x, int n) { double res = 1.0; for(int i = n ; i != 0 ; i /=2){ if(i % 2 != 0) res = res * x; x *= x; } return n &lt; 0 ? 1/res : res; }} 4. 寻找两个正序数组的中位数1234567891011121314151617181920212223242526272829303132333435363738class Solution { public double findMedianSortedArrays(int[] nums1, int[] nums2) { int len1 = nums1.length; int len2 = nums2.length; int l = (len1 + len2 + 1) / 2; int r = (len1 + len2 + 2) / 2; int sum = getK(nums1 , 0 , len1-1 , nums2 , 0 , len2-1 , l) + getK(nums1 , 0 , len1-1 , nums2 , 0 , len2-1 , r); return sum / 2.0; } // 从两个正序数组中获取 第 K 大的数 int getK(int[] nums1 , int s1 , int e1 , int[] nums2 , int s2 , int e2 , int k){ int len1 = e1 - s1 + 1; int len2 = e2 - s2 + 1; if(len1 &gt; len2) return getK(nums2 , s2 , e2 , nums1 , s1 , e1 , k); // 确保 len1 &lt;= len2 // 递归终止条件 if(len1 == 0) return nums2[s2 + k - 1]; if(len2 == 0) return nums1[s1 + k -1]; if(k == 1) return Math.min(nums1[s1] , nums2[s2]); // 1 3 5 7 9 取 k / 2 个数 // 2 4 6 8 10 取 k / 2 个数 // k = 4 int a = s1 + Math.min(len1 , k/2) - 1; // nums1数组和nums2 数组都取到中间的值 int b = s2 + Math.min(len2 , k/2) - 1; if(nums1[a] &gt; nums2[b]){ //把 nums2 的前 s2 - b 个数干掉 return getK(nums1 , s1 , e1 , nums2 , b+1 , e2 , k-(b-s2+1)); }else return getK(nums1 , a+1 , e1 , nums2 , s2 , e2 , k-(a-s1+1)); }} 540. 有序数组中的单一元素12345678910111213141516171819202122232425class Solution { public int singleNonDuplicate(int[] nums) { int n = nums.length; int l = 0 , r = n -1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; // 1 2 2 4 4 //如果当前数和左边的数相等,判断左边的数是不是偶数，如果是偶数说明唯一的数在左边 if(nums[mid] == nums[mid-1]){ if((mid-l) % 2 == 0) r = mid-2;//如果左边的数是偶数,说明唯一的数在左边 else l = mid+1; ////如果当前数和右边的数相等，判断右边的数是不是偶数，如果是偶数说明唯一的数在右边 // 1 1 2 2 3 3 4 4 6 }else if(nums[mid] == nums[mid+1]){ if((r-mid) % 2 == 0) l = mid+2; else r = mid-1; } else return nums[mid]; // 说明当前数和左右都不相等 } return nums[l]; }} 旋转排序数组153. 寻找旋转排序数组中的最小值(无重复值)1234567891011121314class Solution { public int findMin(int[] nums) { int n = nums.length; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; //34512 if(nums[mid] &gt; nums[r]) l = mid+1; else r = mid; } return nums[l]; }} 154. 寻找旋转排序数组中的最小值 II12345678910111213class Solution { public int findMin(int[] nums) { int n = nums.length; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; if(nums[mid] &gt; nums[r]) l = mid+1; else if(nums[mid] &lt; nums[r]) r = mid; else r--; } return nums[l]; }} 33. 搜索旋转排序数组(无重复值)1234567891011121314151617181920class Solution { public int search(int[] nums, int target) { int n = nums.length; if(n == 0) return -1; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; //34512 if(nums[mid] &gt; nums[r]){ if(target &gt;= nums[l] &amp;&amp; target &lt;= nums[mid]) r = mid; else l = mid+1; }else{ //561234 if(target &gt; nums[mid] &amp;&amp; target &lt;= nums[r]) l = mid+1; else r = mid; } } return nums[l] == target ? l : -1; }} 二维矩阵74. 搜索二维矩阵1234567891011121314151617181920212223242526272829303132class Solution { public boolean searchMatrix(int[][] matrix, int target) { int m = matrix.length; int n = matrix[0].length; if(m == 0 || n == 0) return false; int l = 0 , r = n-1; while(l &lt; m &amp;&amp; r &gt;= 0){ if(matrix[l][r] &lt; target) l++; else if(matrix[l][r] &gt; target) r--; else return true; } return false; }}class Solution { public boolean searchMatrix(int[][] matrix, int target) { int m = matrix.length; //行数 int n = matrix[0].length; //列数 int l = 0 , r = m *n -1; while(l &lt; r){ int mid = l + r + 1 &gt;&gt; 1; //将一维的数组转换成二维的坐标 if(matrix[mid / n][ mid % n] &gt; target) r = mid-1; else l = mid; } return matrix[l/n][l%n] == target; }} 240. 搜索二维矩阵 II12345678910111213141516class Solution { public boolean searchMatrix(int[][] matrix, int target) { int m = matrix.length; if(m == 0) return false; int n = matrix[0].length; int l = 0 , r = n-1; while(l &lt; m &amp;&amp; r &gt;= 0){ if(target &gt; matrix[l][r]) l++; else if(target &lt; matrix[l][r]) r--; else return true; } return false; }} 378. 有序矩阵中第 K 小的元素12345678910111213141516171819202122232425class Solution { public int kthSmallest(int[][] matrix, int k) { int raw = matrix.length; int col = matrix[0].length; // 注意这里的l 和 r 取的是数组中的最小值和最大值 int l = matrix[0][0] ; int r = matrix[raw-1][col-1]; while(l &lt; r){ int mid = l + r &gt;&gt; 1; int count = 0; for(int i = 0 ; i &lt; raw ; i++){ for(int j = 0 ; j &lt; col ; j++){ if(matrix[i][j] &lt;= mid) count++; } } if(count &lt; k) l = mid+1; else r = mid; } return l; //就是刚好取到矩阵中的值得时候，循环刚好满足，然后退出 }} 162. 寻找峰值12345678910111213class Solution { public int findPeakElement(int[] nums) { int n = nums.length; int l = 0 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; if(nums[mid] &lt; nums[mid+1]) l = mid+1; else r = mid; } return l; }} 1095. 山脉数组中查找目标值12345678910111213141516171819202122232425262728293031323334353637383940class Solution { public int findInMountainArray(int target, MountainArray mountainArr) { int len = mountainArr.length(); int top = searchPeak(mountainArr,0,len-1); //找到峰顶位置 int res = searchFirst(mountainArr,0,top,target);//在前面升序数组中找 if(mountainArr.get(res) == target) return res;//如果找到了就返回 else { res = searchLast(mountainArr,top+1,len-1,target);//在后面降序数组中找 if(mountainArr.get(res) == target) return res;//如果找到了就返回 else return -1; } } public static int searchPeak(MountainArray mountainArr , int l , int r){ while(l&lt;r){ int mid = l+r&gt;&gt;&gt;1; if(mountainArr.get(mid) &lt; mountainArr.get(mid+1)) l=mid+1; else r = mid; } return l; } //在前面升序数组中找 public static int searchFirst(MountainArray mountainArr , int l , int r ,int target){ while(l&lt;r){ int mid = l+r+1&gt;&gt;&gt;1; if(mountainArr.get(mid) &gt; target) r = mid-1; else l = mid; } return l; } //在后面降序数组中找 public static int searchLast(MountainArray mountainArr , int l , int r ,int target){ while(l&lt;r){ int mid = l+r&gt;&gt;&gt;1; if(mountainArr.get(mid) &gt; target) l=mid+1; else r=mid; } return l; }} 887. 鸡蛋掉落12345678910111213141516171819202122232425class Solution { public int superEggDrop(int k, int n) { //1. 反向dp思维： 有k个鸡蛋，可以扔m次，那么能确定的最大楼层是多少 // 比如 dp[1][7] 表示 一个 鸡蛋 扔7次可以确定 7楼 int[][] dp = new int[k+1][n+1]; //2. 现在要求的 是 n层楼有k个鸡蛋，扔几次。 也就是 dp[k][m] = n 是的m // 无论在哪一层扔鸡蛋，都只有碎了和没碎两种可能： // 1. 如果没碎，说明在楼上 . 如果碎了，说明在楼下 // 2. 无论你上楼还是下楼，总的楼层数 = 楼上的楼层数 + 楼下的楼层数 + 1（当前这层楼） // dp[i][j] = dp[i][j-1] + dp[i-1][j-1] + 1; int m = 0; while(dp[k][m] &lt; n){ m++; // 没多扔一次鸡蛋，有 1 - k 个鸡蛋的所有可能 for(int i = 1 ; i &lt;= k ; i++){ dp[i][m] = dp[i][m-1] + dp[i-1][m-1] + 1; } } return m; }}","link":"/2021/10/10/%E7%AE%97%E6%B3%95%E9%A2%98%EF%BC%88%E4%B8%89%EF%BC%89%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"},{"title":"算法题（五）树结构","text":"二叉树专题 二叉树的遍历问题144. 二叉树的前序遍历12345678910111213141516171819202122232425262728293031323334// 递归class Solution { // 前序 ： 中左右 public List&lt;Integer&gt; preorderTraversal(TreeNode root) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if(root == null) return res; dfs(root , res); return res; } void dfs(TreeNode root , List&lt;Integer&gt; res){ if(root == null) return; res.add(root.val); dfs(root.left , res); dfs(root.right , res); }}class Solution { public List&lt;Integer&gt; preorderTraversal(TreeNode root) { List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); if(root == null) return list; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while(!stack.isEmpty()){ TreeNode curNode = stack.pop(); list.add(curNode.val); if(curNode.right != null) stack.push(curNode.right); if(curNode.left != null) stack.push(curNode.left); } return list; }} 94. 二叉树的中序遍历12345678910111213141516171819202122232425262728293031323334353637383940414243// 递归解法class Solution { public List&lt;Integer&gt; inorderTraversal(TreeNode root) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if(root == null) return res; dfs(root , res); return res; } void dfs(TreeNode root , List&lt;Integer&gt; res){ if(root == null) return; dfs(root.left , res); res.add(root.val); dfs(root.right, res); }}// 非递归class Solution { // 中序遍历： 左中右 // 2 // 3 4 //1 5 6 7 1352647 public List&lt;Integer&gt; inorderTraversal(TreeNode root) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if(root == null) return res; // 用栈 Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root; while(cur != null || !stack.empty()){ // 先一路压左子节点 while(cur != null){ stack.push(cur); cur = cur.left; } // 每弹出一个就加入到结果集中，cur 指向右子节点 cur = stack.pop(); res.add(cur.val); cur = cur.right; } return res; }} 145. 二叉树的后序遍历123456789101112131415161718class Solution { // 后序 ： 左右中 // 中左右 public List&lt;Integer&gt; postorderTraversal(TreeNode root) { LinkedList&lt;Integer&gt; res = new LinkedList&lt;&gt;(); if(root == null) return res; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while(!stack.empty()){ TreeNode curNode = stack.pop(); res.addFirst(curNode.val); if(curNode.left != null) stack.push(curNode.left); if(curNode.right != null) stack.push(curNode.right); } return res; }} 102. 二叉树的层序遍历123456789101112131415161718192021class Solution { public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); if(root == null) return res; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while(!queue.isEmpty()){ int count = queue.size(); //这一层的节点数量 List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); while(count &gt; 0){ TreeNode curNode = queue.poll(); //弹出一个节点就加入 list.add(curNode.val); count--; if(curNode.left != null) queue.add(curNode.left); //将下一层的节点加入 if(curNode.right != null) queue.add(curNode.right); } res.add(list); } return res; }} 103. 二叉树的锯齿形层序遍历123456789101112131415161718192021222324class Solution { public List&lt;List&lt;Integer&gt;&gt; zigzagLevelOrder(TreeNode root) { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); if(root == null) return res; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while(!queue.isEmpty()){ int count = queue.size(); List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); while(count &gt; 0){ TreeNode curNode = queue.poll(); count--; list.add(curNode.val); if(curNode.left != null) queue.add(curNode.left); if(curNode.right != null) queue.add(curNode.right); } if(res.size() % 2 == 1){ Collections.reverse(list); } res.add(list); } return res; }} 107. 二叉树的层序遍历 II12345678910111213141516171819202122class Solution { public List&lt;List&lt;Integer&gt;&gt; levelOrderBottom(TreeNode root) { LinkedList&lt;List&lt;Integer&gt;&gt; res = new LinkedList&lt;&gt;(); if(root == null) return res; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while(!queue.isEmpty()){ int count = queue.size(); List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); while(count &gt; 0){ TreeNode curNode = queue.poll(); list.add(curNode.val); count--; if(curNode.left != null) queue.add(curNode.left); if(curNode.right != null) queue.add(curNode.right); } res.addFirst(list); } return res; }} 199. 二叉树的右视图12345678910111213141516171819class Solution { public List&lt;Integer&gt; rightSideView(TreeNode root) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if(root == null) return res; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while(!queue.isEmpty()){ int count = queue.size(); while(count &gt; 0){ TreeNode curNode = queue.poll(); if(count == 1) res.add(curNode.val); count--; if(curNode.left != null) queue.add(curNode.left); if(curNode.right != null) queue.add(curNode.right); } } return res; }} 297. 二叉树的序列化与反序列化12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class Codec { // Encodes a tree to a single string. public String serialize(TreeNode root) { if(root == null) return &quot;[]&quot;; StringBuilder sb = new StringBuilder(); sb.append('['); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while(!queue.isEmpty()){ TreeNode curNode = queue.poll(); if(curNode == null) sb.append(&quot;null,&quot;); else{ sb.append(curNode.val + &quot;,&quot;); queue.add(curNode.left); // 注意这里不能判空 queue.add(curNode.right); } } sb.deleteCharAt(sb.length()-1); sb.append(']'); return sb.toString(); } // Decodes your encoded data to tree. public TreeNode deserialize(String data) { if(data.equals(&quot;[]&quot;)) return null; String[] nums = data.substring(1 , data.length()-1).split(&quot;,&quot;); TreeNode root = new TreeNode(Integer.parseInt(nums[0])); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); int index = 1; while(!queue.isEmpty()){ TreeNode curNode = queue.poll(); if(!nums[index].equals(&quot;null&quot;)){ curNode.left = new TreeNode(Integer.parseInt(nums[index])); queue.add(curNode.left); } index++; if(!nums[index].equals(&quot;null&quot;)){ curNode.right = new TreeNode(Integer.parseInt(nums[index])); queue.add(curNode.right); } index++; } return root; }} 331. 验证二叉树的前序序列化二叉树的性质：所有节点的入度之和等于出度之和 每个空节点（ &quot;#&quot; ）会提供 0 个出度和 1 个入度。 每个非空节点会提供 2 个出度和 1 个入度（根节点的入度是 0）。 12345678910111213141516171819202122class Solution { //我们只要把字符串遍历一次，每个节点都累加 diff = 出度 - 入度 。在遍历到任何一个节点的时候，要求diff &gt;= 0，原因是还没遍历到该节点的子节点，所以此时的出度应该大于等于入度。当所有节点遍历完成之后，整棵树的 diff == 0 。//这里解释一下为什么下面的代码中 diff 的初始化为 1。因为，我们加入一个非空节点时，都会对 diff 先减去 1（入度），再加上 2（出度）。但是由于根节点没有父节点，所以其入度为 0，出度为 2。因此 diff 初始化为 1，是为了在加入根节点的时候，diff 先减去 1（入度），再加上 2（出度），此时 diff 正好应该是2. public boolean isValidSerialization(String preorder) { int diff = 1; for(String s : preorder.split(&quot;,&quot;)){ diff--; // 每加入一个节点 都要先减去一个入度 若该节点是非空节点 还要再加上两个出度 // 遍历完之前，出度是大于等于入度的 1个出度认为提供一个挂载点 1个入度认为消耗一个挂载点 // 若小于等于 消耗的挂载点 大于等于 提供的挂载点 不可能再继续遍历挂载剩下的节点了 if (diff &lt; 0){ return false; } if(!s.equals(&quot;#&quot;)){ diff += 2; } } return diff == 0; }} 二叉树的性质问题100. 相同的树12345678class Solution { public boolean isSameTree(TreeNode p, TreeNode q) { if(p == null &amp;&amp; q == null) return true; if(p == null || q == null) return false; if(p.val != q.val) return false; return isSameTree(p.left , q.left) &amp;&amp; isSameTree(p.right , q.right); }} 101. 对称二叉树12345678910111213class Solution { public boolean isSymmetric(TreeNode root) { if(root == null) return true; return isSame(root.left ,root.right); } boolean isSame(TreeNode l ,TreeNode r){ if(l == null &amp;&amp; r == null) return true; if(l == null || r == null) return false; if(l.val != r.val) return false; return isSame(l.left , r.right) &amp;&amp; isSame(l.right , r.left); }} 104. 二叉树的最大深度123456class Solution { public int maxDepth(TreeNode root) { if(root == null) return 0; return Math.max(maxDepth(root.left) , maxDepth(root.right)) + 1; }} 111. 二叉树的最小深度123456789101112class Solution { public int minDepth(TreeNode root) { if(root == null) return 0; int l = minDepth(root.left); int r = minDepth(root.right); if(l == 0) return r + 1; else if(r == 0) return l+1; return Math.min(l, r) + 1; }} 572. 另一棵树的子树1234567891011121314class Solution { public boolean isSubtree(TreeNode s, TreeNode t) { if(s == null) return false; if(t == null) return true; return isSame(s , t) || isSubtree(s.left , t) || isSubtree(s.right , t); } boolean isSame(TreeNode s , TreeNode t){ if(s == null &amp;&amp; t == null) return true; if(s == null || t == null) return false; if(s.val != t.val) return false; return isSame(s.left , t.left) &amp;&amp; isSame(s.right , t.right); }} 662. 二叉树最大宽度123456789101112131415161718192021222324252627282930// 用两个队列，一个用于层序遍历、一个用于计算索引class Solution { public int widthOfBinaryTree(TreeNode root) { if(root == null) return 0; int res = 1; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); LinkedList&lt;Integer&gt; indexQueue = new LinkedList&lt;&gt;();//记录节点的索引位置 queue.add(root); indexQueue.add(1); while(!queue.isEmpty()){ int count = queue.size(); int l = indexQueue.peek(); while(count &gt; 0){ TreeNode curNode = queue.poll(); int r = indexQueue.poll(); // curNode的索引值 count--; res = Math.max(res , r - l + 1); //更新 res if(curNode.left != null){ queue.add(curNode.left); indexQueue.add(r * 2); } if(curNode.right != null){ queue.add(curNode.right); indexQueue.add(r * 2 + 1); } } } return res; }} 226. 翻转二叉树1234567891011121314class Solution { public TreeNode invertTree(TreeNode root) { if(root == null) return null; TreeNode tem = root.left; root.left = root.right; root.right = tem; invertTree(root.left); invertTree(root.right); return root; }} 404. 左叶子之和123456789101112class Solution { public int sumOfLeftLeaves(TreeNode root) { if(root == null) return 0; int res = 0; //判断当前节点的 左子树是否是左叶子节点，如果是则将它的和累计起来 if(root.left != null &amp;&amp; root.left.left == null &amp;&amp; root.left.right == null){ res += root.left.val; } return sumOfLeftLeaves(root.left) + sumOfLeftLeaves(root.right) + res; }} 236. 二叉树的最近公共祖先题解：https://leetcode-cn.com/problems/lowest-common-ancestor-of-a-binary-tree/solution/236-er-cha-shu-de-zui-jin-gong-gong-zu-xian-hou-xu/ 123456789101112131415161718192021class Solution { /** 如果 root 是 p 和 q 的最近公共祖先,那么只有下面三种情况 1. p 和 q 分别在 root 的左右子树中 2. p = root，且q在root的左右子树中 3. q = root，且p在root的左右子树中 */ public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { // 递归截止条件 if(root == null) return root; if(root == p || root == q) return root; TreeNode l = lowestCommonAncestor(root.left , p , q); TreeNode r = lowestCommonAncestor(root.right , p , q); if(l == null) return r; else if(r == null) return l; return root; }} 二叉树的路径问题257. 二叉树的所有路径1234567891011121314151617181920212223242526272829class Solution { List&lt;String&gt; res = new ArrayList&lt;&gt;(); public List&lt;String&gt; binaryTreePaths(TreeNode root) { if(root == null) return res; StringBuilder sb = new StringBuilder(); dfs(root , sb); return res; } void dfs(TreeNode root , StringBuilder sb){ if(root == null) return; int n = sb.length(); sb.append(root.val); sb.append(&quot;-&gt;&quot;); if(root.left == null &amp;&amp; root.right == null){ sb.deleteCharAt(sb.length()-1); sb.deleteCharAt(sb.length()-1); res.add(new String(sb)); } dfs(root.left , sb); dfs(root.right , sb); sb.delete(n,sb.length()); //撤销选择，将这回合加入的全部删除 }} 112. 路径总和12345678class Solution { public boolean hasPathSum(TreeNode root, int targetSum) { if(root == null) return false; //注意递归到了 叶子节点，终止条件 if(root.left == null &amp;&amp; root.right == null) return root.val == targetSum; return hasPathSum(root.left , targetSum - root.val) || hasPathSum(root.right , targetSum - root.val); }} 113. 路径总和 II1234567891011121314151617181920212223242526class Solution { public List&lt;List&lt;Integer&gt;&gt; pathSum(TreeNode root, int targetSum) { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); if(root == null) return res; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); dfs(root , targetSum , list , res); return res; } void dfs(TreeNode root , int sum , List&lt;Integer&gt; list , List&lt;List&lt;Integer&gt;&gt; res){ if(root == null) return; // 递归截止条件 // 先将当前节点加入 sum -= root.val; list.add(root.val); // 判断是否满足 if(root.left == null &amp;&amp; root.right == null &amp;&amp; sum == 0){ res.add(new ArrayList&lt;&gt;(list)); // 注意这里不能return！！！ } dfs(root.left , sum , list , res); dfs(root.right , sum , list , res); list.remove(list.size()-1); // 注意回溯！！！ }} https://leetcode-cn.com/problems/diameter-of-binary-tree/solution/yi-pian-wen-zhang-jie-jue-suo-you-er-cha-6g00/ 543. 二叉树的直径12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Solution { //遍历每一个节点，以每一个节点为中心点计算最长路径（左子树深度+右子树深度），更新全局变量max。 int res = 0; public int diameterOfBinaryTree(TreeNode root) { if(root == null) return res; dfs(root); return res; } void dfs(TreeNode root){ if(root == null) return; int l = maxDepth(root.left); // 左子树深度 int r = maxDepth(root.right); // 右子树深度 res = Math.max(res , l + r); // 更新res ， 因为是边长，所以是 l+r dfs(root.left); dfs(root.right); } int maxDepth(TreeNode root){ if(root == null) return 0; return Math.max(maxDepth(root.left) , maxDepth(root.right)) + 1; }}class Solution { //遍历每一个节点，以每一个节点为 起始点 计算最长路径（左子树边长+右子树边长），更新全局变量max。 int res = 0; public int diameterOfBinaryTree(TreeNode root) { if(root == null) return res; dfs(root); return res; } // 以root为路径起始点的最长路径 int dfs(TreeNode root){ if(root == null) return; int l = root.left == null ? 0 : dfs(root.left) + 1; int r = root.right == null ? 0 : dfs(root.right) + 1; res = Math.max(res , l + r); // 更新res ， 因为是边长，所以是 l+r return Math.max(l , r); }} 124. 二叉树中的最大路径和和上面一题差不多，唯一的区别就是求的不是直径而是 路径和。 12345678910111213141516171819202122232425262728class Solution { //遍历所有节点，用一个 res 更新经过 ·每个节点的最大路径和· int res = Integer.MIN_VALUE; public int maxPathSum(TreeNode root) { /** 对于任意一个节点, 如果最大和路径包含该节点, 那么只可能是两种情况: 1. 其左右子树中所构成的和路径值较大的那个加上该节点的值后向父节点回溯构成最大路径 2. 左右子树都在最大路径中, 加上该节点的值构成了最终的最大路径 **/ if(root == null) return 0; dfs(root); // 用res 更新 root为根节点的树的最大路径和 return res; } // 返回 root的左子树和右子树 中的 较大路径和 int dfs(TreeNode root){ if(root == null) return 0; //分别找到root的左子树和右子树的最大路径和，如果是负数就不要了 int ls = Math.max(0 , dfs(root.left)); int rs = Math.max(0 , dfs(root.right)); // 更新res res = Math.max(res , ls + rs + root.val); // 判断在该节点包含左右子树的路径和是否大于当前最大路径和 return Math.max(ls , rs) + root.val; //返回经过当前节点向上回溯的最大路径和 }} 687. 最长同值路径12345678910111213141516171819class Solution { int res = 0; public int longestUnivaluePath(TreeNode root) { if(root == null) return 0; dfs(root , root.val); return res; } int dfs(TreeNode root , int val){ if(root == null) return 0; int l = dfs(root.left , root.val); int r = dfs(root.right , root.val); res = Math.max(res , l + r); // 因为要求同值, 所以在判断左右子树能构成的同值路径时要加入当前节点的值作为判断依据 if(root.val == val) return Math.max(l , r) + 1; return 0; }} 129. 求根节点到叶节点数字之和1234567891011121314151617181920class Solution { int res = 0; public int sumNumbers(TreeNode root) { if(root == null) return 0; dfs(root , 0); return res; } void dfs(TreeNode root , int sum){ if(root == null) return; sum = sum * 10 + root.val;//将当前数加上 // 如果到根节点了，就将 sum 加给res if(root.left == null &amp;&amp; root.right == null) res += sum; dfs(root.left , sum); dfs(root.right , sum); }} 114. 二叉树展开为链表先序遍历 12345678910111213141516171819202122class Solution { public void flatten(TreeNode root) { if(root == null) return ; //1. 先把 左子树 连到右子树上 TreeNode tem = root.right; root.right = root.left; //2. 记得把原来的左子树 置空 ！！！ root.left = null; //3. 找到最右边的节点，将之前root节点的右子树放过去 TreeNode cur = root; while(cur.right != null){ cur = cur.right; } cur.right = tem; //4. 递归的执行左右子树 flatten(root.left); flatten(root.right); }} 116. 填充每个节点的下一个右侧节点指针12345678910111213141516class Solution { public Node connect(Node root) { if(root == null || root.left == null) return root; //每个 node 左子树的 next , 就是 node 的右子树 //每个 node 右子树的 next, 就是 node next 的 左子树 root.left.next = root.right; if(root.next != null){ root.right.next = root.next.left; } connect(root.left); connect(root.right); return root; }} 117. 填充每个节点的下一个右侧节点指针 II上面一题也可以用下面的做法来做 1234567891011121314151617181920class Solution { public Node connect(Node root) { if(root == null) return root; Queue&lt;Node&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); root.next = null;//其实null不用显式的指 while (!queue.isEmpty()){ int count = queue.size(); while (count &gt; 0){ Node curNode = queue.poll(); count--; //如果size还大于0，说明这个一层后面还有节点，所以next指针要指向后面 if (count &gt; 0) curNode.next = queue.peek(); if (curNode.left != null) queue.add(curNode.left); if (curNode.right!= null) queue.add(curNode.right); } } return root; }} 863. 二叉树中所有距离为 K 的结点123456789101112131415161718192021222324252627282930313233343536373839class Solution { Map&lt;TreeNode,TreeNode&gt; map = new HashMap&lt;&gt;(); //1. 用一个map记录树中的每个节点的 父节点 Set&lt;TreeNode&gt; isv = new HashSet&lt;&gt;(); //2.用一个set来表示 节点是否被访问过 public List&lt;Integer&gt; distanceK(TreeNode root, TreeNode target, int k) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if(root == null) return res; getParents(root , null); //3, 从 target开始搜索 dfs(target , k , res); return res; } // 将 root 这颗树中的每个节点 和 父节点的对应关系都存在map中 void getParents(TreeNode root , TreeNode p){ if(root == null) return; map.put(root , p); getParents(root.left , root); getParents(root.right , root); } void dfs(TreeNode target , int k ,List&lt;Integer&gt; res){ if(target == null || isv.contains(target)) return; isv.add(target); //标记为已访问 if(k == 0){ res.add(target.val); return; } //从三个方向来搜索 dfs(target.left , k-1 , res); dfs(target.right , k-1 , res); dfs(map.get(target) , k-1 , res); }} 构造二叉树问题105. 从前序与中序遍历序列构造二叉树12345678910111213141516171819class Solution { //前序 ： 中左右 中序： 左中右 Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); public TreeNode buildTree(int[] preorder, int[] inorder) { for(int i = 0 ; i &lt; inorder.length ; i++) map.put(inorder[i] , i); return dfs(preorder , 0 , preorder.length-1 , inorder , 0 , inorder.length-1); } TreeNode dfs(int[] preorder , int ps , int pe , int[] inorder , int is , int ie){ if(ps &gt; pe || is &gt; ie) return null; TreeNode root = new TreeNode(preorder[ps]); int index = map.get(preorder[ps]); //中序遍历中root的位置 int l = index - is; root.left = dfs(preorder , ps + 1, ps + l , inorder , is , index-1); root.right = dfs(preorder , ps +1 +l , pe , inorder , index + 1 , ie); return root; }} 106. 从中序与后序遍历序列构造二叉树12345678910111213141516171819class Solution { //后序： 左右中 Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); public TreeNode buildTree(int[] inorder, int[] postorder) { for(int i = 0 ; i &lt; inorder.length ; i++) map.put(inorder[i] , i); return dfs(inorder , 0 , inorder.length-1, postorder , 0 , postorder.length-1); } TreeNode dfs(int[] inorder , int is , int ie , int[] postorder , int ps , int pe){ if(is &gt; ie || ps &gt; pe) return null; TreeNode root = new TreeNode(postorder[pe]); // 和上面一题就这里不太一样 int index = map.get(postorder[pe]); int l = index - is; root.left = dfs(inorder , is , index-1 , postorder , ps , ps+l-1); root.right = dfs(inorder , index+1 , ie , postorder , ps+l, pe-1); return root; }} 108. 将有序数组转换为二叉搜索树12345678910111213141516class Solution { public TreeNode sortedArrayToBST(int[] nums) { if(nums == null || nums.length == 0) return null; int n = nums.length; return dfs(nums , 0 , n-1); } TreeNode dfs(int[] nums , int l , int r){ if(l &gt; r) return null; int mid = l + r &gt;&gt; 1; TreeNode root = new TreeNode(nums[mid]); root.left = dfs(nums , l , mid-1); root.right = dfs(nums , mid+1 , r); return root; }} 109. 有序链表转换二叉搜索树123456789101112131415161718192021class Solution { public TreeNode sortedListToBST(ListNode head) { if(head == null) return null; return dfs(head , null); } TreeNode dfs(ListNode head , ListNode end){ // 递归截止条件 if(head == end) return null; ListNode fast = head; ListNode slow = head; while(fast.next != end &amp;&amp; fast.next.next != end){ fast = fast.next.next; slow = slow.next; } TreeNode root = new TreeNode(slow.val); root.left = dfs(head , slow); root.right = dfs(slow.next , end); return root; }} 二叉搜索树98. 验证二叉搜索树123456789101112131415class Solution { public boolean isValidBST(TreeNode root) { if(root == null) return true; // 用long的最小值和最大值作为整个BST的最小值和最大值 return validate(root , Long.MIN_VALUE , Long.MAX_VALUE); } boolean validate(TreeNode root , long min , long max){ if(root == null) return true; //当前节点不满足要求 if(root.val &lt;= min || root.val &gt;= max) return false; //递归的看 左右子节点是否满足要求 return validate(root.left , min , root.val) &amp;&amp; validate(root.right , root.val , max); }} 99. 恢复二叉搜索树123456789101112131415161718192021222324class Solution { // 中序遍历，用t1 和 t2 来记录下这个逆序对 TreeNode t1 , t2 , pre; public void recoverTree(TreeNode root) { if(root == null) return; inOrder(root); int tem = t1.val; t1.val = t2.val; t2.val = tem; } public void inOrder(TreeNode root){ if(root == null) return; inOrder(root.left); // 如果前面一个不为null且大于当前的节点值，说明逆序对出现了 if(pre != null &amp;&amp; pre.val &gt; root.val){ if(t1 == null) t1 = pre; t2 = root; } // 更新pre pre = root; inOrder(root.right); }} 230. 二叉搜索树中第K小的元素1234567891011121314151617181920class Solution { int res = 0; int count = 0; public int kthSmallest(TreeNode root, int k) { dfs(root , k); return res; } void dfs(TreeNode root , int k){ if(root == null) return; dfs(root.left , k); if(++count == k){ res = root.val; return; } dfs(root.right , k); }}//todo 不能直接用k-- =0 来判断，为啥？ 剑指 Offer 54. 二叉搜索树的第k大节点1234567891011121314151617181920212223242526class Solution { int res = 0; int count = 0; public int kthLargest(TreeNode root, int k) { if(root == null) return 0; dfs(root , k); return res; } // 注意是 第k大 ，应该是中序遍历反过来 void dfs(TreeNode root , int k){ if(root == null) return; dfs(root.right , k); count++; if(k == count){ res = root.val; return; } dfs(root.left , k); }} 235. 二叉搜索树的最近公共祖先123456789class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if(root == null) return root; if(root.val &lt; p.val &amp;&amp; root.val &lt; q.val) return lowestCommonAncestor(root.right , p , q); else if(root.val &gt; p.val &amp;&amp; root.val &gt; q.val) return lowestCommonAncestor(root.left , p ,q); else return root; }} 剑指 Offer 36. 二叉搜索树与双向链表123456789101112131415161718192021222324252627class Solution { Node pre , head; public Node treeToDoublyList(Node root) { if(root == null) return null; dfs(root); // 题目要求 头尾指针相连 head.left = pre; pre.right = head; return head; } void dfs(Node root){ if(root == null) return; dfs(root.left); // 如果前面一个节点是 null ，说明当前节点是头节点，用head保存起来 if(pre == null) head = root; else pre.right = root; // 否则就将前面一个节点的right 指针指向当前节点 root.left = pre; // 将当前节点的left节点指向前一个节点 pre = root; // 更新pre指针 dfs(root.right); }} 538. 把二叉搜索树转换为累加树12345678910111213141516171819202122232425262728```# 完全二叉树问题#### [222. 完全二叉树的节点个数](https://leetcode-cn.com/problems/count-complete-tree-nodes/)```javaclass Solution { public int countNodes(TreeNode root) { if(root == null) return 0; //完全二叉树的高度可以直接通过不断地访问左子树就可以获取 int l = getDepth(root.left); int r = getDepth(root.right); // 左右子树高度相同，说明左子树必满 则节点数=左子树节点 + root节点(=1) + 递归找右子树 // 1 &lt;&lt; l 左移l位表示 x2 乘了l次 if(l == r) return (1 &lt;&lt; l) + countNodes(root.right); else return (1 &lt;&lt; r) + countNodes(root.left); } // int getDepth(TreeNode root){ if(root == null) return 0; return Math.max(getDepth(root.left) , getDepth(root.right)) + 1; }} 958. 二叉树的完全性检验123456789101112131415161718192021class Solution { public boolean isCompleteTree(TreeNode root) { if(root == null) return true; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); boolean flag = false; while(!queue.isEmpty()){ TreeNode curNode = queue.poll(); if(curNode == null){ flag = true; continue; } //代码能到这里说明 前面遇到了 null 之后又开始遍历了，如果是完全二叉树遇到null之后应该就没有了 if(flag) return false; queue.add(curNode.left); queue.add(curNode.right); } return true; }}","link":"/2021/10/10/%E7%AE%97%E6%B3%95%E9%A2%98%EF%BC%88%E4%BA%94%EF%BC%89%E6%A0%91%E7%BB%93%E6%9E%84/"},{"title":"算法题（七）数组","text":"数组专题 数和问题1. 两数之和123456789101112class Solution { public int[] twoSum(int[] nums, int target) { int n = nums.length; if(n &lt; 2) return new int[0]; Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); for(int i = 0 ; i &lt; n ; i++){ if(map.containsKey(target - nums[i])) return new int[]{i , map.get(target - nums[i])}; map.put(nums[i] , i); } return new int[0]; }} 15. 三数之和1234567891011121314151617181920212223class Solution { public List&lt;List&lt;Integer&gt;&gt; threeSum(int[] nums) { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); Arrays.sort(nums); //排序为了去重 for(int i = 0 ; i &lt; nums.length ; i++){ if(i &gt; 0 &amp;&amp; nums[i] == nums[i-1]) continue; //去重i int L = i+1; int R = nums.length-1; while(L &lt; R){ int sum = nums[i] + nums[L] + nums[R]; if(sum == 0){ res.add(Arrays.asList(nums[i] , nums[L] , nums[R])); while(L &lt; R &amp;&amp; nums[L] == nums[L+1]) L++;//去重L while(L &lt; R &amp;&amp; nums[R] == nums[R-1]) R--;//去重R L++; R--; }else if(sum &lt; 0) L++; else R--; } } return res; }} 16. 最接近的三数之和12345678910111213141516171819202122class Solution { public int threeSumClosest(int[] nums, int target) { int n = nums.length; if(n &lt; 3) return 0; Arrays.sort(nums); int res = nums[0] + nums[1] + nums[2]; //注意res的初始值 for(int i = 0 ; i &lt; n ; i++){ if(i &gt; 0 &amp;&amp; nums[i] == nums[i-1]) continue; int L = i+1; int R = n-1; while(L &lt; R){ int sum = nums[i] + nums[R] + nums[L]; if(sum == target) return sum; else if(sum &gt; target) R--; else L++; if(Math.abs(sum - target) &lt; Math.abs(res - target)) res = sum; //更新res } } return res; }} 18. 四数之和1234567891011121314151617181920212223242526272829class Solution { public List&lt;List&lt;Integer&gt;&gt; fourSum(int[] nums, int target) { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); if(nums == null || nums.length &lt; 4) return res; Arrays.sort(nums); // 注意1：要排序 int n = nums.length; for(int i = 0 ; i &lt; n ; i++){ if(i &gt; 0 &amp;&amp; nums[i] == nums[i-1]) continue; //注意2： 要去重 for(int j = n-1 ; j &gt;= 0 ;j--){ if(j &lt; n-1 &amp;&amp; nums[j] == nums[j+1]) continue; int L = i+1; int R = j-1; while(L &lt; R){ int sum = nums[i] + nums[j] + nums[L] + nums[R]; if(sum == target){ res.add(Arrays.asList(nums[i] , nums[L] , nums[R] , nums[j])); while(L &lt; R &amp;&amp; nums[L] == nums[L+1]) L++; //注意3：要去重 while(L &lt; R &amp;&amp; nums[R] == nums[R-1]) R--; L++; R--; }else if(sum &lt; target) L++; else R--; } } } return res; }} 数对和https://leetcode-cn.com/problems/pairs-with-sum-lcci/ 1234567891011121314151617181920212223class Solution { public List&lt;List&lt;Integer&gt;&gt; pairSums(int[] nums, int target) { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); //1.先排序，然后双指针 Arrays.sort(nums); int n = nums.length; if(n &lt; 2) return res; int l = 0 , r = n-1; while(l &lt; r){ int sum = nums[l] + nums[r]; if(sum &lt; target) l++; else if(sum &gt; target) r--; else { res.add(Arrays.asList(nums[l] , nums[r])); l++; r--; } } return res; }} 数组加1https://leetcode-cn.com/problems/plus-one/ 1234567891011121314151617181920class Solution { public int[] plusOne(int[] digits) { // 从后往前遍历，找到第一个不是9的数字，进行+1 // 如果遍历完了还有就说明全是9 int n = digits.length; for(int i = n-1 ; i &gt;= 0 ; i--){ if(digits[i] != 9){ digits[i]++; return digits; } digits[i] = 0; } int[] res = new int[n+1]; res[0] = 1; return res; }} 原地哈希思想一般都会告诉你数组中的元素大小范围为 1 ≤ a[i] ≤ n ( n = 数组大小 ) : 交换 i 和 nums[i]-1 287. 寻找重复数1234567891011121314151617181920212223class Solution { public int findDuplicate(int[] nums) { int n = nums.length; if(n == 0) return 0; // 二分查找 1 - n-1 之间的某个数 （数组元素从1开始的） int l = 1 , r = n-1; while(l &lt; r){ int mid = l + r &gt;&gt; 1; // 看数组中右多少个数小于等于mid，计算个数 int count = 0; for(int num : nums){ if(num &lt;= mid) count++; } // 根据抽屉原理，小于等于 4 的个数如果严格大于 4 个，此时重复元素一定出现在 [1..4] 区间里 if(count &gt; mid) r = mid; else l = mid+1; } return l; }} https://leetcode-cn.com/problems/search-insert-position/solution/te-bie-hao-yong-de-er-fen-cha-fa-fa-mo-ban-python-/ 41. 缺失的第一个正数nums[i]-1 和 i 交换位置的话，最后得到的就是 1 2 3 4 … 1234567891011121314151617181920212223class Solution { public int firstMissingPositive(int[] nums) { int n = nums.length; //[3,4,-1,1] //把大于等于1的和小于数组大小的值放到原数组对应位置 for(int i = 0 ; i &lt; n ; i++){ while(nums[i] &gt;= 1 &amp;&amp; nums[i] &lt;= n &amp;&amp; nums[i] != nums[nums[i]-1]) swap(nums , i , nums[i]-1); } //[1,-1,3,4] for(int i = 0 ; i &lt; n ; i++){ if(nums[i] != i+1) return i+1; } return n+1; } void swap(int[] nums , int a , int b){ int tem = nums[a]; nums[a] = nums[b]; nums[b] = tem; }} 442. 数组中重复的数据123456789101112131415161718192021222324252627// 数组中的数据范围在 [1-N]之间，所以比较的时候 是 i 和 nums[i]-1 比较class Solution { public List&lt;Integer&gt; findDuplicates(int[] nums) { // [4,3,2,7,8,2,3,1] List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); int n = nums.length; if(n == 0) return res; for(int i = 0 ; i &lt; n ; i++){ while(nums[i] != nums[nums[i]-1]) swap(nums , i , nums[i]-1); } // 排序完 stdout [1, 2, 3, 4, 3, 2, 7, 8] //System.out.println(Arrays.toString(nums)); for(int i = 0 ; i &lt; n ;i++){ if(nums[i] != i+1) res.add(nums[i]); } return res; } void swap(int[] nums , int a , int b){ int tem = nums[a]; nums[a] = nums[b]; nums[b] = tem; }} 剑指 Offer 03. 数组中重复的数字12345678910111213141516171819202122class Solution { public int findRepeatNumber(int[] nums) { int n = nums.length; for(int i = 0 ; i &lt; n ; i++){ while(nums[i] != nums[nums[i]]) swap(nums , nums[i] , i); } for(int i = 0 ; i &lt; n ; i++){ if(nums[i] != i) return nums[i]; } return 0; } void swap(int[] nums , int a ,int b){ int tem = nums[a]; nums[a] = nums[b]; nums[b] = tem; }} 268. 丢失的数字123456789101112131415161718192021222324// 数组中的数据范围在 [0-N]之间，所以比较的时候 是 i 和 nums[i] 比较 class Solution { public int missingNumber(int[] nums) { int n = nums.length; if(n == 0) return 0; // 原地哈希排序 for(int i = 0 ; i &lt; n ; i++){ while(nums[i] &lt; n &amp;&amp; nums[i] != nums[nums[i]]) swap(nums , i , nums[i]); } for(int i = 0 ; i &lt; n ; i++){ if(nums[i] != i) return i; } return n; } void swap(int[] nums , int a , int b){ int tem = nums[a]; nums[a] = nums[b]; nums[b] = tem; }} 寻找唯一一个重复的数https://leetcode-cn.com/problems/find-the-duplicate-number/ 【1，n】 12345678910111213141516171819class Solution { public int findDuplicate(int[] nums) { int n = nums.length; for(int i = 0 ; i &lt; n ; i++){ while(nums[i] != nums[nums[i]-1]) swap(nums , i , nums[i]-1); } for(int i = 0 ; i &lt; n ; i++){ if(nums[i] != i+1) return nums[i]; } return 0; } void swap(int[] nums , int a , int b){ int tem = nums[a]; nums[a] = nums[b]; nums[b] = tem; }} 7.找到数组中消失的数字https://leetcode-cn.com/problems/find-all-numbers-disappeared-in-an-array/ 123456789101112131415161718192021222324252627class Solution { public List&lt;Integer&gt; findDisappearedNumbers(int[] nums) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); int n = nums.length; for(int i = 0 ; i &lt; n ; i++){ //题目中已经说了 1 ≤ a[i] ≤ n （n为数组长度） // 8 3 2 7 4 2 3 1 // 1 3 2 7 4 2 3 8 // 1 2 3 7 4 2 3 8 // 1 2 3 4 7 2 3 8 // 1 2 3 4 3 2 7 8 // 1 2 3 4 3 2 7 8 // 发现相同，加入 while(nums[nums[i] - 1] != nums[i]) swap(nums , i , nums[i]-1); } for(int i = 0 ; i &lt; n ; i++){ if(nums[i] != i+1) res.add(i+1); } return res; } void swap(int[] nums , int a , int b){ int tem = nums[a]; nums[a] = nums[b]; nums[b] = tem; }} https://leetcode-cn.com/problems/missing-number/ 【0-n】 12345678910111213141516171819202122class Solution { public int missingNumber(int[] nums) { int n = nums.length; for(int i = 0 ; i &lt; n ; i++){ //数字是在[0-n]之间的原地哈希条件 while(i != nums[i] &amp;&amp; nums[i] &lt; nums.length){ swap(nums, i, nums[i]); } } for(int i = 0; i &lt; n ; i++){ if(nums[i] != i) return i; } return n; } void swap(int[] nums , int a , int b){ int tem = nums[a]; nums[a] = nums[b]; nums[b] = tem; }} 子数组问题53. 最大子序和12345678910111213class Solution { public int maxSubArray(int[] nums) { //如果当前数前面的数总和 &gt; 0 ,则加上前面的数的总和 int res = Integer.MIN_VALUE; int sum = 0; for(int num : nums){ if(sum &gt; 0) sum += num; else sum = num; res = Math.max(res,sum); } return res; }} 前缀和 或者 滑动窗口 前缀和思想560. 和为 K 的子数组123456789101112131415161718192021class Solution { public int subarraySum(int[] nums, int k) { int n = nums.length; if(n == 0) return 0; //1. 用一个Map来记录数组的前缀和 , key为前缀和 ，value为出现的次数 Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); //2. 和为0的连续子数组出现1次,不能忘记 map.put(0,1); int sum = 0 , res = 0; for(int i = 0 ; i &lt; nums.length ; i++){ sum += nums[i]; //3. 当前前缀和为 sum , 所有如果 map中的前缀和已经有 sum - k 了， 直接加起来即可 if(map.containsKey(sum - k)) res += map.get(sum - k); //4. 更新当前前缀和 map.put(sum , map.getOrDefault(sum , 0) + 1); } return res; }} 10.和可以被K整除的子数组个数https://leetcode-cn.com/problems/subarray-sums-divisible-by-k/ 12345678910111213141516class Solution { public int subarraysDivByK(int[] nums, int k) { Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); map.put(0 , 1); int res = 0; int sum = 0; for(int i = 0; i &lt; nums.length; i++){ sum += nums[i]; ////当前 presum 与 K的关系，余数是几，当被除数为负数时取模结果为负数，需要纠正 int mod = (sum % k + k) % k; if(map.containsKey(mod)) res += map.get(mod); map.put(mod , map.getOrDefault(mod , 0) + 1); } return res; }} 11.统计优美子数组个数https://leetcode-cn.com/problems/count-number-of-nice-subarrays/ 如果某个 连续 子数组中恰好有 k 个奇数数字，我们就认为这个子数组是「优美子数组」 123456789101112131415class Solution { public int numberOfSubarrays(int[] nums, int k) { //将前缀区间的奇数个数保存到区间内即可 Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); int res = 0; int count = 0; map.put(0,1); for(int num : nums){ count += num &amp; 1; //奇数+1 if(map.containsKey(count - k)) res += map.get(count - k); map.put(count , map.getOrDefault(count , 0) + 1); } return res; }} 12.和为NK的子数组https://leetcode-cn.com/problems/continuous-subarray-sum/ 123456789101112131415161718class Solution { public boolean checkSubarraySum(int[] nums, int k) { Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); map.put(0,-1); //key 为前缀和%k ， value 为索引的位置 int sum = 0; for(int i = 0 ; i &lt; nums.length ; i++){ //另外一个就是之前我们都是统计个数，value 里保存的是次数，但是此时我们加了一个条件就是长度至少为 2，保存的是索引，所以我们不能继续 map.put(0,1)，应该赋初值为 map.put(0,-1)。这样才不会漏掉一些情况，例如我们的数组为[2,3,4],k = 1,当我们 map.put(0,-1) 时，当我们遍历到 nums[1] 即 3 时，则可以返回 true，因为 1-（-1）= 2，5 % 1=0 , 同时满足 sum += nums[i]; int mod = sum % k; if(map.containsKey(mod)){ if(i - map.get(mod) &gt; 1) return true; }else map.put(mod , i); } return false; }} 209. 长度最小的子数组滑动窗口 1234567891011121314151617181920class Solution { public int minSubArrayLen(int target, int[] nums) { if(nums == null || nums.length == 0) return 0; int n = nums.length; int l = 0 , r = 0; int sum = 0; int res = 0; while(r &lt; n){ sum += nums[r]; while(sum &gt;= target){ res = res == 0 ? (r - l + 1) : Math.min(res , r - l+1); sum -= nums[l++]; } r++; // 注意 r++ 必须要放到最后 } return res; }} 581. 最短无序连续子数组12345678910111213141516171819202122232425class Solution { public int findUnsortedSubarray(int[] nums) { int n = nums.length; if(n &lt; 2) return 0; int max = nums[0]; int min = nums[n-1]; int l = 0 , r = 0; for(int i = 0 ; i &lt; n ; i++){ // 从左往右扫描，记录最大值，如果 nums[i] &lt; max, 则说明 i 需要调整 max = Math.max(max , nums[i]); if(nums[i] &lt; max) r = i; //从右到左循环，记录最小值为 min, 若 nums[i] &gt; min, 则表明位置 i 需要调整 min = Math.min(min , nums[n-i-1]); if(nums[n-1-i] &gt; min) l = n-1-i; } return r &gt; l ? r-l+1 : 0; }} 718. 最长重复子数组1234567891011121314151617181920212223class Solution { public int findLength(int[] nums1, int[] nums2) { int l1 = nums1.length; int l2 = nums2.length; // dp[i][j] ：以下标i - 1为结尾的A，和以下标j - 1为结尾的B，最长重复子数组长度 int[][] dp = new int[l1+1][l2+1]; dp[0][0] = 0; int res = 0; // 用res记录 for(int i = 1 ; i &lt;= l1 ; i++){ for(int j = 1; j &lt;= l2 ; j++){ if(nums1[i-1] == nums2[j-1]) dp[i][j] = dp[i-1][j-1] + 1; else dp[i][j] = 0; // 更新res res = Math.max(res , dp[i][j]); } } return res; }} 128. 最长连续序列12345678910111213141516171819202122class Solution { public int longestConsecutive(int[] nums) { int res = 0; // 1. 先把所有数放到一个Set中去 Set&lt;Integer&gt; set = new HashSet&lt;&gt;(); for(int num : nums) set.add(num); int count = 0; for(int num : nums){ // 如果 set 中不包含 num-1 ， 说明序列从 num开始 if(!set.contains(num-1)){ while(set.contains(num)){ count++; num++; } res = Math.max(res, count); count = 0; // 记得还原 count } } return res; }} 674. 最长连续递增子数组1234567891011121314151617class Solution { public int findLengthOfLCIS(int[] nums) { if(nums == null || nums.length == 0) return 0; int res = 0; int count = 0; for(int i = 0 ; i &lt; nums.length ; i++){ int j = i + 1; while(j &lt; nums.length &amp;&amp; nums[j] &gt; nums[j-1]) j++; count += (j-i); res = Math.max(res , count); count = 0; } return res; }} 907. 所有子数组的最小值之和123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657//中心扩展法//1. 将数组的每一个元素视为最小值，找左右边界//2. 找到边界后，求区间个数// 区间个数 = 左区间宽度 + 右区间宽度 + 左区间宽度*右区间宽度 + 1（元素本身）public int sumSubarrayMins(int[] arr) { int mod = 1_000_000_007 ; int n = arr.length ; long count = 0 ; for( int i = 0 ; i &lt; n ; i++ ){ int l = i ; while( l-1 &gt;= 0 &amp;&amp; arr[l-1] &gt; arr[i] ){ //关键：这里不能有等号， 解决这种情况[71,55,82,55] l-- ; } int r = i ; while( r+1 &lt; n &amp;&amp; arr[i] &lt;= arr[r+1] ){ r++ ; } //r-i + i-l + r-i * i-l + 1 long temp = r-l+(r-i)*(i-l)+1 ; count += arr[i]*temp ; count %= mod ; } return (int)count ;}哪里不对？class Solution { //将数组的每一个元素视为最小值，找左右边界 public int sumSubarrayMins(int[] nums) { if(nums == null || nums.length == 0) return 0; int n = nums.length; long count = 0; for(int i = 0 ; i &lt; n ; i++){ // 找到当前数左边第一个比当前数小的位置，这里不能相等解决这种情况[71,55,82,55] int l = i; while(l - 1 &gt;= 0 &amp;&amp; nums[l-1] &gt; nums[i]) l--; // 找到当前数右边第一个比当前数小的位置 ， 注意可以 == int r = i; while(r &lt; n-1 &amp;&amp; nums[r+1] &gt;= nums[i]) r++; //区间个数:区间个数 = 左区间宽度 + 右区间宽度 + 左区间宽度*右区间宽度 + 1（元素本身） long tem = (i - l) + (r - i) + (i-l) * (r-i) + 1; count += nums[i] * tem; } return (int)count % (10^9 + 7); }} 1013. 将数组分成和相等的三个部分1234567891011121314151617181920212223class Solution { public boolean canThreePartsEqualSum(int[] nums) { if(nums == null || nums.length == 0) return true; int sum = 0; for(int num : nums) sum += num; if(sum % 3 != 0) return false; sum = sum / 3; int curSum = 0 , count = 0; // 注意 ： i只能到倒数第二个元素，保证有3段 for(int i = 0 ; i &lt; nums.length-1 ; i++){ curSum += nums[i]; if(curSum == sum){ count++; if(count == 2) return true; curSum = 0; } } return false; }} 双指针思想11. 盛最多水的容器123456789101112131415161718192021class Solution { public int maxArea(int[] height) { int n = height.length; if(n &lt; 2) return 0; int l = 0 , r = n-1; int res = 0; while(l &lt; r){ // 注意根据height[l] 和 height[r]的大小关系来判断后续是对l还是r指针的变动 if(height[l] &lt; height[r]){ res = Math.max(res , (r - l) * height[l]); l++; }else { res = Math.max(res , (r - l) * height[r]); r--; } } return res; }} 单调栈84. 柱状图中最大的矩形123456789101112131415161718192021222324252627282930313233class Solution { public int largestRectangleArea(int[] heights) { //O(1) 的获取柱体 i 左边第一个比它小的柱体吗？答案就是单调增栈 // 思路是以当前值为高度，找到左边第一个比他小的柱体，然后更新面积 // 利用 单调栈找到左右两边第一个比当前数小的数 int n = heights.length; if(n == 0) return 0; int res = 0; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); //单调栈,栈顶最大，栈底最小 for(int i = 0 ; i &lt; n ; i++){ while(!stack.empty() &amp;&amp; heights[stack.peek()] &gt;= heights[i]){ int index = stack.pop(); int l = stack.empty() ? -1 : stack.peek(); //如果弹出之后栈是空的，说明可以一直到-1的位置，否则就是弹出之后的栈顶元素 res = Math.max(res , (i-l-1)*heights[index]); // i是右边界 } stack.push(i); } //for循环结束之后，表示数组中的所有元素全部压入过数组了 //下面的while是结算栈中还剩余的元素 while(!stack.isEmpty()){ int index = stack.pop(); int l = stack.isEmpty() ? -1 : stack.peek(); res = Math.max(res , (n - l -1) * heights[index]); } return res; }} 85. 最大矩形12345678910111213141516171819202122232425262728293031323334353637383940class Solution { public int maximalRectangle(char[][] matrix) { int row = matrix.length; if(row == 0) return 0; int col = matrix[0].length; int[] nums = new int[col]; int res = 0; for(int i = 0 ; i &lt; row ; i++){ for(int j = 0 ; j &lt; col ; j ++){ if(matrix[i][j] == '1') nums[j] += 1; else nums[j] = 0; } res = Math.max(res , help(nums)); // 注意是每一行 } return res; } int help(int[] nums){ Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); int res = 0; for(int i = 0 ; i &lt; nums.length ; i++){ while(!stack.empty() &amp;&amp; nums[stack.peek()] &gt; nums[i]){ int j = stack.pop(); int l = stack.empty() ? -1 : stack.peek(); res = Math.max(res , nums[j] * (i - l - 1)); } stack.push(i); } int n = nums.length; while(!stack.empty()){ int index = stack.pop(); int l = stack.empty() ? -1 : stack.peek(); res = Math.max(res , (n - l -1 ) * nums[index]); } return res; }} 739. 每日温度1234567891011121314151617181920class Solution { public int[] dailyTemperatures(int[] nums) { int n = nums.length; int[] res = new int[n]; res[n-1] = 0; // 本质上是找 nums 中每一个元素后面较大的元素 // 用一个单调递增栈 Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); for(int i = 0 ; i &lt; n ; i++){ while(!stack.empty() &amp;&amp; nums[stack.peek()] &lt; nums[i]){ int j = stack.pop(); res[j] = i - j; } stack.push(i); } return res; }} 496. 下一个更大元素 I1 区间问题56. 合并区间1234567891011121314151617181920212223242526272829class Solution { public int[][] merge(int[][] intervals) { //1. 将数组按照第一个元素排序 Arrays.sort(intervals , (a, b) -&gt; (a[0] - b[0])); int n = intervals.length; if(n == 0) return new int[0][0]; //2. 先用一个list方便收集数组 List&lt;int[]&gt; list = new ArrayList&lt;&gt;(); //3. 用cur指针指向第一个数组开始遍历 int[] cur = intervals[0]; for(int i = 1; i &lt; n ; i++){ //4. 说明不需要合并 if(cur[1] &lt; intervals[i][0]){ list.add(cur); cur = intervals[i]; }else cur[1] = Math.max(cur[1] , intervals[i][1]);//5. 更新右边界 } list.add(cur); //6. 将最后一个数组也加进入 //7. 格式转换 int[][] res = new int[list.size()][2]; for(int i = 0 ; i &lt; list.size() ; i++) res[i] = list.get(i); return res; }} 57. 插入区间1234567891011121314151617181920212223242526272829class Solution { public int[][] insert(int[][] intervals, int[] newInterval) { int n = intervals.length; int[][] nums = new int[n+1][2]; for(int i = 0 ; i &lt; n ; i++) nums[i] = intervals[i]; nums[n] = newInterval; Arrays.sort(nums , (a,b) -&gt; a[0] - b[0]); List&lt;int[]&gt; list = new ArrayList&lt;&gt;(); int[] cur = nums[0]; for(int i = 1; i &lt;= n ; i++){ if(cur[1] &lt; nums[i][0]){ list.add(cur); cur = nums[i]; }else cur[1] = Math.max(cur[1] , nums[i][1]); } list.add(cur); int[][] res = new int[list.size()][2]; for(int i = 0 ; i &lt; list.size() ;i++) res[i] = list.get(i); return res; }} 数组排序问题912. 排序数组12345678910111213141516171819202122232425262728293031323334class Solution { public int[] sortArray(int[] nums) { // 快速排序 quickSort(nums , 0 , nums.length-1); return nums; } void quickSort(int[] nums , int l , int r){ if(l &gt;= r) return; int[] p = partition(nums , l , r); quickSort(nums , l , p[0]-1); quickSort(nums , p[1]+1 , r); } int[] partition(int[] nums , int l , int r){ int less = l - 1; int more = r; int cur = l; while(cur &lt; more){ if(nums[cur] &lt; nums[r]) swap(nums , ++less , cur++); else if(nums[cur] &gt; nums[r]) swap(nums , --more , cur); else cur++; } swap(nums , r , more); return new int[]{less+1 , more}; } void swap(int[] nums, int a , int b){ int tem = nums[a]; nums[a] = nums[b]; nums[b] = tem; }} 215. 数组中的第K个最大元素1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Solution { public int findKthLargest(int[] nums, int k) { //1. 用堆来做 Queue&lt;Integer&gt; heap = new PriorityQueue&lt;&gt;(); for(int num : nums){ heap.add(num); if(heap.size() &gt; k) heap.poll(); } return heap.poll(); }}class Solution { public int findKthLargest(int[] nums, int k) { //2. 用快排来做 int n = nums.length; quickSort(nums , n-k); // 排序前面 n-k个数 return nums[n - k]; // 第k大其实就是 第 n-k 小 } void quickSort(int[] nums , int k){ int l = 0, r = nums.length-1; while(l &lt; r){ int mid = partition(nums , l , r); if(mid &lt; k) l = mid+1; else if(mid &gt; k) r = mid-1; else break; } } int partition(int[] nums, int l , int r){ int less = l-1; int more = r; // more 就是r，因为以 nums[r] 为比较 int cur = l; while(cur &lt; more){ // 以 nums[r] 为比较值 , 如果当前值比 nums[r]小， 则和 ++less 交换，交换后cur++ if(nums[cur] &lt; nums[r]) swap(nums , ++less , cur++); else if(nums[cur] &gt; nums[r]) swap(nums , cur , --more); // 和 --more 交换，交换后cur不变，需要再次判断 else cur++; } // 记得交换最后一个值,和more交换。 这样 就得到 123345 swap(nums , r , more); return less+1; } void swap(int[] nums , int a , int b){ int tem = nums[a]; nums[a] = nums[b]; nums[b] = tem; }} 75. 颜色分类1234567891011121314151617181920212223242526class Solution { public void sortColors(int[] nums) { int n = nums.length; if(n == 0 || n == 1) return; // 用快排的partition过程来解决 int l = 0 , r = nums.length-1; int less = l - 1 , more = r+1; // 以1为分界值，大于1的放右边，小于1的放左边 int cur = l; while(cur &lt; more){ if(nums[cur] &lt; 1){ swap(nums , ++less , cur++); }else if(nums[cur] &gt; 1){ swap(nums , --more , cur); }else cur++; } } void swap(int[] nums , int a , int b){ int tem = nums[a]; nums[a] = nums[b]; nums[b] = tem; }} 414. 第三大的数123456789101112131415161718192021class Solution { // 用3个变量来保存 最大的三个数，动态的变化 public int thirdMax(int[] nums) { long i = Long.MIN_VALUE; //1.假设 a &gt; b &gt; c long a = i , b = i , c = i; for (int num : nums){ if(num &gt; a){ //干掉a c = b ; b = a ; a = num; }else if (num &lt; a &amp;&amp; num &gt; b){ //干掉b c = b ; b = num; }else if (num &lt; b &amp;&amp; num &gt; c){ //干掉c c = num; } } return c == i ? (int)a : (int)c; }} 1464. 数组中两元素的最大乘积12345678910111213141516class Solution { public int maxProduct(int[] nums) { // 求出数组中最大的两个数，分别用a、b表示 int a = Integer.MIN_VALUE; int b = Integer.MIN_VALUE; for(int num : nums){ if(num &gt; a){ b = a; a = num; }else if(num &gt; b) b = num; } return (a-1)*(b-1); }} 628. 三个数的最大乘积12345678910111213class Solution { public int maximumProduct(int[] nums) { // 最大的三个数的乘积 = 最大的三个正数 或者 最大的一个正数 + 最小的两个负数 if(nums == null || nums.length == 0) return -1; int res = Integer.MIN_VALUE; int n = nums.length; Arrays.sort(nums); res = Math.max(nums[n-1] * nums[n-2] * nums[n-3] , nums[0] * nums[1] * nums[n-1]); return res; }} 238. 除自身以外数组的乘积 1234&gt;原数组： [1 2 3 4]&gt;左部分的乘积： 1 1 1*2 1*2*3&gt;右部分的乘积： 2*3*4 3*4 4 1&gt;结果： 1*2*3*4 1*3*4 1*2*4 1*2*3*1 12345678910111213141516171819202122class Solution { public int[] productExceptSelf(int[] nums) { int n = nums.length; int[] res = new int[n]; //1. 第一次遍历求 左边的数的乘积 int l = 1 , r = 1; for(int i = 0 ; i &lt; n ;i++){ res[i] = l; l *= nums[i]; } //2. 第二次遍历，求右边的数乘积 for(int i = n-1 ; i &gt; 0 ; i--){ r *= nums[i]; res[i-1] *= r; } return res; }} 面试题 16.06. 最小差1234567891011121314151617class Solution { public int smallestDifference(int[] a, int[] b) { Arrays.sort(a); Arrays.sort(b); int res = Integer.MAX_VALUE; int i = 0 , j = 0; int la = a.length, lb = b.length; while(i &lt; la &amp;&amp; j &lt; lb){ ////使用 long，防止 -2147483648 转正数后还是 -2147483648 long tem = a[i] - b[j]; res = (int) Math.min(res, Math.abs(tem)); if(a[i] &lt; b[j]) i++; else j++; } return (int)res; }} 905. 按奇偶排序数组12345678910111213141516171819202122class Solution { public int[] sortArrayByParity(int[] nums) { int n = nums.length; if(n &lt; 2) return nums; int l = 0 , r = n-1; while( l &lt; r){ // //左指针对应奇数值，右指针对应偶数值，进行交换 if(nums[l] % 2 == 1 &amp;&amp; nums[r] % 2 == 0) swap(nums , l , r); else if(nums[l] % 2 == 0) l++; //左指针对应的是偶数值，符合题意，继续向右移动 else if(nums[r] % 2 == 1) r--; //右指针对应的是奇数值，符合题意，继续向左移动 } return nums; } void swap(int[] nums , int a , int b){ int tem = nums[a]; nums[a] = nums[b]; nums[b] = tem; }} 922. 按奇偶排序数组 II12345678910111213141516171819202122232425262728293031323334353637383940class Solution { public int[] sortArrayByParityII(int[] nums) { int n = nums.length; if(n == 0) return nums; int a = 1; //表示奇数下标 int b = 0; //表示偶数下标 int[] res = new int[n]; for(int i = 0 ; i &lt; n ; i++){ if(nums[i] % 2 == 1){ res[a] = nums[i]; a += 2; }else{ res[b] = nums[i]; b += 2; } } return res; }}class Solution { public int[] sortArrayByParityII(int[] A) { int j = 1; for (int i = 0; i &lt; A.length - 1; i = i + 2) { if ((A[i] &amp; 1) != 0) { while ((A[j] &amp; 1) != 0) { j = j + 2; } int tmp = A[i]; A[i] = A[j]; A[j] = tmp; } } return A; }} 原地删除数组中元素问题26. 删除有序数组中的重复项12345678910111213class Solution { public int removeDuplicates(int[] nums) { int n = nums.length; if(nums == null || nums.length == 0) return 0; int index = 1; for(int i = 1; i &lt; n ; i++){ if(nums[i] != nums[i-1]) nums[index++] = nums[i]; } return index; }} 80. 删除有序数组中的重复项 II123456789101112class Solution { public int removeDuplicates(int[] nums) { if(nums == null || nums.length == 0) return 0; int index = 2; for(int i = 2 ; i &lt; nums.length ; i++){ if(nums[i] != nums[index-2]) nums[index++] = nums[i]; } return index; }} 通解： 1234567891011121314// 通解扩展class Solution { public int removeDuplicates(int[] nums) { return process(nums, 2); } int process(int[] nums, int k) { // 最多保留k位相同数字 int slow = 0; // 慢指针从0开始 for (int fast : nums) { // 快指针遍历整个数组 // 检查被保留的元素nums[slow−k]是否和当前待检查元素fast相同 if (slow &lt; k || nums[slow - k] != fast) nums[slow++] = fast; } return slow; // 从nums[0]到nums[slow−1]的每个元素都不相同 }} 27. 移除元素1234567891011class Solution { public int removeElement(int[] nums, int val) { if(nums == null || nums.length == 0) return 0; int index = 0; for(int i = 0 ; i &lt; nums.length ; i++){ if(nums[i] != val) nums[index++] = nums[i]; } return index; }} 283. 移动零12345678910111213141516class Solution { public void moveZeroes(int[] nums) { //1. 先用一个指针把所有 非0项移动到前面来 int index = 0; for(int i = 0 ; i &lt; nums.length ; i++){ if(nums[i] != 0) nums[index++] = nums[i]; } //2. 再把数组后面的值全部设为0 for(int i = index ; i &lt; nums.length ; i++) nums[i] = 0; } } 数组中出现n次的元素136. 只出现一次的数字1234567class Solution { public int singleNumber(int[] nums) { int res = 0; for(int num : nums) res ^= num; return res; }} 169. 多数元素123456789101112131415161718class Solution { public int majorityElement(int[] nums) { int res = nums[0]; int count = 1; for(int num : nums){ if(res == num) count++; else { count--; if(count == 0){ res = num; count = 1; } } } return res; }} 数组中的多数元素https://leetcode-cn.com/problems/majority-element/ 12345678910111213141516171819class Solution { public int majorityElement(int[] nums) { int res = nums[0]; int count = 1; for(int num : nums){ if(num == res) count++; else{ count--; if(count == 0){ res = num; count = 1; } } } return res; }} 双指针31. 下一个排列12345678910111213141516171819202122232425class Solution { public void nextPermutation(int[] nums) { if(nums == null || nums.length == 0) return; int n = nums.length; // 双指针 : i在前 ，j在后 , 注意i和j的排序 for(int i = n-1 ; i &gt;= 0 ; i--){ for(int j = n-1 ; j &gt; i ; j--){ // 找到i后面的最后一个比 nums[i] 的数 if(nums[j] &gt; nums[i]){ // 1 2 3 int tem = nums[j]; nums[j] = nums[i]; nums[i] = tem; // 交换完之后后面的要重新排序 1 4 3 2 --- 2 1 3 4 Arrays.sort(nums , i+1 , n); return; } } } // 如果没找到直接排序即可 Arrays.sort(nums); }} 1365. 有多少小于当前数字的数字1 1299. 将每个元素替换为右侧最大元素12345678910111213141516class Solution { public int[] replaceElements(int[] nums) { int n = nums.length; int[] res = new int[n]; res[n-1] = -1; //从后往前遍历 , 假设后面最大的数 是最后一个 int max = n-1; for(int i = n-1 ; i &gt; 0 ; i--){ // 如果当前数大于 后面一个最大的数 ，则替换 if(nums[i] &gt; nums[max]) max = i; res[i-1] = nums[max]; } return res; }} 670. 最大交换1234567891011121314151617181920212223242526272829303132class Solution { public int maximumSwap(int num) { char[] chars = Integer.toString(num).toCharArray(); // 1. 用一个数组记录这个数后面最大的数 的索引 , 如果没有还是本身 比如 2736 int[] maxIndex = new int[chars.length]; int max = chars.length-1; //2. 从后往前遍历, 比如 1993 的 maxIndex为 [2,2,2,3] ,这样可以避免重复数的影响 // 2 7 8 6 for(int i = chars.length-1 ; i &gt;= 0 ;i--){ if(chars[i] &gt; chars[max]) max = i; maxIndex[i] = max; } //System.out.println(Arrays.toString(maxIndex));stdout[1, 1, 3, 3] //3. 正序遍历数组，如果当前数和他后面最大的值不相等，则交换。交换一次之后就退出 for(int i = 0 ; i &lt; chars.length ; i++){ if(chars[i] != chars[maxIndex[i]]){ char tem = chars[i]; chars[i] = chars[maxIndex[i]]; chars[maxIndex[i]] = tem; break; } } //4. 返回 return Integer.parseInt(new String(chars)); }} 179. 最大数123456789101112131415161718192021class Solution { public String largestNumber(int[] nums) { if(nums == null || nums.length == 0) return &quot;&quot;; int n = nums.length; //1.转换成字符串数组 String[] str = new String[n]; for(int i = 0 ; i &lt; n ; i++) str[i] = String.valueOf(nums[i]); //2.重写比较规则,注意用 compareTo方法，就是比较两个字符串的每个字符大小 Arrays.sort(str , (a,b) -&gt; (b+a).compareTo(a+b)); //3.拼接字符数组即可 StringBuilder sb = new StringBuilder(); for(String s : str) sb.append(s); //4.排除0开头的情况 ，注意 String res = sb.toString(); if(res.charAt(0) == '0') res = &quot;0&quot;; return res; }} 611. 有效三角形的个数123456789101112131415161718192021222324class Solution { public int triangleNumber(int[] nums) { // 先固定最大值，再双指针找 int n = nums.length; if(n == 0) return 0; Arrays.sort(nums); int count = 0; for(int i = n-1; i &gt;= 0 ; i--){ int l = 0 , r = i-1; // 2 3 4 5 6 while(l &lt; r){ if(nums[l] + nums[r] &gt; nums[i]){ count += ( r - l); // 注意这里是 + （r - l） , 也就是 2 + 5 可以，那么 3+5、4+5都可以 r--; }else l++; } } return count; }} 42. 接雨水123456789101112131415161718192021class Solution { public int trap(int[] height) { int n = height.length; if(n &lt; 3) return 0; //1. 用两个数组分别记录左边/右边第一个比当前数大的元素 int[] maxL = new int[n]; for(int i = 1 ; i &lt; n ; i++) maxL[i] = Math.max(maxL[i-1] , height[i-1]); int[] maxR = new int[n]; for(int i = n-2 ; i &gt;= 0 ;i--) maxR[i] = Math.max(maxR[i+1] , height[i+1]); ////2. 遍历数组，取左右两边的较小值和当前位置的值比较即为该位置可以存的雨水 int res = 0; for(int i = 1 ; i &lt; n ;i++){ int tem = Math.min(maxR[i] , maxL[i]); if(tem &gt; height[i]) res += (tem - height[i]); } return res; }} 55. 跳跃游戏123456789101112131415161718192021class Solution { public boolean canJump(int[] nums) { // 从最后一个数往前数，遇到0就查看这个0能不能跳过去，只要有一个0是不能跳过去的则返回false int n = nums.length; for(int i = n-2 ; i &gt;= 0 ; i--){ if(nums[i] == 0){ if(!canGo(nums , i)) return false; } } return true; } boolean canGo(int[] nums , int index){ //从index往前找 for(int i = index-1 ; i &gt;= 0 ; i--){ if(nums[i] + i &gt; index) return true; } return false; }} 45. 跳跃游戏 II1234567891011121314151617181920212223242526class Solution { public int jump(int[] nums) { if(nums == null || nums.length &lt; 2) return 0; int res = 0; // begin 为当前起跳的起点 , end 为当前起跳的终点 int begin = 0 , end = 0; // 因为只要跳到最后一个位置即可，所以 end == nums.length-1 即可退出循环 while(end &lt; nums.length-1){ // tem 为下一步能跳到的最大位置 // 从 0 位置开始起跳，你落脚的必定是 [1, 4] 位置中能够跳得更远的， int tem = 0; for(int i = begin ; i &lt;= end ;i++) tem = Math.max(tem , nums[i] + i); // 必须先记录下一次起跳位置再更新end begin = end + 1; end = tem; res++; } return res; }} 最大连续1的数量https://leetcode-cn.com/problems/max-consecutive-ones/ 12345678910111213class Solution { public int findMaxConsecutiveOnes(int[] nums) { int res = 0; int tem = 0; for(int i = 0 ; i &lt; nums.length ; i++){ if(nums[i] == 1) tem++; else tem = 0; res = Math.max(res , tem); } return res; }} 部分排序https://leetcode-cn.com/problems/sub-sort-lcci/ 123456789101112131415161718192021class Solution { public int[] subSort(int[] array) { int n = array.length; int l = -1 , r = -1; int min = Integer.MAX_VALUE , max = Integer.MIN_VALUE; // 从前往后找目标末位，使得从该位到最后，数组保持递增 for(int i = 0 ; i &lt; n ; i++){ if(array[i] &gt;= max) max = array[i]; else r = i; } // 数组恒递增，说明数组是有序的，直接返回 if(r == -1) return new int[]{-1,-1}; // 从后往前找目标首位，使得从该位到最前，数组保持递减 for(int i = n-1 ; i &gt;= 0 ; i--){ if(array[i] &lt;= min) min = array[i]; else l = i; } return new int[]{l ,r}; }} 矩阵问题867. 转置矩阵1234567891011121314151617181920212223class Solution { // 注意是转置矩阵，而不是正方形 public int[][] transpose(int[][] matrix) { int m = matrix.length; if(m == 0) return matrix; int n = matrix[0].length; int[][] res = new int[n][m]; // 1 2 3 // 4 5 6 // 1 4 // 2 5 // 3 6 for(int i = 0 ; i &lt; n ; i++){ for(int j = 0 ; j &lt; m ; j++){ res[i][j] = matrix[j][i]; } } return res; }} 54. 螺旋矩阵123456789101112131415161718192021class Solution { public List&lt;Integer&gt; spiralOrder(int[][] matrix) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); int m = matrix.length; int n = matrix[0].length; int l = 0 , r = n-1; int t = 0 , b = m-1; while(l &lt;= r &amp;&amp; t &lt;= b){ for(int i = l ; i &lt;= r ; i++) res.add(matrix[t][i]); if(++ t &gt; b) break; for(int i = t ; i &lt;= b ; i++) res.add(matrix[i][r]); if(--r &lt; l) break; for(int i = r ; i &gt;= l ; i--) res.add(matrix[b][i]); if(--b &lt; t) break; for(int i = b ; i &gt;= t ; i--) res.add(matrix[i][l]); if(++l &gt; r) break; } return res; }} 59. 螺旋矩阵 II123456789101112131415161718192021class Solution { public int[][] generateMatrix(int n) { int[][] res = new int[n][n]; int index = 1; int l = 0 , r = n-1; int t = 0 , b = n-1; while(true){ for(int i = l ; i &lt;= r ; i++) res[t][i] = index++; if(++t &gt; b) break; for(int i = t ; i &lt;= b ; i++) res[i][r] = index++; if(--r &lt; l) break; for(int i = r ; i &gt;= l ; i--) res[b][i] = index++; if(--b &lt; t) break; for(int i = b ; i &gt;= t ; i--) res[i][l] = index++; if(++l &gt; r) break; } return res; }} 48. 旋转图像1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class MatrixDemo { public static void main(String[] args) { int[][] matrix = {{1,2,3} , {4,5,6} , {7,8,9}}; printMatrix(matrix); rotate(matrix); System.out.println(&quot;=====&quot;); printMatrix(matrix); } public static void rotate(int[][] matrix) { int m = matrix.length; int n = matrix[0].length; // 先转置矩阵: 行变列 //1 2 3 //4 5 6 //7 8 9 for(int i = 0 ; i &lt; m ; i++){ for(int j = 0 ; j &lt; i ;j++){ int tem = matrix[i][j]; matrix[i][j] = matrix[j][i]; matrix[j][i] = tem; } } //1 4 7 //2 5 8 //3 6 9 int mid = n &gt;&gt; 1; //2. 再把每一行以中间的值翻转 for (int i = 0 ; i &lt; m ; i++){ for (int j = 0 ; j &lt; mid ;j++){ int tem = matrix[i][j]; matrix[i][j] = matrix[i][n-j-1]; matrix[i][n-j-1] = tem; } } } public static void printMatrix(int[][] matrix){ for (int[] nums : matrix){ StringBuilder sb = new StringBuilder(); for (int num : nums) sb.append(num + &quot; &quot;); sb.deleteCharAt(sb.length()-1); System.out.println(sb.toString()); } }} 73. 矩阵置零1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Solution { public void setZeroes(int[][] matrix) { int raw = matrix.length; if(raw == 0) return; int col = matrix[0].length; //1. 用两个遍历 第一行 和 第一列 是否要置为0 boolean rawFlag = false; boolean colFlag = false; for(int i = 0 ; i &lt; col ; i++){ if (matrix[0][i] == 0){ rawFlag = true; break; } } for (int i = 0 ; i &lt; raw ; i++){ if (matrix[i][0] == 0){ colFlag = true; break; } } //2. 从 [1,1] 开始遍历，如果遇到0，就把对应的 第一行、第一列置为0 for (int i = 1; i &lt; raw ; i++){ for (int j = 1 ; j &lt; col ; j++){ if (matrix[i][j] == 0) matrix[i][0] = matrix[0][j] = 0; } } //3. 遍历第一行和第一列，遇到0就把那一行那一列都置为0 for (int i = 1 ; i &lt; raw ; i++){ if (matrix[i][0] == 0){ for (int j = 0 ; j &lt; col ; j++) matrix[i][j] = 0; } } for (int i = 1 ; i &lt; col ; i++){ if (matrix[0][i] == 0){ for (int j = 0 ; j &lt; raw ; j++) matrix[j][i] = 0; } } //4. 确定第一行和第一列要不要变为0 if (rawFlag){ for (int i = 0 ; i &lt; col ; i++) matrix[0][i] = 0; } if (colFlag){ for (int i = 0 ; i &lt; raw ; i++) matrix[i][0] = 0; } }} 79. 单词搜索1234567891011121314151617181920212223242526272829303132333435363738394041class Solution { public boolean exist(char[][] board, String word) { int m = board.length; if(m == 0) return false; int n = board[0].length; boolean[][] isv = new boolean[m][n]; for(int i = 0 ; i &lt; m ; i++){ for(int j = 0 ; j &lt; n ; j++){ if(board[i][j] == word.charAt(0) &amp;&amp; dfs(board , word , 0 , i , j ,isv)) return true; } } return false; } // 判断 从 word的第index位开始，是否存在对应的路径，从（i , j）开始 boolean dfs(char[][] board , String word , int index , int i , int j , boolean[][] isv){ //1. 递归截止条件 if(index == word.length()) return true; //2. 不符合的情况 if(i &lt; 0 || i &gt;= board.length || j &lt; 0 || j &gt;= board[0].length) return false; if(word.charAt(index) != board[i][j] || isv[i][j]) return false; //3. 将当前位置标记为已经访问过了 isv[i][j] = true; //4. 遍历后续的上下左右四个方向 if(dfs(board , word , index +1 , i+1 , j ,isv) || dfs(board , word , index +1 , i-1 , j ,isv) || dfs(board , word , index +1 , i , j+1 ,isv) || dfs(board , word , index +1 , i , j-1 ,isv)) return true; //4. 回溯 isv[i][j] = false; return false; }} 329. 矩阵中的最长递增路径12345678910111213141516171819202122232425262728293031class Solution { public int longestIncreasingPath(int[][] matrix) { int m = matrix.length; if(m == 0) return 0; int n = matrix[0].length; //1. dp[i][j] 表示以 matrix[i][j]结尾的最长递增路径 int[][] dp = new int[m][n]; int res = 0; for(int i = 0 ; i &lt; m ; i++){ for(int j = 0 ; j &lt; n ; j++){ res = Math.max(res , dfs(i , j , Integer.MIN_VALUE , matrix , dp)); } } return res; } int dfs(int i , int j , int cur , int[][] matrix , int[][] dp){ if(i &lt; 0 || i &gt;= matrix.length || j &lt; 0 || j &gt;= matrix[0].length) return 0; //2. 如果当前值不小于 matrix[i][j] , 就不构成递增路径 if(matrix[i][j] &lt;= cur) return 0; if(dp[i][j] != 0) return dp[i][j]; int res = 0; res = Math.max(res , dfs(i+1 , j , matrix[i][j] , matrix , dp)); res = Math.max(res , dfs(i-1 , j , matrix[i][j] , matrix , dp)); res = Math.max(res , dfs(i , j+1 , matrix[i][j] , matrix , dp)); res = Math.max(res , dfs(i , j-1 , matrix[i][j] , matrix , dp)); dp[i][j] = res + 1; return res+1; }} 74. 搜索二维矩阵12345678910111213141516class Solution { public boolean searchMatrix(int[][] matrix, int target) { int m = matrix.length; int n = matrix[0].length; if(m == 0 || n == 0) return false; int l = 0 , r = n-1; while(l &lt; m &amp;&amp; r &gt;= 0){ if(matrix[l][r] &lt; target) l++; else if(matrix[l][r] &gt; target) r--; else return true; } return false; }} 面试题 17.24. 最大和子矩阵123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class Solution_6789 { public int[] getMaxMatrix(int[][] matrix) { // 返回的是二维矩阵中两个点的坐标 int[] res = new int[4]; /** * 1 2 3 * 4 5 6 * 7 8 9 */ int m = matrix.length; int n = matrix[0].length; int max = Integer.MIN_VALUE; //1. 构造前缀和二维数组 int[][] preSum = new int[m+1][n]; for (int i = 1; i &lt;= m ; i++){ for (int j = 0 ; j &lt; n ;j++){ preSum[i][j] = preSum[i-1][j] + matrix[i-1][j]; } } /** * 0 0 0 * 1 2 3 * 5 7 9 * 12 15 18 */ //2. 遍历所有可能的 上行 和 下行。转换成1维数组 for (int t = 0 ; t &lt; m ; t++){ for (int b = 0 ; b &lt; m ; b++){ int[] nums = new int[n]; for (int i = 0 ; i &lt; n ; i++) nums[i] = preSum[b+1][i] - preSum[t][i]; /** * [1, 2, 3] * [5, 7, 9] * [12, 15, 18] * [0, 0, 0] * [4, 5, 6] * [11, 13, 15] * [-4, -5, -6] * [0, 0, 0] * [7, 8, 9] */ //3. 看这个数组的最大子数组和，当最大的时候更新 int sum = nums[0]; int start = 0; for (int i = 0 ; i &lt; n ; i++){ if(sum &gt; 0) sum += nums[i]; else { sum = nums[i]; start = i ; // 记录起始位置 } if (sum &gt; max){ // 更新最大值并记录坐标 max = sum; res[0] = t; res[1] = start; res[2] = b; res[3] = i; } } } } return res;} 566. 重塑矩阵1234567891011121314151617181920212223class Solution { public int[][] matrixReshape(int[][] matrix, int r, int c) { int[][] res = new int[r][c]; int raw = matrix.length; int col = matrix[0].length; if(r * c != raw * col) return matrix; int index = 0; for(int i = 0 ; i &lt; raw ; i++){ for(int j = 0 ; j &lt; col ; j++){ res[index/c][index%c] = matrix[i][j]; index++; } } return res; }} 378. 有序矩阵中第 K 小的元素left 和 right 最终会相等，并且会变成数组中第k小的数字 [1 212 100]k = 3那么刚开始 left = 1, right = 100, mid = 50, 遍历完 cnt = 3，此时 right 更新为 50此时 left = 1, right = 50, mid = 25, 遍历完之后 cnt = 3, 此时 right 更新为 25此时 left = 1, right = 25, mid = 13, 遍历完之后 cnt = 3, 此时 right 更新为 13此时 left = 1, right = 13, mid = 7, 遍历完之后 cnt = 2, 此时 left 更新为8此时 left = 8, right = 13, mid = 10, 遍历完之后 cnt = 2, 此时 left 更新为 11此时 left = 11, right = 12, mid = 11, 遍历完之后 cnt = 2, 此时 left 更新为 12循环结束，left 和 right 均为 12，任意返回一个即可。 12345678910111213141516171819202122232425class Solution { public int kthSmallest(int[][] matrix, int k) { int raw = matrix.length; int col = matrix[0].length; // 注意这里的l 和 r 取的是数组中的最小值和最大值 int l = matrix[0][0] ; int r = matrix[raw-1][col-1]; while(l &lt; r){ int mid = l + r &gt;&gt; 1; int count = 0; for(int i = 0 ; i &lt; raw ; i++){ for(int j = 0 ; j &lt; col ; j++){ if(matrix[i][j] &lt;= mid) count++; } } if(count &lt; k) l = mid+1; else r = mid; } return l; }} 85. 最大矩形12345678910111213```# 其他问题#### [611. 有效三角形的个数](https://leetcode-cn.com/problems/valid-triangle-number/)```JAVA 118. 杨辉三角12345678910111213141516171819class Solution { public List&lt;List&lt;Integer&gt;&gt; generate(int numRows) { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); if(numRows == 0) return res; res.add(Arrays.asList(1)); for(int i = 1 ; i &lt; numRows ; i++){ List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(1); for(int j = 1 ; j &lt; i ; j++){ list.add(res.get(i-1).get(j-1) + res.get(i-1).get(j)); } list.add(1); res.add(list); } return res; }} 1233. 删除子文件夹1234567891011121314151617181920212223242526class Solution { public List&lt;String&gt; removeSubfolders(String[] strs) { List&lt;String&gt; res = new ArrayList&lt;&gt;(); // 1 用一个Map记录 每个字符串 和他的长度 Map&lt;String , Integer&gt; map = new HashMap&lt;&gt;(); for(String s : strs) map.put(s , s.length()); for(String s : strs){ int i = s.length()-1; boolean flag = true; //2. 遍历每个字符串，从后往前看 ，如果遇到了 '/' 就看map中有没有之前的子串，如果有说明当前文件夹是一个子文件夹，可以删除 while(i &gt; 1){ if(s.charAt(i) == '/'){ if(map.containsKey(s.substring(0,i))) flag = false; } i--; } if(flag) res.add(s); } return res; }}","link":"/2021/10/10/%E7%AE%97%E6%B3%95%E9%A2%98%EF%BC%88%E4%B8%83%EF%BC%89%E6%95%B0%E7%BB%84/"},{"title":"算法题（二）字符串","text":"字符串专题 双指针思想3. 无重复字符的最长子串1234567891011121314151617181920class Solution { public int lengthOfLongestSubstring(String s) { if(s == null || s.length() == 0) return 0; int n = s.length(); Map&lt;Character , Integer&gt; map = new HashMap&lt;&gt;(); int res = 0; for(int i = 0 , j = 0 ; i &lt; n ; i++){ //如果map中已经有当前字符，则更新j指针的位置 if(map.containsKey(s.charAt(i))){ j = Math.max(j , map.get(s.charAt(i))); } map.put(s.charAt(i) , i + 1); // 如果没有就放入map中 res = Math.max(res , i - j+ 1); // 更新res, 注意是 i在前，j在后 } return res; }} 14. 最长公共前缀12345678910111213class Solution { public String longestCommonPrefix(String[] strs) { if(strs == null || strs.length == 0) return &quot;&quot;; String res = strs[0]; for(int i = 1; i &lt; strs.length ; i++){ int j = 0; // 找到 res 和 strs[i]的最长公共前缀 while(j &lt; res.length() &amp;&amp; j &lt; strs[i].length() &amp;&amp; res.charAt(j) == strs[i].charAt(j)) j++; res = res.substring(0,j); // 更新res } return res; }} 6. Z 字形变换123456789101112131415161718class Solution { public String convert(String s, int numRows) { if(numRows &lt; 2) return s; List&lt;StringBuilder&gt; rows = new ArrayList&lt;&gt;(); for(int i = 0 ; i &lt; numRows ; i++) rows.add(new StringBuilder()); int i = 0 , flag = -1;S for(char c : s.toCharArray()){ rows.get(i).append(c); if(i == 0 || i == numRows -1) flag = - flag; i += flag; } StringBuilder res = new StringBuilder(); for(StringBuilder row : rows) res.append(row); return res.toString(); }} 8. 字符串转换整数 (atoi)1234567891011121314151617181920212223242526272829class Solution { public int myAtoi(String s) { //1. 干掉字符串前后的空格 s = s.trim(); int n = s.length(); if(n == 0) return 0; boolean isN = false; int index = 0; //2. 如果开头既没有 + 又没有 - 则不需要转换 if(!Character.isDigit(s.charAt(index)) &amp;&amp; s.charAt(index) != '+' &amp;&amp; s.charAt(index) != '-' ) return 0; if(s.charAt(index) == '-'){ index++; isN = true; //3. 说明是负数，因为是负号开头的 }else if(s.charAt(index) == '+') index++; //4.判断后续的数字 long res = 0; while(index &lt; s.length() &amp;&amp; Character.isDigit(s.charAt(index))){ res = res * 10 + s.charAt(index) - '0'; index ++; //5. 如果超过了Integer的范围，则直接返回 if(res &gt; Integer.MAX_VALUE) return isN ? Integer.MIN_VALUE : Integer.MAX_VALUE; } return isN ? -(int)res : (int)res; }} 38. 外观数列1234567891011121314151617181920class Solution { public String countAndSay(int n) { String s = &quot;1&quot;; //1. 从第一行到第n行 for(int i = 1; i &lt; n ; i++){ String tem = &quot;&quot;; //2. 遍历每一行时，都从上一行的s来遍历，看有几个一样的数字 for(int j = 0 ; j &lt; s.length() ; j++){ int k = j; while(k &lt; s.length() &amp;&amp; s.charAt(k) == s.charAt(j)) k++; String count = k - j + &quot;&quot;; tem = tem + count + s.charAt(j); j = k-1; } //3. 更新s 为当前行的字符串 s = tem; } return s; }} 49. 字母异位词分组123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Solution { public List&lt;List&lt;String&gt;&gt; groupAnagrams(String[] strs) { List&lt;List&lt;String&gt;&gt; res = new ArrayList&lt;&gt;(); Map&lt;String , List&lt;String&gt;&gt; map = new HashMap&lt;&gt;(); for(String s : strs){ char[] chars = s.toCharArray(); Arrays.sort(chars); if(map.containsKey(new String(chars))) map.get(new String(chars)).add(s); else{ List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(s); map.put(new String(chars) , list); } } for(List&lt;String&gt; s : map.values()) res.add(s); return res; }}#### [43. 字符串相乘](https://leetcode-cn.com/problems/multiply-strings/)![image-20210930161425433](/Users/shengbinbin/Library/Application%20Support/typora-user-images/image-20210930161425433.png)​```javaclass Solution { public String multiply(String num1, String num2) { // num1的第i位(高位从0开始)和num2的第j位相乘的结果在乘积中的位置是[i+j, i+j+1] int n1 = num1.length()-1; int n2 = num2.length()-1; if(n1 &lt; 0 || n2 &lt; 0) return &quot;&quot;; int[] res = new int[n1+n2+2]; for(int i = n1 ; i &gt;= 0 ; i--){ for(int j = n2 ; j&gt;= 0 ; j--){ int num = (num1.charAt(i) - '0') * (num2.charAt(j) - '0'); num += res[i+j+1]; // 加上低位的进位 res[i+j] += num / 10; res[i+j+1] = num % 10; } } StringBuilder sb = new StringBuilder(); int i = 0; while(i &lt; res.length-1 &amp;&amp; res[i] == 0) i++; // 去掉前置0 for( ; i &lt; res.length ; i++) sb.append(res[i]); return sb.toString(); }} 71. 简化路径123456789101112131415161718192021222324class Solution { public String simplifyPath(String path) { Stack&lt;String&gt; stack = new Stack&lt;&gt;(); String[] strs = path.split(&quot;/&quot;); for(String s : strs){ if(s.length() == 0 || s.equals(&quot;.&quot;)) continue; //空字符串 或 . 直接忽略 if(s.equals(&quot;..&quot;)){ // 回到上一个目录 , 弹一个栈 if(!stack.empty()) stack.pop(); }else stack.push(s); // 说明都是目录了，加进去 } // 将栈中的字符串 用 &quot;/&quot; 连接起来 StringBuilder sb = new StringBuilder(); sb.append(&quot;/&quot;); // 题目说了 if(!stack.empty()){ for(String s : stack) sb.append(s + &quot;/&quot;); sb.deleteCharAt(sb.length()-1); } return sb.toString(); }} 151. 翻转字符串里的单词123456789101112131415161718class Solution { public String reverseWords(String s) { s = s.trim(); int n = s.length(); if(n == 0 || n == 1) return s; int l = n-1 , r = n-1; StringBuilder sb = new StringBuilder(); while(l &gt;= 0){ while(l &gt;= 0 &amp;&amp; s.charAt(l) != ' ') l--; sb.append(s.substring(l+1 , r+1) + &quot; &quot;); while(l &gt;= 0 &amp;&amp; s.charAt(l) == ' ') l--; r = l; } return sb.toString().trim(); }} 557. 反转字符串中的单词 III123456789101112131415161718192021class Solution { public String reverseWords(String s) { StringBuilder sb = new StringBuilder(); int n = s.length(); String[] strs = s.split(&quot; &quot;); for(String tem : strs){ sb.append(reverse(tem) + &quot; &quot;); } sb.deleteCharAt(sb.length()-1); return sb.toString(); } String reverse(String s){ StringBuilder sb = new StringBuilder(); for(int i = s.length()-1 ; i &gt;= 0 ; i--) sb.append(s.charAt(i)); return sb.toString(); }} 165. 比较版本号1234567891011121314151617class Solution { public int compareVersion(String version1, String version2) { //转换成字符数组，从头开始往后比较，如果超过长度就取0 String[] a1 = version1.split(&quot;\\\\.&quot;); //按点分隔要加转义符 String[] a2 = version2.split(&quot;\\\\.&quot;); int n = Math.max(a1.length , a2.length); for (int i = 0; i &lt; n; i++) { int a = i &lt; a1.length ? Integer.parseInt(a1[i]) : 0; int b = i &lt; a2.length ? Integer.parseInt(a2[i]) : 0; if (a &gt; b) return 1; else if (a &lt; b) return -1; // 注意这里时else if } return 0; }} 166. 分数到小数123456789101112131415161718192021222324class Solution { public String fractionToDecimal(int numerator, int denominator) { long a = numerator , b = denominator; StringBuilder sb = new StringBuilder(); //1. 确定符号 if((a &gt; 0 &amp;&amp; b &lt; 0) || (a &lt; 0 &amp;&amp; b &gt; 0)) sb.append('-'); a = Math.abs(a); b = Math.abs(b); sb.append(a / b); //2. 如果可以整除直接返回 if(a % b == 0) return sb.toString(); sb.append('.'); //3. 用一个map记录小数点后面的余数的位置 Map&lt;Long , Integer&gt; map = new HashMap&lt;&gt;(); while((a = (a % b) * 10) &gt; 0 &amp;&amp; !map.containsKey(a)){ map.put(a , sb.length()); sb.append(a / b); } // 如果除尽了，a最后等于0,如果除不尽就在 之前出现的位置数之前 加上() if(a == 0) return sb.toString(); return sb.insert(map.get(a).intValue() , '(').append(')').toString(); }} 面试题 01.06. 字符串压缩12345678910111213141516171819202122class Solution { public String compressString(String s) { StringBuilder sb = new StringBuilder(); int l = 0; int n = s.length(); if(n == 0 || n == 1) return s; for(int i = 0 ; i &lt; n ;i++){ int tem = 1; // sb.append(s.charAt(i)); int j = i+1; while(j &lt; n &amp;&amp; s.charAt(j) == s.charAt(i)){ tem ++; j++; } sb.append(s.charAt(i)).append(tem); i = j-1; } return sb.length() &lt; n ? sb.toString() : s; }} 97. 交错字符串12345678910111213141516171819202122232425class Solution { public boolean isInterleave(String s1, String s2, String s3) { int n1 = s1.length(); int n2 = s2.length(); int n3 = s3.length(); if(n1 + n2 != n3) return false; // dp[i][j] 表示 s1的前i个字符 和 s2的前j个字符 能否组成 s3的前（i+j）个字符 boolean[][] dp = new boolean[n1+1][n2+1]; // base case: 只用 s1 或 s2 来组成 s3 dp[0][0] = true; for(int i = 1 ; i &lt;= n1 ; i++) dp[i][0] = dp[i-1][0] &amp;&amp; s1.charAt(i-1) == s3.charAt(i-1); for(int i = 1 ; i &lt;= n2 ; i++) dp[0][i] = dp[0][i-1] &amp;&amp; s2.charAt(i-1) == s3.charAt(i-1); // 遍历 for(int i = 1 ; i &lt;= n1 ; i++){ for(int j = 1; j &lt;= n2 ;j++){ // 两种可能性，是由s1的字符还是s2的字符组成 dp[i][j] = (dp[i-1][j] &amp;&amp; s1.charAt(i-1) == s3.charAt(i+j-1)) || (dp[i][j-1] &amp;&amp; s2.charAt(j-1) == s3.charAt(i+j-1)); } } return dp[n1][n2]; }} 8. 字符串转换整数 (atoi)1234567891011121314151617181920212223242526272829class Solution { public int myAtoi(String s) { //1. 干掉字符串前后的空格 s = s.trim(); int n = s.length(); if(n == 0) return 0; boolean isN = false; int index = 0; //2. 如果开头既没有 + 又没有 - 则不需要转换 if(!Character.isDigit(s.charAt(index)) &amp;&amp; s.charAt(index) != '+' &amp;&amp; s.charAt(index) != '-' ) return 0; if(s.charAt(index) == '-'){ index++; isN = true; //3. 说明是负数，因为是负号开头的 }else if(s.charAt(index) == '+') index++; //4.判断后续的数字 long res = 0; while(index &lt; s.length() &amp;&amp; Character.isDigit(s.charAt(index))){ res = res * 10 + s.charAt(index) - '0'; index ++; //5. 如果超过了Integer的范围，则直接返回 if(res &gt; Integer.MAX_VALUE) return isN ? Integer.MIN_VALUE : Integer.MAX_VALUE; } return isN ? -(int)res : (int)res; }} 468. 验证IP地址12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution { public String validIPAddress(String IP) { if(isIpV4(IP)) return &quot;IPv4&quot;; if(isIpV6(IP)) return &quot;IPv6&quot;; return &quot;Neither&quot;; } private boolean isIpV4(String ip) { //IP4的地址至少有3个点和4个数，所以至少要有7个数 if (ip.length() &lt; 7 || ip.charAt(0) =='.' || ip.charAt(ip.length()-1)=='.') return false; String[] s = ip.split(&quot;\\\\.&quot;); if (s.length != 4) return false; //必须要有4段 for (String tem : s){ //每一段字符串不能为0且不能以0开头的字符串，可以只有0 if (tem.length() == 0 || (tem.charAt(0) == '0' &amp;&amp; tem.length() != 1)) return false; int num = 0; for (int i = 0; i &lt; tem.length(); i++) { if (!Character.isDigit(tem.charAt(i))) return false; //如果不是数字则不是 num = num * 10 + tem.charAt(i) - '0'; if (num &gt; 255) return false; //每一段字符串表示的数字不能超过255 } } return true; } //判断是不是 ipv6的地址 boolean isIpV6(String ip){ //IPV6的地址至少有8个冒号和8个数字组成，必须要16个数 if (ip.length() &lt; 15 || ip.charAt(0) == ':' || ip.charAt(ip.length()-1) == ':') return false; String[] str = ip.split(&quot;:&quot;); if(str.length != 8) return false; for(String s : str){ if(s.length() &gt; 4 || s.length() == 0) return false; for(int i = 0 ; i &lt; s.length() ;i++){ char c = s.charAt(i); if (!((c &gt;= 'a' &amp;&amp; c &lt;= 'f') || (c &gt;='A' &amp;&amp; c &lt;= 'F') || (c &gt;= '0' &amp;&amp; c &lt;= '9'))) return false; } } return true; }} 459. 重复的子字符串1234567891011class Solution { public boolean repeatedSubstringPattern(String s) { // s = nx , n &gt; 1 // t = 2s = 2nx 去掉开头和结尾两位字符 还有 2n - 2 个字符串 // 又 n &gt; 1 ,所以 2n - 2 &gt; 1 // 也就是说 t 中 s 必须出现1次以上 String tem = s + s; return tem.substring(1 , tem.length()-1).contains(s); }} 回文串问题125. 验证回文串12345678910111213141516171819202122232425class Solution { public boolean isPalindrome(String s) { int n = s.length(); if(n == 0) return true; int l = 0 , r = n-1; // 熟悉API while(l &lt; r){ if(!Character.isLetterOrDigit(s.charAt(l))){ l++; continue; } if(!Character.isLetterOrDigit(s.charAt(r))){ r--; continue; } if(Character.toLowerCase(s.charAt(l)) != Character.toLowerCase(s.charAt(r))) return false; l++; r--; } return true; }} 680. 验证回文字符串 Ⅱ1234567891011121314151617181920212223242526272829303132class Solution { public boolean validPalindrome(String s) { int n = s.length(); if(n == 0) return true; int l = 0 , r = n-1; // a bdd a 只要看 bd 和 dd 中是否有回文，如果有一个就行 // a badc a adc 和 bad 都不是 // a dadc a dad 是回文 while(l &lt; r){ if(s.charAt(l) != s.charAt(r)){ return isValid(s , l+1 , r) || isValid(s , l , r-1); } l++; r--; } return true; } boolean isValid(String s , int l , int r){ while(l &lt; r){ if(s.charAt(l) != s.charAt(r)) return false; l++; r--; } return true; }} 5. 最长回文子串1234567891011121314151617181920212223242526272829303132class Solution { public String longestPalindrome(String s) { if(s == null || s.length() == 0) return null; int n = s.length(); // dp数组：表示 s[i -j] 之间是不是回文 boolean[][] dp = new boolean[n][n]; // 初始化状态 for(int i = 0 ; i &lt; s.length() ; i++) dp[i][i] = true; int maxLen = 1; int start = 0; // 开始遍历 ，i从倒数第二个字符开始往前遍历， J从i之后往后遍历 for(int i = n-2 ; i &gt;= 0 ; i--){ for(int j = i+1; j &lt; n; j++){ if(s.charAt(i) != s.charAt(j)) dp[i][j] = false; else { if(j -i &lt; 3) dp[i][j] = true; else dp[i][j] = dp[i+1][j-1]; } if(dp[i][j] &amp;&amp; j-i+1 &gt; maxLen){ start = i; maxLen = j-i+1; } } } return s.substring(start , start + maxLen); }} 131. 分割回文串12345678910111213141516171819202122232425262728293031323334353637383940class Solution { List&lt;List&lt;String&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;String&gt;&gt; partition(String s) { if(s == null || s.length() == 0) return res; List&lt;String&gt; list = new ArrayList&lt;&gt;(); dfs(s , 0 , list); return res; } void dfs(String s , int start , List&lt;String&gt; list){ // 递归截止条件 if(start == s.length()){ res.add(new ArrayList&lt;&gt;(list)); return; } for(int i = start ; i &lt; s.length() ; i++){ String a = s.substring(start , i+1); // 如果这个子串是回文的话，就加入并回溯 if(isHuiwen(a)){ list.add(a); dfs(s , i+1 , list); list.remove(list.size()-1); } } } boolean isHuiwen(String s){ int n = s.length(); int l = 0 , r = n-1; while(l &lt; r){ if(s.charAt(l) != s.charAt(r)) return false; l++; r--; } return true; }} 132. 分割回文串 II123456789101112131415161718192021222324252627282930313233343536373839class Solution { public int minCut(String s) { int n = s.length(); if(n &lt; 2) return 0; // dp[i] 表示 s[0-i]拆成回文的最小次数 int[] dp = new int[n]; // base case: 将每个字符都拆 for(int i = 0 ; i &lt; n ;i++) dp[i] = i; //isHui[i][j] 表示 s[i-j]是否是回文 boolean[][] isHui = new boolean[n][n]; for(int i = 0 ; i &lt; n ; i++) isHui[i][i] = true; for(int i = n-2; i &gt;= 0 ;i--){ for(int j = i+1; j &lt; n ; j++){ if(s.charAt(i) != s.charAt(j)) isHui[i][j] = false; else { if(j - i &lt; 3) isHui[i][j] = true; else isHui[i][j] = isHui[i+1][j-1]; } } } // 开始遍历 for(int i = 0 ; i &lt; n ; i++){ // 如果 0 - i 已经是回文了，直接设置为0 if(isHui[0][i]){ dp[i] = 0; continue; } // 如果不是回文， 遍历 0 - i之间找到j使得 [j+1 , i]是回文,取较小值 for(int j = 0 ; j &lt; i ;j++){ if(isHui[j+1][i]) dp[i] = Math.min(dp[i] , dp[j]+1); } } return dp[n-1]; }} 647. 回文子串12345678910111213141516171819202122232425class Solution { public int countSubstrings(String s) { int n = s.length(); int res = n; boolean[][] dp = new boolean[n][n]; for(int i = 0 ; i &lt; n ; i++) dp[i][i] = true; for(int i = n-2 ; i &gt;= 0 ; i--){ for(int j = i+1 ; j &lt; n ; j++){ if(s.charAt(i) != s.charAt(j)) dp[i][j] = false; else { if(j - i &lt; 3) dp[i][j] = true; else dp[i][j] = dp[i+1][j-1]; } if(dp[i][j]) res++; } } return res; }} 139. 单词拆分123456789101112131415161718192021222324class Solution { public boolean wordBreak(String s, List&lt;String&gt; wordDict) { if(s == null || s.length() == 0) return false; int n = s.length(); // dp[i] 表示s[0,i]之间能不能拆分 boolean[] dp = new boolean[n+1]; //base case dp[0] = true; //开始遍历 for(int i = 1 ; i &lt;= n ; i++){ // 从i往前找，找到一个字符串 在 字符数组中，更新 dp[i] for(int j = i-1 ; j &gt;= 0 ;j--){ String tem = s.substring(j , i); if(wordDict.contains(tem) &amp;&amp; dp[j]){ dp[i] = true; break; } } } return dp[n]; }} 括号问题20. 有效的括号12345678910111213141516class Solution { public boolean isValid(String s) { if(s.length() == 0) return true; Stack&lt;Character&gt; stack = new Stack&lt;&gt;(); for(int i = 0 ; i &lt; s.length() ; i++){ if(s.charAt(i) == '(') stack.push(')'); else if(s.charAt(i) == '[') stack.push(']'); else if(s.charAt(i) == '{') stack.push('}'); else if(stack.empty() || stack.pop() != s.charAt(i)) return false; } if(!stack.empty()) return false; return true; }} 32. 最长有效括号1234567891011121314151617181920212223242526272829303132class Solution { public int longestValidParentheses(String s) { if(s == null) return 0; int[] nums = new int[s.length()]; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); for(int i = 0 ; i &lt; nums.length ; i++){ if(s.charAt(i) == '(') stack.push(i); else{ if(stack.empty()) nums[i] = 1; // 说明这个位置是不合法的，设为1 else stack.pop(); } } while(!stack.empty()) nums[stack.pop()] = 1; // 问题转换成 找数组中连续0的个数 int res = 0; int count = 0; for(int i = 0 ; i &lt; nums.length ; i++){ if(nums[i] == 0){ count++; res = Math.max(res , count); }else count = 0; } return res; }} 678. 有效的括号字符串12345678910111213141516171819202122232425262728class Solution { public boolean checkValidString(String s) { //用两个栈分别存 括号 和 *号 Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); Stack&lt;Integer&gt; temp = new Stack&lt;&gt;(); for(int i = 0 ; i &lt; s.length() ; i++){ if(s.charAt(i) == '(') stack.push(i); else if(s.charAt(i) == '*') temp.push(i); else{ if(stack.empty() &amp;&amp; temp.empty()) return false; //如果两个栈都空了,说明这个 ）无法匹配 if(!stack.empty()) stack.pop(); else temp.pop(); } } // 如果两个栈都不为空，说明还有 ( 和 * ，但是 * 不能在 （ 前面 while(!stack.empty() &amp;&amp; !temp.empty()){ if(stack.peek() &gt; temp.peek()) return false; // *( stack.pop(); temp.pop(); } return stack.empty(); // 最后主要是看 （ 有没有被匹配完 ，因为单纯的 * 也合法 }} 子串问题76. 最小覆盖子串滑动窗口 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution { public String minWindow(String s, String t) { //1. 用一个 map 记录 t中的所有字符及出现次数 Map&lt;Character , Integer&gt; need = new HashMap&lt;&gt;(); for(char c : t.toCharArray()) need.put(c , need.getOrDefault(c , 0) + 1); //2. 用一个 map 记录窗口中出现的字符 及 次数 Map&lt;Character,Integer&gt; win = new HashMap&lt;&gt;(); int l = 0 , r = 0; // 左右边界 int count = 0; int len = Integer.MAX_VALUE , start = 0 , end = 0; // 记录最大值和起始位置、终止位置 while(r &lt; s.length()){ //3. 右边界开始移动,判断当前字符是否是需要的 char a = s.charAt(r); if(need.containsKey(a)){ win.put(a , win.getOrDefault(a , 0) + 1); if(win.get(a).intValue() == need.get(a).intValue()) count++; } r++; //4. 当满足条件时， 更新len之后 ， 左边界开始移动 while(count == need.size()){ if(len &gt; r - l){ len = r-l; start = l; end = r; } char b = s.charAt(l); if(need.containsKey(b)){ win.put(b , win.getOrDefault(b ,0) -1); if(win.get(b) &lt; need.get(b)) count--; } l++; } } return s.substring(start , end); }} 字符串计算问题415. 字符串相加1234567891011121314151617181920212223class Solution { public String addStrings(String num1, String num2) { if(num1 == null || num2 == null) return num1 == null ? num2 : num1; StringBuilder sb = new StringBuilder(); int l1 = num1.length() - 1; int l2 = num2.length() - 1; int carry = 0; while(l1 &gt;= 0 || l2 &gt;= 0){ int a = l1 &gt;= 0 ? num1.charAt(l1) - '0' : 0; int b = l2 &gt;= 0 ? num2.charAt(l2) - '0' : 0; int sum = a+b+carry; carry = sum / 10; sb.append(sum % 10); if(l1 &gt;= 0) l1--; if(l2 &gt;= 0) l2--; } if(carry == 1) sb.append(&quot;1&quot;); return sb.reverse().toString(); }} 字符串相减https://mp.weixin.qq.com/s/RtAoA1hdf0h1PaVxRj_fzA 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class TestStringJian { public static void main(String[] args) { String num1 = &quot;123&quot;; String num2 = &quot;251&quot;; System.out.println(stringJian(num1 , num2)); } // 字符串相减 , 注意题目限制了 nums1 和 num2 为非负整数 public static String stringJian(String num1 , String num2){ String res; //1. 先判断两个字符串哪个更大,将更大的放前面 if (Integer.parseInt(num1) &gt; Integer.parseInt(num2)){ res = sub(num1 , num2); }else if (Integer.parseInt(num1) &lt; Integer.parseInt(num2)){ res = sub(num2 , num1); res = &quot;-&quot; + res; // 需要加上负号 }else res = &quot;0&quot;; return res; } // nums1 - num2 默认 nums1 比 nums2 大 private static String sub(String num1, String num2) { StringBuilder sb = new StringBuilder(); int l1 = num1.length()-1; int l2 = num2.length()-1; int carry = 0; // carry 表示当前位置的数有没有被上一位借走1，只有0和1两种可能 while (l1 &gt;= 0 || l2 &gt;= 0){ int a = l1 &gt;= 0 ? num1.charAt(l1) - '0' : 0; int b = l2 &gt;= 0 ? num2.charAt(l2) - '0' : 0; int sum = 0; if (a - b - carry &gt; 0){ sum = a - b - carry; carry = 0; }else { sum = (a - b - carry + 10) % 10; carry = 1; } sb.append(sum); l1--; l2--; } //先翻转再删除前面的0 sb = sb.reverse(); int index = 0; while(index &lt; sb.length() &amp;&amp; sb.charAt(index) == '0') index++; return sb.substring(index); }} 43. 字符串相乘1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution { public String multiply(String s, String p) { // num1的第i位(高位从0开始)和num2的第j位相乘的结果在乘积中的位置是[i+j, i+j+1] /** 1 2 3 4 5 6 1 8 1 2 6 1 5 1 0 5 1 2 8 4 = [5 6 0 8 8] */ if(s.equals(&quot;0&quot;) || p.equals(&quot;0&quot;)) return &quot;0&quot;; int ls = s.length() , lp = p.length(); // 最多 就是 ls + lp 位数 int[] res = new int[ls + lp]; for(int i = ls - 1; i &gt;= 0 ; i--){ for(int j = lp-1 ; j &gt;= 0 ; j--){ int a = s.charAt(i) - '0'; int b = p.charAt(j) - '0'; int sum = a * b + nums[i+j+1]; nums[i+j+1] = sum % 10; nums[i+j] += sum/10; } } StringBuilder sb = new StringBuilder(); for(int i= 0 ; i&lt;ls+lp ; i++){ if(i == 0 &amp;&amp; res[i] == 0) continue; // 去掉第一个0 sb.append(res[i]); } return sb.toString(); }} 227. 基本计算器 II(没有括号)12345678910111213141516171819202122232425262728293031323334class Solution { public int calculate(String s) { Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); char tem = '+'; for(int i = 0 ; i &lt; s.length() ; i++){ char c = s.charAt(i); if(c == ' ') continue; // 如果当前字符是数字的话 if(Character.isDigit(c)){ int num = c - '0'; int j = i+1; // 注意j指针从后面位数开始 while(j &lt; s.length() &amp;&amp; Character.isDigit(s.charAt(j))){ num = num * 10 + s.charAt(j) - '0'; j++; } // 注意更新i指针，更新为 j前面一个 i = j-1; // 判断符号了 if(tem == '+') stack.push(num); else if(tem == '-') stack.push(-num); else if(tem == '*') stack.push(stack.pop() * num); else if(tem == '/') stack.push(stack.pop() / num); }else tem = c; //当前字符不是数字，说明是符号，用tem指代 } int res = 0; for(int num : stack) res += num; return res; }} 补充题36进制加法12345678910111213141516171819202122232425262728293031323334353637383940// 将数字转换成 字符 static char getChar(int n){ if (n &lt;= 9) return (char) n; else return (char) (n - 10 + 'a'); // a是10 } //将字符转换成数字 static int getNum(char c){ if (c &gt;= '0' &amp;&amp; c &lt;= '9' ) return c - '0'; else return c + 10 - 'a'; } //36进制由0-9，a-z，共36个字符表示,如下 // 0123456789abcdefghigklmnopqrstuvwxyz public static String add36Strings(String num1 , String num2){ int len1 = num1.length(); int len2 = num2.length(); int carry = 0; StringBuilder sb = new StringBuilder(); int i = len1-1; int j = len2-1; // 下面和字符串相加是一样的,把10改成36即可 while (i &gt;= 0 || j &gt;= 0){ int a = i &gt;= 0 ? getNum(num1.charAt(i)) : 0; int b = j &gt;= 0 ? getNum(num2.charAt(j)) : 0; int sum = a + b + carry; carry = sum / 36; sb.append(sum % 36); i--; j--; } if (carry == 1) sb.append(1); return sb.reverse().toString(); } System.out.println(getNum('b')); // 11 System.out.println(getChar(25)); // p String s = add36Strings(&quot;1b&quot;, &quot;2x&quot;); System.out.println(s); // 48 67. 二进制求和1234567891011121314151617181920212223class Solution { public String addBinary(String a, String b) { StringBuilder sb = new StringBuilder(); int i = a.length()-1; int j = b.length()-1; int carry = 0; while(i &gt;= 0 || j &gt;= 0){ int num1 = i &gt;= 0 ? a.charAt(i)-'0' : 0; int num2 = j &gt;= 0 ? b.charAt(j)-'0' : 0; int sum = num1 + num2 + carry; carry = sum / 2; sb.append(sum % 2); i--; j--; } if(carry == 1) sb.append(1); return sb.reverse().toString(); }} 动态规划91. 解码方法1234567891011121314151617181920212223class Solution { public int numDecodings(String s) { int n = s.length(); if (n == 0) return 0; //dp定义为由前i个数字解码得到的字符串 //对于最后一个字母进行情况的划分= 最后一个字母是1位数 + 最后一个字母是两位数 //f(i) = f(i-1) + f(i-2) int[] dp = new int[n+1]; dp[0] = 1; for (int i = 1; i &lt;= n ; i++) { //1. 如果最后一个数字不是0，则 23/234 的是一样的 //2. 如果最后一个数字是0，只能看0前面一个数满不满足条件了 比如 1520 是可以的，但是 1550其实就不能编码了 if (s.charAt(i-1) != '0') dp[i] = dp[i-1]; if (i &gt;= 2){ //2. 如果最后一个数是10-26之间的话，比如 1520 说明除了dp[i-1] 外还可以 加上dp[i-2] int num = Integer.parseInt(s.substring(i-2 , i)); if (num &gt;= 10 &amp;&amp; num &lt;= 26) dp[i] += dp[i-2]; } } return dp[n]; }} IP地址与int整数的转换https://www.nowcoder.com/questionTerminal/66ca0e28f90c42a196afd78cc9c496ea 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import java.util.*;//遇到的问题，数组越界，输入格式不正确，应都为String输入；//int型溢出，要换为long型public class Main{ public static void main(String[] args) { Scanner in=new Scanner(System.in); while(in.hasNext()){ String ip=in.nextLine(); String p=in.nextLine(); System.out.println(IptoTen(ip)); TentoIp(p); } } private static void TentoIp(String p) { long temp=Long.parseLong(p); String ip=Long.toBinaryString(temp); StringBuilder sb=new StringBuilder(); if(ip.length()&lt;32){ for(int i=0;i&lt;(32-ip.length());i++){ sb.append(0); } sb.append(ip); }else if(ip.length()==32){ sb.append(ip); } for(int i=0;i&lt;sb.length()-8;i=i+8){ System.out.print(Integer.parseInt(sb.substring(i,i+8),2)+&quot;.&quot;); } System.out.println(Integer.parseInt(sb.substring(sb.length()-8,sb.length()),2)); } private static long IptoTen(String ip) { String[] arr=ip.split(&quot;\\\\.&quot;); long n=Long.parseLong(arr[0]); for(int i=1;i&lt;arr.length;i++){ n=n&lt;&lt;8; n=n+Long.parseLong(arr[i]); } return n; } } 阿拉伯数字转汉字（有小数点）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class NumberFormatUtil { static String[] units = {&quot;&quot;,&quot;十&quot;,&quot;百&quot;,&quot;千&quot;,&quot;万&quot;,&quot;十万&quot;,&quot;百万&quot;,&quot;千万&quot;,&quot;亿&quot;,&quot;十亿&quot;,&quot;百亿&quot;,&quot;千亿&quot;,&quot;万亿&quot; }; static char[] numArray = {'零','一','二','三','四','五','六','七','八','九'}; //将整数转换成汉字数字 public static String formatInteger(int num) { char[] val = String.valueOf(num).toCharArray(); int len = val.length; StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; len; i++) { String m = val[i] + &quot;&quot;; int n = Integer.valueOf(m); boolean isZero = n == 0; String unit = units[(len - 1) - i]; if (isZero) { if ('0' == val[i - 1]) { continue; } else { sb.append(numArray[n]); } } else { sb.append(numArray[n]); sb.append(unit); } } return sb.toString(); } //将小数转换成汉字数字 public static String formatDecimal(double decimal) { String decimals = String.valueOf(decimal); int decIndex = decimals.indexOf(&quot;.&quot;); int integ = Integer.valueOf(decimals.substring(0, decIndex)); int dec = Integer.valueOf(decimals.substring(decIndex + 1)); String result = formatInteger(integ) + &quot;.&quot; + formatFractionalPart(dec); return result; } //格式化小数部分的数字 public static String formatFractionalPart(int decimal) { char[] val = String.valueOf(decimal).toCharArray(); int len = val.length; StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; len; i++) { int n = Integer.valueOf(val[i] + &quot;&quot;); sb.append(numArray[n]); } return sb.toString(); } private static void print(Object arg0) { System.out.println(arg0); } public static void main(String[] args) { int num = 245000006; String numStr = formatInteger(num); print(&quot;num= &quot; + num + &quot;, convert result: &quot; + numStr); double decimal = 245006.234206; print(&quot;============================================================&quot;); String decStr = formatDecimal(decimal); print(&quot;decimal= &quot; + decimal + &quot;, decStr: &quot; + decStr); }}","link":"/2021/10/09/%E7%AE%97%E6%B3%95%E9%A2%98%EF%BC%88%E4%BA%8C%EF%BC%89%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"title":"算法题（八）滑动窗口","text":"239. 滑动窗口最大值123456789101112131415161718192021222324252627class Solution { public int[] maxSlidingWindow(int[] nums, int k) { int n = nums.length; if(n == 0) return new int[0]; //1. 长度 为 n 的数组中有 n-k+1 个连续窗口 int[] res = new int[n-k+1]; //2. 用一个单调队列，保证严格递减，最大值在对头 LinkedList&lt;Integer&gt; list = new LinkedList&lt;&gt;(); for(int i = 0 ; i &lt; n ; i++){ //3. 如果 当前值比队列中的最后一个值要大， 则 把队列的最后一个值弹出 // 1 3 当到3时，需要把1弹出 while(!list.isEmpty() &amp;&amp; nums[list.peekLast()] &lt; nums[i]) list.pollLast(); list.addLast(i); //4. 如果当前值已经 比队列的 头部形成的窗口大于 k，则把 队列头部最大的弹出 // 1 3 -1 -3 5 当进入 5 的时候 ，队列中 只有 3 和 5，但是 5的索引 - 3的索引 = 3，如果大于k就说明不在一个窗口内了 if(i - k + 1 &gt; list.peekFirst()) list.pollFirst(); //5. 如果已经形成窗口了，就用res记录 if(i - k + 1 &gt;=0) res[i-k+1] = nums[list.peekFirst()]; } return res; }} 76. 最小覆盖子串1234567891011121314151617181920212223242526272829303132333435363738394041class Solution { public String minWindow(String s, String t) { //1. 用一个 map 记录 t中的所有字符及出现次数 Map&lt;Character , Integer&gt; need = new HashMap&lt;&gt;(); for(char c : t.toCharArray()) need.put(c , need.getOrDefault(c , 0) + 1); //2. 用一个 map 记录窗口中出现的字符 及 次数 Map&lt;Character,Integer&gt; win = new HashMap&lt;&gt;(); int l = 0 , r = 0; // 左右边界 int count = 0; int len = Integer.MAX_VALUE , start = 0 , end = 0; // 记录最大值和起始位置、终止位置 while(r &lt; s.length()){ //3. 右边界开始移动,判断当前字符是否是需要的 char a = s.charAt(r); if(need.containsKey(a)){ win.put(a , win.getOrDefault(a , 0) + 1); if(win.get(a).intValue() == need.get(a).intValue()) count++; } r++; //4. 当满足条件时， 更新len之后 ， 左边界开始移动 while(count == need.size()){ if(len &gt; r - l){ len = r-l; start = l; end = r; } char b = s.charAt(l); if(need.containsKey(b)){ win.put(b , win.getOrDefault(b ,0) -1); if(win.get(b) &lt; need.get(b)) count--; } l++; } } return s.substring(start , end); }}","link":"/2021/10/10/%E7%AE%97%E6%B3%95%E9%A2%98%EF%BC%88%E5%85%AB%EF%BC%89%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/"},{"title":"算法题（四）回溯算法","text":"回溯算法专题 全排列问题需要一个 boolean 数组来判断 46. 全排列1234567891011121314151617181920212223242526class Solution { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) { if(nums == null || nums.length == 0) return res; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); boolean[] isv = new boolean[nums.length]; // 需要一个布尔数组来判断前面这个数是否已经加入了 dfs(nums , list , isv); return res; } void dfs(int[] nums , List&lt;Integer&gt; list , boolean[] isv){ if(list.size() == nums.length){ res.add(new ArrayList&lt;&gt;(list)); return; } for(int i = 0 ; i &lt; nums.length ; i++){ if(isv[i]) continue; list.add(nums[i]); isv[i] = true; dfs(nums , list , isv); list.remove(list.size()-1); //回溯 isv[i] = false; } }} 47. 全排列 II1234567891011121314151617181920212223242526272829303132class Solution { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; permuteUnique(int[] nums) { List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); boolean[] isv = new boolean[nums.length]; Arrays.sort(nums); // 排序方便去重 dfs(nums , isv , list); return res; } void dfs(int[] nums , boolean[] isv , List&lt;Integer&gt; list){ if(nums.length == list.size()){ res.add(new ArrayList&lt;&gt;(list)); return; } for(int i = 0 ; i &lt; nums.length ; i++){ if(isv[i]) continue; // 如果当前数和前一个数一样，且前一个数已经被访问了，就跳过 if(i &gt; 0 &amp;&amp; nums[i] == nums[i-1] &amp;&amp; isv[i-1]) continue; list.add(nums[i]); isv[i] = true; dfs(nums ,isv , list); list.remove(list.size()-1); isv[i] = false; } }} 组合总和问题39. 组合总和12345678910111213141516171819202122232425class Solution { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; combinationSum(int[] nums, int target) { if(nums == null || nums.length == 0) return res; int n = nums.length; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); dfs(nums , 0 , target , list); return res; } void dfs(int[] nums, int start , int target , List&lt;Integer&gt; list){ if(target &lt; 0) return; if(target == 0){ res.add(new ArrayList&lt;&gt;(list)); return; } for(int i = start ; i &lt; nums.length ; i++){ list.add(nums[i]); dfs(nums , i , target - nums[i] , list); list.remove(list.size()-1); } }} 40. 组合总和 II123456789101112131415161718192021222324252627282930class Solution { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; combinationSum2(int[] nums, int target) { if(nums == null || nums.length == 0) return res; int n = nums.length; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); Arrays.sort(nums); dfs(nums , 0 , target , list ); return res; } void dfs(int[] nums , int start , int target , List&lt;Integer&gt; list){ if(target &lt; 0) return; if(target == 0){ res.add(new ArrayList&lt;&gt;(list)); return; } for(int i = start ; i &lt; nums.length ; i++){ if(i &gt; start &amp;&amp; nums[i] == nums[i-1]) continue; list.add(nums[i]); dfs(nums , i+1 , target-nums[i] , list) ; list.remove(list.size()-1); } }} 77. 组合1234567891011121314151617181920212223242526class Solution { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; combine(int n, int k) { int[] nums = new int[n]; for(int i = 0 ; i &lt; n ; i++) nums[i] = i+1; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); dfs(nums , 0 , list , k); return res; } void dfs(int[] nums , int start, List&lt;Integer&gt; list , int k){ if(list.size() == k){ res.add(new ArrayList&lt;&gt;(list)); return; } for(int i = start ; i &lt; nums.length ; i++){ list.add(nums[i]); dfs(nums , i+1 , list , k); list.remove(list.size()-1); } }} 子集问题78. 子集1234567891011121314151617181920class Solution { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums) { if(nums == null || nums.length == 0) return res; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); dfs(nums , 0 , list); return res; } void dfs(int[] nums, int start , List&lt;Integer&gt; list){ res.add(new ArrayList&lt;&gt;(list)); for(int i = start ; i &lt; nums.length ; i++){ list.add(nums[i]); dfs(nums , i+1 , list); list.remove(list.size()-1); } } } 90. 子集 II12345678910111213141516171819202122232425class Solution { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; subsetsWithDup(int[] nums) { if(nums == null || nums.length == 0) return res; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); Arrays.sort(nums); // 有重复元素就需要排序方便去重 dfs(nums , 0 , list); return res; } void dfs(int[] nums , int start , List&lt;Integer&gt; list){ res.add(new ArrayList&lt;&gt;(list)); for(int i = start ; i &lt; nums.length ; i++){ if(i &gt; start &amp;&amp; nums[i] == nums[i-1]) continue; list.add(nums[i]); dfs(nums , i+1 , list); list.remove(list.size()-1); } }} 矩阵中的问题79. 单词搜索1234567891011121314151617181920212223242526272829303132333435363738394041class Solution { public boolean exist(char[][] board, String word) { int m = board.length; if(m == 0) return false; int n = board[0].length; boolean[][] isv = new boolean[m][n]; for(int i = 0 ; i &lt; m ; i++){ for(int j = 0 ; j &lt; n ; j++){ if(board[i][j] == word.charAt(0) &amp;&amp; dfs(board , word , 0 , i , j ,isv)) return true; } } return false; } // 判断 从 word的第index位开始，是否存在对应的路径，从（i , j）开始 boolean dfs(char[][] board , String word , int index , int i , int j , boolean[][] isv){ //1. 递归截止条件 if(index == word.length()) return true; //2. 不符合的情况 if(i &lt; 0 || i &gt;= board.length || j &lt; 0 || j &gt;= board[0].length) return false; if(word.charAt(index) != board[i][j] || isv[i][j]) return false; //3. 将当前位置标记为已经访问过了 isv[i][j] = true; //4. 遍历后续的上下左右四个方向 if(dfs(board , word , index +1 , i+1 , j ,isv) || dfs(board , word , index +1 , i-1 , j ,isv) || dfs(board , word , index +1 , i , j+1 ,isv) || dfs(board , word , index +1 , i , j-1 ,isv)) return true; //4. 回溯 isv[i][j] = false; return false; }}","link":"/2021/10/10/%E7%AE%97%E6%B3%95%E9%A2%98%EF%BC%88%E5%9B%9B%EF%BC%89%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/"},{"title":"计算机网络（三）网络层","text":"网络层的主要作用是实现主机与主机之间的通信，主要讲述网络层的几个协议： IP基础知识 ARP协议 NAT协议 ICMP协议 IP 的作⽤是主机之间通信⽤的，⽽ MAC 的作⽤则是实现「直连」的两个设备之间通信，⽽ IP 则负责在「没有直连」的两个⽹络之间进⾏通信传输 IP地址IP 地址（IPv4 地址）由 32 位正整数来表示，IP 地址在计算机是以⼆进制的⽅式处理的。 分类基础分类互联网诞生之初，IP 地址分类成了 5 种类型，分别是 A 类、B 类、C 类、D 类、E 类。 ABC类的IP地址主要是网络号和主机号的位数不同，可以理解为小区地址和单元楼不一样。 如何看C类地址的最大主机个数呢？ 其实就是看主机号的个数，比如C类地址主机号有8位，所以最大主机个数为 2^8 - 2 = 254个。 这里减去2是因为有两个ip地址是特殊的： 主机号全是1：用于制定网络下的所有主机，用于广播 主机号全是0：用于指定某个网络 广播地址用于什么？ IP地址的优点： 不管是路由器还是主机解析到⼀个 IP 地址时候，我们判断其 IP 地址的⾸位是否为 0，为 0 则为 A 类地址，那么就能很快的找出⽹络地址和主机地址。 第一位是0，说明是A类地址，第二位是0说明是B类地址，第三位为0说明是C类地址，第四位为0说明是D类地址。。 IP地址的缺点： 同⼀⽹络下没有地址层次，⽐如⼀个公司⾥⽤了 B 类地址，但是可能需要根据⽣产环境、测试环境、开发环境来划分地址层次，⽽这种 IP 分类是没有地址层次划分的功能，所以这就缺少地址的灵活性。 A、B、C类有个尴尬处境，就是不能很好的与现实⽹络匹配。C 类地址能包含的最⼤主机数ᰁ实在太少了，⽽ B 类地址能包含的最⼤主机数ᰁ⼜太多了， 无分类地址CIDR32 ⽐特的 IP 地址被划分为两部分，前⾯是⽹络号，后⾯是主机号。 表示形式 a.b.c.d/x ，其中 /x 表示前 x 位属于⽹络号， x 的范围是 0 ~ 32 ，这就使得 IP 地址更加具有灵活性。 ⽐如 10.100.122.2/24，这种地址表示形式就是 CIDR，/24 表示前 24 位是⽹络号，剩余的 8 位是主机号。 还有另⼀种划分⽹络号与主机号形式，那就是⼦⽹掩码，掩码的意思就是掩盖掉主机号，剩余的就是⽹络号。 将⼦⽹掩码和 IP 地址按位计算 AND，就可得到⽹络号： 为什么要分离网络号和主机号 因为两台计算机要通讯，⾸先要判断是否处于同⼀个⼴播域内，即⽹络地址是否相同。如果⽹络地址相同，表明接受⽅在本⽹络上，那么可以把数据包直接发送到⽬标主机。 路由器寻址⼯作中，也就是通过这样的⽅式来找到对应的⽹络号的，进⽽把数据包转发给对应的⽹络内。 通过子网掩码可以划分出网络号和主机号，实际上还可以划分子网。 ⼦⽹划分实际上是将主机地址分为两个部分：⼦⽹⽹络地址和⼦⽹主机地址 假设对 C 类地址进⾏⼦⽹划分，⽹络地址 192.168.1.0，使⽤⼦⽹掩码 255.255.255.192 对其进⾏⼦⽹划分。 C 类地址中前 24 位是⽹络号，最后 8 位是主机号，根据⼦⽹掩码可知从 8 位主机号中借⽤ 2 位作为⼦⽹号。 由于⼦⽹⽹络地址被划分成 2 位，那么⼦⽹地址就有 4 个，分别是 00、01、10、11，具体划分如下图： 公有IP地址和私有IP地址IP地址和路由控制IP地址中的网络号部分主要是用于路由控制。 在主机和路由器上都会有各⾃的路由器控制表。路由控制表中记录着⽹络地址与下⼀步应该发送⾄路由器的地址。 在发送 IP 包时，⾸先要确定 IP 包⾸部中的⽬标地址，再从路由控制表中找到与该地址具有相同⽹络地址的记录，根据该记录将 IP 包转发给相应的下⼀个路由器。如果路由控制表中存在多条相同⽹络地址的记录，就选择相同位数最多的⽹络地址，也就是最⻓匹配。 主机 A 要发送⼀个 IP 包，其源地址是 10.1.1.30 和⽬标地址是 10.1.2.10 ，由于没有在主机 A 的路由表找到与⽬标地址 10.1.2.10 的⽹络地址，于是包被转发到默认路由（路由器 1 ） 路由器 1 收到 IP 包后，也在路由器 1 的路由表匹配与⽬标地址相同的⽹络地址记录，发现匹配到了，于是就把 IP 数据包转发到了 10.1.0.2 这台路由器 2 路由器 2 收到后，同样对⽐⾃身的路由表，发现匹配到了，于是把 IP 包从路由器 2 的 10.1.2.1 这个接⼝出去，最终经过交换机把 IP 数据包转发到了⽬标主机 127.0.0.1作为本地地址，使用这个ip时，数据包是不会流向网络的 IP数据分片其中，我们最常⻅数据链路是以太⽹，它的 MTU 是 1500 字节。 那么当 IP 数据包⼤⼩⼤于 MTU 时， IP 数据包就会被分⽚。 经过分⽚之后的 IP 数据报在被᯿组的时候，只能由⽬标主机进⾏，路由器是不会进⾏重组的。 在分⽚传输中，⼀旦某个分⽚丢失，则会造成整个 IP 数据报作废，所以 TCP 引⼊了 MSS 也就是在 TCP 层进⾏ 分⽚不由 IP 层分⽚，那么对于 UDP 我们尽量不要发送⼀个⼤于 MTU 的数据报⽂。 ARP协议在传输⼀个 IP 数据报的时候，确定了源 IP 地址和⽬标 IP 地址后，就会通过主机「路由表」确定 IP 数据包下⼀跳。 然⽽，⽹络层的下⼀层是数据链路层，所以我们还要知道「下⼀跳」的 MAC 地址。 由于主机的路由表中可以找到下⼀跳的 IP 地址，所以可以通过 ARP 协议，求得下⼀跳的 MAC 地址。 如何实现的呢ARP 是借助 ARP 请求与 ARP 响应两种类型的包确定 MAC 地址的： 简单来说就是： 主机会通过⼴播发送 ARP 请求，这个包中包含了想要知道的 MAC 地址的主机 IP 地址。 当同个链路中的所有设备收到 ARP 请求时，会去拆开 ARP 请求包⾥的内容，如果 ARP 请求包中的⽬标 IP 地址与⾃⼰的 IP 地址⼀致，那么这个设备就将⾃⼰的 MAC 地址塞⼊ ARP 响应包返回给主机。 操作系统通常会把第⼀次通过 ARP 获取的 MAC 地址缓存起来，以便下次直接从缓存中找到对应 IP 地址的 MAC地址。 NAT协议⽹络地址转换 NAT 的⽅法用于缓解了 IPv4 地址耗尽的问题。其实就是把私有IP地址转换成公有IP地址。 普通的 NAT 转换（因为要1对1）没什么意义。 所以由于绝⼤多数的⽹络应⽤都是使⽤传输层协议 TCP 或 UDP 来传输数据的，可以把 IP 地址 + 端⼝号⼀起进⾏转换，这就叫NAPT。 图中有两个客户端 192.168.1.10 和 192.168.1.11 同时与服务器 183.232.231.172 进⾏通信，并且这两个客户端的本地端⼝都是 1025。 此时，两个私有 IP 地址都转换 IP 地址为公有地址 120.229.175.121，但是以不同的端⼝号作为区分。于是，⽣成⼀个 NAPT 路由器的转换表，就可以正确地转换地址跟端⼝的组合，令客户端 A、B 能同时与服务器之间进⾏通信。 这种转换表在 NAT 路由器上⾃动⽣成。例如，在 TCP 的情况下，建⽴ TCP 连接⾸次握⼿时的 SYN 包⼀经发出，就会⽣成这个表。⽽后⼜随着收到关闭连接时发出 FIN 包的确认应答从表中被删除 NAPT的缺点： todo ICMP协议ICMP 全称是 Internet Control Message Protocol，也就是互联⽹控制报⽂协议。大致可以分为两类： ⼀类是⽤于诊断的查询消息，也就是「查询报⽂类型」 另⼀类是通知出错原因的错误消息，也就是「差错报⽂类型」 ICMP 主要的功能包括：确认 IP 包是否成功送达⽬标地址、报告发送过程中 IP 包被废弃的原因和改善⽹络设置等。 ping是利用了查询报文类型。 原理： ping 命令执⾏的时候，源主机⾸先会构建⼀个 ICMP 回送请求消息数据包。 ICMP 数据包内包含多个字段，最᯿要的是两个： 第⼀个是类型，对于回送请求消息⽽⾔该字段为 8 ； 另外⼀个是序号，主要⽤于区分连续 ping 的时候发出的多个数据包。每发出⼀个请求数据包，序号会⾃动加 1 。 为了能够计算往返时间 RTT ，它会在报⽂的数据部分插⼊发送时间。 然后，由 ICMP 协议将这个数据包连同地址 192.168.1.2 ⼀起交给 IP 层。IP 层将以 192.168.1.2 作为⽬的地址， 本机 IP 地址作为源地址，协议字段设置为 1 表示是 ICMP 协议，再加上⼀些其他控制信息，构建⼀个 IP 数据包。 接下来，需要加⼊ MAC 头。如果在本地 ARP 映射表中查找出 IP 地址 192.168.1.2 所对应的 MAC 地址，则可以直接使⽤；如果没有，则需要发送 ARP 协议查询 MAC 地址，获得 MAC 地址后，由数据链路层构建⼀个数据帧，⽬的地址是 IP 层传过来的 MAC 地址，源地址则是本机的 MAC 地址；还要附加上⼀些控制信息，依据以太⽹的介质访问规则，将它们传送出去。 主机 B 收到这个数据帧后，先检查它的⽬的 MAC 地址，并和本机的 MAC 地址对⽐，如符合，则接收，否则就 丢弃。 接收后检查该数据帧，将 IP 数据包从帧中提取出来，交给本机的 IP 层。同样，IP 层检查后，将有⽤的信息提取后 交给 ICMP 协议。 主机 B 会构建⼀个 ICMP 回送响应消息数据包，回送响应数据包的类型字段为 0 ，序号为接收到的请求数据包中的序号，然后再发送出去给主机 A。 在规定的时候间内，源主机如果没有接到 ICMP 的应答包，则说明⽬标主机不可达；如果接收到了 ICMP 回送响应消息，则说明⽬标主机可达。 此时，源主机会检查，⽤当前时刻减去该数据包最初从源主机上发出的时刻，就是 ICMP 数据包的时间延迟。 traceroutetraceroute 的第⼀个作⽤就是故意设置特殊的 TTL，来追踪去往⽬的地时沿途经过的路由器 12345# shengbinbin @ binshow in ~ [9:29:33]$ traceroute 192.168.1.100traceroute to 192.168.1.100 (192.168.1.100), 64 hops max, 52 byte packets 1 hiwifi (192.168.199.1) 10.218 ms 3.498 ms 2.885 ms 2 192.168.1.3 (192.168.1.3) 3032.726 ms !H 3065.937 ms !H 3071.972 ms !H 原理就是利⽤ IP 包的⽣存期限 从 1 开始按照顺序递增的同时发送 UDP 包，强制接收 ICMP 超时消息的⼀种⽅法 ⽐如，将 TTL 设置 为 1 ，则遇到第⼀个路由器，就牺牲了，接着返回 ICMP 差错报⽂⽹络包，类型是时间超时。 接下来将 TTL 设置为 2 ，第⼀个路由器过了，遇到第⼆个路由器也牺牲了，也同时返回了 ICMP 差错报⽂数据包，如此往复，直到到达⽬的主机。 这样的过程，traceroute 就可以拿到了所有的路由器 IP。 发送⽅如何知道发出的 UDP 包是否到达了⽬的主机呢？ traceroute 在发送 UDP 包时，会填⼊⼀个不可能的端⼝号值作为 UDP ⽬标端⼝号（⼤于 3000 ）。当⽬的主机，收到 UDP 包后，会返回 ICMP 差错报⽂消息，但这个差错报⽂消息的类型是「端⼝不可达」。 所以，当差错报⽂类型是端⼝不可达时，说明发送⽅发出的 UDP 包到达了⽬的主机。 traceroute 还有⼀个作⽤是故意设置不分⽚，从⽽确定路径的 MTU。因为有的时候我们并不知道路由器的 MTU ⼤⼩，以太⽹的数据链路上的 MTU 通常是 1500 字节，但是⾮以外⽹的 MTU 值就不⼀样了，所以我们要知道 MTU 的⼤⼩，从⽽控制发送的包⼤⼩。 ⾸先在发送端主机发送 IP 数据报时，将 IP 包⾸部的分⽚禁⽌标志位设置为 1。根据这个标志位，途中的路由器不会对⼤数据包进⾏分⽚，⽽是将包丢弃。 随后，通过⼀个 ICMP 的不可达消息将数据链路上 MTU 的值⼀起给发送主机，不可达消息的类型为「需要进⾏分⽚但设置了不分⽚位」。 发送主机端每次收到 ICMP 差错报⽂时就减少包的⼤⼩，以此来定位⼀个合适的 MTU 值，以便能到达⽬标主机","link":"/2021/10/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%89%EF%BC%89%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"title":"算法题（六）动态规划","text":"动态规划专题！ 回文串问题5. 最长回文子串12345678910111213141516171819202122232425262728293031class Solution { public String longestPalindrome(String s) { if(s == null || s.length() == 0) return &quot;&quot;; int n = s.length(); //1. definition : dp[i][j] 表示 s[i-j]是否可以组成回文串 boolean[][] dp = new boolean[n][n]; //2. base case : 一个字符肯定是回文 for(int i = 0 ; i &lt; n ; i++) dp[i][i] = true; //3. 遍历 i往前， j往后 int start = 0; int maxLen = 1; for(int i = n-2 ; i &gt;= 0 ; i--){ for(int j = i+1 ; j &lt; n ;j++){ if(s.charAt(i) != s.charAt(j)) dp[i][j] = false; else{ if( j - i &lt; 3) dp[i][j] = true; // aba else dp[i][j] = dp[i+1][j-1]; } if(dp[i][j] &amp;&amp; j - i + 1 &gt; maxLen){ start = i; maxLen = j - i + 1; } } } return s.substring(start , start + maxLen); }} 131. 分割回文串1234567891011121314151617181920212223242526272829303132333435class Solution { List&lt;List&lt;String&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;String&gt;&gt; partition(String s) { if(s == null || s.length() == 0) return res; List&lt;String&gt; list = new ArrayList&lt;&gt;(); dfs(s , 0 , list); return res; } void dfs(String s , int start , List&lt;String&gt; list){ if(start == s.length()){ res.add(new ArrayList&lt;&gt;(list)); return; } for(int i = start ; i &lt; s.length() ; i++){ String tem = s.substring(start , i+1); if(isHuiWen(tem)){ list.add(tem); dfs(s , i+1 , list); list.remove(list.size()-1); } } } boolean isHuiWen(String s){ int l = 0 , r = s.length()-1; while(l &lt; r){ if(s.charAt(l) != s.charAt(r)) return false; l++; r--; } return true; }} 132. 分割回文串 II12345678910111213141516171819202122232425262728293031323334353637383940class Solution { public int minCut(String s) { if(s == null || s.length() == 0) return 0; int n = s.length(); // defination: dp[i] 表示 s[0-i]拆成回文的最小次数 int[] dp = new int[n]; // base case for(int i = 0 ; i &lt; n ; i++) dp[i] = i; // 辅助 dp, is[i][j] 表示 s[I-J]是否可以构成回文 boolean[][] is = new boolean[n][n]; for(int i = 0 ; i &lt; n ; i++) is[i][i] = true; for(int i = n-2 ; i &gt;= 0 ; i--){ for(int j = i+1 ; j &lt; n ; j++){ if(s.charAt(i) != s.charAt(j)) is[i][j] = false; else { if(j - i &lt; 3) is[i][j] = true; else is[i][j] = is[i+1][j-1]; } } } //借助辅助dp开始遍历 for(int i = 0 ; i &lt; n ; i++){ //如果已经构成回文了，则直接设置为1 if(is[0][i]){ dp[i] = 0; continue; } //往前找，一直找到能构成回文的 for(int j = 0; j &lt; i ; j++){ if(is[j+1][i]) dp[i] = Math.min(dp[i] , dp[j]+1); } } return dp[n-1]; }} 647. 回文子串个数123456789101112131415161718192021222324class Solution { public int countSubstrings(String s) { if(s == null || s.length() == 0) return 0; int n = s.length(); int res = n; //每一个字符都是回文 // dp[i][j] 表示 s[i-j]是否是回文 boolean[][] dp = new boolean[n][n]; for(int i = 0 ; i &lt; n ; i++) dp[i][i] = true; for(int i = n-2 ; i &gt;= 0 ; i--){ for(int j = i+1 ; j &lt; n ; j++){ if(s.charAt(i) != s.charAt(j)) dp[i][j] = false; else { if( j - i &lt; 3) dp[i][j] = true; else dp[i][j] = dp[i+1][j-1]; } if(dp[i][j]) res++; } } return res; }} 10. 正则表达式匹配123456789101112131415161718192021222324class Solution { public boolean isMatch(String s, String p) { int ls = s.length(); int lp = p.length(); // dp[i][j] 表示 s[0-i] 和 p[0-j] 之间是否匹配 boolean[][] dp = new boolean[ls+1][lp+1]; // base case dp[0][0] = true; for(int i = 2 ; i &lt;= lp ; i++) dp[0][i] = dp[0][i-2] &amp;&amp; p.charAt(i-1) =='*'; //开始遍历两个字符串,注意现在是从 1-ls 之间了 for(int i = 1 ; i &lt;= ls ; i++){ for(int j = 1 ; j &lt;= lp ; j++){ int m = i-1; int n = j-1; // aa ** // aa a* a. 这两种情况 if(p.charAt(n) == '*') dp[i][j] = dp[i][j-2] || dp[i-1][j] &amp;&amp; (s.charAt(m) == p.charAt(n-1) || p.charAt(n-1) == '.'); else if(p.charAt(n) == '.' || p.charAt(n) == s.charAt(m)) dp[i][j] = dp[i-1][j-1]; } } return dp[ls][lp]; }} 矩阵路径问题62. 不同路径123456789101112131415161718192021class Solution { public int uniquePaths(int m, int n) { if(m == 0 || n == 0) return 1; //1. defination: dp[i][j] 表示从 (0,0) - (i-1 , j-1)的路径数 int[][] dp = new int[m][n]; //2. base case dp[0][0] = 1; for(int i = 1 ; i &lt; m ; i++) dp[i][0] = dp[i-1][0]; for(int j = 1; j &lt; n ; j++) dp[0][j] = dp[0][j-1]; //3. 开始遍历 for(int i = 1; i &lt; m ; i++){ for(int j = 1 ; j &lt; n ; j++){ dp[i][j] = dp[i-1][j] + dp[i][j-1]; } } return dp[m-1][n-1]; }} 63. 不同路径 II12345678910111213141516171819202122232425262728class Solution { public int uniquePathsWithObstacles(int[][] matrix) { int m = matrix.length; if(m == 0) return 0; int n = matrix[0].length; //1. defination int[][] dp = new int[m][n]; //2. base case for(int i = 0 ; i &lt; m ; i++){ if (matrix[i][0] != 1) dp[i][0] = 1; else break; } for(int i = 0; i &lt; n ; i++){ if(matrix[0][i] != 1) dp[0][i] = 1; else break; } //3. 开始遍历 for(int i = 1 ; i &lt; m ; i++){ for(int j = 1; j &lt; n ;j++){ if(matrix[i][j] != 1) dp[i][j] = dp[i-1][j] + dp[i][j-1]; } } return dp[m-1][n-1]; }} 64. 最小路径和123456789101112131415161718192021class Solution { public int minPathSum(int[][] grid) { int m = grid.length; int n = grid[0].length; //1. defination : dp[i][j] 表示从[0,0] - [m-1 , n-1]之间的路径总和 int[][] dp = new int[m][n]; dp[0][0] = grid[0][0]; //2. base case for(int i = 1; i &lt; m ; i++) dp[i][0] = dp[i-1][0] + grid[i][0]; for(int i = 1; i &lt; n ; i++) dp[0][i] = dp[0][i-1] + grid[0][i]; //3. 开始遍历 for(int i = 1; i &lt; m ; i++){ for(int j = 1 ; j &lt; n ; j++){ dp[i][j] = Math.min(dp[i-1][j] , dp[i][j-1]) + grid[i][j]; } } return dp[m-1][n-1]; }} 120. 三角形最小路径和12345678910111213141516171819202122232425class Solution { // 看出一个矩形即可 public int minimumTotal(List&lt;List&lt;Integer&gt;&gt; triangle) { int m = triangle.size(); int n = triangle.get(m-1).size(); // dp[i][j] 表示 从 三角形的左上角到 mn位置的路径和 int[][] dp = new int[m][n]; //base case dp[0][0] = triangle.get(0).get(0); //开始遍历，需要考虑三种情况 for(int i = 1 ; i &lt; m ; i++){ for(int j = 0 ; j &lt;= i ; j++){ if(i == j) dp[i][j] = dp[i-1][j-1] + triangle.get(i).get(j); else if(j == 0) dp[i][j] = dp[i-1][j] + triangle.get(i).get(j); else dp[i][j] = Math.min(dp[i-1][j] , dp[i-1][j-1]) + triangle.get(i).get(j); } } // 找到最后一行的最小值 int res = Integer.MAX_VALUE; for(int i = 0 ; i &lt; n ; i++) res = Math.min(res , dp[m-1][i]); return res; }} 174. 地下城游戏12345678910111213141516171819202122class Solution { public int calculateMinimumHP(int[][] matrix) { //倒着dp int m = matrix.length; int n = matrix[0].length; int[][] dp = new int[m][n]; //需要在遍历的时候保证 dp[i][j] &gt;= 1 for(int i = m - 1 ; i &gt;= 0 ; i--){ for(int j = n-1 ; j &gt;= 0 ;j--){ if(i == m-1 &amp;&amp; j == n-1) dp[i][j] = Math.max(1, 1-matrix[i][j]); else if(i == m-1) dp[i][j] = Math.max(1 , dp[i][j+1] - matrix[i][j]); // 最后一行：只能从右往左走 else if(j == n-1) dp[i][j] = Math.max(1 , dp[i+1][j] - matrix[i][j]); //最后一列 else dp[i][j] = Math.max(1 , Math.min(dp[i+1][j] , dp[i][j+1]) - matrix[i][j]); } } return dp[0][0]; }} 329. 矩阵中的最长递增路径长度12345678910111213141516171819202122232425262728293031class Solution { public int longestIncreasingPath(int[][] matrix) { int m = matrix.length; if(m == 0) return 0; int n = matrix[0].length; //1. dp[i][j] 表示以 matrix[i][j]结尾的最长递增路径 int[][] dp = new int[m][n]; int res = 0; for(int i = 0 ; i &lt; m ; i++){ for(int j = 0 ; j &lt; n ; j++){ res = Math.max(res , dfs(i , j , Integer.MIN_VALUE , matrix , dp)); } } return res; } int dfs(int i , int j , int cur , int[][] matrix , int[][] dp){ if(i &lt; 0 || i &gt;= matrix.length || j &lt; 0 || j &gt;= matrix[0].length) return 0; //2. 如果当前值不小于 matrix[i][j] , 就不构成递增路径 if(matrix[i][j] &lt;= cur) return 0; if(dp[i][j] != 0) return dp[i][j]; int res = 0; res = Math.max(res , dfs(i+1 , j , matrix[i][j] , matrix , dp)); res = Math.max(res , dfs(i-1 , j , matrix[i][j] , matrix , dp)); res = Math.max(res , dfs(i , j+1 , matrix[i][j] , matrix , dp)); res = Math.max(res , dfs(i , j-1 , matrix[i][j] , matrix , dp)); dp[i][j] = res + 1; return res+1; }} 79. 单词搜索12345678910111213141516171819202122232425262728293031323334353637383940class Solution { public boolean exist(char[][] board, String word) { int m = board.length; int n = board[0].length; if(m == 0) return false; boolean[][] isv = new boolean[m][n]; for(int i = 0 ; i &lt; m ; i++){ for(int j = 0 ; j &lt; n ; j++){ if(board[i][j] == word.charAt(0) &amp;&amp; dfs(board , word ,0, i , j , isv)){ return true; } } } return false; } boolean dfs(char[][] board , String word , int index , int i , int j , boolean[][] isv){ //1. 递归截止条件 if(index == word.length()) return true; //2. 检查不符合的条件：i，j超了边界、或当前值和字符串的值不相等、或当前位置的字符串已经被访问了 if(i &lt; 0 || i &gt;= board.length || j &lt; 0 || j &gt;= board[0].length) return false; if(board[i][j] != word.charAt(index) || isv[i][j]) return false; //3. 将当前位置标记为已访问 isv[i][j] = true; //4. 遍历字符串后面的字符 if( dfs(board , word , index+1 , i+1 , j , isv) || dfs(board , word , index+1 , i-1 , j ,isv) || dfs(board , word , index+1 , i , j+1 , isv) || dfs(board , word , index+1 , i , j-1 , isv)) return true; //5. 回溯 isv[i][j] = false; return false; }} 221. 最大正方形为什么取三者中的最小值： 1234567891011121314151617181920212223242526class Solution { public int maximalSquare(char[][] matrix) { int raw = matrix.length; if(raw == 0) return 0; int col = matrix[0].length; // dp[i][j]表示以第i行第j列为右下角所能构成的最大正方形边长 int[][] dp = new int[raw+1][col+1]; int res = 0; // 记录最大的边长 for(int i = 1 ; i &lt;= raw ; i++){ for(int j = 1 ; j &lt;= col ;j++){ //如果当前数为1，则更新 dp[i][j] if(matrix[i-1][j-1] == '1'){ dp[i][j] = 1 + Math.min(Math.min(dp[i-1][j] , dp[i][j-1]) , dp[i-1][j-1]); res = Math.max(res , dp[i][j]); } } } return res*res; }} 85. 最大矩形1234567891011121314151617181920212223242526```# =====#### [70. 爬楼梯](https://leetcode-cn.com/problems/climbing-stairs/)```javaclass Solution { public int climbStairs(int n) { if(n == 1 || n == 2) return n; int[] dp = new int[n+1]; dp[0] = 1; dp[1] = 1; for(int i = 2 ; i &lt;= n ; i++) dp[i] = dp[i-2] + dp[i-1]; return dp[n]; }} 圆环回原点问题圆环上有10个点，编号为0~9。从0点出发，每次可以逆时针和顺时针走一步，问走n步回到0点共有多少种走法。 输入: 2输出: 2解释：有2种方案。分别是0-&gt;1-&gt;0和0-&gt;9-&gt;0 123456789101112131415161718192021222324/** * 圆环回原点问题 * @param step：走几步回到原点 * @param len : 圆环长度 * @return */ public static int backToOrigin(int step , int len){ //走n步到0的方案数= 走n-1步到1的方案数 + 走n-1步到9的方案数。 // dp[i][j] 表示 dp[i][j]表示从0出发，走i步到j的方案数 int[][] dp = new int[step+1][len]; //base case dp[0][0] = 1; for (int i = 1 ; i &lt;= step ; i++){ for (int j = 0 ; j &lt; len ; j++){ //dp[i][j]表示从0出发，走i步到j的方案数 //公式之所以取余是因为j-1或j+1可能会超过圆环0~9的范围 // 可能是从前面走，也可能是从后面走 dp[i][j] = dp[i-1][(j-1 +len)%len] + dp[i-1][(j+1)%len]; } } // 返回走了 step 步 回到 0 起点的走法。 return dp[step][0]; } 72. 编辑距离123456789101112131415161718192021222324class Solution { public int minDistance(String s, String p) { int ls = s.length(); int lp = p.length(); //1. defination: dp[i][j] 表示 s[0-i] 到 p[0-j]之间的编辑距离 int[][] dp = new int[ls+1][lp+1]; //2. base case: 全部插入或全部删除 for(int i = 0 ; i &lt;= ls ; i++) dp[i][0] = i; for(int j = 0; j &lt;= lp ; j++) dp[0][j] = j; //3. 遍历 for(int i = 1 ; i &lt;= ls; i++){ for(int j = 1 ; j &lt;= lp ;j++){ if(s.charAt(i-1) == p.charAt(j-1)) dp[i][j] = dp[i-1][j-1]; //从 新增、删除、 替换中找出最小的值 + 1 else dp[i][j] = Math.min(dp[i-1][j] , Math.min(dp[i][j-1] , dp[i-1][j-1])) + 1; } } return dp[ls][lp]; }} 91. 解码方法1234567891011121314151617181920212223class Solution { public int numDecodings(String s) { if(s == null || s.length() == 0) return 0; if(s.charAt(0) == '0') return 0; int n = s.length(); //1. dp defination: dp[i] 表示 s[0-i] 有多少种编码方式 int[] dp = new int[n+1]; //2. base case dp[0] = 1; //3. 开始遍历：如果最后一个字符为'0' ,则 dp[i] = dp[i-2] // 如果最后一个字符不为'0' ,则 dp[i] = dp[i-1] + 后面两个字符组成的数字可能性 for(int i = 1 ; i &lt;= n ; i++){ dp[i] = s.charAt(i-1) == '0' ? 0 : dp[i-1]; if(i &gt;= 2){ int num = Integer.parseInt(s.substring(i-2 ,i)); if(num &gt;= 10 &amp;&amp; num &lt;= 26) dp[i] += dp[i-2]; } } return dp[n]; }} 96. 不同的二叉搜索树123456789101112131415161718192021222324class Solution { public int numTrees(int n) { if(n == 0 || n == 1) return 1; //1. defination: dp[n] 表示 1-n 组成的BST个数 int[] dp = new int[n+1]; dp[0] = 1; dp[1] = 1; //2. 推理题： fn 表示 以n为根节点的BST个数 // dp[n] = f1 + f2 + f3 + .. + fn; // 而 fi = dp[i-1] * dp[n-i] // 所以 dp[n] = dp[0]* dp[n-1] + dp[1]*dp[n-2] + .. for(int i = 2 ; i &lt;= n ; i++){ for(int j = 1; j &lt;i+1 ; j++){ dp[i] += dp[j-1] * dp[i-j]; } } return dp[n]; }} 97. 交错字符串12345678910111213141516171819202122232425class Solution { public boolean isInterleave(String s1, String s2, String s3) { int n1 = s1.length(); int n2 = s2.length(); int n3 = s3.length(); if(n1 + n2 != n3) return false; // dp[i][j] 表示 s1的前i个字符 和 s2的前j个字符 能否组成 s3的前（i+j）个字符 boolean[][] dp = new boolean[n1+1][n2+1]; // base case: 只用 s1 或 s2 来组成 s3 dp[0][0] = true; for(int i = 1 ; i &lt;= n1 ; i++) dp[i][0] = dp[i-1][0] &amp;&amp; s1.charAt(i-1) == s3.charAt(i-1); for(int i = 1 ; i &lt;= n2 ; i++) dp[0][i] = dp[0][i-1] &amp;&amp; s2.charAt(i-1) == s3.charAt(i-1); // 遍历 for(int i = 1 ; i &lt;= n1 ; i++){ for(int j = 1; j &lt;= n2 ;j++){ // 两种可能性，是由s1的字符还是s2的字符组成 dp[i][j] = (dp[i-1][j] &amp;&amp; s1.charAt(i-1) == s3.charAt(i+j-1)) || (dp[i][j-1] &amp;&amp; s2.charAt(j-1) == s3.charAt(i+j-1)); } } return dp[n1][n2]; }} 377. 组合总和 Ⅳ12345678910111213141516171819class Solution { public int combinationSum4(int[] nums, int target) { int n = nums.length; // dp[i] 表示 总和为i的组合个数 int[] dp = new int[target+1]; // dp[i] = dp[i-nums[0]] + dp[i-nums[1]] + ... // 比如说 134 组合成7 dp[7] = 1和dp[6] + 3和dp[4] + 4和dp[3] dp[0] = 1; for(int i = 1; i &lt;= target ; i++){ for(int num : nums){ if( i &gt;= num) dp[i] += dp[i-num]; } } return dp[target]; }} 子序列问题115. 不同的子序列12345678910111213141516171819202122232425class Solution { public int numDistinct(String s, String p) { int ls = s.length(); int lp = p.length(); if(ls == 0 || lp == 0) return 0; //1. dp defination: dp[i][j] 表示 s[0-i] 中 p[0-j]出现的个数 int[][] dp = new int[ls+1][lp+1]; //2. base case dp[0][0] = 1; //两个都是空串 for(int i = 1 ; i &lt;= ls ;i++) dp[i][0] = 1; //3. 开始遍历 for(int i = 1 ; i &lt;= ls ; i++){ for(int j = 1 ; j &lt;= lp; j++){ // 如果最后一个字符相等，则有两种选择：选和不选 if(s.charAt(i-1) == p.charAt(j-1)) dp[i][j] = dp[i-1][j-1] + dp[i-1][j]; else dp[i][j] = dp[i-1][j]; //只能不选 } } return dp[ls][lp]; }} 300. 最长递增子序列长度123456789101112131415161718192021class Solution { public int lengthOfLIS(int[] nums) { if(nums == null || nums.length == 0) return 0; int n = nums.length; // dp[i] 表示 到 nums[0-i] 中 以nums[i]结尾的最长子序列长度 int[] dp = new int[n]; // base case dp[0] = 1; Arrays.fill(dp , 1); for(int i = 1; i &lt; n ; i++){ // 从 i 往前找，只到找到第一个比 nums[i] 小的数 for(int j = i-1 ; j &gt;= 0 ; j--){ if(nums[j] &lt; nums[i]) dp[i] = Math.max(dp[i] , dp[j] + 1); } } //res为 dp数组中的最大值 int res = 1; for(int num : dp) res = Math.max(res , num); return res; }} 673. 最长递增子序列的个数1234567891011121314151617181920212223242526272829```#### [516. 最长回文子序列的长度](https://leetcode-cn.com/problems/longest-palindromic-subsequence/)```javaclass Solution { public int longestPalindromeSubseq(String s) { if(s == null || s.length() == 0) return 0; int n = s.length(); // dp[i][j] 表示 s[i-j]之间的最长回文子序列长度 int[][] dp = new int[n][n]; // base case for(int i = 0 ; i &lt; n ; i ++) dp[i][i] = 1; // 开始遍历 for(int i = n-2 ; i &gt;= 0 ; i--){ for(int j = i+1 ; j &lt; n ;j++){ if(s.charAt(i) == s.charAt(j)) dp[i][j] = dp[i+1][j-1] + 2; else dp[i][j] = Math.max(dp[i+1][j] , dp[i][j-1]); } } return dp[0][n-1]; }} 1143. 最长公共子序列12345678910111213141516171819class Solution { public int longestCommonSubsequence(String s, String p) { int ls = s.length(); int lp = p.length(); int[][] dp = new int[ls+1][lp+1]; dp[0][0] = 0; for(int i = 1 ; i &lt;= ls ; i++){ for(int j = 1 ; j &lt;= lp ; j++){ if(s.charAt(i-1) == p.charAt(j-1)) dp[i][j] = dp[i-1][j-1] + 1; else dp[i][j] = Math.max(dp[i-1][j] , dp[i][j-1]); // 注意不相等的情况 } } return dp[ls][lp]; }} 416. 能否分割等和子集123456789101112131415161718192021222324252627class Solution { public boolean canPartition(int[] nums) { //1. 先计算 sum ，如果不是偶数直接排除 if(nums == null || nums.length == 0) return false; int n = nums.length; int sum = 0; for(int num : nums) sum += num; if(sum % 2 != 0) return false; sum = sum / 2; //2. dp[i][j] 表示用 前i个数能否组成j boolean[][] dp = new boolean[n+1][sum+1]; for(int i = 0 ; i &lt;= n ; i++) dp[i][0] = true; for(int i = 1; i &lt;= n ; i++){ for(int j = 1;j&lt;=sum ;j++){ // 如果当前 数 大于 j ，说明不能用当前数 if(nums[i-1] &gt; j) dp[i][j] = dp[i-1][j]; // 否则就可以用当前数 else dp[i][j] = dp[i-1][j] || dp[i-1][j-nums[i-1]]; } } return dp[n][sum]; } } 118. 杨辉三角12345678910111213141516171819202122class Solution { public List&lt;List&lt;Integer&gt;&gt; generate(int numRows) { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); if(numRows == 0) return res; res.add(Arrays.asList(1)); for(int i = 1 ; i &lt; numRows ; i++){ List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(1); for(int j = 1 ; j &lt; i ; j++){ list.add(res.get(i-1).get(j-1) + res.get(i-1).get(j)); } list.add(1); res.add(list); } return res; }} 子数组问题53. 最大子序和12345678910111213141516171819202122232425262728293031323334353637383940class Solution { public int maxSubArray(int[] nums) { int res = Integer.MIN_VALUE; int sum = 0; for(int num : nums){ if(sum &gt; 0) sum += num; else sum = num; res = Math.max(res , sum); } return res; }}衍生： 如果返回的是和最大的子数组呢？ public static int maxSubArray(int[] nums) { int res = Integer.MIN_VALUE; int sum = 0; int m = 0 , n = 0; int l = 0; for(int i = 0 ; i &lt; nums.length ; i++){ if (sum &gt; 0){ sum += nums[i]; }else{ sum = nums[i]; l = i; //用一个l记录左边界 } if (sum &gt; res){ res = sum; m = l; n = i; } } int[] maxNums = Arrays.copyOfRange(nums, m, n+1); // 这个就是最大和的子数组 System.out.println(Arrays.toString(maxNums)); return res; } 152. 乘积最大子数组123456789101112131415161718192021class Solution { public int maxProduct(int[] nums) { // 因为乘积的话，负数也可能变成正数,所以需要保存最小值和最大值 int res = Integer.MIN_VALUE; int max = 1 , min = 1; for(int num : nums){ // 如果当前数是负数，则需要交换最大值和最小值 if(num &lt; 0){ int tem = max; max = min; min = tem; } max = Math.max( max * num , num); min = Math.min( min * num , num); res = Math.max(max ,res); } return res; }} 零钱兑换问题322. 零钱兑换最小硬币个数12345678910111213141516171819202122232425class Solution { public int coinChange(int[] coins, int amount) { int n = coins.length; if(n == 0) return n; // dp[i] 表示 凑出i的最小硬币个数 int[] dp = new int[amount+1]; Arrays.fill(dp , amount+1); // 可能凑不出来，就用amount+1来表示 // base case dp[0] = 0; //遍历所有的硬币 for(int i = 0 ; i &lt; n ;i++){ // 遍历所有的数额 for(int j = 1 ; j &lt;= amount ; j++){ // 如果当前数额 &gt; 当前硬币数值，说明可以用当前硬币 if(j &gt;= coins[i]) dp[j] = Math.min(dp[j] , dp[j-coins[i]]+1); } } return dp[amount] == amount+1 ? -1 : dp[amount]; }} 518. 零钱兑换 II 组合总数123456789101112131415161718192021222324class Solution { public int change(int amount, int[] coins) { int n = coins.length; //1. dp[i] 表示 凑出金额 i的组合数 int[] dp = new int[amount+1]; //2. base case: 凑出0元只有一种组合 dp[0] = 1; //3. 遍历所有的硬币，先遍历硬币！！！！！！ for(int i = 0 ;i &lt; n ;i++){ for(int j = 1; j &lt;= amount ; j++){ //4. 说明可以用当前硬币凑金额 if(coins[i] &lt;= j) dp[j] += dp[j-coins[i]]; } } return dp[amount]; }} 买卖股票问题121. 买卖股票的最佳时机只能买卖一次 123456789101112131415class Solution { public int maxProfit(int[] prices) { int res = Integer.MIN_VALUE; int min = Integer.MAX_VALUE; //前i天的最大收益 = max{前i-1天的最大收益，第i天的价格-前i-1天中的最小价格} for(int num : prices){ //如果是今天卖，更新 利润 res = Math.max(res , num - min); //更新最小值 min = Math.min(min ,num); } return res &lt; 0 ? 0 : res; }} 122. 买卖股票的最佳时机 II可以买卖无限次 1234567891011121314class Solution { public int maxProfit(int[] prices) { int res = 0; if(prices == null || prices.length == 0) return 0; int n = prices.length; if(n == 1) return 0; // 策略：将每两天的利润都加起来 for(int i = 1 ; i &lt; n ; i++){ res += Math.max(0 , prices[i] - prices[i-1]); } return res; }} 123. 买卖股票的最佳时机 III只能买两次 12345678910111213141516171819202122232425262728293031class Solution { public int maxProfit(int[] prices) { if(prices == null || prices.length == 0) return 0; int n = prices.length; if(n &lt; 2) return 0; // 三个参数 ： 第几天 、 买股票次数有0，1，2 、 手里还有股票的状态 0 1 int[][][] dp = new int[n][3][2]; for(int i = 0 ; i &lt; n ; i++){ for(int j = 0 ; j &lt; 3 ; j++){ // 第一天的base case if(i == 0){ dp[0][j][0] = 0; dp[0][j][1] = -prices[0]; continue; } //还没开始买的basecase if(j == 0){ dp[i][0][0] = 0; dp[i][0][1] = Integer.MIN_VALUE; //不存在的情况 continue; } // 只剩下两种情况：前一天有今天卖了，或者前一天也没有 dp[i][j][1] = Math.max(dp[i-1][j][1] , dp[i-1][j-1][0] - prices[i]); dp[i][j][0] = Math.max(dp[i-1][j][0] , dp[i-1][j][1] + prices[i]); } } return dp[n-1][2][0]; }} 188. 买卖股票的最佳时机 IV123456789101112131415161718192021222324252627282930313233343536class Solution { public int maxProfit(int k, int[] prices) { int n = prices.length; if(k &gt;= n/2){ //等同于无限次 int res = 0; for(int i = 1 ; i &lt; prices.length ; i++) res += Math.max(0 , prices[i] - prices[i-1]); return res; } //只能买卖k次,注意是 k+1次 int[][][] dp = new int[n][k+1][2]; for(int i = 0 ; i &lt; n ; i++){ for(int j = 0 ; j &lt;= k ; j++){ // 第一天 if(i == 0){ dp[0][j][0] = 0; dp[0][j][1] = - prices[0]; continue; } //还没买股票 if(j == 0){ dp[i][0][0] = 0; dp[i][0][1] = -Integer.MIN_VALUE; continue; } // 前面一天也没有 + 前面一天有今天卖了 dp[i][j][0] = Math.max(dp[i-1][j][0] , dp[i-1][j][1] + prices[i]); // 前面一天也没有 + 前面一天没有今天买了 dp[i][j][1] = Math.max(dp[i-1][j][1] , dp[i-1][j-1][0] - prices[i]); } } return dp[n-1][k][0]; }} 309. 最佳买卖股票时机含冷冻期714. 买卖股票的最佳时机含手续费1 打家劫舍问题198. 打家劫舍1234567891011121314class Solution { public int rob(int[] nums) { int n = nums.length; if(n == 1) return nums[0]; if(n == 2) return Math.max(nums[0] , nums[1]); int[] dp = new int[n]; dp[0] = nums[0]; dp[1] = Math.max(nums[0] , nums[1]); // 偷前面一家 + 不偷前面一家 for(int i = 2 ; i &lt; n ; i++) dp[i] = Math.max(dp[i-1] , dp[i-2] + nums[i]); return dp[n-1]; }} 213. 打家劫舍 II12345678910111213141516171819202122232425262728class Solution { public int rob(int[] nums) { int n = nums.length; if(n == 0) return 0; if(n == 1) return nums[0]; if(n == 2) return Math.max(nums[0] , nums[1]); // 偷第一家 int[] dp = new int[n]; dp[0] = nums[0]; dp[1] = nums[0]; for(int i = 2 ; i &lt; n ;i++){ dp[i] = Math.max(dp[i-1] , dp[i-2] + nums[i]); } //偷最后一家,第一家不偷 int[] dp1 = new int[n]; dp1[0] = 0; dp1[1] = nums[1]; dp1[2] = Math.max(nums[1] , nums[2]); for(int i =3 ; i &lt; n ; i++){ dp1[i] = Math.max(dp1[i-1] , dp1[i-2] + nums[i]); } return Math.max(dp[n-2] , dp1[n-1]); }} 字符串拆分问题139. 单词拆分123456789101112131415161718192021222324class Solution { public boolean wordBreak(String s, List&lt;String&gt; wordDict) { if(s == null || s.length() == 0) return false; int n = s.length(); // dp[i] 表示s[0,i]之间能不能拆分 boolean[] dp = new boolean[n+1]; //base case dp[0] = true; //开始遍历 for(int i = 1 ; i &lt;= n ; i++){ // 从i往前找，找到一个字符串 在 字符数组中，更新 dp[i] for(int j = i-1 ; j &gt;= 0 ;j--){ String tem = s.substring(j , i); if(wordDict.contains(tem) &amp;&amp; dp[j]){ dp[i] = true; break; } } } return dp[n]; }} 343. 整数拆分的最大乘积 1234567891011121314151617181920212223242526class Solution { public int integerBreak(int n) { if(n &lt; 2) return 1; // dp[i] 表示 拆分数字i得到的最大乘积 int[] dp = new int[n+1]; // base case dp[0] = 0; dp[1] = 0; dp[2] = 1; //开始遍历 // dp[i] 有下面几种拆分方法： // 1. j * (i-j) 拆分成2个数相乘 // 2. j * dp[i-j] 将 i-j继续拆分相乘 // 为什么不拆成 dp[j] * (i-j)呢 ？ 因为j是从1 - i 的，这样拆和上面的2相同 // 3. 为什么还要和 dp[i] 比较呢？ 因为在递推公式推导的过程中，每次计算dp[i]，取最大的而已。 for(int i = 3; i &lt;= n ; i++){ for(int j = 1 ; j &lt; i ; j++){ dp[i] = Math.max(dp[i] , Math.max(dp[i-j]*j , (i-j) * j)); } } return dp[n]; }} 410. 分割数组的最大值123456789101112131415161718192021222324252627282930313233343536373839```#### [312. 戳气球](https://leetcode-cn.com/problems/burst-balloons/)```javaclass Solution { public int maxCoins(int[] arr) { if(arr == null || arr.length == 0) return 0; //1. 新建一个数组，包含超过边界的情况 int n = arr.length; int[] nums = new int[n+2]; nums[0] = 1; nums[n+1] = 1; for(int i = 1; i&lt;= n ; i++) nums[i] = arr[i-1]; n = nums.length; // dp[i][j] 表示戳破 （i,j）内气球可以得到的最大金币数（左开右开） int[][] dp = new int[n][n]; //两个for循环控制左右边界，最后一个for循环确定i-j之间去掉哪个气球能取最大值 for(int i = n-2 ; i &gt;= 0 ; i--){ for(int j = i+1 ; j&lt; n ; j++){ for(int k = i+1 ; k &lt; j ;k++){ dp[i][j] = Math.max(dp[i][j] , dp[i][k] + dp[k][j] + nums[i]*nums[j]*nums[k]); } } } return dp[0][n-1]; }} 264. 丑数 II1 494. 目标和96. 不同的二叉搜索树","link":"/2021/10/10/%E7%AE%97%E6%B3%95%E9%A2%98%EF%BC%88%E5%85%AD%EF%BC%89%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"title":"计算机网络（二）传输层","text":"主要讲述的是传输层的一些协议： UDP TCP UDP协议UDP报文段结构源端口号 + 目标端口号 + 长度 + 检验和 ， 共 8 个字节。 UDP的三大特点UDP只是做了运输协议能够做的最少工作，除了复用/分解功能及少量的差错检测外，它几乎没有增加别的东西，比如DNS就是一个使用UDP的典型例子，UDP协议具有以下特点： 无需连接建立: UDP无需任何准备可以立即开始进行数据传输，因此不会引入建立连接的时延。 无连接状态: TCP通过一系列的参数来保证数据的可靠传输，而UDP无需这些参数，因此也不会保证可靠传输。 首部开销小: UDP首部只有8字节，而TCP首部有20个字节。 UDP的应用场景 需要资源少，在网络情况比较好的内网，或者对于丢包不敏感的应用： DHCP 就是基于 UDP 协议的。一般的获取 IP 地址都是内网请求，而且一次获取不到 IP 又没事，过一会儿还有机会。 不需要一对一沟通，建立连接，而是可以广播的应用：DHCP 就是一种广播的形式，就是基于 UDP 协议的，而广播包的格式前面说过了。 需要处理速度快，时延低，可以容忍少数丢包，但是要求即便网络拥塞，也毫不退缩，一往无前的时候 UDP的改进举例todo QUIC而QUIC（全称Quick UDP Internet Connections，快速 UDP 互联网连接）是 Google提出的一种基于 UDP 改进的通信协议，其目的是降低网络通信的延迟，提供更好的用户互动体验。QUIC 在应用层上，会自己实现快速连接建立、减少重传时延，自适应拥塞控制，是应用层“城会玩”的代表。 流媒体的协议直播协议多使用 RTMP，不适合TCP。很多直播应用，都基于UDP 实现了自己的视频传输协议。 实时游戏实时游戏中客户端和服务端要建立长连接，来保证实时传输。游戏对实时要求较为严格的情况下，采用自定义的可靠 UDP 协议，自定义重传策略，能够把丢包产生的延迟降到最低，尽量减少网络问题对游戏性造成的影响。 TCP协议全称是 Transmisslon Control Protocol 数据传输协议 TCP报文段结构占了20个字节 源端口号和目标端口号：确定数据是从哪个应用要发送到哪个应用去。 序号：为了解决乱序问题 确认序号：可以解决不丢包的问题 首部长度 状态位：SYN 是发起一个连接，ACK 是回复，RST 是重新连接，FIN 是结束连接等 窗口大小：该字段用于流量控制，用于指示接收方愿意接受的字节数量 校验和 TCP协议的特点Transmisslon Control Protocol 数据传输协议 面向连接的:一个进程向另一个进程发送数据之前，必须相互发送某些预备报文段，以建立确保数据传输的参数作为 TCP 连接建立的 部分，连接的双方都将初始化与 TCP 连接相关的许多 TCP 状态变量 全双工服务：如果一台主机上的进程A与另一台主机上的进程B存在一条 TCP 连接，那么应用层数据就可在从进程A流向进程B的同时，也从进程B流向进程A 点对点的服务：在单个发送方与单个接收方之间的连接 基于字节流：消息是「没有边界」的，所以⽆论我们消息有多⼤都可以进⾏传输。并且消息是「有序的」，当「前⼀个」消息没有收到的时候，即使它先收到了后⾯的字节，那么也不能扔给应⽤层去处理，同时对「重复」的报⽂会⾃动丢弃。 简述建立连接的三次握手 客户端向服务端发送SYN报文（TCP首部的标志位上SYN=1，序号seq=x），进入SYN_SEND状态即同步已发送。 服务端受到客户端发来的SYN报文之后，会返回一个响应报文，这个响应报文中（首部标志位上SYN= 1，ACK = 1 ， 序号seq = y ， 确认序号ack =x+1），此时服务器进入SYN_RECV状态即同步已收到状态。 客户端受到服务端发送的响应报文之后，会继续向服务器发送一个确认报文（首部标志位SYN = 1 , 序号seq = x+1 , 确认序号 ack = y+1），这个报文发送完毕后客户端和服务端完成了三次握手。（这次的响应报文是可以携带数据的） 为什么是三次握手 防止 重复历史连接的初始化 同步双方的初始的序列号seq，以保证应用层接收到的数据不会因为网络上的传输问题而乱序，即Tcp会用这个序号进行拼接数据 避免资源浪费 不采用四次握手原因：可以将服务器对客户端SYN同步包的响应回复ACK，和服务器发送给客户端的SYN同步包合并起来，提高传输效率。 不采用两次握手原因： 两次握手无法保证服务器和客户端就服务器的初始序列值没有达成一致。 TCP不会为没有数据的ACK重传！ 两次握⼿会造成消息滞留情况下，服务器重复接受⽆⽤的连接请求 SYN 报⽂，⽽造成᯿复分配资源。 如果客户端发送了一次SYN包之后就掉线了会发生什么？Server服务器端收到Client客户端发送的SYN，Server服务器端回复Client客户端SYN-ACK的时候，如果此时Client客户端掉线了，Server服务器端未收到Client客户端的ACK确认。那么该连接就处于一个中间状态，即未连接成功也未连接失败状态。 此时Server服务器端不断重试直至超时，Linux默认重试5次，重试间隔从1秒开始，每次翻倍，等待63秒钟，tcp才会断开连接。 SYN Flood攻击该如何防御？我们都知道 TCP 连接建⽴是需要三次握⼿，假设攻击者短时间伪造不同 IP 地址的 SYN 报⽂，服务端每接收到⼀个 SYN 报⽂，就进⼊ SYN_RCVD 状态，但服务端发送出去的 ACK + SYN 报⽂，⽆法得到未知 IP 主机的 ACK 应答，久⽽久之就会占满服务端的 SYN 接收队列（未连接队列），使得服务器不能为正常⽤户服务。此时服务器可能遭受SYN Flood攻击的风险。 针对SYN Flood攻击风险的防护措施。 比如恶意程序给服务器发送SYN报文，发送以后就进行了下线，然后服务器默认等待63秒才会断开，攻击者就会将服务器的SYN连接的队列耗尽，让正常的连接请求不能处理。 Linux系统给出了一个方案，当SYN队列满后，通过tcp_syncookies参数回发SYN Cookie。TCP根据源地址端口，目标目的端口，时间戳生成一个特殊的Sequence Number即SYN Cookie回发给客户端。如果是攻击者是不会有响应的，若为正常连接则Client客户端会回发服务器端SYN Cookie，直接建立连接。通过SYN Cookie创建的连接，即使现在SYN队列满后，本次连接请求不在队列中，也可以创建连接。 如果建立连接后，Client客户端出现故障怎么办呢，其实TCP设置保活机制，在一段时间内，该时间被称为保活时间keep alive time，在这段时间内，连接处于非活动状态，开启保活功能的一端将向对方发送保活探测报文。向对方发送保活探测报文，如果发送端未收到响应报文，如果在保活时间内即提前配置好的keep alive time则继续发送。直到尝试次数达到保活探测数仍未收到响应则中断连接。对方直接将被确认为不可达，连接即被中断。 简述TCP的四次挥手 客户端向服务端发送终止报文（TCP首部标志位 FIN = 1 , 序号seq = u）,客户端进入 FIN_WAIT_1状态即中止等待1的状态。这里Client作为先发起断开连接的一端说明客户端已经没有数据要发送了。 服务端收到客户端的终止报文后，会发送一个响应报文（TCP首部ACK = 1，序号seq = v , 确认序号ack = u+1），服务端进入CLOSE_WAIT状态即关闭等待状态。 服务端此时仍然可以继续发送数据。 服务端发送完数据之后，也会向客户端发送一个终止报文（TCP标志位 FIN = 1 ，ACK = 1 ，seq = w ，确认序号ack = u+1），Server进入LAST_ACK状态即最后确认状态。 客户端受到服务端的终止报文后，会进入TIME_WAIT状态即时间等待状态，最后发送一个确认报文给服务端（TCP标志位 ACK = 1， seq = u+1,确认序号ack = w+1）。需要等待2倍的MSL最大报文段生存时间，来保证连接的可靠关闭，之后client进入CLOSE关闭状态，server收到client的ack以后，直接进入close状态。 为什么会有TIME_WAIT状态？TIME-WAIT时间等待状态到CLOSE关闭状态，有一个超时设置，这个超时设置是2乘以MSL。为什么要等待这一段时间呢，TIME-WAIT状态主要是要确保有足够的时间让对方收到ACK包，如果被动关闭的哪一方没有收到ACK确认，就会触发被动端重发FIN包（FINISH包）一来一去正好是2乘以MSL。 2MSL 的时间是从客户端接收到 FIN 后发送 ACK 开始计时的。如果在 TIME-WAIT 时间内，因为客户端的 ACK 没有传输到服务端，客户端⼜接收到了服务端᯿发的 FIN 报⽂，那么 2MSL 时间将重新计时。 这个时间⾜以让两个⽅向上的数据包都被丢弃，使得原来连接的数据包在⽹络中都⾃然消失，再出现的数据包⼀定都是新建⽴连接所产⽣的。 是等待⾜够的时间以确保最后的 ACK 能让被动关闭⽅接收，从⽽帮助其正常关闭。 为什么需要四次挥手才能断开连接呢？ 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。 服务器收到客户端的 FIN 报⽂时，先回⼀个 ACK 应答报⽂，⽽服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 FIN 报⽂给客户端来表示同意现在关闭连接。 建立连接的时候，server端的SYN和ACK包是合并为一次发送。而断开连接的时候，两个方向的数据发送的停止时间可能是不同的，所以无法合并FIN和ACK包，FIN和ACK是分开发送的。 2MSL后服务端还没收到ACK咋办按照 TCP 的原理，B 当然还会重发 FIN，这个时候 A 再收到这个包之后，A 就表示，我已经在这里等了这么长时间了，已经仁至义尽了，之后的我就都不认了，于是就直接发送 RST，B 就知道 A 早就跑了。 TIME_WAIT状态过多的危害 第⼀是内存资源占⽤； 第⼆是对端⼝资源的占⽤，⼀个 TCP 连接⾄少消耗⼀个本地端⼝ TCP如何保证可靠传输的1.将应用数据被分割成 TCP 认为最适合发送的数据块。2.确认机制，发送报文后，等待确认。3.重发机制，没有收到确认，将重发数据段。4.保持它首部和数据的校验和。 确认数据的准确性。5.排序，丢弃重复的，流量控制。 重传机制在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回⼀个确认应答消息，表示已收到消息。 一种方法就是超时重试，也即对每一个发送了，但是没有 ACK 的包，都有设一个定时器，超过了一定的时间，就重新尝试。但是这个超时的时间如何评估呢？这个时间不宜过短，时间必须大于往返时间 RTT，否则会引起不必要的重传。也不宜过长，这样超时时间变长，访问就变慢了。估计往返时间，需要 TCP 通过采样 RTT 的时间，然后进行加权平均，算出一个值，而且这个值还是要不断变化的，因为网络状况不断的变化。除了采样 RTT，还要采样 RTT 的波动范围，计算出一个估计的超时时间。由于重传时间是不断变化的，我们称为自适应重传算法（Adaptive Retransmission Algorithm）。每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送 估计往返时间，通常需要采样以下两个： 需要 TCP 通过采样 RTT 的时间，然后进⾏加权平均，算出⼀个平滑 RTT 的值，⽽且这个值还是要不断变化的，因为⽹络状况不断地变化。 除了采样 RTT，还要采样 RTT 的波动范围，这样就避免如果 RTT 有⼀个⼤的波动的话，很难被发现的情况。 有一个可以快速重传的机制，当接收方收到一个序号大于下一个所期望的报文段时，就检测到了数据流中的一个间格，于是发送三个冗余的 ACK，客户端收到后，就在定时器过期之前，重传丢失的报文段。快速重传解决了超时时间的问题，但是没解决是重传之前的一个还是重传所有的问题 SACK （ Selective Acknowledgment 选择性确认）： 这种方式需要在 TCP 头里加一个 SACK 的东西，可以将缓存的地图发送给发送方。例如可以发送 ACK6、SACK8、SACK9，有了地图，发送方一下子就能看出来是 7 丢了。 滑动窗口微信聊天的时候是你发一句我发一句吗？这样效率是不是太低了？因此引入窗口的概念： 窗口的大小：指的是无需等待确认应答，可以继续发送数据的最大值。 我们可以将窗口理解为操作系统的一个缓存空间，发送方主机在等待确认应答返回之前，必须在缓冲区中保留已经发送的数据，如果按期受到了确认应答，此时数据就可以从缓存区中清除。 窗口大小是由哪一边决定的？ TCP 头⾥有⼀个字段叫 Window ，也就是窗⼝⼤⼩。这个字段是接收端告诉发送端⾃⼰还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能⼒来发送数据，⽽不会导致接收端处理不过来。所以，通常窗⼝的⼤⼩是由接收⽅的窗⼝⼤⼩来决定的。 发送⽅发送的数据⼤⼩不能超过接收⽅的窗⼝⼤⼩，否则接收⽅就⽆法正常接收到数据。 发送方的滑动窗口： 当发送⽅把数据「全部」都⼀下发送出去后，可⽤窗⼝的⼤⼩就为 0 了，表明可⽤窗⼝耗尽，在没收到ACK 确认之前是⽆法继续发送数据了： 当3236之间的字节ACK确认应答之后，如果发送窗口没有发送变化，则滑动窗口往右边移动5个字节，接下来的5256就又变成了可用窗口了： 接收方的滑动窗口： 发送方的滑动窗口和接收方的滑动窗口一定是相等的吗？ 并不是完全相等，接收窗⼝的⼤⼩是约等于发送窗⼝的⼤⼩的。 因为滑动窗⼝并不是⼀成不变的。⽐如，当接收⽅的应⽤进程读取数据的速度⾮常快的话，这样的话接收窗⼝可以很快的就空缺出来。那么新的接收窗⼝⼤⼩，是通过 TCP 报⽂中的 Windows 字段来告诉发送⽅。那么这个传输过程是存在时延的，所以接收窗⼝和发送窗⼝是约等于的关系。 流量控制TCP 提供⼀种机制可以让「发送⽅」根据「接收⽅」的实际接收能⼒控制发送的数据量，这就是所谓的流量控制。 流量控制是为了避免 发送方的数据填满了接收方的缓存，并不知道网络中发送了什么。 实际上，发送窗⼝和接收窗⼝中所存放的字节数，都是放在操作系统内存缓冲区中的，⽽操作系统的缓冲区，会被操作系统调整。 利用滑动窗口机制可以很方便的在TCP连接上实现对发送方的流量控制。那怎么进行流量控制呢？ 接收方每次收到数据包，可以在发送确定报文的时候，同时告诉发送方自己的缓存区还剩余多少是空闲的，我们也把缓存区的剩余大小称之为接收窗口大小。发送方收到之后，便会调整自己的发送速率，也就是调整自己发送窗口的大小，当发送方收到接收窗口的大小为0时，发送方就会停止发送数据，防止出现大量丢包情况的发生。 那发送方何时再继续发送数据呢? 当发送方收到接受窗口 win = 0 时，这时发送方停止发送报文，并且同时开启一个定时器(持续计时器），每隔一段时间就发个测试报文去询问接收方，打听是否可以继续发送数据了，如果可以，接收方就告诉他此时接受窗口的大小；如果接受窗口大小还是为0，则发送方再次刷新启动定时器。 那窗口大小怎么设置呢？ 举例：当应用程序没有及时读取缓存时，发送窗口和接受窗口的变化： 客户端发送 140 字节的数据，于是可⽤窗⼝减少到了 220。 服务端因为现在⾮常的繁忙，操作系统于是就把接收缓存减少了 120 字节，当收到 140 字节数据后，⼜因为应⽤程序没有读取任何数据，所以 140 字节留在了缓冲区中，于是接收窗⼝⼤⼩从 360 收缩成了 100，最后发送确认信息时，通告窗⼝⼤⼩给对⽅。 此时客户端因为还没有收到服务端的通告窗⼝报⽂，所以不知道此时接收窗⼝收缩成了 100，客户端只会看⾃⼰的可⽤窗⼝还有 220，所以客户端就发送了 180 字节数据，于是可⽤窗⼝减少到 40。 服务端收到了 180 字节数据时，发现数据⼤⼩超过了接收窗⼝的⼤⼩，于是就把数据包丢失了。 客户端收到第 2 步时，服务端发送的确认报⽂和通告窗⼝报⽂，尝试减少发送窗⼝到 100，把窗⼝的右端向左收缩了 80，此时可⽤窗⼝的⼤⼩就会出现诡异的负值。 所以，如果发⽣了先减少缓存，再收缩窗⼝，就会出现丢包的现象。 为了防⽌这种情况发⽣，TCP 规定是不允许同时减少缓存⼜收缩窗⼝的，⽽是采⽤先收缩窗⼝，过段时间再减少缓存，这样就可以避免了丢包情况。 窗口关闭的潜在风险： 接收⽅向发送⽅通告窗⼝⼤⼩时，是通过 ACK 报⽂来通告的。 那么，当发⽣窗⼝关闭时，接收⽅处理完数据后，会向发送⽅通告⼀个窗⼝⾮ 0 的 ACK 报⽂，如果这个通告窗⼝的 ACK 报⽂在⽹络中丢失了，那麻烦就⼤了。 解决办法：\u0001\u0001\u0001\u0001\u0001\u0001\u0001加一个计时器打破这个死锁 拥塞控制所谓拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载，维护了一个拥塞窗口的概念。 拥塞窗口cwnd ： 发送方维护的一个状态变量，会根据网络的拥塞程度动态的变化。 有了拥塞窗口后，发送窗口的值 swnd = Math.min(cwnd , rwnd),也就是拥塞窗⼝和接收窗⼝中的最⼩值。 如何判断网络发生了堵塞：发生了超时重传即认为网络阻塞了。 TCP拥塞控制机制有两个版本：TCP Tahoe和TCP Reno，两者的差别在于是否采用快速恢复机制。 慢启动: 当新建连接时，cwnd初始化为1个最大报文段(MSS)大小，发送端开始按照拥塞窗口大小发送数据，每当有一个报文段被确认，cwnd就增加1个MSS大小。这样cwnd的值就随着网络往返时间(RTT)呈指数级增长，事实上，慢启动的速度一点也不慢，只是它的起点比较低一点而已。 拥塞避免: 从慢启动可以看到，cwnd可以很快的增长上来，从而最大程度利用网络带宽资源，但是cwnd不能一直这样无限增长下去，一定需要某个限制。TCP使用了一个叫慢启动门限(ssthresh)的变量，当cwnd超过该值后，慢启动过程结束，进入拥塞避免阶段。对于大多数TCP实现来说，ssthresh的值是65536(同样以字节计算)。拥塞避免的主要思想是加法增大，也就是cwnd的值不再指数级往上升，开始加法增加。此时当窗口中所有的报文段都被确认时，cwnd的大小加1，cwnd的值就随着RTT开始线性增加，这样就可以避免增长过快导致网络拥塞，慢慢的增加调整到网络的最佳值。 快速重传: 当发生了超时重传时，慢启动门限变为原来的一般，拥塞窗口重置为1： 当发生了快速重传：拥塞窗口变为原来的一半，慢启动门限 = 拥塞窗口当前值，进入快速恢复算法 总结： TCP连接中的计时器有哪几种？ 超时重传计时器（60s）：当TCP发送报文段时，就创建该特定报文段的重传计时器。 持续计时器（60s）：用于流量控制，在接收端发送接收窗口大小=0报文后，发送端在持续计时器超时后，发送探询消息试探接收窗口的大小，以此来避免接收端重新发送&gt;0的窗口报文给发送方时出现报文丢失，造成双方僵持等待的死锁情况。 保活计时器（2h）：保活计时器用来防止两个TCP之间的连续出现长时间的空闲。 时间等待计时器(2MSL)：当客户端进入TIME-WAIT状态的时候，链接还没有释放掉，必须等待2倍的MSL(最长报文段寿命)后，客户端才能关闭连接。在时间等待期间，链接还处于一种过渡状态。 简述流量控制（滑动窗口rwnd）和拥塞控制（拥塞窗口cwnd）的相同点和区别同： 两者的现象都是丢包，实现机制都是让发送方发的慢一点，发的少一点。异： 拥塞控制所要做的都有一个前提，就是网络能承受现有的网络负荷。流量控制往往指的是点对点通信量的控制，是个端到端的问题。流量控制所要做的就是控制发送端发送数据的速率，以便使接收端来得及接受。拥塞控制的丢包发生在路由器上，流量控制的丢包发生在接收端。 滑动窗口一般指接收窗口，接收器窗口是接收器可以获取数据包的缓冲区，用于流量控制，而拥塞窗口用于拥塞控制。 TCP真正的发送窗口=min(rwnd,cwnd)。","link":"/2021/10/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%EF%BC%88%E4%BA%8C%EF%BC%89%E4%BC%A0%E8%BE%93%E5%B1%82/"},{"title":"计算机网络（四）数据链路层和物理层","text":"","link":"/2021/10/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%EF%BC%88%E5%9B%9B%EF%BC%89%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82%E5%92%8C%E7%89%A9%E7%90%86%E5%B1%82/"},{"title":"设计模式（一）面向对象","text":"本篇描述了： 面向对象 接口和抽象类 组合和继承 贫血模式的传统架构和充血模式的DDD 面向对象的四大特性封装 封装也叫作信息隐藏或者数据访问保护。类通过暴露有限的访问接口，授权外部仅能通过类提供的方式（或者叫函数）来访问内部信息或者数据。 举例验证：设计一个虚拟钱包 12345678910111213141516171819202122232425262728293031323334353637383940public class Wallet { private String id; //钱包的唯一编号 private long createTime; //创建时间 private BigDecimal balance; //余额 private long balanceLastModifiedTime; //上次余额修改的时间 // ... 省略其他属性... public Wallet() { this.id = IdGenerator.getInstance().generate(); this.createTime = System.currentTimeMillis(); this.balance = BigDecimal.ZERO; this.balanceLastModifiedTime = System.currentTimeMillis(); } // 注意：下面对 get 方法做了代码折叠，是为了减少代码所占文章的篇幅 public String getId() { return this.id; } public long getCreateTime() { return this.createTime; } public BigDecimal getBalance() { return this.balance; } public long getBalanceLastModifiedTime() { return this.balanceLastModifiedTime} public void increaseBalance(BigDecimal increasedAmount) { if (increasedAmount.compareTo(BigDecimal.ZERO) &lt; 0) { throw new InvalidAmountException(&quot;...&quot;); } this.balance.add(increasedAmount); this.balanceLastModifiedTime = System.currentTimeMillis(); } public void decreaseBalance(BigDecimal decreasedAmount) { if (decreasedAmount.compareTo(BigDecimal.ZERO) &lt; 0) { throw new InvalidAmountException(&quot;...&quot;); }if (decreasedAmount.compareTo(this.balance) &gt; 0) { throw new InsufficientAmountException(&quot;...&quot;); } this.balance.subtract(decreasedAmount); this.balanceLastModifiedTime = System.currentTimeMillis(); } } 细节总结 private 关键字修饰的属性只能类本身访问，这样外部就不能通过wallet.id来进行修改了 对于钱包来说，id和创建时间是不能修改的，就不会对外暴露set方法。 对于余额来说，只能增加或减少，不能重新设置，也不能对外暴露set方法 上次余额的修改时间这个属性 和余额增加/减少是捆绑的，也不对外暴露修改这个属性。 抽象 抽象讲的是如何隐藏方法的具体实现，让调用者只需要关心方法提供了哪些功能，并不需要知道这些功能是如何实现的。 12345678910111213public interface IPictureStorage { void savePicture(Picture picture); Image getPicture(String pictureId); void deletePicture(String pictureId); void modifyMetaInfo(String pictureId, PictureMetaInfo metaInfo); }public class PictureStorage implements IPictureStorage { // ... 省略其他属性... @Override public void savePicture(Picture picture) { ... } @Override public Image getPicture(String pictureId) { ... } @Override public void deletePicture(String pictureId) { ... } @Override public void modifyMetaInfo(String pictureId, PictureMetaInfo metaInfo) { ... }} 细节总结： 调用者在使用图片存储功能时，只需要了解相应的接口暴露了哪些功能即可，不需要去查看具体类中方法的具体实现。 抽象作为一种只关注功能点不关注实现的设计思路，正好帮我们的大脑过滤掉许多非必要的信息。 继承 继承是用来表示类之间的is-a 关系，比如猫是一种哺乳动物。从继承关系上来讲，继承可以分为两种模式，单继承和多继承。单继承表示一个子类只继承一个父类，多继承表示一个子类可以继承多个父类，比如猫既是哺乳动物，又是爬行动物。 继承最大的一个好处就是代码复用。假如两个类有一些相同的属性和方法，我们就可以将这些相同的部分，抽取到父类中，让两个子类继承父类。这样，两个子类就可以重用父类中的代码，避免代码重复写多遍。 继承来关联两个类，反应真实世界中的这种关系，非常符合人类的认知。 多态 多态是指，子类可以替换父类，在实际的代码运行过程中，调用子类的方法实现。 通过继承加方法重写： 父类对象可以引用子类对象 支持继承 子类要重写父类中的方法 通过接口和实现类来实现。 面向对象和面向过程的优势定义面向对象的定义： 面向对象编程是一种编程范式或编程风格。它以类或对象作为组织代码的基本单元，并将封装、抽象、继承、多态四个特性，作为代码设计和实现的基石 。 面向对象编程语言是支持类或对象的语法机制，并有现成的语法机制，能方便地实现面向对象编程四大特性（封装、抽象、继承、多态）的编程语言。 面向过程的定义： 面向过程编程也是一种编程范式或编程风格。它以过程（可以为理解方法、函数、操作）作为组织代码的基本单元，以数据（可以理解为成员变量、属性）与方法相分离为最主要的特点。面向过程风格是一种流程化的编程风格，通过拼接一组顺序执行的方法来操作数据完成一项功能。 举例：需求：假设我们有一个记录了用户信息的文本文件 users.txt，每行文本的格式是 name&amp;age&amp;gender（比如，小王 &amp;28&amp; 男）。我们希望写一个程序，从 users.txt 文件中逐行读取用户信息，然后格式化成 name\\tage\\tgender（其中，\\t 是分隔符）这种文本格式，并且按照 age 从小到达排序之后，重新写入到另一个文本文件 formatted_users.txt 中。 12345678910111213141516171819202122232425262728293031323334353637//面向过程：面向过程风格的代码被组织成了一组方法集合及其数据结构（struct User），方法和数据结构的定义是分开的struct User { char name[64]; int age; char gender[16]; };struct User parse_to_user(char* text) { // 将 text(“小王 &amp;28&amp; 男”) 解析成结构体 struct User }char* format_to_text(struct User user) { // 将结构体 struct User 格式化成文本（&quot; 小王\\t28\\t 男 &quot;） }void sort_users_by_age(struct User users[]) { // 按照年龄从小到大排序 users }void format_user_file(char* origin_file_path, char* new_file_path) { // open files... struct User users[1024]; // 假设最大 1024 个用户 int count = 0; while(1) { // read until the file is empty struct User user = parse_to_user(line); users[count++] = user; } sort_users_by_age(users); for (int i = 0; i &lt; count; ++i) { char* formatted_user_text = format_to_text(users[i]); // write to new file... } // close files... }int main(char** args, int argv) { format_user_file(&quot;/home/zheng/user.txt&quot;, &quot;/home/zheng/formatted_users.txt&quot;); } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//面向对象的思想： 方法和数据结构被绑定一起，定义在类中。public class User { private String name; private int age; private String gender; public User(String name, int age, String gender) { this.name = name; this.age = age; this.gender = gender; } public static User praseFrom(String userInfoText) { // 将 text(“小王 &amp;28&amp; 男”) 解析成类 User } public String formatToText() { // 将类 User 格式化成文本（&quot; 小王\\t28\\t 男 &quot;） } }public class UserFileFormatter { public void format(String userFile, String formattedUserFile) { // Open files... List users = new ArrayList&lt;&gt;(); while (1) { // read until file is empty // read from file into userText... User user = User.parseFrom(userText); users.add(user); } // sort users by age... for (int i = 0; i &lt; users.size(); ++i) { String formattedUserText = user.formatToText(); // write to new file... } // close files... } } public class MainApplication { public static void main(Sring[] args) { UserFileFormatter userFileFormatter = new UserFileFormatter(); userFileFormatter.format(&quot;/home/zheng/users.txt&quot;, &quot;/home/zheng/formatted_us } } 面向对象的优势 OOP更加能够应对大规模复杂程序的开发 对于大规模复杂程序的开发来说，整个程序的处理流程错综复杂，并非只有一条主线。如果把整个程序的处理流程画出来的话，会是一个网状结构。如果我们再用面向过程编程这种流程化、线性的思维方式，去翻译这个网状结构，去思考如何把程序拆解为一组顺序执行的方法，就会比较吃力。这个时候，面向对象的编程风格的优势就比较明显了。 面向对象编程是以类为思考对象。在进行面向对象编程的时候，我们并不是一上来就去思考，如何将复杂的流程拆解为一个一个方法，而是采用曲线救国的策略，先去思考如何给业务建模，如何将需求翻译为类，如何给类之间建立交互关系，而完成这些工作完全不需要考虑错综复杂的处理流程。当我们有了类的设计之后，然后再像搭积木一样，按照处理流程，将类组装起来形成整个程序。这种开发模式、思考问题的方式，能让我们在应对复杂程序开发的时候，思路更加清晰。 OOP 风格的代码更易复用、易扩展、易维护 多态特性。基于这个特性，我们在需要修改一个功能实现的时候，可以通过实现一个新的子类的方式，在子类中重写原来的功能逻辑，用子类替换父类。在实际的代码运行过程中，调用子类新的功能逻辑，而不是在原有代码上做修改。这就遵从了“对修改关闭、对扩展开放”的设计原则，提高代码的扩展性。除此之外，利用多态特性，不同的类对象可以传递给相同的方法，执行不同的代码逻辑，提高了代码的复用性。 OOP 语言更加人性化、更加高级、更加智能 接口和抽象类定义抽象类Logger 是一个记录日志的抽象类，FileLogger 和 MessageQueueLogger 继承 Logger，分别实现两种不同的日志记录方式：记录日志到文件中和记录日志到消息队列中。FileLogger 和MessageQueueLogger 两个子类复用了父类 Logger 中的 name、enabled、minPermittedLevel 属性和 log() 方法，但因为这两个子类写日志的方式不同，它们又各自重写了父类中的 doLog() 方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 // 抽象类public abstract class Logger { private String name; private boolean enabled; private Level minPermittedLevel; public Logger(String name, boolean enabled, Level minPermittedLevel) { this.name = name; this.enabled = enabled; this.minPermittedLevel = minPermittedLevel; } public void log(Level level, String message) { boolean loggable = enabled &amp;&amp; (minPermittedLevel.intValue() &lt;= level.intValue()); if (!loggable) return; doLog(level, message); } protected abstract void doLog(Level level, String message); } // 抽象类的子类：输出日志到文件public class FileLogger extends Logger { private Writer fileWriter; public FileLogger(String name, boolean enabled, Level minPermittedLevel, String filepath) { super(name, enabled, minPermittedLevel); this.fileWriter = new FileWriter(filepath); } @Override public void doLog(Level level, String mesage) { // 格式化 level 和 message, 输出到日志文件 fileWriter.write(...); } } // 抽象类的子类: 输出日志到消息中间件 (比如 kafka) public class MessageQueueLogger extends Logger { private MessageQueueClient msgQueueClient; public MessageQueueLogger(String name, boolean enabled, Level minPermittedLevel, MessageQueueClient msgQueueClient) { super(name, enabled, minPermittedLevel); this.msgQueueClient = msgQueueClient; } @Override protected void doLog(Level level, String mesage) { // 格式化 level 和 message, 输出到消息中间件 msgQueueClient.send(...); } } 抽象类的特性： 不允许被实例化，只能被继承。也就是说不能直接new一个抽象类出来 抽象类可以包含属性和方法。方法既可以包含代码实现也可以不包含代码实现。不包含代码实现的方法为抽象方法 子类继承抽象类，必须实现抽象类中的所有抽象方法 定义接口1234567891011121314151617181920212223242526272829303132333435//接口public interface Filter { void doFilter(RpcRequest req) throws RpcException;}// 接口实现类：鉴权过滤器public class AuthencationFilter implements Filter { @Override public void doFilter(RpcRequest req) throws RpcException { //... 鉴权逻辑.. }}// 接口实现类：限流过滤器public class RateLimitFilter implements Filter { @Override public void doFilter(RpcRequest req) throws RpcException { //... 限流逻辑... }}// 过滤器使用demopublic class Application { filters.add(new AuthencationFilter()); filters.add(new RateLimitFilter()); private List&lt;Filter&gt; filters = new ArrayList&lt;&gt;(); public void handleRpcRequest(RpcRequest req) { try { for (Filter filter : fitlers) { filter.doFilter(req); } } catch(RpcException e) { // ... 处理过滤结果... }// ... 省略其他处理逻辑... }} 接口特性： 接口不能包含属性（也就是成员变量）。 接口只能声明方法，方法不能包含代码实现。 类实现接口的时候，必须实现接口中声明的所有方法。 相对于抽象类的 is-a 关系来说，接口表示一种 has-a 关系 抽象类和接口存在的意义抽象类也是为代码复用而生的。多个子类可以继承抽象类中定义的属性和方法，避免在子类中，重复编写相同的代码。 应用场景 如果要表示一种is-a 的关系，并且是为了解决代码复用问题，我们就用抽象类； 如果要表示 一种 has-a 关系，并且是为了解决抽象而非代码复用问题，那我们就用接口。 基于接口而非实现编程 越抽象、越顶层、越脱离具体某一实现的设计，越能提高代码的灵活性，越能应对未来的需求变化。好的代码设计，不仅能应对当下的需求，而且在将来需求发生变化的时候，仍然能够在不破坏原有代码设计的情况下灵活应对。而抽象就是提高代码扩展性、灵活性、可维护性最有效的手段之一。 图片存储案例1234567891011121314151617181920212223242526272829303132333435363738public class AliyunImageStore { //... 省略属性、构造函数等... //创建存储目录 public void createBucketIfNotExisting(String bucketName) { // ... 创建 bucket 代码逻辑... // ... 失败会抛出异常.. } //生成访问凭证 public String generateAccessToken() { // ... 根据 accesskey/secrectkey 等生成 access token } //携带 token 和图片上传到指定目录中 public String uploadToAliyun(Image image, String bucketName, String accessToken){ //... 上传图片到阿里云... //... 返回图片存储在阿里云上的地址 (url）... } public Image downloadFromAliyun(String url, String accessToken) { //... 从阿里云下载图片... }}// AliyunImageStore 类的使用举例public class ImageProcessingJob { private static final String BUCKET_NAME = &quot;ai_images_bucket&quot;; //... 省略其他无关代码... public void process() { Image image = ...; // 处理图片，并封装为 Image 对象 AliyunImageStore imageStore = new AliyunImageStore(/* 省略参数 */); imageStore.createBucketIfNotExisting(BUCKET_NAME); String accessToken = imageStore.generateAccessToken(); imagestore.uploadToAliyun(image, BUCKET_NAME, accessToken); }} 业务需求发生了变化，图片不再存储在阿里云上，而是存储在私有云上，该怎么处理呢？ 这就要求我们必须将 AliyunImageStore 类中所定义的所有 public 方法，在 PrivateImageStore 类中都逐一定义并重新实现一遍。而这样做就会存在一些问题，我总结了下面两点： AliyunImageStore 类中有些函数命名暴露了实现细节，比如，uploadToAliyun()和 downloadFromAliyun()。 将图片存储到阿里云的流程，跟存储到私有云的流程，可能并不是完全一致的。比如，阿里云的图片上传和下载的过程中，需要生产 access token，而私有不需要 accesstoken。一方面，AliyunImageStore 中定义的 generateAccessToken() 方法不能照抄到PrivateImageStore 中；另一方面，我们在使用AliyunImageStore 上传、下载图片的时候，代码中用到了 generateAccessToken() 方法，如果要改为私有云的上传下载流程，这些代码都需要做调整。 解决思路： 函数的命名不能暴露任何实现细节。比如：upload()。 封装具体的实现细节。比如，跟阿里云相关的特殊上传（或下载）流程不应该暴露给调用者。我们对上传（或下载）流程进行封装，对外提供一个包裹所有上传（或下载）细节的方法，给调用者使用。 为实现类定义抽象的接口。具体的实现类都依赖统一的接口定义，遵从一致的上传功能协议。使用者依赖接口，而不是具体的实现类来编程 代码重构： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657//图片存储接口public interface ImageStore { String upload(Image image, String bucketName); Image download(String url);}public class AliyunImageStore implements ImageStore { //... 省略属性、构造函数等... public String upload(Image image, String bucketName) { createBucketIfNotExisting(bucketName); String accessToken = generateAccessToken(); //... 上传图片到阿里云... //... 返回图片在阿里云上的地址 (url)... } public Image download(String url){ String accessToken = generateAccessToken(); } private void createBucketIfNotExisting(String bucketName) { // ... 创建 bucket... // ... 失败会抛出异常.. } private String generateAccessToken() { // ... 根据 accesskey/secrectkey 等生成 access token }}// 上传下载流程改变：私有云不需要支持 access tokenpublic class PrivateImageStore implements ImageStore { public String upload(Image image, String bucketName) { createBucketIfNotExisting(bucketName); //... 上传图片到私有云... //... 返回图片的 url... } public Image download(String url) { //... 从私有云下载图片... } private void createBucketIfNotExisting(String bucketName) { // ... 创建 bucket... // ... 失败会抛出异常.. }}// ImageStore 的使用举例public class ImageProcessingJob { private static final String BUCKET_NAME = &quot;ai_images_bucket&quot;; //... 省略其他无关代码... public void process() { Image image = ...;// 处理图片，并封装为 Image 对象 ImageStore imageStore = new PrivateImageStore(...); imagestore.upload(image, BUCKET_NAME); }} 这条原则的设计初衷是，将接口和实现相分离，封装不稳定的实现，暴露稳定的接口。上游系统面向接口而非实现编程，不依赖不稳定的实现细节，这样当实现发生变化的时候，上游系统的代码基本上不需要做改动，以此来降低代码间的耦合性，提高代码的扩展性。 组合优于继承为什么不推荐使用继承虽然继承有诸多作用，但继承层次过深、过复杂，也会影响到代码的可维护性。 案例：设计一个鸟类： 123456789101112//抽象类鸟public class AbstractBird { //... 省略其他属性和方法... public void fly() { //... } }如果很多鸟不会飞怎么办呢？ 每种不会飞的鸟都要重写fly方法来抛出异常吗？如果再加一个是否会叫的抽象方法呢？如果再加一个是否会下蛋的抽象方法呢？ 继承关系变的越来越复杂了！！！ 组合的使用还是上面的例子： 1234567891011121314151617181920212223242526272829303132333435public interface Flyable { void fly(); }public interface Tweetable { void tweet(); }public interface EggLayable { void layEgg(); }public class Ostrich implements Tweetable, EggLayable {// 鸵鸟//... 省略其他属性和方法... @Override public void tweet() { //... } @Override public void layEgg() { //... } }public class Sparrow impelents Flayable, Tweetable, EggLayable {// 麻雀//... 省略其他属性和方法... @Override public void fly() { //... } @Override public void tweet() { //... } @Override public void layEgg() { //... }} 每个会下蛋的鸟都要实现一遍layEgg() 方法，并且实现逻辑是一样的，这就会导致代码重复的问题。那这个问题又该如何解决呢？ 我们可以针对三个接口再定义三个实现类，它们分别是：实现了 fly() 方法的 FlyAbility类、实现了 tweet() 方法的 TweetAbility 类、实现了 layEgg() 方法的 EggLayAbility 类。然后，通过组合和委托技术来消除代码重复： 12345678910111213141516171819202122232425public interface Flyable { void fly(); }public class FlyAbility implements Flyable { @Override public void fly() { //... } }// 省略 Tweetable/TweetAbility/EggLayable/EggLayAbilitypublic class Ostrich implements Tweetable, EggLayable {// 鸵鸟 private TweetAbility tweetAbility = new TweetAbility(); // 组合 private EggLayAbility eggLayAbility = new EggLayAbility(); // 组合 //... 省略其他属性和方法... @Override public void tweet() { tweetAbility.tweet(); // 委托 } @Override public void layEgg() { eggLayAbility.layEgg(); // 委托 } } 如何判断该用组合还是继承？如果类之间的继承结构稳定，层次比较浅，关系不复杂，我们就可以大胆地使用继承。反之，我们就尽量使用组合来替代继承。除此之外，还有一些设计模式、特殊的应用场景，会固定使用继承或者组合。 对于业务开发的MVC架构探讨贫血模型的传统架构MVC 三层架构中的 M 表示 Model，V 表示 View，C 表示 Controller。它将整个项目分为三层：展示层、逻辑层、数据层。 现在很多 Web 或者 App 项目都是前后端分离的，后端负责暴露接口给前端调用。这种情况下，我们一般就将后端项目分为 Repository 层、Service 层、Controller 层。其中，Repository 层负责数据访问，Service 层负责业务逻辑，Controller 层负责暴露接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748////////// Controller+VO(View Object) //////////public class UserController { private UserService userService; // 通过构造函数或者 IOC 框架注入 public UserVo getUserById(Long userId) { UserBo userBo = userService.getUserById(userId); UserVo userVo = [...convert userBo to userVo...]; return userVo; }}public class UserVo {// 省略其他属性、get/set/construct 方法 private Long id; private String name; private String cellphone;}////////// Service+BO(Business Object) //////////public class UserService { private UserRepository userRepository; // 通过构造函数或者 IOC 框架注入 public UserBo getUserById(Long userId) { UserEntity userEntity = userRepository.getUserById(userId); UserBo userBo = [...convert userEntity to userBo...]; return userBo; }}public class UserBo {// 省略其他属性、get/set/construct 方法 private Long id; private String name; private String cellphone;}////////// Repository+Entity //////////public class UserRepository { public UserEntity getUserById(Long userId) { //... }}public class UserEntity {// 省略其他属性、get/set/construct 方法 private Long id; private String name; private String cellphone;} 像 UserBo 这样，只包含数据，不包含业务逻辑的类，就叫作贫血模型（Anemic Domain Model）。同理，UserEntity、UserVo 都是基于贫血模型设计的。这种贫血模型将数据与操作分离，破坏了面向对象的封装特性，是一种典型的面向过程的编程风格。 充血模型的 DDD 开发模式？在贫血模型中，数据和业务逻辑被分割到不同的类中。 充血模型（Rich Domain Model）正好相反，数据和对应的业务逻辑被封装到同一个类中。因此，这种充血模型满足面向对象的封装特性，是典型的面向对象编程风格。 什么是DDD领域驱动设计，即 DDD，主要是用来指导如何解耦业务系统，划分业务模块，定义业务领域模型及其交互。 我们知道，除了监控、调用链追踪、API 网关等服务治理系统的开发之外，微服务还有另外一个更加重要的工作，那就是针对公司的业务，合理地做微服务拆分。而领域驱动设计恰好就是用来指导划分服务的。所以，微服务加速了领域驱动设计的盛行。 实际上，基于充血模型的 DDD 开发模式实现的代码，也是按照 MVC 三层架构分层的。Controller 层还是负责暴露接口，Repository 层还是负责数据存取，Service 层负责核心业务逻辑。它跟基于贫血模型的传统开发模式的区别主要在 Service 层。 在基于贫血模型的传统开发模式中，Service 层包含 Service 类和 BO 类两部分，BO 是贫血模型，只包含数据，不包含具体的业务逻辑。业务逻辑集中在 Service 类中。在基于充血模型的 DDD 开发模式中，Service 层包含 Service 类和 Domain 类两部分。Domain 就相当于贫血模型中的 BO。不过，Domain 与 BO 的区别在于它是基于充血模型开发的，既包含数据，也包含业务逻辑。而 Service 类变得非常单薄。总结一下的话就是，基于贫血模型的传统的开发模式，重 Service 轻 BO；基于充血模型的 DDD 开发模式，轻 Service 重Domain。","link":"/2021/06/28/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%B8%80%EF%BC%89%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"title":"设计模式（三）规范和重构实战","text":"aaaa 实战：ID生成器需求背景ID（identity）翻译为标识。在软件开发中，ID 常用来表示一些业务信息的唯一标识，比如订单的单号或者数据库中的唯一主键，比如地址表中的 ID 字段（实际上是没有业务含义的，对用户来说是透明的，不需要关注。 为了方便在请求出错时排查问题，我们在编写代码的时候会在关键路径上打印日志。某个请求出错之后，我们希望能搜索出这个请求对应的所有日志，以此来查找问题的原因。而实际情况是，在日志文件中，不同请求的日志会交织在一起。如果没有东西来标识哪些日志属于同一个请求，我们就无法关联同一个请求的所有日志。 借鉴微服务调用链追踪的实现思路，我们可以给每个请求分配一个唯一 ID，并且保存在请求的上下文（Context）中，比如，处理请求的工作线程的局部变量中。在 Java 语言中，我们可以将 ID 存储在 Servlet 线程的 ThreadLocal 中，或者利用 Slf4j 日志框架的MDC（Mapped Diagnostic Contexts）来实现（实际上底层原理也是基于线程的ThreadLocal）。每次打印日志的时候，我们从请求上下文中取出请求 ID，跟日志一块输出。这样，同一个请求的所有日志都包含同样的请求 ID 信息，我们就可以通过请求 ID 来搜索同一个请求的所有日志了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 初始 id 生成器代码 */public class IdGenerator { private static final Logger logger = LoggerFactory.getLogger(IdGenerator.class); public static String generate(){ String id = &quot;&quot;; try{ //第一部分是本机名的最后一个字段 String hostname = InetAddress.getLocalHost().getHostName(); String[] tokens = hostname.split(&quot;\\\\.&quot;); if (tokens.length &gt; 0) hostname = tokens[tokens.length-1]; //第三部分是 8 位的随机字符串，包含大小写字母和数字 char[] randomChars = new char[8]; int count = 0; Random random = new Random(); while (count &lt; 8){ int randomAscii = random.nextInt(122); if (randomAscii &gt;= 48 &amp;&amp; randomAscii &lt;= 57){ randomChars[count] = (char) ('0' + (randomAscii - 48)); count++; }else if (randomAscii &gt;= 65 &amp;&amp; randomAscii &lt;= 90){ randomChars[count] = (char) ('A' + (randomAscii - 65)); count++; }else if (randomAscii &gt;= 97 &amp;&amp; randomAscii &lt;= 122){ randomChars[count] = (char) ('a' + (randomAscii - 97)); count++; } } //第二部分是当前时间戳，精确到毫秒 id = String.format(&quot;%s-%d-%s&quot;,hostname,System.currentTimeMillis() , new String(randomChars)); }catch (UnknownHostException e) { logger.warn(&quot;Failed to get the host name&quot;,e); e.printStackTrace(); } return id; } public static void main(String[] args) { System.out.println(&quot;IdGenerator.generate() = &quot; + IdGenerator.generate()); //107-1625029281729-ZwlgYXh9 }} 如何分析代码质量问题代码质量角度： 目录设置是否合理、模块划分是否清晰、代码结构是否满足“高内聚、松耦合”？ 是否遵循经典的设计原则和设计思想（SOLID、DRY、KISS、YAGNI、LOD 等）？ 设计模式是否应用得当？是否有过度设计？ 代码是否容易扩展？如果要添加新功能，是否容易实现？ 代码是否可以复用？是否可以复用已有的项目代码或类库？是否有重复造轮子？ 代码是否容易测试？单元测试是否全面覆盖了各种正常和异常的情况？ 代码是否易读？是否符合编码规范（比如命名和注释是否恰当、代码风格是否一致等）？ 问题： 将IdGenerator直接设计成实现类，如果后续项目中需要两种id生成算法，该怎么办呢？是否应该定义成接口比较好，针对不同的生成算法定义不同的生成类 generate为静态函数，会影响代码的可测试性。同时依赖运行环境，没有单元测试代码，是否需要进行补充？ 按照一下的思考顺序： 代码是否实现了预期的业务需求？ 逻辑是否正确？是否处理了各种异常情况？ – 并未处理“hostName 为空” 日志打印是否得当？是否方便 debug 排查问题？ 接口是否易用？是否支持幂等、事务等？ 代码是否存在并发问题？是否线程安全？ 性能是否有优化空间，比如，SQL、算法是否可以优化？–hostName变量不应该被重复使用 是否有安全漏洞？比如输入输出校验是否全面？ 第一轮重构：提高可读性 hostName 变量不应该被重复使用，尤其当这两次使用时的含义还不同的时候； 将获取 hostName 的代码抽离出来，定义为 getLastfieldOfHostName() 函数； 删除代码中的魔法数，比如，57、90、97、122； 将随机数生成的代码抽离出来，定义为 generateRandomAlphameric() 函数； generate() 函数中的三个 if 逻辑重复了，且实现过于复杂，我们要对其进行简化； 对IdGenerator进行重命名并且抽象出相应的接口 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * Id生成器接口 */public interface IdGenerate { String generate();}/** * 日志的随机id生成器 */public interface LogTraceIdGenerator extends IdGenerate{}/** * 随机id生成器 - 第一次重构 */public class RandomIdGenerator implements LogTraceIdGenerator { private static final Logger logger = LoggerFactory.getLogger(RandomIdGenerator.class); @Override public String generate() { String substrOfHostName = getLastFieldOfHostName(); long currentTimeMillis = System.currentTimeMillis(); String randomString = generateRandomAlphameric(8); String id = String.format(&quot;%s-%d-%s&quot; , substrOfHostName ,currentTimeMillis,randomString); return id; } /** * @return 获得主机的最后一位 */ private String getLastFieldOfHostName(){ String substrOfHostName = null; try { String hostName = InetAddress.getLocalHost().getHostName(); String[] tokens = hostName.split(&quot;\\\\.&quot;); substrOfHostName = tokens[tokens.length-1]; return substrOfHostName; } catch (UnknownHostException e) { logger.warn(&quot;Failed to get the host name&quot;,e); } return substrOfHostName; } /** * @param length 随机数字和字母的长度 * @return 返回随机数字和字母的组合 */ private String generateRandomAlphameric(int length){ char[] randomChars = new char[length]; int count = 0; Random random = new Random(); while (count &lt; length){ int maxAscii = 'z'; int randomAscii = random.nextInt(maxAscii); boolean isDigit = randomAscii &gt;= '0' &amp;&amp; randomAscii &lt;= '9'; boolean isUppercase = randomAscii &gt;= 'A' &amp;&amp; randomAscii &lt;= 'Z'; boolean isLowercase = randomAscii &gt;= 'a' &amp;&amp; randomAscii &lt;= 'z'; if (isDigit || isLowercase || isUppercase){ randomChars[count] = (char) randomAscii; count++; } } return new String(randomChars); } } 第二轮重构：提高代码的可测试性 generate() 函数定义为静态函数，会影响使用该函数的代码的可测试性； – 第一轮已经解决了。 generate() 函数的代码实现依赖运行环境（本机名）、时间函数、随机函数，所以generate() 函数本身的可测试性也不好。 重构如下： 从 getLastfieldOfHostName() 函数中，将逻辑比较复杂的那部分代码剥离出来，定义为 getLastSubstrSplittedByDot() 函数。因为 getLastfieldOfHostName() 函数依赖本地主机名，所以，剥离出主要代码之后这个函数变得非常简单，可以不用测试。我们重点测试 getLastSubstrSplittedByDot() 函数即可。 将 generateRandomAlphameric() 和 getLastSubstrSplittedByDot() 这两个函数的访问权限设置为 protected。这样做的目的是，可以直接在单元测试中通过对象来调用两个函数进行测试。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * 随机id生成器 - 第2次重构 */public class RandomIdGenerator implements LogTraceIdGenerator { private static final Logger logger = LoggerFactory.getLogger(RandomIdGenerator.class); @Override public String generate() { String substrOfHostName = getLastFieldOfHostName(); long currentTimeMillis = System.currentTimeMillis(); String randomString = generateRandomAlphameric(8); String id = String.format(&quot;%s-%d-%s&quot; , substrOfHostName ,currentTimeMillis,randomString); return id; } /** * @return 获得主机的最后一位 */ private String getLastFieldOfHostName(){ String substrOfHostName = null; try { String hostName = InetAddress.getLocalHost().getHostName(); substrOfHostName = getLastSubstrSplittedByDot(hostName); } catch (UnknownHostException e) { logger.warn(&quot;Failed to get the host name&quot;,e); } return substrOfHostName; } /** * //Google Guava 的 annotation @VisibleForTesting ,只起到标识的作用，告诉其他人说，这两个函数本该是 private 访问权限的， * 之所以提升访问权限到 protected，只是为了测试，只能用于单元测试中 */ @VisibleForTesting protected String getLastSubstrSplittedByDot(String hostName){ String[] tokens = hostName.split(&quot;\\\\.&quot;); String substrOfHostName = tokens[tokens.length-1]; return substrOfHostName; } /** * @param length 随机数字和字母的长度 * @return 返回随机数字和字母的组合 */ @VisibleForTesting private String generateRandomAlphameric(int length){ char[] randomChars = new char[length]; int count = 0; Random random = new Random(); while (count &lt; length){ int maxAscii = 'z'; int randomAscii = random.nextInt(maxAscii); boolean isDigit = randomAscii &gt;= '0' &amp;&amp; randomAscii &lt;= '9'; boolean isUppercase = randomAscii &gt;= 'A' &amp;&amp; randomAscii &lt;= 'Z'; boolean isLowercase = randomAscii &gt;= 'a' &amp;&amp; randomAscii &lt;= 'z'; if (isDigit || isLowercase || isUppercase){ randomChars[count] = (char) randomAscii; count++; } } return new String(randomChars); }} 第三轮重构：编写单元测试1234567891011121314151617181920212223242526272829303132333435363738394041424344public class RandomIdGeneratorTest { @Test public void testGetLastSubstrSplittedByDot(){ RandomIdGenerator idGenerator = new RandomIdGenerator(); String actualSubstr = idGenerator.getLastSubstrSplittedByDot(&quot;field1.field2.field3&quot;); Assertions.assertEquals(&quot;field3&quot;, actualSubstr); actualSubstr = idGenerator.getLastSubstrSplittedByDot(&quot;field1&quot;); Assertions.assertEquals(&quot;field1&quot;, actualSubstr); actualSubstr = idGenerator.getLastSubstrSplittedByDot(&quot;field1#field2$field3&quot;); Assertions.assertEquals(&quot;field1#field2$field3&quot;, actualSubstr); } @Test //// 此单元测试会失败，因为我们在代码中没有处理hostName为null或空字符串的情况 public void testGetLastSubstrSplittedByDot_nullOrEmpty(){ RandomIdGenerator idGenerator = new RandomIdGenerator(); String actualSubstr = idGenerator.getLastSubstrSplittedByDot(null); Assertions.assertNull(actualSubstr); actualSubstr = idGenerator.getLastSubstrSplittedByDot(&quot;&quot;); Assertions.assertEquals(&quot;&quot;, actualSubstr); } @Test public void testGenerateRandomAlphameric(){ RandomIdGenerator idGenerator = new RandomIdGenerator(); String actualRandomString = idGenerator.generateRandomAlphameric(6); Assertions.assertNotNull(actualRandomString); Assertions.assertEquals(6, actualRandomString.length()); for (char c : actualRandomString.toCharArray()) { Assertions.assertTrue(('0' &lt; c &amp;&amp; c &gt; '9') || ('a' &lt; c &amp;&amp; c &gt; 'z') || ('A' &lt; c &amp;&amp; c &gt; 'Z')); } } @Test // 此单元测试会失败，因为我们在代码中没有处理length&lt;=0的情况 public void testGenerateRandomAlphameric_lengthEqualsOrLessThanZero(){ RandomIdGenerator idGenerator = new RandomIdGenerator(); String actualRandomString = idGenerator.generateRandomAlphameric(0); Assertions.assertEquals(&quot;&quot;, actualRandomString); actualRandomString = idGenerator.generateRandomAlphameric(-1); Assertions.assertNull(actualRandomString); }}","link":"/2021/06/30/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%B8%89%EF%BC%89%E8%A7%84%E8%8C%83%E5%92%8C%E9%87%8D%E6%9E%84%E5%AE%9E%E6%88%98/"},{"title":"设计模式（二）设计原则","text":"六大软件设计原则： 单一职责 开闭原则 里式替换 迪米特法则 接口隔离 依赖反转 单一职责原则Single Responsibility Principe 简单的来说就是：一个类或者模块只负责完成一个职责（或者功能）, 它约定了一个类应该有且只有一个改变类的原因 换个角度来讲就是，一个类包含了两个或者两个以上业务不相干的功能，那我们就说它职责不够单一，应该将它拆分成多个功能更加单一、粒度更细的类。比如，一个类里既包含订单的一些操作，又包含用户的一些操作。而订单和用户是两个独立的业务领域模型，我们将两个不相干的功能放到同一个类中，那就违反了单一职责原则。 如何判断类的职责是否单一不同的应用场景、不同阶段的需求背景、不同的业务层面，对同一个类的职责是否单一，可能会有不同的判定结果。 比如下面的userinfo类表示用户信息： 12345678910111213141516public class UserInfo { private long userId; private String username; private String email; private String telephone; private long createTime; private long lastLoginTime; private String avatarUrl; private String provinceOfAddress; // 省 private String cityOfAddress; // 市 private String regionOfAddress; // 区 private String detailedAddress; // 详细地址 // ... 省略其他属性和方法... } 问题： 用户的信息占比很重，是否要单独抽出来建一个新的类userAddress呢？ 如果在这个社交产品中，用户的地址信息跟其他信息一样，只是单纯地用来展示，那 UserInfo 现在的设计就是合理的。 如果这个社交产品发展得比较好，之后又在产品中添加了电商的模块，用户的地址信息还会用在电商物流中，那我们最好将地址信息从 UserInfo 中拆分出来，独立成用户物流信息（或者叫地址信息、收货信息等）。 公司希望支持统一账号系统，也就是用户一个账号可以在公司内部的所有产品中登录。这个时候，我们就需要继续对 UserInfo 进行拆分，将跟身份认证相关的信息（比如，email、telephone 等）抽取成独立的类。 在真实的业务开发中，我们可以先写一个粗粒度的类，满足业务需求。随着业务的发展，如果粗粒度的类越来越庞大，代码越来越多，这个时候，我们就可以将这个粗粒度的类，拆分成几个更细粒度的类。这就是所谓的持续重构 下面几条判断原则： 类中的代码行数、函数或者属性过多； 类依赖的其他类过多，或者依赖类的其他类过多； 私有方法过多； 比较难给类起一个合适的名字； 类中大量的方法都是集中操作类中的某几个属性。 模拟场景一个视频网站用户分类的例子： 访客用户，视频清晰度只有480P，并时刻提醒用户需要注册才能看高清视频。 普通会员，可以看高清视频，但会有广告 VIP会员，可以选择是否跳过广告 违背SRP的设计1234567891011121314151617181920212223242526/** * @author shengbinbin * @description: 违背 SRP 原则 * @date 2021/8/1510:51 下午 */public class VideoUserService { public void serveGrade(String userType){ if (&quot;VIP会员&quot;.equals(userType)){ System.out.println(&quot;VIP会员，视频1080p蓝光&quot;); }else if (&quot;普通会员&quot;.equals(userType)){ System.out.println(&quot;普通会员，视频720p高清&quot;); }else if (&quot;访客用户&quot;.equals(userType)){ System.out.println(&quot;访客用户，视频360p标清&quot;); } } // 测试这个类的使用 public static void main(String[] args) { VideoUserService service = new VideoUserService(); /** 调用方法时，所有的用户都走的是一个方法，很难维护 */ service.serveGrade(&quot;VIP会员&quot;); service.serveGrade(&quot;普通会员&quot;); service.serveGrade(&quot;访客用户&quot;); }} SRP改善视频播放是核心功能，不能用一个类把所有的职责行为混为一谈。而是提供一个上层的接口类，对不同的差异化用户提供自己的实现类，拆分各自的职责边界。 上层接口，核心功能类： 123456public interface IVideoUserService { // 定义视频的清晰级别 void definition(); // 定义是否有广告 void advertisement();} 不同的实现类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * @author shengbinbin * @description: 访客用户在视频网站上的广告和视频清晰度 * @date 2021/8/1511:01 下午 */public class GuestVideoUserServiceImpl implements IVideoUserService{ @Override public void definition() { System.out.println(&quot;访客用户，视频360p标清&quot;); } @Override public void advertisement() { System.out.println(&quot;访客用户，存在广告&quot;); }}/** * @author shengbinbin * @description: 普通会员在视频网站上的广告和视频清晰度 * @date 2021/8/1511:03 下午 */public class OrdinaryVideoUserServiceImpl implements IVideoUserService{ @Override public void definition() { System.out.println(&quot;普通会员，视频720p标清&quot;); } @Override public void advertisement() { System.out.println(&quot;普通会员，存在广告&quot;); }}/** * @author shengbinbin * @description: vip会员在视频网站上的广告和视频清晰度 * @date 2021/8/1511:04 下午 */public class VipVideoUserServiceImpl implements IVideoUserService { @Override public void definition() { System.out.println(&quot;VIP会员，视频1080p蓝光&quot;); } @Override public void advertisement() { System.out.println(&quot;VIP会员，可以选择是否有广告&quot;); }} 通过SRP优化后，现在每个类都只负责自己的用户行为。后续的无论是扩展还是修改某个用户行为类都比较方便。 在项目开发的过程中，要尽可能的保证接口的定义、类的实现以及方法的开发保持单一职责。 开闭原则 Open Closed Principle：软件实体（模块、类、方法等）应该“对扩展开放、对修改关闭”，这意味着应该抽象的定义结构，面向抽象编程。 举例：API接口监控的代码： 12345678910111213141516171819202122public class Alert { private AlertRule rule; // AlertRule 存储报警规则 private Notification notification; // Notification 告警通知类 public Alert(AlertRule rule, Notification notification) { this.rule = rule; this.notification = notification; } public void check(String api, long requestCount, long errorCount, long duration){ long tps = requestCount / durationOfSeconds; //当接口的 TPS 超过某个预先设置的最大值时 if (tps &gt; rule.getMatchedRule(api).getMaxTps()) { notification.notify(NotificationEmergencyLevel.URGENCY, &quot;...&quot;); } //接口请求出错数大于某个最大允许值时 if (errorCount &gt; rule.getMatchedRule(api).getMaxErrorCount()) notification.notify(NotificationEmergencyLevel.SEVERE, &quot;...&quot;); } 需求：增加功能：当每秒钟接口超时请求个数超过某个预先设置的最大值时，也会触发告警通知。 方案一： 1234567891011121314151617181920212223242526272829public class Alert { private AlertRule rule; // AlertRule 存储报警规则 private Notification notification; // Notification 告警通知类 public Alert(AlertRule rule, Notification notification) { this.rule = rule; this.notification = notification; } // 1. 增加参数 超时请求个数timeoutCount public void check(String api, long requestCount, long errorCount, long duration, long timeoutCount){ long tps = requestCount / durationOfSeconds; //当接口的 TPS 超过某个预先设置的最大值时 if (tps &gt; rule.getMatchedRule(api).getMaxTps()) { notification.notify(NotificationEmergencyLevel.URGENCY, &quot;...&quot;); } //接口请求出错数大于某个最大允许值时 if (errorCount &gt; rule.getMatchedRule(api).getMaxErrorCount()) notification.notify(NotificationEmergencyLevel.SEVERE, &quot;...&quot;); //2. 增加相应的处理逻辑 long timeoutTps = timeoutCount / durationOfSeconds; if (timeoutTps &gt; rule.getMatchedRule(api).getMaxTimeoutTps()) { notification.notify(NotificationEmergencyLevel.URGENCY, &quot;...&quot;); } } 问题： 对接口进行了修改，调用这个接口的代码都要修改 修改了check函数，单元测试也需要修改 方案二: 进行重构: 第一部分是将 check() 函数的多个入参封装成 ApiStatInfo 类； 第二部分是引入 handler 的概念，将 if 判断逻辑分散在各个 handler 中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class Alert { //讲处理的规则封装成handler private List&lt;AlertHandler&gt; alertHandlers = new ArrayList&lt;&gt;(); public void addAlertHandler(AlertHandler alertHandler) { this.alertHandlers.add(alertHandler); } public void check(ApiStatInfo apiStatInfo) { for (AlertHandler handler : alertHandlers) { handler.check(apiStatInfo); } }}public class ApiStatInfo {// 省略 constructor/getter/setter 方法 private String api; private long requestCount; private long errorCount; private long durationOfSeconds; }public abstract class AlertHandler { protected AlertRule rule; protected Notification notification; public AlertHandler(AlertRule rule, Notification notification) { this.rule = rule; this.notification = notification; } public abstract void check(ApiStatInfo apiStatInfo); }public class TpsAlertHandler extends AlertHandler { public TpsAlertHandler(AlertRule rule, Notification notification) { super(rule, notification); } @Override public void check(ApiStatInfo apiStatInfo) { long tps = apiStatInfo.getRequestCount()/ apiStatInfo.getDurationOfSeconds if (tps &gt; rule.getMatchedRule(apiStatInfo.getApi()).getMaxTps()) { notification.notify(NotificationEmergencyLevel.URGENCY, &quot;...&quot;); } } }public class ErrorAlertHandler extends AlertHandler { public ErrorAlertHandler(AlertRule rule, Notification notification){ super(rule, notification); } @Override public void check(ApiStatInfo apiStatInfo) { if (apiStatInfo.getErrorCount() &gt; rule.getMatchedRule(apiStatInfo.getApi()) { notification.notify(NotificationEmergencyLevel.SEVERE, &quot;...&quot;); } } 使用代码： 12345678910111213141516171819202122232425262728293031public class ApplicationContext { private AlertRule alertRule; private Notification notification; private Alert alert; public void initializeBeans() { alertRule = new AlertRule(/*. 省略参数.*/); // 省略一些初始化代码 notification = new Notification(/*. 省略参数.*/); // 省略一些初始化代码 alert = new Alert(); alert.addAlertHandler(new TpsAlertHandler(alertRule, notification)); alert.addAlertHandler(new ErrorAlertHandler(alertRule, notification)); } public Alert getAlert() { return alert; } // 饿汉式单例 private static final ApplicationContext instance = new ApplicationContext(); private ApplicationContext() { instance.initializeBeans(); } public static ApplicationContext getInstance() { return instance; }}public class Demo { public static void main(String[] args) { ApiStatInfo apiStatInfo = new ApiStatInfo(); // ... 省略设置 apiStatInfo 数据值的代码 ApplicationContext.getInstance().getAlert().check(apiStatInfo); } } 增加功能改动如下： 第一处改动是：在 ApiStatInfo 类中添加新的属性 timeoutCount。 第二处改动是：添加新的 TimeoutAlertHander 类。 第三处改动是：在 ApplicationContext 类的 initializeBeans() 方法中，往 alert 对象中注册新的 timeoutAlertHandler。 第四处改动是：在使用 Alert 类的时候，需要给 check() 函数的入参 apiStatInfo 对象设置 timeoutCount 的值。 这样的话，check函数的逻辑不需要改变，只需要创建新的handler类即可。 里式替换原则Liskov Substitution Principle： 继承必须保证超类所拥有的性质必须在子类中仍然成立。 子类对象（object ofsubtype/derived class）能够替换程序（program）中父类对象（object of base/parentclass）出现的任何地方，并且保证原来程序的逻辑行为（behavior）不变及正确性不被破坏。 在编码上的体现也就是：当子类继承父类时，除了新添加的方法且完成新增功能外，尽量不要重写父类的方法，包含了以下四点： 子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法。 子类可以增加自己特有的方法 当子类的方法重载父类的方法时，方法的输入参数要比父类的方法更加宽松。 当子类的方法实现父类的方法时，方法的输出或返回值要比父类更严格或相等。 模拟场景不同种类的银行卡比如信用卡，储蓄卡都具备一定的消费能力，但是又有所不同，比如信用卡不宜提现，如果提现会产生高额的利息。 信用卡和储蓄卡都有支付、还款、提现、充值的功能。但对于支付来说，储蓄卡是账户扣款，信用卡是生成贷款单的动作。信用卡继承储蓄卡 违背LSP的代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * @author shengbinbin * @description: 储蓄卡 * @date 2021/8/1511:26 下午 */public class CashCard { private Logger logger = LoggerFactory.getLogger(CashCard.class); /** * 提现功能 * @param orderId ：单号 * @param amount ：金额 * @return 0000 成功 0001 失败 0002 重复 */ public String withdrawal(String orderId , BigDecimal amount){ logger.debug(&quot;提现成功，单号: {} , 金额: {}&quot;, orderId, amount); return &quot;0000&quot;; } /** * 储蓄 * @param orderId 单号 * @param amount 金额 * @return */ public String recharge(String orderId , BigDecimal amount){ //模拟充值成功 logger.debug(&quot;储蓄成功，单号:{},金额:{}&quot;, orderId,amount); return &quot;0000&quot;; } /** * 交易流水查询 * @return 交易流水 */ public List&lt;String&gt; tradeFlow(){ logger.debug(&quot;交易流水查询&quot;); List&lt;String&gt; tradeList = new ArrayList&lt;&gt;(); tradeList.add(&quot;100001,100.00&quot;); tradeList.add(&quot;100001,80.00&quot;); tradeList.add(&quot;100001,76.50&quot;); tradeList.add(&quot;100001,126.00&quot;); return tradeList; }}/** * @author shengbinbin * @description: 信用卡 * @date 2021/8/1610:45 下午 */public class CreditCard extends CashCard { private Logger logger = LoggerFactory.getLogger(CreditCard.class); @Override public String withdrawal(String orderId, BigDecimal amount) { //1. 校验 if (amount.compareTo(new BigDecimal(1000)) &gt;= 0){ logger.debug(&quot;贷款金额校验（限额1000元）, 单号:{} , 金额:{}&quot; , orderId, amount); return &quot;0001&quot;; } // 2. 模拟生成贷款单 logger.debug(&quot;生成贷款单,单号:{} , 金额:{}&quot; , orderId, amount); // 3. 模拟支付成功 logger.debug(&quot;贷款成功,单号:{} , 金额:{}&quot; , orderId, amount); return &quot;0000&quot;; } @Override public String recharge(String orderId, BigDecimal amount) { logger.debug(&quot;生成还款单,单号:{} , 金额:{}&quot; , orderId, amount); logger.debug(&quot;还款成功,单号:{} , 金额:{}&quot; , orderId, amount); return &quot;0000&quot;; } @Override public List&lt;String&gt; tradeFlow() { return super.tradeFlow(); }} 此时继承父类实现的信用卡类并不满足里式替换原则。 LSP改善代码储蓄卡和信用卡在功能使用上有些许类似，有很多可以复用的属性和逻辑。因此最好的方式是抽象一个抽象类，由抽象类定义所有卡的公用核心属性和逻辑： 12345678910111213141516171819202122232425```# 迪米特法则又称最小知道原则，指一个对象类对于其他对象类而言，知道的越少越好。这样相互之间的耦合才会最小。## 模拟场景校长想知道一个班级的总分和平均分，是应该去找老师要还是应该找每个学生要来统计呢？## 违背代码```java 接口隔离原则 Interface Segregation Principle依赖反转原则控制反转举例: 下面的代码都是由程序员来控制的 123456789101112public class UserServiceTest { public static boolean doTest(){ if (System.currentTimeMillis() % 2 == 0) return true; else return false; } public static void main(String[] args) { //这部分逻辑可以放入框架中 if (doTest()) System.out.println(&quot;Test succeed&quot;); else System.out.println(&quot;Test failed&quot;); }} 使用框架来实现同样的功能： 1234567891011121314151617181920212223242526272829303132333435//1. 写一个抽象类public abstract class TestCase { public void run(){ if (doTest()) System.out.println(&quot;Test succeed&quot;); else System.out.println(&quot;Test failed&quot;); } public abstract boolean doTest();}//2. 写一个抽象类的子类，重写doTest方法public class UserServiceTest extends TestCase{ @Override public boolean doTest() { //这里就是写业务逻辑，啥时候返回true，啥时候返回false if (System.currentTimeMillis() % 2 == 0) return true; else return false; }}//3. 编写一个启动类,只需要在框架预留的扩展点，也就是TestCase 类中的 doTest() 抽象函数中，填充具体的测试代码就可以实现之前的功能了，完全不需要写负责执行流程的 main() 函数了public class JunitApplication { private static final List&lt;TestCase&gt; testCases = new ArrayList&lt;&gt;(); public static void register(TestCase testCase){ testCases.add(testCase); } public static void main(String[] args) { register(new UserServiceTest()); // 注册操作还可以通过配置的方式来实现，不需要程序员显示调用 register() for (TestCase testCase : testCases) testCase.run(); }} 程序员利用框架进行开发的时候，只需要往预留的扩展点上，添加跟自己业务相关的代码，就可以利用框架来驱动整个程序流程的执行。 依赖注入不通过 new() 的方式在类内部创建依赖类对象，而是将依赖的类对象在外部创建好之后，通过构造函数、函数参数等方式传递（或注入）给类使用。 举例： 1234567891011121314151617181920212223242526//不使用依赖注入的方式：public class MessageSender { public void send(String cellphone, String message){ //发送逻辑 }}/** * Notification 依赖 MessageSender 类来发送通知 */public class Notification { private MessageSender messageSender; public Notification(){ this.messageSender = new MessageSender(); //硬编码，在类内部通过new的方式创建依赖类对象 } public void sendMessage(String cellphone , String message){ //校验逻辑省略 this.messageSender.send(cellphone,message); }}// 使用 Notification 的方式Notification notification = new Notification(); 1234567891011121314151617//使用依赖注入的方式：public class Notification { private MessageSender messageSender; public Notification(MessageSender messageSender){ //通过构造方法传入，而不是直接在内部创建 this.messageSender = messageSender; } public void sendMessage(String cellphone , String message){ //校验逻辑省略 this.messageSender.send(cellphone,message); }}// 使用 Notification MessageSender messageSender = new MessageSender(); Notification notification = new Notification(messageSender); 123456789101112131415161718192021//继续优化，讲MessageSender定义为接口public interface MessageSender { public void send(String cellphone, String message);}//短信发送类public class SmsSender implements MessageSender { @Override public void send(String cellphone, String message) { //.... } }// 站内信发送类public class InboxSender implements MessageSender { @Override public void send(String cellphone, String message) { //.... } }// 使用 Notification MessageSender messageSender = new SmsSender(); Notification notification = new Notification(messageSender); 依赖倒置原则（DIP） High-level modules shouldn’t depend on low-level modules. Both modules should depend on abstractions. In addition, abstractions shouldn’t depend on details. Details depend on abstractions. 高层模块（high-level modules）不要依赖低层模块（low-level）。高层模块和低层模块应该通过抽象（abstractions）来互相依赖。除此之外，抽象（abstractions）不要依赖具体实现细节（details），具体实现细节（details）依赖抽象（abstractions）","link":"/2021/06/29/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%BA%8C%EF%BC%89%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"title":"计算机网络（一）应用层","text":"主要讲述的是应用层的一些协议： HTTP DNS DHCP FTP 一、概述应⽤层是⼯作在操作系统中的⽤户态，传输层及以下则⼯作在内核态。 1.浏览器输入网址，发生哪些过程 你先在浏览器里面输入URL: https://www.taobao.com 。浏览器只知道名字是“www.taobao.com”，但是不知道具体的地点，所以不知道应该如何访问。于是，它打开地址簿去查找。可以使用一般的地址簿协议**DNS**去查找，还可以使用另一种更加精准的地址簿查找协议**HTTPDNS**。 通过DNS获得ip地址，这是是互联网世界的“门牌号” 浏览器开始打包请求，普通的请求用http，加密的用https。 下一层是传输层。传输层有两种协议，一种是无连接的协议UDP，一种是面向连接的协议TCP。TCP 协议里面会有两个端口，一个是浏览器监听的端口，一个是电商的服务器监听的端口。操作系统往往通过端口来判断，它得到的包应该给哪个进程。 传输层封装完毕后，浏览器会将包交给操作系统的网络层。网络层的协议是 IP 协议。在 IP协议里面会有源 IP 地址，即浏览器所在机器的 IP 地址和目标 IP 地址，也即电商网站所在服务器的 IP 地址。 操作系统往往会判断，这个目标 IP 地址是本地人，还是外地人（是否在同一个网段），如果在同一个网段，通过ip地址就能找到目标主机，如果不在，就需要通过网关。而操作系统启动的时候，就会被 DHCP 协议配置 IP 地址，以及默认的网关的 IP 地址192.168.1.1。 操作系统根据192.168.1.1的IP地址，在本地网段中大吼一声，网关就会回复它自己的MAC地址。（ARP协议） 操作系统将 IP 包交给了下一层，也就是MAC 层。网卡再将包发出去。由于这个包里面是有 MAC 地址的，因而它能够到达网关 网关收到包之后，会根据自己的知识，判断下一步应该怎么走。网关往往是一个路由器，到某个 IP 地址应该怎么走，这个叫作路由表。相邻的路由器会经常沟通去哪里该怎么走，这种沟通的协议称为路由协议，常用的有OSPF和BGP。 通过封装下一个路由器的MAC地址，找到下一个路由器。就这样依次下去 最后一个路由器就是和服务端IP地址在同一个网段中，大吼一声，目标服务器就会传过来一个MAC地址，通过这个 MAC 地址就能找到目标服务器。 目标服务器发现 MAC 地址对上了，取下 MAC 头来，发送给操作系统的网络层。发现 IP也对上了，就取下 IP 头。IP 头里会写上一层封装的是 TCP 协议，然后将其交给传输层，即TCP 层。 在TCP层，对每一个收到的包都会有一个回复，说明包收到了。发送端的TCP如果一段时间内没收到回复确认，就会闷头重试。 当网络包平安到达 TCP 层之后，TCP 头中有目标端口号，通过这个端口号，可以找到电商网站的进程正在监听这个端口号、 电商网站的进程得到 HTTP 请求的内容，知道了要买东西，买多少。往往一个电商网站最初接待请求的这个 Tomcat 只是个接待员，负责统筹处理这个请求，而不是所有的事情都自己做。通过RPC调用找到负责相关服务的服务主机进行请求。 当接待员发现相应的部门都处理完毕，就回复一个 HTTPS 的包，告知下单成功。这个HTTPS 的包，会像来的时候一样，经过千难万险到达你的个人电脑，最终进入浏览器，显示支付成功。 2.为什么根据IP地址就能找到目标主机，却还需要Mac地址呢？局域网内IP地址是动态分配的，假如我是192.168.2.100，如果我下线了，可能IP就分配给了另一台电脑。IP和设备并不总是对应的，这对通信就产生了问题，但是MAC地址不同，MAC地址和设备是一一对应且全球唯一的。所以局域网使用MAC地址通信没有问题。 HTTP协议基本概念URL解析 协议方案名。 使用http：或https：等协议方案名获取访问资源时要指定协议类型。不区分字母大小写，最后附一个冒号（:）。 也可使用data：或javascript：这类指定数据或脚本程序的方案名。 登录信息（认证） 指定用户名和密码作为从服务器端获取资源时必要的登录信息（身份认证）。此项是可选项。 服务器地址 使用绝对URI必须指定待访问的服务器地址。地址可以是类似hackr.jp这种DNS可解析的名称，或是192.168.1.1这类IPv4地址名，还可以是[0:0:0:0:0:0:0:1]这样用方括号括起来的IPv6地址名 查询字符串 针对已指定的文件路径内的资源，可以使用查询字符串传入任意参数。此项可选。 片段标识符 使用片段标识符通常可标记出已获取资源中的子资源（文档内的某个位置）。但在RFC中并没有明确规定其使用方法。该项也为可选项。 请求报文 将URL通过DNS拿到对应的IP地址 HTTP是基于 TCP 协议的，会先进行三次握手（后续章节再讲） 发送HTTP请求报文： 请求报文三个组成 请求行（方法 + URI + 协议版本） 请求首部字段 内容实体 不同的HTTP方法 GET ，就是去服务器获取一些资源。对于访问网页来讲，要获取的资源往往是一个页面。其实也有很多其他的格式，比如说返回一个 JSON 字符串，到底要返回什么，是由服务器端的实现决定的 POST，它需要主动告诉服务端一些信息，而非获取。要告诉服务端什么呢？一般会放在正文里面。正文可以有各种各样的格式。常见的格式也是 JSON，比如支付场景，客户端就需要把“我是谁？我要支付多少？我要买啥？”告诉服务器，这就需要通过 POST 方法 PUT，就是向指定资源位置上传最新内容。但是，HTTP 的服务器往往是不允许上传文件的，所以 PUT 和 POST 就都变成了要传给服务器东西的方法。在实际使用过程中，这两者还会有稍许的区别。POST 往往是用来创建一个资源的，而PUT 往往是用来修改一个资源的。 DELETE。这个顾名思义就是用来删除资源的。例如，我们要删除一个云主机，就会调用 DELETE 方法 HEAD，和get方法一样，只是不放回报文的主体部分。 OPTIONS：询问支持的方法，响应报文会返回当前服务器所支持的方法：比如 Allow:GET,POST,HEAD,OPTIONS Get和Post的区别先说明下安全和幂等的概念： 在 HTTP 协议⾥，所谓的「安全」是指请求⽅法不会「破坏」服务器上的资源。 所谓的「幂等」，意思是多次执⾏相同的操作，结果都是「相同」的。 再说一下两者的应用： Get ⽅法的含义是请求从服务器获取资源，这个资源可以是静态的⽂本、⻚⾯、图⽚视频等。 POST ⽅法则是相反操作，它向 URI 指定的资源提交数据，数据就放在报⽂的 body ⾥。 所以 Get方法是安全且幂等的，而post方法由于会修改服务器上的资源，所以是不安全的。 请求报文常见首部字段请求行下面就是我们的首部字段。首部是 key value，通过冒号分隔。这里面，往往保存了一些非常重要的字段。 Accept-Charset，表示客户端可以接受的字符集。防止传过来的是另外的字符集，从而导致出现乱码 Content-Type是指正文的格式。例如，我们进行 POST 的请求，如果正文是JSON，那么我们就应该将这个值设置为 JSON Cache-control是用来控制缓存的，当客户端发送的请求中包含 max\u0002-age 指令时，如果判定缓存层中，资源的缓存时间数值比指定时间的数值小，那么客户端可以接受缓存的资源；当指定 max-age 值为 0，那么缓存层通常需要将请求转发给应用集群。 If-Modified-Since：如果服务器的资源在某个时间之后更新了，那么客户端就应该下载最新的资源；如果没有更新，服务端会返回“304 Not Modified”的响应，那客户端就不用下载了，也会节省带宽。 Host: 用于客户端发送请求时用来指定服务器的域名。比如www.baidu.com中的baidu就是域名。 Connection: 客户端要求服务器使⽤ TCP 持久连接,，以便其他请求复⽤。从HTTP1.1开始默认都是持久连接了。 响应报文响应报文三个组成 状态行（版本 + 状态码 + 状态码的原因短语） 响应首部字段 内容实体 常见状态码1XX：正在允许 2XX：运行成功 3XX：重定向： 状态码 状态码意思 100 continue 200 OK 请求成功，实体的主体部分已经包含了资源 201 Created 用于创建服务器对象的请求（比如put） 202 Accepted 请求已经被接收，但服务器还没对其执行任何动作 204 No Content 请求已经成功处理，但是返回的响应报文不包含实体的主体部分 206 Partial Content 表示客户端进行了范围请求，响应报文包含由 Content-Range 指定范围的实体内容。 301 Moved Permanently 永久重定向，返回的响应首部字段中Location包含了资源现在的URL 302 Found 表示临时重定向，说明请求的资源还在，但暂时需要⽤另⼀个 URL 来访问 303 see other 告知客户端应该用另一个URL获取资源。 304 Not Modified 客户端发起get请求而资源没有发生改变，说明资源没有发生改变，响应中也不应该包含实体的主体部分 305 Use Proxy 用来说明必须通过一个代理来访问资源，代理的位置由Location给出 307 Temporary Redirect 临时重定向，客户端应该使用Location给出的URL来定位临时资源，但将来的请求还是应该用老的URL 301 和 302 都会在响应头⾥使⽤字段 Location ，指明后续要跳转的 URL，浏览器会⾃动᯿定向新的 URL。 4XX 客户端错误 400 Bad Request 请求报文中存在语法错误 401 Unauthorized 未认证，需要认证 403 Forbidden 请求被服务器拒绝（通常不说明拒绝原因） 404 Not Found 无法找到对应的URL资源 405 Method Not Allowed 方法不被允许 408 Request Timeout 请求超时 5XX 服务器错误 500 Internal Server Error 服务器正在执行请求时发生错误 502 Bad Gateway 网关或代理服务器从上游服务器收到无效的请求 503 Service Unavailable 服务器暂时处于超负载或正在进行停机维护，现在无法处理请求，这个是临时的，一段时间后恢复 504 Gateway Timeout 网关或代理服务器没能及时从上游服务器收到响应 501 Not Implemented 表示客户端请求的功能还不⽀持，类似“即将开业，敬请期待”的意思 响应报文常见首部字段 Retry-After表示，告诉客户端应该在多长时间以后再次尝试一下。“503 错误”是说“服务暂时不再和这个值配合使用 Content-Type，表示返回的是 HTML，还是 JSON。 Http1.1的优化HTTP/1.1 相⽐ HTTP/1.0 性能上的改进： 使⽤ TCP ⻓连接的⽅式改善了 HTTP/1.0 短连接造成的性能开销。 ⽀持管道（pipeline）⽹络传输，只要第⼀个请求发出去了，不必等其回来，就可以发第⼆个请求出去，可以减少整体的响应时间。 优化思路： 避免发送HTTP请求 — 缓存优化 减少HTTP请求： 减少HTTP响应的数据大小 缓存的应用客户端会把第⼀次请求以及响应的数据保存在本地磁盘上，其中将请求的 URL 作为 key，⽽响应作为 value，两者形成映射关系。 这样当后续发起相同的请求时，就可以先在本地磁盘上通过 key 查到对应的 value，也就是响应，如果找到了，就直接从本地读取该响应： 既然有缓存，可以就需要解决缓存一致性的问题了： 万⼀缓存的响应不是最新的，⽽客户端并不知情，那么该怎么办呢？ 服务器在发送 HTTP 响应时，会估算⼀个过期的时间，并把这个信息放到响应头部中，这样客户端在查看响应头部的信息时，⼀旦发现缓存的响应是过期的，则就会重新新发送⽹络请求。 如果客户端从第⼀次请求得到的响应头部中发现该响应过期了，客户端重新发送请求，假设服务器上的资源并没有变更，还是⽼样⼦，那么你觉得还要在服务器的响应带上这个资源吗？ 只需要客户端在重新发送请求时，在请求的 Etag 头部带上第⼀次请求的响应头部中的摘要，这个摘要是唯⼀标识响应的资源，当服务器收到请求后，会将本地资源的摘要与请求中的摘要做个⽐较。 如果不同，那么说明客户端的缓存已经没有价值，服务器在响应中带上最新的资源。 如果相同，说明客户端的缓存还是可以继续使⽤的，那么服务器仅返回不含有包体的 304 Not Modified 304 Not Modified 响应，告诉客户端仍然有效，这样就可以减少响应资源在⽹络中传输的延时， 减少请求方法： 减少重定向 合并请求 延迟发送请求 重定向请求是指服务器上的一个资源转移到了另外一个url上面了，而客户端并不知道。它还是会继续请求原来的url，这时候服务器需要通过返回302状态码和Location头部字段表示资源已经换地方存储了，这时客户端根据URL2再去获取资源。 因此重定向请求需要多次发送HTTP请求，需要减少。 另外，服务端这⼀⽅往往不只有⼀台服务器，⽐如源服务器上⼀级是代理服务器，然后代理服务器才与客户端通信，这时客户端᯿定向就会导致客户端与代理服务器之间需要 2 次消息传递，如下图： 如果重定向的⼯作交由代理服务器完成，就能减少 HTTP 请求次数了，如下图： 合并请求的⽅式就是合并资源，以⼀个⼤资源的请求替换多个⼩资源的请求。 延迟发送请求的意思：请求⽹⻚的时候，没必要把全部资源都获取到（比如全部的URL），⽽是只获取当前⽤户所看到的⻚⾯资源，当⽤户向下滑动⻚⾯的时候，再向服务器获取接下来的资源，这样就达到了延迟发送请求的效果。 减少响应数据大小通过压缩响应资源，降低传输资源的⼤⼩，从⽽提⾼传输效率，所以应当选择更优秀的压缩算法。 Http2.0的优化头部压缩头部压缩：HTTP 1.1 在应用层以纯文本的形式进行通信。每次通信都要带完整的 HTTP 的头，而且不考虑 pipeline 模式的话，每次的过程总是像上面描述的那样一去一回。这样在实时性、并发性上都存在问题。为了解决这些问题，HTTP 2.0 会对 HTTP 的头进行一定的压缩，将原来每次都要携带的大量 key value 在两端建立一个索引表，对相同的头只发送索引表中的索引。 为⾼频出现在头部的字符串和字段建⽴了⼀张静态表表中的 Index 表示索引（Key）， Header Value 表示索引对应的 Value， Header Name 表示字段的名字，⽐如Index 为 2 代表 GET，Index 为 8 代表状态码 200。 ** 二进制分帧 二进制分帧：HTTP/2 厉害的地⽅在于将 HTTP/1 的⽂本格式改成⼆进制格式传输数据，极⼤提⾼了 HTTP 传输效率，⽽且⼆进制数据使⽤位运算能⾼效解析。常见的帧有Header 帧，用于传输 Header 内容，并且会开启一个新的流。再就是Data 帧，用来传输正文实体。多个 Data 帧属于同一个流。 通过这两种机制，HTTP 2.0 的客户端可以将多个请求分到不同的流中，然后将请求内容拆成帧，进行二进制传输。这些帧可以打散乱序发送， 然后根据每个帧首部的流标识符重新组装，并且可以根据优先级，决定优先处理哪个流的数据。 举例子：假设我们的一个页面要发送三个独立的请求，一个获取 css，一个获取 js，一个获取图片jpg。如果使用 HTTP 1.1 就是串行的，但是如果使用 HTTP 2.0，就可以在一个连接里，客户端和服务端都可以同时发送多个请求或回应，而且不用按照顺序一对一对应。 多路复用 HTTP/1.1 的实现是基于请求-响应模型的。同⼀个连接中，HTTP 完成⼀个事务（请求与响应），才能处理下⼀个事务，也就是说在发出请求等待响应的过程中，是没办法做其他事情的，如果响应迟迟不来，那么后续的请求是⽆法发送的，也造成了队头阻塞的问题。 HTTP 2.0 协议将一个 TCP 的连接中，切分成多个流，每个流都有自己的 ID，而且流可以是客户端发往服务端，也可以是服务端发往客户端。它其实只是一个虚拟的通道。 Stream ⾥可以包含 1 个或多个 Message，Message 对应 HTTP/1 中的请求或响应，由 HTTP 头部和包体构成； Message ⾥包含⼀条或者多个 Frame，Frame 是 HTTP/2 最⼩单位，以⼆进制压缩格式存放 HTTP/1 中的内容（头部和包体）； HTTP 消息可以由多个 Frame 构成，以及 1 个 Frame 可以由多个 TCP 报⽂构成。 在 HTTP/2 连接上，不同 Stream 的帧是可以乱序发送的（因此可以并发不同的 Stream ），因为每个帧的头部会携带 Stream ID 信息，所以接收端可以通过 Stream ID 有序组装成 HTTP 消息，⽽同⼀ Stream 内部的帧必须是严格有序的。 HTTP/2 通过 Stream 实现的并发，⽐ HTTP/1.1 通过 TCP 连接实现并发要⽜逼的多，因为当 HTTP/2 实现 100 个并发 Stream 时，只需要建⽴⼀次 TCP 连接，⽽ HTTP/1.1 需要建⽴ 100 个 TCP 连接，每个 TCP 连接都要经过TCP 握⼿、慢启动以及 TLS 握⼿过程，这些都是很耗时的。 服务端推送HTTP/1.1 不⽀持服务器主动推送资源给客户端，都是由客户端向服务器发起请求后，才能获取到服务器响应的资源。 ⽐如，客户端通过 HTTP/1.1 请求从服务器那获取到了 HTML ⽂件，⽽ HTML 可能还需要依赖 CSS 来渲染⻚⾯，这时客户端还要再发起获取 CSS ⽂件的请求，需要两次消息往返，如下图左边部分： 如上图右边部分，在 HTTP/2 中，客户端在访问 HTML 时，服务器可以直接主动推送 CSS ⽂件，减少了消息传递的次数。 如何实现服务端推送的呢？ 客户端发起的请求，必须使⽤的是奇数号 Stream，服务器主动的推送，使⽤的是偶数号 Stream。服务器在推送资源时，会通过 PUSH_PROMISE 帧传输 HTTP 头部，并通过帧中的 Promised Stream ID 字段告知客户端，接下来会在哪个偶数号 Stream 中发送包体。 服务器推送资源时，会先发送 PUSH_PROMISE帧，告诉客户端接下来在哪个 Stream 发送资源，然后⽤偶数号 Stream 发送资源给客户端 HTTPS协议 HTTP 是超⽂本传输协议，信息是明⽂传输，存在安全⻛险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在TCP 和 HTTP ⽹络层之间加⼊了 SSL/TLS 安全协议，使得报⽂能够加密传输。 HTTP 连接建⽴相对简单， TCP 三次握⼿之后便可进⾏ HTTP 的报⽂传输。⽽ HTTPS 在 TCP 三次握⼿之后，还需进⾏ SSL/TLS 的握⼿过程，才可进⼊加密报⽂传输。 HTTP 的端⼝号是 80，HTTPS 的端⼝号是 443。 HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。 HTTP的问题HTTPS的解决方法混合加密的⽅式实现信息的机密性，解决了窃听的⻛险。 摘要算法的⽅式来实现完整性，它能够为数据⽣成独⼀⽆⼆的「指纹」，指纹⽤于校验数据的完整性，解决了篡改的⻛险。 将服务器公钥放⼊到数字证书中，解决了冒充的⻛险。 SSL 是洋⽂ “Secure Sockets Layer 的缩写，中⽂叫做「安全套接层」。它是在上世纪 90 年代中期，由⽹景公司 设计的。 到了1999年，SSL 因为应⽤⼴泛，已经成为互联⽹上的事实标准。IETF 就在那年把 SSL 标准化。标准化之后的名 称改为 TLS（是 “Transport Layer Security” 的缩写），中⽂叫做 「传输层安全协议」。 很多相关的⽂章都把这两者并列称呼（SSL/TLS），因为这两者可以视作同⼀个东⻄的不同阶段。 对称加密和非对称加密 在对称加密算法中，加密和解密使用的密钥是相同的。也就是说，加密和解密使用的是同一个密钥。 在非对称加密算法中，加密使用的密钥和解密使用的密钥是不相同的。一把是作为公开的公钥，另一把是作为谁都不能给的私钥。公钥加密的信息，只有私钥才能解密。私钥加密的信息，只有公钥才能解密。 因为对称加密算法相比非对称加密算法来说，效率要高得多，性能也好，所以交互的场景下多用对称加密。 混合加密的工作方式 对称加密只使⽤⼀个密钥，运算速度快，密钥必须保密，⽆法做到安全的密钥交换。 ⾮对称加密使⽤两个密钥：公钥和私钥，公钥可以任意分发⽽私钥保密，解决了密钥交换问题但速度慢。 摘要算法主要用于实现数据的完整性： 客户端在发送明⽂之前会通过摘要算法算出明⽂的「指纹」，发送的时候把「指纹 + 明⽂」⼀同加密成密⽂后，发送给服务器，服务器解密后，⽤相同的摘要算法算出发送过来的明⽂，通过⽐较客户端携带的「指纹」和当前算出的「指纹」做⽐较，若「指纹」相同，说明数据是完整的。 数字证书不对称加密也会有同样的问题，如何将不对称加密的公钥给对方呢？一种是放在一个公网的地址上，让对方下载；另一种就是在建立连接的时候，传给对方。 证书里面有什么呢？当然应该有公钥，这是最重要的；还有证书的所有者，就像户口本上有你的姓名和身份证号，说明这个户口本是你的；另外还有证书的发布机构和证书的有效期，这个有点像身份证上的机构是哪个区公安局，有效期到多少年。 证书请求可以通过这个命令生成： 1openssl req -key cliu8siteprivate.key -new -out cliu8sitecertificate.req 将这个请求发给权威机构，权威机构会给这个证书卡一个章，我们称为签名算法。问题又来了，那怎么签名才能保证是真的权威机构签名的呢？当然只有用只掌握在权威机构手里的东西签名了才行，这就是 CA 的私钥。 签名算法大概是这样工作的：一般是对信息做一个 Hash 计算，得到一个 Hash 值，这个过程是不可逆的，也就是说无法通过 Hash 值得出原来的信息内容。在把信息发送出去时，把这个 Hash 值加密后，作为一个签名和信息一起发出去。 权威机构给证书签名的命令是这样的。 1openssl x509 -req -in cliu8sitecertificate.req -CA cacertificate.pem -CAkey caprivate.ke 这个命令会返回 Signature ok，而 cliu8sitecertificate.pem 就是签过名的证书。CA 用自己的私钥给外卖网站的公钥签名，就相当于给外卖网站背书，形成了外卖网站的证书。我们来查看这个证书的内容。 1openssl x509 -in cliu8sitecertificate.pem -noout -text 这里面有个 Issuer，也即证书是谁颁发的；Subject，就是证书颁发给谁；Validity 是证书期限；Public-key 是公钥内容；Signature Algorithm 是签名算法。 这下好了，你不会从外卖网站上得到一个公钥，而是会得到一个证书，这个证书有个发布机构 CA，你只要得到这个发布机构 CA 的公钥，去解密外卖网站证书的签名，如果解密成功了，Hash 也对的上，就说明这个外卖网站的公钥没有啥问题。你有没有发现，又有新问题了。要想验证证书，需要 CA 的公钥，问题是，你怎么确定 CA的公钥就是对的呢？ 所以，CA 的公钥也需要更牛的 CA 给它签名，然后形成 CA 的证书。要想知道某个 CA 的证书是否可靠，要看 CA 的上级证书的公钥，能不能解开这个 CA 的签名。就像你不相信区公安局，可以打电话问市公安局，让市公安局确认区公安局的合法性。这样层层上去，直到全球皆知的几个著名大 CA，称为root CA，做最后的背书。通过这种层层授信背书的方式，从而保证了非对称加密模式的正常运转。除此之外，还有一种证书，称为Self-Signed Certificate，就是自己给自己签名。这个给人一种“我就是我，你爱信不信”的感觉。这里我就不多说了。 TLS握手过程公钥私钥主要用于传输对称加密的秘钥，而真正的双方大数据量的通信都是通过对称加密进行的。 客户端向服务器发起加密通信请求ClientHello，在这一步客户端主要向服务端发送下面的数据 客户端支持的SSL/TLS的版本 客户端产生的随机数a 客户端支持的加密算法 服务器受到客户端的ClientHello之后，会回复一个响应，响应主要包括： 确认SSL/TLS版本号 服务器产生的随机数b 确认加密算法 服务器的数字证书 客户端受到服务器的回应之后，会先通过浏览器或者操作系统中的CA公钥确认服务器给的数字证书真实性。如果证书没问题。会再生成一个随机数c，从服务器给的证书中的公钥对这个随机数c进行加密传输给服务器。 这样的话客户端和服务端都有3个随机数abc了，通过前面协商的加密算法，计算出本次会话的对称密钥，后续就用这个对称密钥来加密传输了。 上面的过程只包含了 HTTPS 的单向认证，也即客户端验证服务端的证书 Cookie和SessionHTTP是无状态协议，它不对之前发生过的请求和响应的状态进行管理。也就是说，无法根据之前的状态进行本次的请求处理。 Cookie技术通过在请求和响应报文中写入Cookie信息来控制客户端的状态,用来让服务端记住客户端。 过程： 请求报文(首次请求) 123GET /reader/ HTTP/1.1HOST: hacker.jp* 首部字段内没有Cookie的相关信息 响应报文(服务端生成Cookie信息) 123HTTP/1.1 200 OK.....server: Apache &lt;Set-Cookie: sid=12313123121; path=/; .......&gt; 请求报文(第二次请求) 1234GET /image/ HTTP/1.1Host: hacker.jpCookie: sid=12313123121这样二次请求的时候，服务端通过cookie会记住上一次访问的是谁 DNS协议素质三连DNS是什么： 将ip地址和域名相互转换的协议。比如：下面的www.baidu.com 的ip地址就是110.242.68.4 123456789101112131415# shengbinbin @ binshow in ~ [12:45:53]$ ping www.baidu.comPING www.a.shifen.com (110.242.68.4): 56 data bytes64 bytes from 110.242.68.4: icmp_seq=0 ttl=53 time=14.868 ms64 bytes from 110.242.68.4: icmp_seq=1 ttl=53 time=23.293 ms64 bytes from 110.242.68.4: icmp_seq=2 ttl=53 time=14.054 ms64 bytes from 110.242.68.4: icmp_seq=3 ttl=53 time=23.775 ms64 bytes from 110.242.68.4: icmp_seq=4 ttl=53 time=20.637 ms64 bytes from 110.242.68.4: icmp_seq=5 ttl=53 time=22.940 ms64 bytes from 110.242.68.4: icmp_seq=6 ttl=53 time=22.268 ms64 bytes from 110.242.68.4: icmp_seq=7 ttl=53 time=22.558 ms^C--- www.a.shifen.com ping statistics ---8 packets transmitted, 8 packets received, 0.0% packet lossround-trip min/avg/max/stddev = 14.054/20.549/23.775/3.625 ms 为什么要DNS： IP地址的长度是固定32位的（如果是IPv6，就是128位），人们很难记住。转换成域名就比较好记住了。 DNS是怎么实现的： 通过使用分布式的域名系统DNS：当一个应用程序需要把主机名解析为IP地址时，该应用进程就调用解析程序，并成为DNS的一个客户，把待解析的域名放在DNS请求报文中，以UDP用户数据报方式发给本地域名解析器，本地域名服务器查找后，就返回对应的IP地址。应用进程获得目的的IP地址后即可进行通信。若本地域名服务器无法解析，就向上一层的域名服务器上查找，直到找到。 域名的结构 mail.cctv.com中 com 为顶级域名，cctv为二级域名,mail为三级域名。级别最低的写在最左边，而级别最高的顶级域名写在最右边。 通用顶级域名：com(公司企业) net(网络服务机构) org(非营利性组织) int(国际组织) edu(美国专用教育机构) gov(美国政府机构) mil(美国军事机构) 等等共有20个。 国家顶级域名： cn(中国) us(美国) 等等 反向域名：arpa,用于反向解析 域名服务器采用层次树状结构的命名方法，并使用分布式的域名系统DNS。 互联网的域名系统DNS被设计成为一个联机分布式数据库系统，并采用客户服务器方式。DNS让大多数名字都在本地进行解析，仅少量解析需要在互联网上通信，因此NDS系统效率很高。由于DNS是分布式系统，即便单个计算机出了故障，也不会妨碍整个DNS系统正常运行。 域名到IP地址的解析是由分布在互联网上的许多域名服务器程序共同完成的。域名服务器程序在专设的结点上运行，而人们也常把运行域名服务器程序的机器称为 域名服务器。 分类： 根域名服务器。最高层次，最重要的域名服务器。 顶级域名服务器。诸如com cn gov 这种的。 权限域名服务器。负责一个区的域名服务器。 本地域名服务器。当一台主机发出DNS查询请求，这个查询请求报文就发送给本地域名服务器。每个ISP，或一个大学都可以拥有一个本地域名服务器。本地域名服务器离用户较近，一般不超过几个路由器的距离。当所要查询的主机也同属于同一个ISP时，该本地域名服务器立即就能把要查询的主机名转换为她的IP地址，而不需要去询问其他的域名服务器。 域名查询的两种方式 递归查询 假如主机A想访问主机B。主机A输入B的域名后，先是到本地域名服务器查询，查询不到，本地域名服务器以DNS客户的身份代表主机A去访问根域名服务器，若还是查询不到，根域名服务器如法炮制去到下面一级的顶级域名服务器查询，直到拿到主机B的IP地址。然后原路返回，给主机A。 迭代查询 当根域名服务器收到本地域名服务器发出的请求报文时，要么给出IP地址，要么告诉本地域名服务器“下一步应该向哪一个域名服务器进行查询”，直到有域名服务器给出IP地址。 注意：主机向本地域名服务器查询一般采用递归查询。本地域名服务器向根域名服务器查询通常采用迭代查询。 高速缓存加速为了提高DNS查询效率，并减轻根域名服务器的负荷和减少互联网上的DNS查询报文数量，在域名系统服务器中广泛使用了高速缓存。高速缓存用来存放最近查询过的域名以及从何处获得域名映射信息的记录。 许多主机在启动时从本地域名服务器下载名字和地址的全部数据库，维护存放自己最近使用的域名的高速缓存，并且在缓存中找不到名字时才使用域名服务器。 DNS解析详细流程 电脑客户端会发出一个 DNS 请求，问 www.163.com 的 IP 是啥啊，会先查本地的DNS缓存。如果没有进行第二步 发给本地域名服务器 (本地 DNS)。那本地域名服务器 (本地 DNS) 是什么呢？如果是通过 DHCP 配置，本地 DNS 由你的网络服务商（ISP），如电信、移动等自动分配，它通常就在你网络服务商的某个机房。 本地 DNS 收到来自客户端的请求。你可以想象这台服务器上缓存了一张域名与之对应IP 地址的大表格。如果能找到 www.163.com，它直接就返回 IP 地址。如果没有，本地DNS 会去问它的根域名服务器：“老大，能告诉我 www.163.com 的 IP 地址吗？”根域名服务器是最高层次的，全球共有 13 套。它不直接用于域名解析，但能指明一条道路。 根 DNS 收到来自本地 DNS 的请求，发现后缀是 .com，说：“哦，www.163.com啊，这个域名是由.com 区域管理，我给你它的顶级域名服务器的地址，你去问问它吧。” 本地 DNS 转向问顶级域名服务器：“老二，你能告诉我 www.163.com 的 IP 地址吗？”顶级域名服务器就是大名鼎鼎的比如 .com、.net、 .org 这些一级域名，它负责管理二级域名，比如 163.com，所以它能提供一条更清晰的方向。 顶级域名服务器说：“我给你负责 www.163.com 区域的权威 DNS 服务器的地址，你去问它应该能问到。” 本地 DNS 转向问权威 DNS 服务器：“您好，www.163.com 对应的 IP 是啥呀？”163.com 的权威 DNS 服务器，它是域名解析结果的原出处。为啥叫权威呢？就是我的域名我做主。 权限 DNS 服务器查询后将对应的 IP 地址 X.X.X.X 告诉本地 DNS。 本地 DNS 再将 IP 地址返回客户端，客户端和目标建立连接。 负载均衡站在客户端角度，这是一次DNS 递归查询过程。因为本地 DNS 全权为它效劳，它只要坐等结果即可。在这个过程中，DNS 除了可以通过名称映射为 IP 地址，它还可以做另外一件事，就是负载均衡。 例如，某个应用要访问另外一个应用，如果配置另外一个应用的 IP 地址，那么这个访问就是一对一的。但是当被访问的应用撑不住的时候，我们其实可以部署多个。但是，访问它的应用，如何在多个之间进行负载均衡？只要配置成为域名就可以了。在域名解析的时候，我们只要配置策略，这次返回第一个 IP，下次返回第二个 IP，就可以实现负载均衡了。 全局负载均衡： 为了保证我们的应用高可用，往往会部署在多个机房，每个地方都会有自己的 IP 地址。当用户访问某个域名的时候，这个 IP 地址可以轮询访问多个数据中心。如果一个数据中心因为某种原因挂了，只要在 DNS 服务器里面，将这个数据中心对应的 IP 地址删除，就可以实现一定的高可用。另外，我们肯定希望北京的用户访问北京的数据中心，上海的用户访问上海的数据中心，这样，客户体验就会非常好，访问速度就会超快。这就是全局负载均衡的概念。 动态主机配置协议DHCP素质三连是什么： 一个连接到互联网的计算机的协议软件需要配置的项目包括： IP地址 子网掩码 默认路由器的IP地址 域名服务器的IP地址 为了省去给计算机配置IP地址的麻烦， 能否在计算机的生产过程中，事先给一台计算机配置好一个唯一的IP地址。（如同每一个以太网适配器拥有一个唯一的硬件的地址） 为什么： 之所以出现了静态IP和动态IP，是因为IP地址不够用。现在需要上网的人太多了，但是现有的技术条件满足不了所有人同时上网 如何做： DHCP（动态主机配置协议）是一个局域网的网络协议。指的是由服务器控制一段lP地址范围，客户机登录服务器时就可以自动获得服务器分配的lP地址和子网掩码。它提供一种机制，称为即插即用连网。这种机制允许一台计算机加入新的网络和获取IP地址而不用手工参与。 DHCP配置ip流程解析 需要IP地址的主机在启动时就向DHCP服务器广播发送发现报文（将目的IP设置为全1，即255.255.255.255）。这时，该主机就成为了DHCP客户。 这台主机因为还没有IP地址，所以将IP数据报的源IP地址设置为全0.这样在本地网络上的所有主机都能接收到这个广播报文，但只有DHCP服务器才对这广播进行应答 DHCP服务器先在其数据库中查找该计算机的配置信息，若找到，则返回找到的信息。若找不到，则从服务器的IP地址池中取一个地址分配给该计算机。DHCP服务器的回答报文叫做提供报文，表示提供了IP地址等配置信息。 CDN内容分发网络CDN的就近配送全球有这么多的数据中心，无论在哪里上网，临近不远的地方基本上都有数据中心。是不是可以在这些数据中心里部署几台机器，形成一个缓存的集群来缓存部分数据，那么用户访问数据的时候，就可以就近访问了呢？ 当然是可以的。这些分布在各个地方的各个数据中心的节点，就称为边缘节点。 由于边缘节点数目比较多，但是每个集群规模比较小，不可能缓存下来所有东西，因而可能无法命中，这样就会在边缘节点之上。有区域节点，规模就要更大，缓存的数据会更多，命中的概率也就更大。在区域节点之上是中心节点，规模更大，缓存数据更多。如果还不命中，就只好回源网站访问了。 负载均衡CDN 分发网络也是一个分布在多个区域、多个运营商的分布式系统，也可以用相同的思路选择最合适的边缘节点。 在没有 CDN 的情况下，用户向浏览器输入 www.web.com 这个域名，客户端访问本地DNS 服务器的时候，如果本地 DNS 服务器有缓存，则返回网站的地址；如果没有，递归查询到网站的权威 DNS 服务器，这个权威 DNS 服务器是负责 web.com 的，它会返回网站的 IP 地址。本地 DNS 服务器缓存下 IP 地址，将 IP 地址返回，然后客户端直接访问这个 IP 地址，就访问到了这个网站。 然而有了 CDN 之后，情况发生了变化。在 web.com 这个权威 DNS 服务器上，会设置一个 CNAME 别名，指向另外一个域名 www.web.cdn.com，返回给本地 DNS 服务器。当本地 DNS 服务器拿到这个新的域名时，需要继续解析这个新的域名。这个时候，再访问的就不是 web.com 的权威 DNS 服务器了，而是 web.cdn.com 的权威 DNS 服务器，这是 CDN 自己的权威 DNS 服务器。在这个服务器上，还是会设置一个 CNAME，指向另外一个域名，也即 CDN 网络的全局负载均衡器。接下来，本地 DNS 服务器去请求 CDN 的全局负载均衡器解析域名，全局负载均衡器会为用户选择一台合适的缓存服务器提供服务，选择的依据包括： 根据用户 IP 地址，判断哪一台服务器距用户最近； 用户所处的运营商； 根据用户所请求的 URL 中携带的内容名称，判断哪一台服务器上有用户所需的内容； 查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力。 基于以上这些条件，进行综合分析之后，全局负载均衡器会返回一台缓存服务器的 IP 地址。 本地 DNS 服务器缓存这个 IP 地址，然后将 IP 返回给客户端，客户端去访问这个边缘节点，下载资源。缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内容，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。 缓存的内容 还记得这个接入层缓存的架构吗？在进入数据中心的时候，我们希望通过最外层接入层的缓存，将大部分静态资源的访问拦在边缘。而 CDN 则更进一步，将这些静态资源缓存到离用户更近的数据中心外。越接近客户，访问性能越好，时延越低。 CDN 最擅长的是缓存静态数据，除此之外还可以缓存流媒体数据，这时候要注意使用防盗链机制。","link":"/2021/10/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89%E5%BA%94%E7%94%A8%E5%B1%82/"},{"title":"设计模式（五）结构型","text":"结构型的设计模式结构型模式旨在通过改变代码结构来达到解耦的目的，使得我们的代码容易维护和扩展。主要包含： 代理模式 适配器模式 桥梁模式 装饰模式 门面模式 组合模式 享元模式 代理模式代理模式简单来说就是用一个代理类来隐藏具体实现类的实现细节。通常还会在真实的实现前后加上一部分逻辑。对于客户端来说，代理表现的就像真实的实现类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public interface FoodService { Food makeChicken(); Food makeNoodle();}//实际的实现类public class FoodServiceImpl implements FoodService { public Food makeChicken() { Food f = new Chicken() f.setChicken(&quot;1kg&quot;); f.setSpicy(&quot;1g&quot;); f.setSalt(&quot;3g&quot;); return f; } public Food makeNoodle() { Food f = new Noodle(); f.setNoodle(&quot;500g&quot;); f.setSalt(&quot;5g&quot;); return f; }}// 代理要表现得“就像是”真实实现类，所以需要实现 FoodServicepublic class FoodServiceProxy implements FoodService { // 内部一定要有一个真实的实现类，当然也可以通过构造方法注入 private FoodService foodService = new FoodServiceImpl(); public Food makeChicken() { System.out.println(&quot;我们马上要开始制作鸡肉了&quot;); // 如果我们定义这句为核心代码的话，那么，核心代码是真实实现类做的， // 代理只是在核心代码前后做些“无足轻重”的事情 Food food = foodService.makeChicken(); System.out.println(&quot;鸡肉制作完成啦，加点胡椒粉&quot;); // 增强 food.addCondiment(&quot;pepper&quot;); return food; } public Food makeNoodle() { System.out.println(&quot;准备制作拉面~&quot;); Food food = foodService.makeNoodle(); System.out.println(&quot;制作完成啦&quot;) return food; }} 客户端调用： 123// 这里用代理类来实例化FoodService foodService = new FoodServiceProxy();foodService.makeChicken(); 代理模式说白了就是“方法包装” 或做 “方法增强” 在 AOP 中，其实就是动态代理的过程。比如 Spring 中，我们自己不定义代理类，但是 Spring 会帮我们动态来定义代理，然后把我们定义在 @Before、@After、@Around 中的代码逻辑动态添加到代理中。 说到动态代理，又可以展开说 …… Spring 中实现动态代理有两种，一种是如果我们的类定义了接口，如 UserService 接口和 UserServiceImpl 实现，那么采用 JDK 的动态代理，感兴趣的读者可以去看看 java.lang.reflect.Proxy 类的源码；另一种是我们自己没有定义接口的，Spring 会采用 CGLIB 进行动态代理，它是一个 jar 包，性能还不错。 适配器模式适配器模式做的就是，有一个接口需要实现，但是我们现成的对象都不满足，需要加一层适配器来进行适配。 一般分为三种： 默认适配器模式1234567891011// Appache commons-io 包中的 FileAlterationListener :对文件或文件夹进行监控，一旦发生了对应的操作，就会触发相应的方法public interface FileAlterationListener { void onStart(final FileAlterationObserver observer); void onDirectoryCreate(final File directory); void onDirectoryChange(final File directory); void onDirectoryDelete(final File directory); void onFileCreate(final File file); void onFileChange(final File file); void onFileDelete(final File file); void onStop(final FileAlterationObserver observer);} 如果我们只是想要监控文件夹中的文件创建和文件删除事件，可是我们还是不得不实现所有的方法,是不是太冗余了？ 需要一个适配器：实现所有方法，但实现为空。这样的话我们可以自定义类继承这个适配器实现类，只需要重写需要的方法： 123456789101112131415161718192021222324252627public class FileAlterationListenerAdaptor implements FileAlterationListener { public void onStart(final FileAlterationObserver observer) { } public void onDirectoryCreate(final File directory) { } public void onDirectoryChange(final File directory) { } public void onDirectoryDelete(final File directory) { } public void onFileCreate(final File file) { } public void onFileChange(final File file) { } public void onFileDelete(final File file) { } public void onStop(final FileAlterationObserver observer) { }} 123456789101112public class FileMonitor extends FileAlterationListenerAdaptor { public void onFileCreate(final File file) { // 文件创建 doSomething(); } public void onFileDelete(final File file) { // 文件删除 doSomething(); }} 对象适配器模式看看怎么将鸡适配成鸭，这样鸡也能当鸭来用。因为，现在鸭这个接口，我们没有合适的实现类可以用，所以需要适配器： 123456789101112131415161718192021public interface Duck { public void quack(); // 鸭的呱呱叫 public void fly(); // 飞}public interface Cock { public void gobble(); // 鸡的咕咕叫 public void fly(); // 飞}public class WildCock implements Cock { public void gobble() { System.out.println(&quot;咕咕叫&quot;); } public void fly() { System.out.println(&quot;鸡也会飞哦&quot;); }} 鸭接口有 fly() 和 quare() 两个方法，鸡 Cock 如果要冒充鸭，fly() 方法是现成的，但是鸡不会鸭的呱呱叫，没有 quack() 方法。这个时候就需要适配了： 12345678910111213141516171819202122// 毫无疑问，首先，这个适配器肯定需要 implements Duck，这样才能当做鸭来用// 鸡的适配器继承鸭这个接口public class CockAdapter implements Duck { Cock cock; // 构造方法中需要一个鸡的实例，此类就是将这只鸡适配成鸭来用 public CockAdapter(Cock cock) { this.cock = cock; } // 实现鸭的呱呱叫方法 @Override public void quack() { // 内部其实是一只鸡的咕咕叫 cock.gobble(); } @Override public void fly() { cock.fly(); }} 客户端调用： 12345678public static void main(String[] args) { // 有一只野鸡 Cock wildCock = new WildCock(); // 成功将野鸡适配成鸭 Duck duck = new CockAdapter(wildCock); ...} 无非是我们需要一只鸭，但是我们只有一只鸡，这个时候就需要定义一个适配器，由这个适配器来充当鸭，但是适配器里面的方法还是由鸡来实现的。 类适配器模式通过继承的方法，适配器自动获得了所需要的大部分方法。这个时候，客户端使用更加简单，直接 Target t = new SomeAdapter(); 就可以了。 适配器模式总结 类适配和对象适配的异同 一个采用继承，一个采用组合； 类适配属于静态实现，对象适配属于组合的动态实现，对象适配需要多实例化一个对象。 总体来说，对象适配用得比较多。 适配器模式和代理模式的异同 比较这两种模式，其实是比较对象适配器模式和代理模式，在代码结构上，它们很相似，都需要一个具体的实现类的实例。但是它们的目的不一样，代理模式做的是增强原方法的活；适配器做的是适配的活，为的是提供“把鸡包装成鸭，然后当做鸭来使用”，而鸡和鸭它们之间原本没有继承关系。 桥梁模式就是理解代码抽象和解耦。 现在有一个接口定义了一个抽象方法，有不同的实现类： 123456789101112131415161718192021222324public interface DrawAPI { public void draw(int radius, int x, int y);}public class RedPen implements DrawAPI { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用红色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }}public class GreenPen implements DrawAPI { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用绿色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }}public class BluePen implements DrawAPI { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用蓝色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }} 有一个抽象类，使用了上面的接口。 抽象类有一系列的子类，都可以直接使用这个接口的方法 123456789101112131415161718192021222324252627282930313233343536373839public abstract class Shape { protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI){ this.drawAPI = drawAPI; } public abstract void draw(); }// 圆形public class Circle extends Shape { private int radius; public Circle(int radius, DrawAPI drawAPI) { super(drawAPI); this.radius = radius; } public void draw() { drawAPI.draw(radius, 0, 0); }}// 长方形public class Rectangle extends Shape { private int x; private int y; public Rectangle(int x, int y, DrawAPI drawAPI) { super(drawAPI); this.x = x; this.y = y; } public void draw() { drawAPI.draw(0, x, y); }} 客户端调用： 12345678public static void main(String[] args) { Shape greenCircle = new Circle(10, new GreenPen()); Shape redRectangle = new Rectangle(4, 8, new RedPen()); greenCircle.draw(); redRectangle.draw();} 装饰模式既然说是装饰，那么往往就是添加小功能这种，而且，我们要满足可以添加多个小功能。最简单的，代理模式就可以实现功能的增强，但是代理不容易实现多个功能的增强，当然你可以说用代理包装代理的方式，但是那样的话代码就复杂了。 接口 Component 其实已经有了 ConcreteComponentA 和 ConcreteComponentB 两个实现类了，但是，如果我们要增强这两个实现类的话，我们就可以采用装饰模式，用具体的装饰器来装饰实现类，以达到增强的目的。 所有的具体装饰者们 ConcreteDecorator都可以作为 Component 来使用，因为它们都实现了 Component 中的所有接口。它们和 Component 实现类 ConcreteComponent的区别是，它们只是装饰者，起装饰作用。 举例： 最近大街上流行起来了“快乐柠檬”，我们把快乐柠檬的饮料分为三类：红茶、绿茶、咖啡，在这三大类的基础上，又增加了许多的口味，什么金桔柠檬红茶、金桔柠檬珍珠绿茶、芒果红茶、芒果绿茶、芒果珍珠红茶、烤珍珠红茶、烤珍珠芒果绿茶、椰香胚芽咖啡、焦糖可可咖啡等等，每家店都有很长的菜单，但是仔细看下，其实原料也没几样，但是可以搭配出很多组合，如果顾客需要，很多没出现在菜单中的饮料他们也是可以做的。 在这个例子中，红茶、绿茶、咖啡是最基础的饮料，其他的像金桔柠檬、芒果、珍珠、椰果、焦糖等都属于装饰用的。当然，在开发中，我们确实可以像门店一样，开发这些类：LemonBlackTea、LemonGreenTea、MangoBlackTea、MangoLemonGreenTea……但是，很快我们就发现，这样子干肯定是不行的，这会导致我们需要组合出所有的可能，而且如果客人需要在红茶中加双份柠檬怎么办？三份柠檬怎么办？万一有个变态要四份柠檬，所以这种做法是给自己找加班的。 123456789101112131415161718192021222324252627282930// 定义饮料抽象基类public abstract class Beverage { // 返回描述 public abstract String getDescription(); // 返回价格 public abstract double cost();}//三个基础饮料实现类，红茶、绿茶和咖啡：public class BlackTea extends Beverage { public String getDescription() { return &quot;红茶&quot;; } public double cost() { return 10; }}public class GreenTea extends Beverage { public String getDescription() { return &quot;绿茶&quot;; } public double cost() { return 11; }}...// 咖啡省略 定义调料，也就是装饰者的基类，此类必须继承自 Beverage： 12345678910111213141516171819202122232425262728293031323334353637// 调料public abstract class Condiment extends Beverage {}// 具体调料public class Lemon extends Condiment { private Beverage bevarage; // 这里很关键，需要传入具体的饮料，如需要传入没有被装饰的红茶或绿茶， // 当然也可以传入已经装饰好的芒果绿茶，这样可以做芒果柠檬绿茶 public Lemon(Beverage bevarage) { this.bevarage = bevarage; } public String getDescription() { // 装饰 return bevarage.getDescription() + &quot;, 加柠檬&quot;; } public double cost() { // 装饰 return beverage.cost() + 2; // 加柠檬需要 2 元 }}public class Mango extends Condiment { private Beverage bevarage; public Mango(Beverage bevarage) { this.bevarage = bevarage; } public String getDescription() { return bevarage.getDescription() + &quot;, 加芒果&quot;; } public double cost() { return beverage.cost() + 3; // 加芒果需要 3 元 }}...// 给每一种调料都加一个类 客户端调用： 1234567891011public static void main(String[] args) { // 首先，我们需要一个基础饮料，红茶、绿茶或咖啡 Beverage beverage = new GreenTea(); // 开始装饰 beverage = new Lemon(beverage); // 先加一份柠檬 beverage = new Mongo(beverage); // 再加一份芒果 System.out.println(beverage.getDescription() + &quot; 价格：￥&quot; + beverage.cost()); //&quot;绿茶, 加柠檬, 加芒果 价格：￥16&quot;} 如下图： Java中IO的装饰模式 InputStream 代表了输入流，具体的输入来源可以是文件（FileInputStream）、管道（PipedInputStream）、数组（ByteArrayInputStream）等，这些就像前面奶茶的例子中的红茶、绿茶，属于基础输入流。 FilterInputStream 承接了装饰模式的关键节点，其实现类是一系列装饰器，比如 BufferedInputStream 代表用缓冲来装饰，也就使得输入流具有了缓冲的功能，LineNumberInputStream 代表用行号来装饰，在操作的时候就可以取得行号了，DataInputStream 的装饰，使得我们可以从输入流转换为 java 中的基本类型值。 当然，在 java IO 中，如果我们使用装饰器的话，就不太适合面向接口编程了，如： 1InputStream inputStream = new LineNumberInputStream(new BufferedInputStream(new FileInputStream(&quot;&quot;))); 这样的结果是，InputStream 还是不具有读取行号的功能，因为读取行号的方法定义在 LineNumberInputStream 类中。 我们应该像下面这样使用： 123DataInputStream is = new DataInputStream( new BufferedInputStream( new FileInputStream(&quot;&quot;))); 门面模式也叫外观模式，Facade Pattern 常见的编码方式： 1234567891011121314151617181920212223242526272829303132//1. 定义接口public interface Shape { void draw();}//2. 定义实现类public class Circle implements Shape { @Override public void draw() { System.out.println(&quot;Circle::draw()&quot;); }}public class Rectangle implements Shape { @Override public void draw() { System.out.println(&quot;Rectangle::draw()&quot;); }}//3. 客户端调用public static void main(String[] args) { // 画一个圆形 Shape circle = new Circle(); circle.draw(); // 画一个长方形 Shape rectangle = new Rectangle(); rectangle.draw();} 定义门面： 12345678910111213141516171819202122232425262728293031323334353637public class ShapeMaker { private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() { circle = new Circle(); rectangle = new Rectangle(); square = new Square(); } /** * 下面定义一堆方法，具体应该调用什么方法，由这个门面来决定 */ public void drawCircle(){ circle.draw(); } public void drawRectangle(){ rectangle.draw(); } public void drawSquare(){ square.draw(); }}//直接调用public static void main(String[] args) { ShapeMaker shapeMaker = new ShapeMaker(); // 客户端调用现在更加清晰了 shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); } 门面模式的优点显而易见，客户端不再需要关注实例化时应该使用哪个实现类，直接调用门面提供的方法就可以了，因为门面类提供的方法的方法名对于客户端来说已经很友好了。 组合模式组合模式用于表示具有层次结构的数据，使得我们对单个对象和组合对象的访问具有一致性。 享元模式 Flyweight Pattern 。Flyweight 是轻量级的意思，享元分开来说就是 共享 元器件，也就是复用已经生成的对象，这种做法当然也就是轻量级的了。 复用对象最简单的方式是，用一个 HashMap 来存放每次新生成的对象。每次需要一个对象的时候，先到 HashMap 中看看有没有，如果没有，再生成新的对象，然后将这个对象放入 HashMap 中。 总结代理模式是做方法增强的，适配器模式是把鸡包装成鸭这种用来适配接口的，桥梁模式做到了很好的解耦，装饰模式从名字上就看得出来，适合于装饰类或者说是增强类的场景，门面模式的优点是客户端不需要关心实例化过程，只要调用需要的方法即可，组合模式用于描述具有层次结构的数据，享元模式是为了在特定的场景中缓存已经创建的对象，用于提高性能。","link":"/2021/08/22/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%BA%94%EF%BC%89%E7%BB%93%E6%9E%84%E5%9E%8B/"},{"title":"设计模式（六）行为型","text":"行为型模式关注的是各个类之间的相互作用，将职责划分清楚，使得我们的代码更加地清晰： 策略模式 观察者模式 责任链模式 模板方法模式 状态模式 策略模式场景：我们需要画一个图形，可选的策略就是用红色笔来画，还是绿色笔来画，或者蓝色笔来画。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//1. 定义一个策略接口public interface Strategy { public void draw(int radius, int x, int y);}//2. 定义几个具体的策略public class RedPen implements Strategy { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用红色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }}public class GreenPen implements Strategy { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用绿色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }}public class BluePen implements Strategy { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用蓝色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }}//3. 写出使用策略的类public class Context { private Strategy strategy; public Context(Strategy strategy){ this.strategy = strategy; } public int executeDraw(int radius, int x, int y){ return strategy.draw(radius, x, y); }}//4. 客户端调用：public static void main(String[] args) { Context context = new Context(new BluePen()); // 使用绿色笔来画 context.executeDraw(10, 0, 0);} 和桥梁模式非常像：桥梁模式在左侧加了一层抽象而已。桥梁模式的耦合更低，结构更复杂一些。 观察者模式观察者订阅自己关心的主题和主题有数据变化后通知观察者们。 定义主题：每个主题都持有观察者列表的引用，用于数据变更时通知观察者 123456789101112131415161718192021222324252627public class Subject { private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); private int state; // 状态 public int getState() { return state; } public void setState(int state) { this.state = state; // 数据已变更，通知观察者们 notifyAllObservers(); } public void attach(Observer observer){ observers.add(observer); } // 通知观察者们 public void notifyAllObservers(){ for (Observer observer : observers) { observer.update(); } } } 定义观察者接口.其实如果只有一个观察者类的话，接口都不用定义了，不过，通常场景下，既然用到了观察者模式，我们就是希望一个事件出来了，会有多个不同的类需要处理相应的信息。比如，订单修改成功事件，我们希望发短信的类得到通知、发邮件的类得到通知、处理物流信息的类得到通知等。 1234public abstract class Observer { protected Subject subject; public abstract void update();} 定义具体的几个观察者类： 1234567891011121314151617181920212223242526272829303132public class BinaryObserver extends Observer { // 在构造方法中进行订阅主题 public BinaryObserver(Subject subject) { this.subject = subject; // 通常在构造方法中将 this 发布出去的操作一定要小心 this.subject.attach(this); } // 该方法由主题类在数据变更的时候进行调用 @Override public void update() { String result = Integer.toBinaryString(subject.getState()); System.out.println(&quot;订阅的数据发生变化，新的数据处理为二进制值为：&quot; + result); }}public class HexaObserver extends Observer { public HexaObserver(Subject subject) { this.subject = subject; this.subject.attach(this); } @Override public void update() { String result = Integer.toHexString(subject.getState()).toUpperCase(); System.out.println(&quot;订阅的数据发生变化，新的数据处理为十六进制值为：&quot; + result); }} 1234567891011public static void main(String[] args) { // 先定义一个主题 Subject subject1 = new Subject(); // 定义观察者 new BinaryObserver(subject1); new HexaObserver(subject1); // 模拟数据变更，这个时候，观察者们的 update 方法将会被调用 subject.setState(11);} jdk 也提供了相似的支持，具体的大家可以参考 java.util.Observable 和 java.util.Observer 这两个类。 实际生产过程中，观察者模式往往用消息中间件来实现，如果要实现单机观察者模式，笔者建议读者使用 Guava 中的 EventBus，它有同步实现也有异步实现，本文主要介绍设计模式，就不展开说了。 责任链模式责任链通常需要先建立一个单向链表，然后调用方只需要调用头部节点就可以了，后面会自动流转下去。比如流程审批就是一个很好的例子，只要终端用户提交申请，根据申请的内容信息，自动建立一条责任链，然后就可以开始流转了。 有这么一个场景，用户参加一个活动可以领取奖品，但是活动需要进行很多的规则校验然后才能放行，比如首先需要校验用户是否是新用户、今日参与人数是否有限额、全场参与人数是否有限额等等。设定的规则都通过后，才能让用户领走奖品。 定义节点的基类： 123456789101112131415public abstract class RuleHandler { // 后继节点 protected RuleHandler successor; public abstract void apply(Context context); public void setSuccessor(RuleHandler successor) { this.successor = successor; } public RuleHandler getSuccessor() { return successor; }} 定义具体的每个节点 1234567891011121314151617181920212223242526272829303132333435363738394041424344//1. 校验是否是新用户public class NewUserRuleHandler extends RuleHandler { public void apply(Context context) { if (context.isNewUser()) { // 如果有后继节点的话，传递下去 if (this.getSuccessor() != null) { this.getSuccessor().apply(context); } } else { throw new RuntimeException(&quot;该活动仅限新用户参与&quot;); } }}//2. 校验用户所在区是否可以参与public class LocationRuleHandler extends RuleHandler { public void apply(Context context) { boolean allowed = activityService.isSupportedLocation(context.getLocation); if (allowed) { if (this.getSuccessor() != null) { this.getSuccessor().apply(context); } } else { throw new RuntimeException(&quot;非常抱歉，您所在的地区无法参与本次活动&quot;); } }}//3. 校验奖品是否已经领完public class LimitRuleHandler extends RuleHandler { public void apply(Context context) { int remainedTimes = activityService.queryRemainedTimes(context); // 查询剩余奖品 if (remainedTimes &gt; 0) { if (this.getSuccessor() != null) { this.getSuccessor().apply(userInfo); } } else { throw new RuntimeException(&quot;您来得太晚了，奖品被领完了&quot;); } }} 客户端调用： 12345678910public static void main(String[] args) { RuleHandler newUserHandler = new NewUserRuleHandler(); RuleHandler locationHandler = new LocationRuleHandler(); RuleHandler limitHandler = new LimitRuleHandler(); // 假设本次活动仅校验地区和奖品数量，不校验新老用户 locationHandler.setSuccessor(limitHandler); locationHandler.apply(context);} 代码其实很简单，就是先定义好一个链表，然后在通过任意一节点后，如果此节点有后继节点，那么传递下去。 模板方法模式在含有继承结构的代码中，模板方法模式是非常常用的。 定义一个抽象类，含有一个模板方法 123456789101112131415161718public abstract class AbstractTemplate { // 这就是模板方法，有几个抽象方法由子类实现是可以选择的。 public void templateMethod(){ init(); apply(); // 这个是重点 end(); // 可以作为钩子方法 } protected void init() { System.out.println(&quot;init 抽象层已经实现，子类也可以选择覆写&quot;); } // 留给子类实现 protected abstract void apply(); protected void end() { } } 写一个实现类： 123456789public class ConcreteTemplate extends AbstractTemplate { public void apply() { System.out.println(&quot;子类实现抽象方法 apply&quot;); } public void end() { System.out.println(&quot;我们可以把 method3 当做钩子方法来使用，需要的时候覆写就可以了&quot;); }} 客户端调用 123456public static void main(String[] args) { AbstractTemplate t = new ConcreteTemplate(); // 调用模板方法 t.templateMethod();} 状态模式用状态模式来写减库存和补库存： 定义状态接口 123public interface State { public void doAction(Context context);} 定义减库存的状态 1234567891011121314151617181920212223242526public class DeductState implements State { public void doAction(Context context) { System.out.println(&quot;商品卖出，准备减库存&quot;); context.setState(this); //... 执行减库存的具体操作 } public String toString(){ return &quot;Deduct State&quot;; }}//定义补库存状态public class RevertState implements State { public void doAction(Context context) { System.out.println(&quot;给此商品补库存&quot;); context.setState(this); //... 执行加库存的具体操作 } public String toString() { return &quot;Revert State&quot;; }} 前面用到了 context.setState(this)，我们来看看怎么定义 Context 类： 123456789101112131415public class Context { private State state; private String name; public Context(String name) { this.name = name; } public void setState(State state) { this.state = state; } public void getState() { return this.state; }} 客户端调用 12345678910111213141516public static void main(String[] args) { // 我们需要操作的是 iPhone X Context context = new Context(&quot;iPhone X&quot;); // 看看怎么进行补库存操作 State revertState = new RevertState(); revertState.doAction(context); // 同样的，减库存操作也非常简单 State deductState = new DeductState(); deductState.doAction(context); // 如果需要我们可以获取当前的状态 // context.getState().toString();} 如果我们不关心当前 context 处于什么状态，那么 Context 就可以不用维护 state 属性了，那样代码会简单很多。 总结行为型模式部分介绍了策略模式、观察者模式、责任链模式、模板方法模式和状态模式，其实，经典的行为型模式还包括备忘录模式、命令模式等，但是它们的使用场景比较有限。","link":"/2021/08/22/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E5%85%AD%EF%BC%89%E8%A1%8C%E4%B8%BA%E5%9E%8B/"},{"title":"设计模式（三）创建型","text":"创建型的设计模式包含： 单例模式 简单工厂模式 工厂方法模式 抽象工厂模式 建造者模式 原型模式 单例模式为什么要用单例模式单例设计模式（Singleton Design Pattern）理解起来非常简单。一个类只允许创建一个对象（或者实例），那这个类就是一个单例类，这种设计模式就叫作单例设计模式，简称单例模式。 处理资源访问冲突举例： 12345678910111213141516171819202122232425262728293031// 日志类：public class Logger { private FileWriter writer; public Logger() { File file = new File(&quot;/Users/wangzheng/log.txt&quot;); writer = new FileWriter(file, true); //true表示追加写入 } public void log(String message) { writer.write(mesasge); } }public class UserController{ private Logger logger = new Logger(); public void login(String username , String password){ //省略业务代码 logger.log(&quot;&quot;); //记录日志 }}public class OrderController{ private Logger logger = new Logger(); public void create(String username , String password){ //省略业务代码 logger.log(&quot;&quot;); //记录日志 }} 问题分析： 这段代码在 UserController和OrderController中都创建了Logger对象，而在Web容器中servlet多线程条件下，如果两个servlet同时执行这个记录日志的操作，可能会发生日志的覆盖的问题。（两个线程同时向共享资源上面写文件） 如果加锁可以解决这个问题嘛？ 12345public void log(String message) { synchronized(this){ writer.write(mesasge); }} 其实不能，应该锁是对象级别的锁（一个对象在不同的线程下同时调用 log() 函数，会被强制要求顺序执行）。而在上面的例子中是在两个类中创建了不同的logger对象，通过不同的对象执行log函数，锁并无作用。 如何解决呢？ — 将对象级别的锁换成类级别的锁即可。让所有的对象都共享一把锁。 12345public void log(String message) { synchronized(Logger.class) { // 类级别的锁 writer.write(mesasge); } } 并发队列（比如 Java 中的 BlockingQueue）也可以解决这个问题：多个线程同时往并发队列里写日志，一个单独的线程负责将并发队列中的数据，写入到日志文件。 将 Logger 设计成一个单例类，程序中只允许创建一个 Logger 对象，所有的线程共享使用的这一个 Logger 对象，共享一个 FileWriter 对象，而 FileWriter 本身是对象级别线程安全的，也就避免了多线程情况下写日志会互相覆盖的问题 表示全局唯一类比如，配置信息类。在系统中，我们只有一个配置文件，当配置文件被加载到内存之后，以对象的形式存在，也理所应当只有一份。 再比如之前的唯一id生成器。 如何实现单例模式 构造函数需要是 private 访问权限的，这样才能避免外部通过 new 创建实例； 考虑对象创建时的线程安全问题； 考虑是否支持延迟加载； 考虑 getInstance() 性能是否高（是否加锁）。 饿汉式12345678910111213141516/** * 饿汉式：在程序启动时就将这个实例初始化好 */public class SingleInstance { private static final SingleInstance instance = new SingleInstance(); //构造器私有化 private SingleInstance(){ } //对外通过一个方法获取单例 public SingleInstance getInstance() { return instance; }} 懒汉式1234567891011121314/** * 懒汉式：对外提供的方法需要加锁，并法度较低 */public class LazySingleInstance { private static LazySingleInstance instance; private LazySingleInstance(){} public static synchronized LazySingleInstance getInstance(){ if (instance == null) instance = new LazySingleInstance(); return instance; }} 双重检测既支持延迟加载、又支持高并发的单例实现方式，也就是双重检测实现方式。 在这种实现方式中，只要 instance 被创建之后，即便再调用 getInstance() 函数也不会再进入到加锁逻辑中了。 12345678910111213141516/** * 双重检测：需要加volatile */public class DoubleCheckSingleInstance { //加上volatile 是为了instance = new DoubleCheckSingleInstance(); 禁止重排序 private static volatile DoubleCheckSingleInstance instance; private DoubleCheckSingleInstance(){} public static DoubleCheckSingleInstance getInstance(){ if (instance == null){ synchronized (DoubleCheckSingleInstance.class){ //此处为类级别的锁 if (instance == null) instance = new DoubleCheckSingleInstance(); } } return instance; }} 静态内部类insance 的唯一性、创建过程的线程安全性，都由JVM 来保证。所以，这种实现方法既保证了线程安全，又能做到延迟加载 12345678910111213141516/** * 静态内部类: SingletonHolder 是一个静态内部类，当外部类 StaticInnerClassSingleInstance 被加载的时候，并不会创建SingletonHolder 实例对象。 * 只有当调用 getInstance() 方法时，SingletonHolder 才会被加载，这个时候才会创建 instance */public class StaticInnerClassSingleInstance { private StaticInnerClassSingleInstance(){} private static class SingletonHolder{ private static final StaticInnerClassSingleInstance instance = new StaticInnerClassSingleInstance(); } public static StaticInnerClassSingleInstance getInstance(){ return SingletonHolder.instance; }} 枚举1234567891011/** * 枚举类实现单例模式 */public enum EnumSingleInstance { INSTANCE; private AtomicLong id = new AtomicLong(0); public long getId(){ return id.incrementAndGet(); }} 单例模式有哪些问题简单工厂模式 定义一个工厂类，根据传入的参数不同返回不同的实例，被创建的实例具有共同的父类或接口。 适用场景 由于只有一个工厂类，所以工厂类中创建的对象不能太多 客户端不关心对象的创建过程。 需求实例创建一个可以绘制不同形状的绘图工具，可以绘制圆形，正方形，三角形，每个图形都会有一个draw()方法用于绘图， 先根据实例圆形，正方形，三角形抽象出共同的父类或接口 123456/** * @description: 圆形，正方形，三角形 共同的父类或接口 */public interface Shape { void draw();} 针对不同的实例，有不同的实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * @description: CircleShape */public class CircleShape implements Shape{ public CircleShape() { System.out.println(&quot;Circle constructor&quot;); } @Override public void draw() { System.out.println(&quot;draw a circle&quot;); }}/** * @description: 正方形 */public class RectShape implements Shape{ public RectShape() { System.out.println(&quot;RectShape constructor&quot;); } @Override public void draw() { System.out.println(&quot;draw a RectShape&quot;); }}/** * @description: 三角形 */public class TriangleShape implements Shape{ public TriangleShape() { System.out.println(&quot;TriangleShape constructor&quot;); } @Override public void draw() { System.out.println(&quot;draw a TriangleShape&quot;); }} 创建一个工厂，对外提供一个获取实例的方法，针对不同的入参提供不同的返回类型： 12345678910111213141516171819/** * @description: 工厂类 */public class ShapeFactory { public static final String TAG = &quot;ShapeFactory&quot;; //根据传入的参数不同返回不同的类型 public static Shape getShape(String type){ Shape shape = null; if (type.equalsIgnoreCase(&quot;circle&quot;)) { shape = new CircleShape(); } else if (type.equalsIgnoreCase(&quot;rect&quot;)) { shape = new RectShape(); } else if (type.equalsIgnoreCase(&quot;triangle&quot;)) { shape = new TriangleShape(); } return shape; }} 直接使用工厂来创建实例： 12345678910public static void main(String[] args) { Shape circle = ShapeFactory.getShape(&quot;circle&quot;); circle.draw(); Shape rect = ShapeFactory.getShape(&quot;rect&quot;); rect.draw(); Shape triangle = ShapeFactory.getShape(&quot;triangle&quot;); triangle.draw(); } 工厂方法模式工厂方法模式是简单工厂的仅一步深化， 在工厂方法模式中，我们不再提供一个统一的工厂类来创建所有的对象，而是针对不同的对象提供不同的工厂。也就是说每个对象都有一个与之对应的工厂。 工厂方法模式(Factory Method Pattern)又称为工厂模式， 工厂父类负责定义创建产品对象的公共接口，而工厂子类则负责生成具体的产品对象，即通过不同的工厂子类来创建不同的产品对象。 简单工厂是由一个代工厂生产不同的产品，而工厂方法是对工厂进行抽象化，不同产品都由专门的具体工厂来生产 适用场景 客户端不需要知道它所创建的对象的类。例子中我们不知道每个图片加载器具体叫什么名，只知道创建它的工厂名就完成了创建过程。 客户端可以通过子类来指定创建对应的对象。 需求实例现在需要设计一个这样的图片加载类，它具有多个图片加载器，用来加载jpg，png，gif格式的图片，每个加载器都有一个read（）方法，用于读取图片。下面我们完成这个图片加载类。 编写一个加载器的公共接口,有加载图片的方法 1234public interface Reader { //加载图片的方法 void read();} 针对不同的图片格式有不同的实现类 123456789101112131415161718192021public class GIFReader implements Reader{ @Override public void read() { System.out.println(&quot;load GIF picture&quot;); }}public class JPGReader implements Reader{ @Override public void read() { System.out.println(&quot;load JPG picture&quot;); }}public class PNGReader implements Reader{ @Override public void read() { System.out.println(&quot;load PNG picture&quot;); }} 如果是简单工厂的话，就一个工厂类，可以提供不同的 Reader。但现在是工厂方法模式，需要先抽象一个加载器工厂出来： 1234public interface ReaderFactory { Reader getReader();} 针对不同格式的图片有对应的加载工厂，返回相对于的加载器 123456789101112131415161718192021public class GIFReaderFactory implements ReaderFactory{ @Override public Reader getReader() { return new GIFReader(); }}public class JPGReaderFactory implements ReaderFactory{ @Override public Reader getReader() { return new JPGReader(); }}public class PNGReaderFactory implements ReaderFactory{ @Override public Reader getReader() { return new PNGReader(); }} 测试使用： 12345678910public static void main(String[] args) { GIFReaderFactory gifReaderFactory = new GIFReaderFactory(); gifReaderFactory.getReader().read(); JPGReaderFactory jpgReaderFactory = new JPGReaderFactory(); jpgReaderFactory.getReader().read(); PNGReaderFactory pngReaderFactory = new PNGReaderFactory(); pngReaderFactory.getReader().read(); } 抽象工厂模式 抽象工厂模式(Abstract Factory Pattern)：提供一个创建一系列相关或相互依赖对象的接口，而无须指定它们具体的类。 抽象工厂和工厂方法不同的地方在于，生产产品的工厂是抽象的。举例，可口可乐公司生产可乐的同时，也需要生产装可乐的瓶子和箱子，瓶子和箱子也是可口可乐专属定制的，同样百事可乐公司也会有这个需求。这个时候我们的工厂不仅仅是生产可乐饮料的工厂，还必须同时生产同一主题的瓶子和箱子，所以它是一个抽象的主题工厂，专门生产同一主题的不同商品。 抽象工厂模式是工厂方法的仅一步深化，在这个模式中的工厂类不单单可以创建一个对象，而是可以创建一组对象。这是和工厂方法最大的不同点。 抽象工厂和工厂方法一样可以划分为4大部分：AbstractFactory（抽象工厂）声明了一组用于创建对象的方法，注意是一组。ConcreteFactory（具体工厂）：它实现了在抽象工厂中声明的创建对象的方法，生成一组具体对象。AbstractProduct（抽象产品）：它为每种对象声明接口，在其中声明了对象所具有的业务方法。ConcreteProduct（具体产品）：它定义具体工厂生产的具体对象。 实例场景现在需要做一款跨平台的游戏，需要兼容Android，Ios，Wp三个移动操作系统，该游戏针对每个系统都设计了一套操作控制器（OperationController）和界面控制器（UIController），下面通过抽象工厂方式完成这款游戏的架构设计。 定义抽象产品：OperationController 和 UIController 1234567public interface OperationController { void control();}public interface UIController { void display();} 实现具体产品： 12345678910111213public class AndroidOperationController implements OperationController{ @Override public void control() { System.out.println(&quot;android OperationController&quot;); }}public class AndroidUIController implements UIController{ @Override public void display() { System.out.println(&quot;android UIController&quot;); }} 1234567891011121314public class IosOperationController implements OperationController{ @Override public void control() { System.out.println(&quot;ios OperationController&quot;); }}public class IosUIController implements UIController{ @Override public void display() { System.out.println(&quot;ios UIController&quot;); }} 12345678910111213public class WpOperationController implements OperationController{ @Override public void control() { System.out.println(&quot;WpOperationController&quot;); }}public class WpUIController implements UIController{ @Override public void display() { System.out.println(&quot;WpInterfaceController&quot;); }} 定义抽象工厂： 123456// 抽象工厂，可以创建两个OperationController 、 UIControllerpublic interface SystemFactory { public OperationController createOperationController(); public UIController createInterfaceController();} 定义具体工厂： 12345678910111213141516171819202122232425262728293031323334353637383940public class AndroidFactory implements SystemFactory { @Override public OperationController createOperationController() { return new AndroidOperationController(); } @Override public UIController createInterfaceController() { return new AndroidUIController(); }}public class IosFactory implements SystemFactory { @Override public OperationController createOperationController() { return new IosOperationController(); } @Override public UIController createInterfaceController() { return new IosUIController(); }} public class WpFactory implements SystemFactory { @Override public OperationController createOperationController() { return new WpOperationController(); } @Override public UIController createInterfaceController() { return new WpUIController(); }} 针对不同平台只通过创建不同的工厂对象就完成了操作和UI控制器的创建 如果这个游戏使用工厂方法模式搭建需要创建多少个工厂类呢？ 适用场景：（1）和工厂方法一样客户端不需要知道它所创建的对象的类。（2）需要一组对象共同完成某种功能时。并且可能存在多组对象完成不同功能的情况。（3）系统结构稳定，不会频繁的增加对象。（因为一旦增加就需要修改原有代码，不符合开闭原则） 这个模式并不符合开闭原则。实际开发还需要做好权衡。 建造者模式简单的说就是先 new 一个 Builder，然后可以链式地调用一堆方法，最后再调用一次 build() 方法，我们需要的对象就有了。 比如： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970//先把所有的属性都设置给 Builder，然后 build() 方法的时候，将这些属性复制给实际产生的对象。class User { // 下面是“一堆”的属性 private String name; private String password; private String nickName; private int age; // 构造方法私有化，不然客户端就会直接调用构造方法了 private User(String name, String password, String nickName, int age) { this.name = name; this.password = password; this.nickName = nickName; this.age = age; } // 静态方法，用于生成一个 Builder，这个不一定要有，不过写这个方法是一个很好的习惯， // 有些代码要求别人写 new User.UserBuilder().a()...build() 看上去就没那么好 public static UserBuilder builder() { return new UserBuilder(); } public static class UserBuilder { // 下面是和 User 一模一样的一堆属性 private String name; private String password; private String nickName; private int age; private UserBuilder() { } // 链式调用设置各个属性值，返回 this，即 UserBuilder public UserBuilder name(String name) { this.name = name; return this; } public UserBuilder password(String password) { this.password = password; return this; } public UserBuilder nickName(String nickName) { this.nickName = nickName; return this; } public UserBuilder age(int age) { this.age = age; return this; } // build() 方法负责将 UserBuilder 中设置好的属性“复制”到 User 中。 // 当然，可以在 “复制” 之前做点检验 public User build() { if (name == null || password == null) { throw new RuntimeException(&quot;用户名和密码必填&quot;); } if (age &lt;= 0 || age &gt;= 150) { throw new RuntimeException(&quot;年龄不合法&quot;); } // 还可以做赋予”默认值“的功能 if (nickName == null) { nickName = name; } return new User(name, password, nickName, age); } }} 客户端调用： 12345678910public class APP { public static void main(String[] args) { User d = User.builder() .name(&quot;foo&quot;) .password(&quot;pAss12345&quot;) .age(25) .build(); }} 当属性很多，而且有些必填，有些选填的时候，这个模式会使代码清晰很多。我们可以在 Builder 的构造方法中强制让调用者提供必填字段，还有，在 build() 方法中校验各个参数比在 User 的构造方法中校验，代码要优雅一些。 用了 lombok 以后，可以用@Builder 注解快速构建建造者模式 原型模式原型模式很简单：有一个原型实例，基于这个原型实例产生新的实例，也就是“克隆”了。 Object 类中有一个 clone() 方法，它用于生成一个新的对象，当然，如果我们要调用这个方法，java 要求我们的类必须先实现 Cloneable 接口，此接口没有定义任何方法，但是不这么做的话，在 clone() 的时候，会抛出 CloneNotSupportedException 异常。 java 的克隆是浅克隆，碰到对象引用的时候，克隆出来的对象和原对象中的引用将指向同一个对象。通常实现深克隆的方法是将对象进行序列化，然后再进行反序列化。 总结创建型模式总体上比较简单，它们的作用就是为了产生实例对象，算是各种工作的第一步了，因为我们写的是面向对象的代码，所以我们第一步当然是需要创建一个对象了。 简单工厂模式最简单；工厂模式在简单工厂模式的基础上增加了选择工厂的维度，需要第一步选择合适的工厂；抽象工厂模式有产品族的概念，如果各个产品是存在兼容性问题的，就要用抽象工厂模式。单例模式就不说了，为了保证全局使用的是同一对象，一方面是安全性考虑，一方面是为了节省资源；建造者模式专门对付属性很多的那种类，为了让代码更优美；原型模式用得最少，了解和 Object 类中的 clone() 方法相关的知识即可。","link":"/2021/07/01/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E5%9B%9B%EF%BC%89%E5%88%9B%E5%BB%BA%E5%9E%8B/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","link":"/tags/ElasticSearch/"},{"name":"JavaSE","slug":"JavaSE","link":"/tags/JavaSE/"},{"name":"Java并发","slug":"Java并发","link":"/tags/Java%E5%B9%B6%E5%8F%91/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"MapStruct","slug":"MapStruct","link":"/tags/MapStruct/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"Netty","slug":"Netty","link":"/tags/Netty/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"SpringBoot","slug":"SpringBoot","link":"/tags/SpringBoot/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"EffectiveJava","slug":"EffectiveJava","link":"/tags/EffectiveJava/"},{"name":"项目","slug":"项目","link":"/tags/%E9%A1%B9%E7%9B%AE/"},{"name":"操作系统","slug":"操作系统","link":"/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"算法题","slug":"算法题","link":"/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"消息队列","slug":"消息队列","link":"/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"计算机网络","slug":"计算机网络","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[{"name":"技术","slug":"技术","link":"/categories/%E6%8A%80%E6%9C%AF/"}]}